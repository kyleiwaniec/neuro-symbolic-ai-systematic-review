Key,Item Type,Publication Year,Author,Title,Abstract Note,Publication Title,DOI,Url,Date,Date Added,Date Modified
WLW49ADY,journalArticle,2020,"Hitzler, P.; Bianchi, F.; Ebrahimi, M.; Sarker, M.K.",Neural-symbolic integration and the Semantic Web,"Symbolic Systems in Artificial Intelligence which are based on formal logic and deductive reasoning are fundamentally different from Artificial Intelligence systems based on artificial neural networks, such as deep learning approaches. The difference is not only in their inner workings and general approach, but also with respect to capabilities. Neural-symbolic Integration, as a field of study, aims to bridge between the two paradigms. In this paper, we will discuss neural-symbolic integration in its relation to the Semantic Web field, with a focus on promises and possible benefits for both, and report on some current research on the topic. © 2020-IOS Press and the authors. All rights reserved.",Semantic Web,10.3233/SW-190368,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078979352&doi=10.3233%2fSW-190368&partnerID=40&md5=eb9a74a48c2b1364b7000bd5b95c322a,2020,2021-07-20 15:48:23,2021-07-20 15:48:23
YXUZIQIU,journalArticle,2021,"Ebrahimi, M.; Eberhart, A.; Bianchi, F.; Hitzler, P.",Towards bridging the neuro-symbolic gap: deep deductive reasoners,"Symbolic knowledge representation and reasoning and deep learning are fundamentally different approaches to artificial intelligence with complementary capabilities. The former are transparent and data-efficient, but they are sensitive to noise and cannot be applied to non-symbolic domains where the data is ambiguous. The latter can learn complex tasks from examples, are robust to noise, but are black boxes; require large amounts of –not necessarily easily obtained– data, and are slow to learn and prone to adversarial examples. Either paradigm excels at certain types of problems where the other paradigm performs poorly. In order to develop stronger AI systems, integrated neuro-symbolic systems that combine artificial neural networks and symbolic reasoning are being sought. In this context, one of the fundamental open problems is how to perform logic-based deductive reasoning over knowledge bases by means of trainable artificial neural networks. This paper provides a brief summary of the authors’ recent efforts to bridge the neural and symbolic divide in the context of deep deductive reasoners. Throughout the paper we will discuss strengths and limitations of models in term of accuracy, scalability, transferability, generalizabiliy, speed, and interpretability, and finally, will talk about possible modifications to enhance desirable capabilities. More specifically, in terms of architectures, we are looking at Memory-augmented networks, Logic Tensor Networks, and compositions of LSTM models to explore their capabilities and limitations in conducting deductive reasoning. We are applying these models on Resource Description Framework (RDF), first-order logic, and the description logic EL+ respectively. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Applied Intelligence,10.1007/s10489-020-02165-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100570972&doi=10.1007%2fs10489-020-02165-6&partnerID=40&md5=0003bc9ef2baca0720a980936c047d5a,2021,2021-07-20 15:48:23,2021-07-20 15:48:23
EX5ZGV3N,journalArticle,2021,"Manhaeve, R.; Dumančić, S.; Kimmig, A.; Demeester, T.; De Raedt, L.",Neural probabilistic logic programming in DeepProbLog,"We introduce DeepProbLog, a neural probabilistic logic programming language that incorporates deep learning by means of neural predicates. We show how existing inference and learning techniques of the underlying probabilistic logic programming language ProbLog can be adapted for the new language. We theoretically and experimentally demonstrate that DeepProbLog supports (i) both symbolic and subsymbolic representations and inference, (ii) program induction, (iii) probabilistic (logic) programming, and (iv) (deep) learning from examples. To the best of our knowledge, this work is the first to propose a framework where general-purpose neural networks and expressive probabilistic-logical modeling and reasoning are integrated in a way that exploits the full expressiveness and strengths of both worlds and can be trained end-to-end based on examples. © 2021 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2021.103504,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104453726&doi=10.1016%2fj.artint.2021.103504&partnerID=40&md5=9b46926a1251f8176c535e6027367e8f,2021,2021-07-20 15:48:23,2021-07-20 15:48:23
YCQKU49S,journalArticle,2018,"Harder, F.; Besold, T.R.",Learning Łukasiewicz logic,"The integration between connectionist learning and logic-based reasoning is a longstanding foundational question in artificial intelligence, cognitive systems, and computer science in general. Research into neural-symbolic integration aims to tackle this challenge, developing approaches bridging the gap between sub-symbolic and symbolic representation and computation. In this line of work the core method has been suggested as a way of translating logic programs into a multilayer perceptron computing least models of the programs. In particular, a variant of the core method for three valued Łukasiewicz logic has proven to be applicable to cognitive modelling among others in the context of Byrne's suppression task. Building on the underlying formal results and the corresponding computational framework, the present article provides a modified core method suitable for the supervised learning of Łukasiewicz logic (and of a closely-related variant thereof), implements and executes the corresponding supervised learning with the backpropagation algorithm and, finally, constructs a rule extraction method in order to close the neural-symbolic cycle. The resulting system is then evaluated in several empirical test cases, and recommendations for future developments are derived. © 2017 Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2017.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044158917&doi=10.1016%2fj.cogsys.2017.07.004&partnerID=40&md5=7eddcddf7db6d6df3f12c65d94e10e9f,2018,2021-07-20 15:48:23,2021-07-20 15:48:23
NPTPCDHQ,journalArticle,2018,"Tran, S.N.; D'Avila Garcez, A.S.",Deep Logic Networks: Inserting and Extracting Knowledge from Deep Belief Networks,"Developments in deep learning have seen the use of layerwise unsupervised learning combined with supervised learning for fine-tuning. With this layerwise approach, a deep network can be seen as a more modular system that lends itself well to learning representations. In this paper, we investigate whether such modularity can be useful to the insertion of background knowledge into deep networks, whether it can improve learning performance when it is available, and to the extraction of knowledge from trained deep networks, and whether it can offer a better understanding of the representations learned by such networks. To this end, we use a simple symbolic language - a set of logical rules that we call confidence rules - and show that it is suitable for the representation of quantitative reasoning in deep networks. We show by knowledge extraction that confidence rules can offer a low-cost representation for layerwise networks (or restricted Boltzmann machines). We also show that layerwise extraction can produce an improvement in the accuracy of deep belief networks. Furthermore, the proposed symbolic characterization of deep networks provides a novel method for the insertion of prior knowledge and training of deep networks. With the use of this method, a deep neural-symbolic system is proposed and evaluated, with the experimental results indicating that modularity through the use of confidence rules and knowledge insertion can be beneficial to network performance. © 2012 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2016.2603784,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995403891&doi=10.1109%2fTNNLS.2016.2603784&partnerID=40&md5=d7e818ffd09540742abbfab902658516,2018,2021-07-20 15:48:24,2021-07-20 15:48:24
X9XE9Y2I,journalArticle,2018,"Barbosa, R.; Cardoso, D.O.; Carvalho, D.; França, F.M.G.",Weightless neuro-symbolic GPS trajectory classification,"This paper presents a framework for dealing with the problem of GPS trajectory classification in the context of the Rio de Janeiro's public transit system (with hundreds or more classes). Such framework combines the versatile WiSARD classifier with a set of rules defined a priori, resulting in a neuro-symbolic learning system with very interesting characteristics and cutting-edge performance. We also verified the influence of different binarization methods in order to adapt raw data to WiSARD, which feeds from binary data only. These ideas were tested against a large data set of trajectories of buses from the city of Rio de Janeiro. The results confirm the practical applicability of those, since the accomplished performance was as good as that of other state-of-the-art rival methods in most test scenarios. © 2018 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2017.11.075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042642694&doi=10.1016%2fj.neucom.2017.11.075&partnerID=40&md5=06ce33319b0ac725c565ef086be96ca5,2018,2021-07-20 15:48:24,2021-07-20 15:48:24
LIJBLYR5,journalArticle,2019,"Gromann, D.; Espinosa Anke, L.; Declerck, T.",Special issue on Semantic Deep Learning,"Numerous success use cases involving deep learning have recently started to be propagated to the Semantic Web. Approaches range from utilizing structured knowledge in the training process of neural networks to enriching such architectures with ontological reasoning mechanisms. Bridging the neural-symbolic gap by joining deep learning and Semantic Web not only holds the potential of improving performance but also of opening up new avenues of research. This editorial introduces the Semantic Web Journal special issue on Semantic Deep Learning, which brings together Semantic Web and deep learning research. After a general introduction to the topic and a brief overview of recent contributions, we continue to introduce the submissions published in this special issue. © 2019 - IOS Press and the authors. All rights reserved.",Semantic Web,10.3233/SW-180364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072388130&doi=10.3233%2fSW-180364&partnerID=40&md5=d257ab8eba9e946dcca7bf59caecf60e,2019,2021-07-20 15:48:24,2021-07-20 15:48:24
5FLUBV5A,journalArticle,2021,"Roychowdhury, S.; Diligenti, M.; Gori, M.",Regularizing deep networks with prior knowledge: A constraint-based approach[Formula presented],"Deep Learning architectures can develop feature representations and classification models in an integrated way during training. This joint learning process requires large networks with many parameters, and it is successful when a large amount of training data is available. Instead of making the learner develop its entire understanding of the world from scratch from the input examples, the injection of prior knowledge into the learner seems to be a principled way to reduce the amount of require training data, as the learner does not need to induce the rules from the data. This paper presents a general framework to integrate arbitrary prior knowledge into learning. The domain knowledge is provided as a collection of first-order logic (FOL) clauses, where each task to be learned corresponds to a predicate in the knowledge base. The logic statements are translated into a set of differentiable constraints, which can be integrated into the learning process to distill the knowledge into the network, or used during inference to enforce the consistency of the predictions with the prior knowledge. The experimental results have been carried out on multiple image datasets and show that the integration of the prior knowledge boosts the accuracy of several state-of-the-art deep architectures on image classification tasks. © 2021 The Authors",Knowledge-Based Systems,10.1016/j.knosys.2021.106989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104105457&doi=10.1016%2fj.knosys.2021.106989&partnerID=40&md5=80144cf33992912376acc23e9716d914,2021,2021-07-20 15:48:24,2021-07-20 15:48:24
QTYPBWY7,journalArticle,2014,"França, M.V.M.; Zaverucha, G.; D'Avila Garcez, A.S.",Fast relational learning using bottom clause propositionalization with artificial neural networks,"Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph mining and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integrated BCP with a well-known neural-symbolic system, C-IL2P, to perform learning from numerical vectors. C-IL2P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, handles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary results indicating that a reduction of more than 90 % of features can be achieved with a small loss of accuracy. © 2013 The Author(s).",Machine Learning,10.1007/s10994-013-5392-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891373440&doi=10.1007%2fs10994-013-5392-1&partnerID=40&md5=8731ad71554ee4a0916f866be773a0d9,2014,2021-07-20 15:48:24,2021-07-20 15:48:24
PK2SIRSE,journalArticle,2021,"Confalonieri, R.; Coba, L.; Wagner, B.; Besold, T.R.",A historical perspective of explainable Artificial Intelligence,"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision-making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge-based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural-symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human-understandable explainable systems. This article is categorized under: Fundamental Concepts of Data and Knowledge > Explainable AI Technologies > Artificial Intelligence. © 2020 The Authors. WIREs Data Mining and Knowledge Discovery published by Wiley Periodicals LLC.",Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,10.1002/widm.1391,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092933617&doi=10.1002%2fwidm.1391&partnerID=40&md5=a5014127898cb4a0899155d6c248c7ef,2021,2021-07-20 15:48:24,2021-07-20 15:48:24
KIRM4EBZ,journalArticle,2021,"Vonrueden, L.; Mayer, S.; Beckh, K.; Georgiev, B.; Giesselbach, S.; Heese, R.; Kirsch, B.; Walczak, M.; Pfrommer, J.; Pick, A.; Ramamurthy, R.; Garcke, J.; Bauckhage, C.; Schuecker, J.",Informed Machine Learning - A Taxonomy and Survey of Integrating Prior Knowledge into Learning Systems,"Despite its great success, machine learning can have its limits when dealing with insufficient training data. A potential solution is the additional integration of prior knowledge into the training process which leads to the notion of informed machine learning. In this paper, we present a structured overview of various approaches in this field. We provide a definition and propose a concept for informed machine learning which illustrates its building blocks and distinguishes it from conventional machine learning. We introduce a taxonomy that serves as a classification framework for informed machine learning approaches. It considers the source of knowledge, its representation, and its integration into the machine learning pipeline. Based on this taxonomy, we survey related research and describe how different knowledge representations such as algebraic equations, logic rules, or simulation results can be used in learning systems. This evaluation of numerous papers on the basis of our taxonomy uncovers key methods in the field of informed machine learning. CCBY",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2021.3079836,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105863468&doi=10.1109%2fTKDE.2021.3079836&partnerID=40&md5=3945f72c7aa111fb2c91e6d4636de977,2021,2021-07-20 15:48:24,2021-07-20 15:48:24
K8KF6UMX,journalArticle,2018,"Sreelekha, S.",NeuroSymbolic integration with uncertainty,"Most of the tasks which require intelligent behavior have some degree of uncertainty associated with them. The occurrence of uncertainty might be because of several reasons such as the incomplete domain knowledge, unreliable or ambiguous data due to measurement errors, inconsistent data representation. Most of the knowledge-based systems require the incorporation of some form of uncertainty management, in order to handle this kind of indeterminacy present in the system. In this paper, we present one such method to handle the uncertainty in neurules, a neuro-symbolic integration concept. Neuro-Computing is used within the symbolic frame work for improving the performance of symbolic rules. The uncertainty, the personal belief degree that an uncertain event may occur is managed by computing the composite belief values of incomplete or conflicting data. With the implementation of uncertainty management in neurules, the accuracy of the inference mechanism and the generalization performance can be improved. © 2018, Springer Nature Switzerland AG.",Annals of Mathematics and Artificial Intelligence,10.1007/s10472-018-9605-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056089637&doi=10.1007%2fs10472-018-9605-y&partnerID=40&md5=7b536c76a9f545463667915525d71135,2018,2021-07-20 15:48:24,2021-07-20 15:48:24
FVUVUB3N,journalArticle,2019,"Alirezaie, M.; Längkvist, M.; Sioutis, M.; Loutfi, A.",Semantic referee: A neural-symbolic framework for enhancing geospatial semantic segmentation,"Understanding why machine learning algorithms may fail is usually the task of the human expert that uses domain knowledge and contextual information to discover systematic shortcomings in either the data or the algorithm. In this paper, we propose a semantic referee, which is able to extract qualitative features of the errors emerging from deep machine learning frameworks and suggest corrections. The semantic referee relies on ontological reasoning about spatial knowledge in order to characterize errors in terms of their spatial relations with the environment. Using semantics, the reasoner interacts with the learning algorithm as a supervisor. In this paper, the proposed method of the interaction between a neural network classifier and a semantic referee shows how to improve the performance of semantic segmentation for satellite imagery data. © 2019 - IOS Press and the authors. All rights reserved.",Semantic Web,10.3233/SW-180362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072400163&doi=10.3233%2fSW-180362&partnerID=40&md5=0c0a71f7e614556f6e265a952bc2be44,2019,2021-07-20 15:48:24,2021-07-20 15:48:24
2TUT8Z5S,journalArticle,2021,"van Bekkum, M.; de Boer, M.; van Harmelen, F.; Meyer-Vitali, A.; Teije, A.","Modular design patterns for hybrid learning and reasoning systems: a taxonomy, patterns and use cases","The unification of statistical (data-driven) and symbolic (knowledge-driven) methods is widely recognized as one of the key challenges of modern AI. Recent years have seen a large number of publications on such hybrid neuro-symbolic AI systems. That rapidly growing literature is highly diverse, mostly empirical, and is lacking a unifying view of the large variety of these hybrid systems. In this paper, we analyze a large body of recent literature and we propose a set of modular design patterns for such hybrid, neuro-symbolic systems. We are able to describe the architecture of a very large number of hybrid systems by composing only a small set of elementary patterns as building blocks. The main contributions of this paper are: 1) a taxonomically organised vocabulary to describe both processes and data structures used in hybrid systems; 2) a set of 15+ design patterns for hybrid AI systems organized in a set of elementary patterns and a set of compositional patterns; 3) an application of these design patterns in two realistic use-cases for hybrid AI systems. Our patterns reveal similarities between systems that were not recognized until now. Finally, our design patterns extend and refine Kautz’s earlier attempt at categorizing neuro-symbolic architectures. © 2021, The Author(s).",Applied Intelligence,10.1007/s10489-021-02394-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108116690&doi=10.1007%2fs10489-021-02394-3&partnerID=40&md5=d23e1c358cf7b9ded5d7fb5ce7d6ad33,2021,2021-07-20 15:48:24,2021-07-20 15:48:24
Y3PYNSZX,journalArticle,2021,"Han, Z.; Wei, B.; Xi, X.; Chen, B.; Yin, Y.; Li, S.",Unifying neural learning and symbolic reasoning for spinal medical report generation,"Automated medical report generation in spine radiology, i.e., given spinal medical images and directly create radiologist-level diagnosis reports to support clinical decision making, is a novel yet fundamental study in the domain of artificial intelligence in healthcare. However, it is incredibly challenging because it is an extremely complicated task that involves visual perception and high-level reasoning processes. In this paper, we propose the neural-symbolic learning (NSL) framework that performs human-like learning by unifying deep neural learning and symbolic logical reasoning for the spinal medical report generation. Generally speaking, the NSL framework firstly employs deep neural learning to imitate human visual perception for detecting abnormalities of target spinal structures. Concretely, we design an adversarial graph network that interpolates a symbolic graph reasoning module into a generative adversarial network through embedding prior domain knowledge, achieving semantic segmentation of spinal structures with high complexity and variability. NSL secondly conducts human-like symbolic logical reasoning that realizes unsupervised causal effect analysis of detected entities of abnormalities through meta-interpretive learning. NSL finally fills these discoveries of target diseases into a unified template, successfully achieving a comprehensive medical report generation. When employed in a real-world clinical dataset, a series of empirical studies demonstrate its capacity on spinal medical report generation and show that our algorithm remarkably exceeds existing methods in the detection of spinal structures. These indicate its potential as a clinical tool that contributes to computer-aided diagnosis. © 2020",Medical Image Analysis,10.1016/j.media.2020.101872,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094323461&doi=10.1016%2fj.media.2020.101872&partnerID=40&md5=6fb4c90947d3fd6fce24756a93bfd9b9,2021,2021-07-20 15:48:24,2021-07-20 15:48:24
7CM6V7SH,journalArticle,2014,"Hatzilygeroudis, I.; Prentzas, J.",Fuzzy and neuro-symbolic approaches in personal credit scoring: Assessment of bank loan applicants,"Credit scoring is a vital task in the financial domain. An important aspect in credit scoring involves the assessment of bank loan applications. Loan applications are frequently assessed by banking personnel regarding the ability/possibility of satisfactorily dealing with loan demands. Intelligent methods may be employed to assist in the required tasks. In this chapter, we present the design, implementation and evaluation of two separate intelligent systems that assess bank loan applications. The systems employ different knowledge representation formalisms. More specifically, the corresponding intelligent systems are a fuzzy expert system and a neuro-symbolic expert system. The former employs fuzzy rules based on knowledge elicited from experts. The latter is based on neurules, a type of neuro-symbolic rules that combine a symbolic (production rules) and a connectionist (adaline unit) representation. A characteristic of neurules is that they retain the naturalness and modularity of symbolic rules. Neurules were produced from available patterns. Evaluation showed that the performance of both systems is close although their knowledge bases were derived from different types of source knowledge. © 2014 Springer International Publishing Switzerland.",Studies in Computational Intelligence,10.1007/978-3-319-01866-9_10,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958528749&doi=10.1007%2f978-3-319-01866-9_10&partnerID=40&md5=d85f5a1001a31aa8001d51869f5fd356,2014,2021-07-20 15:48:24,2021-07-20 15:48:24
TPT5W9FR,journalArticle,2016,"Prentzas, J.; Hatzilygeroudis, I.",Assessment of life insurance applications: An approach integrating neuro-symbolic rule-based with case-based reasoning,"Assessment of applications for life insurance is an important task in the insurance sector that concerns estimation of potential risks underlying an application, if accepted. This task is accomplished by specialized personnel of insurance companies. Because of recent financial crises, this task is more demanding, and intelligent computer-based methods could be employed to assist. In this paper, we present an intelligent approach to assessment of life insurance applications, which is based on an integration of neurule-based with case-based reasoning. Neurules are a type of neuro-symbolic rules that combine a symbolic (production rules) and a connectionist (adaline unit) representation. A characteristic of neurules is that in contrast to other hybrid neuro-symbolic approaches, they retain the naturalness and modularity of symbolic rules. Neurules are produced from available symbolic rules that represent general knowledge, which however do not completely cover the domain. We use health condition, age, gender, annual income, profession, insurance type and primary life insurance benefit as assessment parameters used in rule conditions. The integration of neurules and cases employs different types of indices for the cases according to different roles they play in neurule-based reasoning. This results in its accuracy improvement. Experimental results demonstrate the effectiveness of the approach. © 2015 Wiley Publishing Ltd.",Expert Systems,10.1111/exsy.12137,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947982891&doi=10.1111%2fexsy.12137&partnerID=40&md5=3f0355d67b192ef1aea3aa52c1ab5584,2016,2021-07-20 15:48:24,2021-07-20 15:48:24
B6HMY5VN,journalArticle,2020,"Hohenecker, P.; Lukasiewicz, T.",Ontology reasoning with deep neural networks,"The ability to conduct logical reasoning is a fundamental aspect of intelligent human behavior, and thus an important problem along the way to human-level artificial intelligence. Traditionally, logic-based symbolic methods from the field of knowledge representation and reasoning have been used to equip agents with capabilities that resemble human logical reasoning qualities. More recently, however, there has been an increasing interest in using machine learning rather than logic-based symbolic formalisms to tackle these tasks. In this paper, we employ state-of-the-art methods for training deep neural networks to devise a novel model that is able to learn how to effectively perform logical reasoning in the form of basic ontology reasoning. This is an important and at the same time very natural logical reasoning task, which is why the presented approach is applicable to a plethora of important real-world problems. We present the outcomes of several experiments, which show that our model is able to learn to perform highly accurate ontology reasoning on very large, diverse, and challenging benchmarks. Furthermore, it turned out that the suggested approach suffers much less from different obstacles that prohibit logic-based symbolic reasoning, and, at the same time, is surprisingly plausible from a biological point of view. © 2020 AI Access Foundation. All rights reserved.",Journal of Artificial Intelligence Research,10.1613/JAIR.1.11661,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091394619&doi=10.1613%2fJAIR.1.11661&partnerID=40&md5=f2c4fe69dd532c4c3c1697020bd217c3,2020,2021-07-20 15:48:25,2021-07-20 15:48:25
HMMJSAAE,journalArticle,2017,"Besold, T.R.; Garcez, A.A.; Stenning, K.; van der Torre, L.; van Lambalgen, M.",Reasoning in Non-probabilistic Uncertainty: Logic Programming and Neural-Symbolic Computing as Examples,"This article aims to achieve two goals: to show that probability is not the only way of dealing with uncertainty (and even more, that there are kinds of uncertainty which are for principled reasons not addressable with probabilistic means); and to provide evidence that logic-based methods can well support reasoning with uncertainty. For the latter claim, two paradigmatic examples are presented: logic programming with Kleene semantics for modelling reasoning from information in a discourse, to an interpretation of the state of affairs of the intended model, and a neural-symbolic implementation of input/output logic for dealing with uncertainty in dynamic normative contexts. © 2017, Springer Science+Business Media Dordrecht.",Minds and Machines,10.1007/s11023-017-9428-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014652011&doi=10.1007%2fs11023-017-9428-3&partnerID=40&md5=7fc0aac7a6a945f1f9ab9c4c5b9a43a1,2017,2021-07-20 15:48:25,2021-07-20 15:48:25
YR46S9XC,journalArticle,2020,"Jia, H.; Chen, S.",Integrated data and knowledge driven methodology for human activity recognition,"Human activity recognition has been a popular research area concerned with identifying the specific movement or action of a person based on variety of sensor data. Conventional human activity recognition approaches are mainly data driven, which are not working well for composite activity recognition due to the complexity and uncertainty of real scenarios. We propose in this paper a hierarchical structure-based framework and methodology for human activity recognition by an integration of data-driven approach and knowledge-based approach, which provides an interesting framework capable of bridging lower-level pattern recognition and higher-level knowledge for reasoning and explanation. More specifically, this approach constructs a hierarchical structure for representing the composite activity by a composition of lower-level actions and gestures according to its semantic meaning. This hierarchical structure is then transformed into formal syntactical logical formulas and rules, based on which the resolution based automated reasoning is applied to recognize the composite activity given the recognized lower-level actions by using data driven machine learning methods. The work is the validated using some open-source data about video based human activity recognition. The present work provides a promising framework and application illustration of integration of machine learning and symbolic reasoning. © 2020 Elsevier Inc.",Information Sciences,10.1016/j.ins.2020.03.081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085927756&doi=10.1016%2fj.ins.2020.03.081&partnerID=40&md5=760f02da321fd6489d045b170c525f04,2020,2021-07-20 15:48:25,2021-07-20 15:48:25
A84GZVVK,journalArticle,2018,"Sünderhauf, N.; Brock, O.; Scheirer, W.; Hadsell, R.; Fox, D.; Leitner, J.; Upcroft, B.; Abbeel, P.; Burgard, W.; Milford, M.; Corke, P.",The limits and potentials of deep learning for robotics,"The application of deep learning in robotics leads to very specific problems and research questions that are typically not addressed by the computer vision and machine learning communities. In this paper we discuss a number of robotics-specific learning, reasoning, and embodiment challenges for deep learning. We explain the need for better evaluation metrics, highlight the importance and unique challenges for deep robotic learning in simulation, and explore the spectrum between purely data-driven and model-driven approaches. We hope this paper provides a motivating overview of important research directions to overcome the current limitations, and helps to fulfill the promising potentials of deep learning in robotics. © 2018, © The Author(s) 2018.",International Journal of Robotics Research,10.1177/0278364918770733,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046709786&doi=10.1177%2f0278364918770733&partnerID=40&md5=0bc3b31b095684a8cbbab2e29e82f2a3,2018,2021-07-20 15:48:25,2021-07-20 15:48:25
NAKSBACG,journalArticle,2021,"Ai, L.; Muggleton, S.H.; Hocquette, C.; Gromowski, M.; Schmid, U.",Beneficial and harmful explanatory machine learning,"Given the recent successes of Deep Learning in AI there has been increased interest in the role and need for explanations in machine learned theories. A distinct notion in this context is that of Michie’s definition of ultra-strong machine learning (USML). USML is demonstrated by a measurable increase in human performance of a task following provision to the human of a symbolic machine learned theory for task performance. A recent paper demonstrates the beneficial effect of a machine learned logic theory for a classification task, yet no existing work to our knowledge has examined the potential harmfulness of machine’s involvement for human comprehension during learning. This paper investigates the explanatory effects of a machine learned theory in the context of simple two person games and proposes a framework for identifying the harmfulness of machine explanations based on the Cognitive Science literature. The approach involves a cognitive window consisting of two quantifiable bounds and it is supported by empirical evidence collected from human trials. Our quantitative and qualitative results indicate that human learning aided by a symbolic machine learned theory which satisfies a cognitive window has achieved significantly higher performance than human self learning. Results also demonstrate that human learning aided by a symbolic machine learned theory that fails to satisfy this window leads to significantly worse performance than unaided human learning. © 2021, The Author(s).",Machine Learning,10.1007/s10994-020-05941-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102509030&doi=10.1007%2fs10994-020-05941-0&partnerID=40&md5=3b780a16d9b0cb76ce2ffa395d3ba073,2021,2021-07-20 15:48:25,2021-07-20 15:48:25
9W2TRGDW,journalArticle,2021,"Zeng, Z.; Li, X.",Application of human computing in image captioning under deep learning,"Image captioning refers to a kind of machine learning technology that makes an input of an image automatically generate some descriptive texts, with this being widely applied in image search, unmanned driving, VR and AR. Deep learning performs well in some designated situations, but since it lacks reasoning ability compared with humans, it has difficulty solving problems like UUs. This paper introduces image captioning under the deep learning method based on human computing to solve UUs problems where the training set is not identical to the test set. We test this method on the data sets of Corel5K and PASCAL VOC 2012, using several popular image captioning models, and the result shows it to be efficient. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Microsystem Technologies,10.1007/s00542-019-04473-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066850932&doi=10.1007%2fs00542-019-04473-5&partnerID=40&md5=f1c21225c9587d8c68a4e36fc66555fe,2021,2021-07-20 15:48:25,2021-07-20 15:48:25
9VD974LX,journalArticle,2020,"Spelda, P.","Machine learning, inductive reasoning, and reliability of generalisations","The present paper shows how statistical learning theory and machine learning models can be used to enhance understanding of AI-related epistemological issues regarding inductive reasoning and reliability of generalisations. Towards this aim, the paper proceeds as follows. First, it expounds Price’s dual image of representation in terms of the notions of e-representations and i-representations that constitute subject naturalism. For Price, this is not a strictly anti-representationalist position but rather a dualist one (e- and i-representations). Second, the paper links this debate with machine learning in terms of statistical learning theory becoming more viable epistemological tool when it abandons the perspective of object naturalism. The paper then argues that machine learning grounds a form of knowing that can be understood in terms of e- and i-representation learning. Third, this synthesis shows a way of analysing inductive reasoning in terms of reliability of generalisations stemming from a structure of e- and i-representations. In the age of Artificial Intelligence, connecting Price’s dual view of representation with Deep Learning provides an epistemological way forward and even perhaps an approach to how knowing is possible. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",AI and Society,10.1007/s00146-018-0860-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052563918&doi=10.1007%2fs00146-018-0860-6&partnerID=40&md5=d325aae4df228fd7019070f5e8e682bf,2020,2021-07-20 15:48:25,2021-07-20 15:48:25
2FCUJH2G,journalArticle,2011,"Silva, J.; Coheur, L.; Mendes, A.C.; Wichert, A.",From symbolic to sub-symbolic information in question classification,"Question Answering (QA) is undoubtedly a growing field of current research in Artificial Intelligence. Question classification, a QA subtask, aims to associate a category to each question, typically representing the semantic class of its answer. This step is of major importance in the QA process, since it is the basis of several key decisions. For instance, classification helps reducing the number of possible answer candidates, as only answers matching the question category should be taken into account. This paper presents and evaluates a rulebased question classifier that partially founds its performance in the detection of the question headword and in its mapping into the target category through the use ofWordNet. Moreover, we use the rule-based classifier as a features' provider of a machine learning-based question classifier. A detailed analysis of the rule-base contribution is presented. Despite using a very compact feature space, state of the art results are obtained. © Springer Science+Business Media B.V. 2010.",Artificial Intelligence Review,10.1007/s10462-010-9188-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956078298&doi=10.1007%2fs10462-010-9188-4&partnerID=40&md5=122e6c4f8a49ee3c6e33182ffe606bf9,2011,2021-07-20 15:48:25,2021-07-20 15:48:25
W25MFCK4,journalArticle,2020,"Cohen, W.W.; Yang, F.; Mazaitis, K.R.",TensorLog: A probabilistic database implemented using deep-learning infrastructure,"We present an implementation of a probabilistic first-order logic called TensorLog, in which classes of logical queries are compiled into differentiable functions in a neural-network infrastructure such as Tensorflow or Theano. This leads to a close integration of probabilistic logical reasoning with deep-learning infrastructure: in particular, it enables high-performance deep learning frameworks to be used for tuning the parameters of a probabilistic logic. The integration with these frameworks enables use of GPU-based parallel processors for inference and learning, making TensorLog the first highly parallellizable probabilistic logic. Experimental results show that TensorLog scales to problems involving hundreds of thousands of knowledge-base triples and tens of thousands of examples. © 2020 AI Access Foundation. All rights reserved.",Journal of Artificial Intelligence Research,10.1613/JAIR.1.11944,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094400673&doi=10.1613%2fJAIR.1.11944&partnerID=40&md5=f90034ac321d4c9e6dbeef4b20363d5e,2020,2021-07-20 15:48:26,2021-07-20 15:48:26
V3QFHYAM,journalArticle,2020,"Dalpiaz, F.; Niu, N.",Requirements Engineering in the Days of Artificial Intelligence,"Artificial Intelligence (AI) has a long tradition in software and requirements engineering (RE). Over the years, many AI techniques have been employed to represent and analyze requirements, ranging from knowledge representation and reasoning in the 1980s to the use of natural language (NL) processing, machine learning, and deep learning since the 2000s. AI techniques have been successfully applied in practice, for example, to manage large-volume requirements. © 1984-2012 IEEE.",IEEE Software,10.1109/MS.2020.2986047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087333846&doi=10.1109%2fMS.2020.2986047&partnerID=40&md5=781d3ad494282e05453adfa0001ee98b,2020,2021-07-20 15:48:26,2021-07-20 15:48:26
ZN3Q37BX,journalArticle,2011,"Krüger, N.; Geib, C.; Piater, J.; Petrick, R.; Steedman, M.; Wörgötter, F.; Ude, A.; Asfour, T.; Kraft, D.; Omrčen, D.; Agostini, A.; Dillmann, R.",ObjectAction Complexes: Grounded abstractions of sensorymotor processes,"This paper formalises ObjectAction Complexes (OACs) as a basis for symbolic representations of sensorymotor experience and behaviours. OACs are designed to capture the interaction between objects and associated actions in artificial cognitive systems. This paper gives a formal definition of OACs, provides examples of their use for autonomous cognitive robots, and enumerates a number of critical learning problems in terms of OACs. © 2011 Elsevier B.V. All rights reserved.",Robotics and Autonomous Systems,10.1016/j.robot.2011.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051824498&doi=10.1016%2fj.robot.2011.05.009&partnerID=40&md5=75c20d1bf699ea8f90f6e481b4132d02,2011,2021-07-20 15:48:26,2021-07-20 15:48:26
TA7789L5,journalArticle,2019,"Zhou, Y.; Xu, T.; Li, S.; Shi, R.",Beyond engagement: an EEG-based methodology for assessing user’s confusion in an educational game,"Confusion is an emotion, which may occur when the learner is confronting inconsistence between new knowledge and existing cognitive structure, or reasoning for solving the puzzle and problem. Although confusion is not pleasant, it is necessary for the learner to engage in understanding and deep learning. Consequently, confusion assessment has attracted increased interest in e-learning. However, current studies have targeted no further than engagement detection and measurement, while there is lack of studies in investigating cognitive and emotional aspects beyond engagement in the context of game-based learning. To quantify confused states in logic reasoning in game-based learning, we propose an EEG-based methodology for assessing the user’s confusion using the OpenBCI device with 8 channels. In the complicated context of game play, it is difficult, and sometimes impossible, to collect the ground truth of the data in real tasks. To solve this issue, this work leverages cross-task and cross-subject methods to build a classifier, that is, training on the data of one standardized cognitive test paradigm (Raven’s test) and testing on the data of real tasks in game play (Sokoban Game). It provides a new possibility to create a classifier based on a small dataset. We also employ the end-to-end algorithm of deep learning in machine learning. Results showed the feasibility of this proposal in the task variation of the classifier, with an accuracy of 91.04%. The proposed EEG-based methodology is suitable to analyze learners’ confusion on the long game-play duration and has a good adaption in real tasks. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Universal Access in the Information Society,10.1007/s10209-019-00678-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069685322&doi=10.1007%2fs10209-019-00678-7&partnerID=40&md5=55f4acb529617e420472243a642b0861,2019,2021-07-20 15:48:26,2021-07-20 15:48:26
JGU2SECC,journalArticle,2020,"Jiang, J.; Wang, H.; Xie, J.; Guo, X.; Guan, Y.; Yu, Q.",Medical knowledge embedding based on recursive neural network for multi-disease diagnosis,"The representation of knowledge based on first-order logic captures the richness of natural language and supports multiple probabilistic inference models. Although symbolic representation enables quantitative reasoning with statistical probability, it is difficult to utilize with machine learning models as they perform numerical operations. In contrast, knowledge embedding (i.e., high-dimensional and continuous vectors) is a feasible approach to complex reasoning that can not only retain the semantic information of knowledge, but also establish the quantifiable relationship among embeddings. In this paper, we propose a recursive neural knowledge network (RNKN), which combines medical knowledge based on first-order logic with a recursive neural network for multi-disease diagnosis. After the RNKN is efficiently trained using manually annotated Chinese Electronic Medical Records (CEMRs), diagnosis-oriented knowledge embeddings and weight matrixes are learned. The experimental results confirm that the diagnostic accuracy of the RNKN is superior to those of four machine learning models, four classical neural networks and Markov logic network. The results also demonstrate that the more explicit the evidence extracted from CEMRs, the better the performance. The RNKN gradually reveals the interpretation of knowledge embeddings as the number of training epochs increases. © 2019 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2019.101772,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078077985&doi=10.1016%2fj.artmed.2019.101772&partnerID=40&md5=3483a63fb7430aa96a98811d66247886,2020,2021-07-20 15:48:26,2021-07-20 15:48:26
Q4V9P98H,journalArticle,2020,"Schmid, U.; Finzel, B.",Mutual Explanations for Cooperative Decision Making in Medicine,"Exploiting mutual explanations for interactive learning is presented as part of an interdisciplinary research project on transparent machine learning for medical decision support. Focus of the project is to combine deep learning black box approaches with interpretable machine learning for classification of different types of medical images to combine the predictive accuracy of deep learning and the transparency and comprehensibility of interpretable models. Specifically, we present an extension of the Inductive Logic Programming system Aleph to allow for interactive learning. Medical experts can ask for verbal explanations. They can correct classification decisions and in addition can also correct the explanations. Thereby, expert knowledge can be taken into account in form of constraints for model adaption. © 2020, The Author(s).",KI - Kunstliche Intelligenz,10.1007/s13218-020-00633-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088044619&doi=10.1007%2fs13218-020-00633-2&partnerID=40&md5=92dc6a4d98b8783fb1e14df29cac7147,2020,2021-07-20 15:48:26,2021-07-20 15:48:26
7MMJY5BM,journalArticle,2019,"Schon, C.; Siebert, S.; Stolzenburg, F.",The CoRg Project: Cognitive Reasoning,"The term cognitive computing refers to new hardware and/or software that mimics the functioning of the human brain. In the context of question answering and commonsense reasoning this means that the reasoning process of humans shall be modeled by adequate technical means. However, since humans do not follow the rules of classical logic, a system designed to model these abilities must be very versatile. The aim of the CoRg project (Cognitive Reasoning) is to successfully complete a reasoning task with commonsense reasoning. We address different benchmarks with focus on the COPA benchmark set (Choice of Plausible Alternatives). Since humans naturally use background knowledge, we have to deal with large background knowledge bases and must be able to reason with multiple input formats and sources in the CoRg system, in order to draw explainable conclusions. For this, we have to find appropriate logics for cognitive reasoning. For a successful reasoning system, nowadays it seems to be important to combine automated reasoning with machine learning technology like recurrent neural networks. © 2019, Gesellschaft für Informatik e.V. and Springer-Verlag GmbH Germany, part of Springer Nature.",KI - Kunstliche Intelligenz,10.1007/s13218-019-00601-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077011935&doi=10.1007%2fs13218-019-00601-5&partnerID=40&md5=e6edfc5075a223b3bbc5a5cbd192fc8c,2019,2021-07-20 15:48:26,2021-07-20 15:48:26
XNAC7N7B,journalArticle,2015,"Prentzas, J.; Hatzilygeroudis, I.",Improving efficiency of merging symbolic rules into integrated rules: Splitting methods and mergability criteria,"Neurules are a type of neuro-symbolic rules integrating neurocomputing and production rules. Each neurule is represented as an adaline unit. Neurules exhibit characteristics such as modularity, naturalness and ability to perform interactive and integrated inferences and provide explanations for reached conclusions. One way of producing a neurule base is through conversion of an existing symbolic rule base yielding an equivalent but more compact rule base. The conversion process merges symbolic rules having the same conclusion into one or more neurules. Because of the inability of the adaline unit to handle inseparability, more than one neurule for each conclusion may be produced by splitting the initial set of symbolic rules into subsets. This paper presents research work improving the conversion process in terms of runtime and number of produced neurules. First, we show how easier it is to construct a neurule base than a connectionist one. Second, we present alternative rule set splitting methods. Finally, we define criteria concerning the ability or inability to convert a rule set into a single, equivalent, but more compact rule. With application of such mergability criteria, the conversion process of symbolic rules into neurules becomes more time-efficient. All the aforementioned are supported by experimental results. © 2014 Wiley Publishing Ltd.",Expert Systems,10.1111/exsy.12085,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027944661&doi=10.1111%2fexsy.12085&partnerID=40&md5=10da2576c3e51fe14d9ac29863fd7f84,2015,2021-07-20 15:48:26,2021-07-20 15:48:26
B7EZ93DX,journalArticle,2020,"Wei, H.; Jia, H.; Li, Y.; Xu, Y.",Verify and measure the quality of rule based machine leaning,"In recent years, explainable AI has been gaining great attention, and there is a surge of interest in studying how prediction models work and how to provide formal guarantees for the models. Rule based machine learning (RBML), which aims to automatically identify and learn a set of relational rules that collectively represent the knowledge captured by the system, are a popular class of techniques in machine learning and data mining. Since inconsistencies in the rule base learnt can have a significant, negative impact on how the system will perform and on the conclusions that it will reach, the present work addresses the issues of verification and evaluation of consistency of rule base resulted from machine learning or domain expert using the logic based automated reasoning method. The main contribution consists of two parts. The first one focused on the consistency of rule base in the classical logic sense, which can be transformed into conjunctive normal form, so the consistency of rule base learnt can be verified via a resolution based automated reasoning method. Due to the uncertainty inevitably included in the rule-base during the learning process, the more detailed work has been presented in the second part, i.e., focused on providing a formal foundation of RBML under uncertainty in order to support logical analysis, verify and measure the consistency degree of the rule-base under uncertainty based on many-valued logic automated reasoning framework and algorithms. Some examples are also provided in both parts to illustrate the feasibility and effectiveness of the present work. © 2020 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2020.106300,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088842235&doi=10.1016%2fj.knosys.2020.106300&partnerID=40&md5=0df33cbbfea3138d48c2b5f7e31a301e,2020,2021-07-20 15:48:26,2021-07-20 15:48:26
6LD73KV7,journalArticle,2016,"Ferilli, S.",Predicate invention-based specialization in Inductive Logic Programming,"Three relevant areas of interest in symbolic Machine Learning are incremental supervised learning, multistrategy learning and predicate invention. In many real-world tasks, new observations may point out the inadequacy of the learned model. In such a case, incremental approaches allow to adjust it, instead of learning a new model from scratch. Specifically, when a negative example is wrongly classified by a model, specialization refinement operators are needed. A powerful way to specialize a theory in Inductive Logic Programming is adding negated preconditions to concept definitions. This paper describes an empowered specialization operator that allows to introduce the negation of conjunctions of preconditions using predicate invention. An implementation of the operator is proposed, and experiments purposely devised to stress it prove that the proposed approach is correct and viable even under quite complex conditions. © 2016, Springer Science+Business Media New York.",Journal of Intelligent Information Systems,10.1007/s10844-016-0412-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968548431&doi=10.1007%2fs10844-016-0412-9&partnerID=40&md5=06ed89f4f309f622601dc2d3e8e06e46,2016,2021-07-20 15:48:26,2021-07-20 15:48:26
676S4D2D,journalArticle,2015,"Furbach, U.; Schon, C.; Stolzenburg, F.; Weis, K.-H.; Wirth, C.-P.",The RatioLog Project: Rational Extensions of Logical Reasoning,"Higher-level cognition includes logical reasoning and the ability of question answering with common sense. The RatioLog project addresses the problem of rational reasoning in deep question answering by methods from automated deduction and cognitive computing. In a first phase, we combine techniques from information retrieval and machine learning to find appropriate answer candidates from the huge amount of text in the German version of the free encyclopedia “Wikipedia”. In a second phase, an automated theorem prover tries to verify the answer candidates on the basis of their logical representations. In a third phase—because the knowledge may be incomplete and inconsistent—we consider extensions of logical reasoning to improve the results. In this context, we work toward the application of techniques from human reasoning: We employ defeasible reasoning to compare the answers w.r.t. specificity, deontic logic, normative reasoning, and model construction. Moreover, we use integrated case-based reasoning and machine learning techniques on the basis of the semantic structure of the questions and answer candidates to learn giving the right answers. © 2015, Springer-Verlag Berlin Heidelberg.",KI - Kunstliche Intelligenz,10.1007/s13218-015-0377-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072861875&doi=10.1007%2fs13218-015-0377-9&partnerID=40&md5=c5095ffbeeaf0de5e98788136f7cf0d4,2015,2021-07-20 15:48:26,2021-07-20 15:48:26
8Y85NX6C,journalArticle,2015,"Hatzilygeroudis, I.; Prentzas, J.",Symbolic-neural rule based reasoning and explanation,"In this paper, we present neurule-based inference and explanation mechanisms. A neurule is a kind of integrated rule, which integrates a symbolic rule with neurocomputing: each neurule is considered as an adaline neural unit. Thus, a neurule base consists of a number of autonomous adaline units (neurules), expressed in a symbolic oriented syntax. There are two inference processes for neurules: the connectionism-oriented process, which gives pre-eminence to neurocomputing approach, and the symbolism-oriented process, which gives pre-eminence to a symbolic backwards chaining like approach. Symbolism-oriented process is proved to be more efficient than the connectionism-oriented one, in terms of the number of required computations (56.47-63.88% average reduction) and the mean runtime gain (59.95-64.89% in average), although both require almost the same average number of input values. The neurule-based explanation mechanism provides three types of explanations: 'how' a conclusion was derived, 'why' a value for a specific input variable was asked from the user and 'why-not' a variable has acquired a specific value. As shown by experiments, the neurule-based explanation mechanism is superior to that provided by known connectionist expert systems, another neuro-symbolic integration category. It provides less in number (64.38-69.28% average reduction) and more natural explanation rules, thus increasing efficiency (mean runtime gain of 56.65-56.73% in average) and comprehensibility of explanations. © 2015 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2015.01.068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923385886&doi=10.1016%2fj.eswa.2015.01.068&partnerID=40&md5=9ceb876cdd0c89753752e164b442c9a4,2015,2021-07-20 15:48:26,2021-07-20 15:48:26
5JDIL466,journalArticle,2018,"Chesani, F.; Galassi, A.; Lippi, M.; Mello, P.",Can deep networks learn to play by the rules? A case study on nine Men's Morris,"Deep networks have been successfully applied to a wide range of tasks in artificial intelligence, and game playing is certainly not an exception. In this paper, we present an experimental study to assess whether purely subsymbolic systems, such as deep networks, are capable of learning to play by the rules, without any a priori knowledge neither of the game, nor of its rules, but only by observing the matches played by another player. Similar problems arise in many other application domains, where the goal is to learn rules, policies, behaviors, or decisions, simply by the observation of the dynamics of a system.We present a case study conducted with residual networks on the popular board game of Nine Men's Morris, showing that this kind of subsymbolic architecture is capable of correctly discriminating legal from illegal decisions, just from the observation of past matches of a single player. 2017 IEEE. © 2018 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",IEEE Transactions on Games,10.1109/TG.2018.2804039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077818519&doi=10.1109%2fTG.2018.2804039&partnerID=40&md5=ece5775b2e637ab476fe1d5cab8d448f,2018,2021-07-20 15:48:26,2021-07-20 15:48:26
E7HRHIKT,journalArticle,2020,"Kraus, J.-L.",Can artificial intelligency revolutionize drug discovery?,"Artificial intelligency can bring speed and reliability to drug discovery process. It represents an additional intelligence, which in any case can replace the strategic and logic creative insight of the medicinal chemist who remains the architect and molecule master designer. In terms of drug design, artificial intelligency, deep learning machines, and other revolutionary technologies will match with the medicinal chemist’s natural intelligency, but for sure never go beyond. This manuscript tries to assess the impact of the artificial intelligency on drug discovery today. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",AI and Society,10.1007/s00146-019-00892-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065245088&doi=10.1007%2fs00146-019-00892-0&partnerID=40&md5=1bfb6c576973c396116796c4230676b9,2020,2021-07-20 15:48:26,2021-07-20 15:48:26
LFHAX2JG,journalArticle,2015,"Skarlatidis, A.; Paliouras, G.; Artikis, A.; Vouros, G.A.",Probabilistic event calculus for event recognition,"Symbolic event recognition systems have been successfully applied to a variety of application domains, extracting useful information in the form of events, allowing experts or other systems to monitor and respond when significant events are recognised. In a typical event recognition application, however, these systems often have to deal with a significant amount of uncertainty. In this article, we address the issue of uncertainty in logic-based event recognition by extending the Event Calculus with probabilistic reasoning. Markov logic networks are a natural candidate for our logic-based formalism. However, the temporal semantics of the Event Calculus introduce a number of challenges for the proposed model. We show how and under what assumptions we can overcome these problems. Additionally,we study how probabilistic modelling changes the behaviour of the formalism, affecting its key property-the inertia of fluents. Furthermore, we demonstrate the advantages of the probabilistic Event Calculus through examples and experiments in the domain of activity recognition, using a publicly available dataset for video surveillance. © 2015 ACM.",ACM Transactions on Computational Logic,10.1145/2699916,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923517534&doi=10.1145%2f2699916&partnerID=40&md5=362ed819692de4e1c73c71bf38005236,2015,2021-07-20 15:48:26,2021-07-20 15:48:26
TNQ9QG45,journalArticle,2020,"Wang, S.; Pathania, A.; Mitra, T.",Neural Network Inference on Mobile SoCs,"Editor's notes: Mobile devices are increasingly being used to run machine-learning-based applications. This article provides a quantitative evaluation of machine learning inference capabilities of the different components on mobile SoCs and explores their performance limits. - Sudeep Pasricha, Colorado State University. © 2013 IEEE.",IEEE Design and Test,10.1109/MDAT.2020.2968258,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078141456&doi=10.1109%2fMDAT.2020.2968258&partnerID=40&md5=cfbc57e1842339bd38b09cd70236ed4a,2020,2021-07-20 15:48:26,2021-07-20 15:48:26
NB39QA35,journalArticle,2018,"Yao, Y.; Xu, J.; Shi, J.; Xu, B.",Learning to activate logic rules for textual reasoning,"Most current textual reasoning models cannotlearn human-like reasoning process, and thus lack interpretability and logical accuracy. To help address this issue, we propose a novel reasoning model which learns to activate logic rules explicitly via deep reinforcement learning. It takes the form of Memory Networks but features a special memory that stores relational tuples, mimicking the “Image Schema” in human cognitive activities. We redefine textual reasoning as a sequential decision-making process modifying or retrieving from the memory, where logic rules serve as state-transition functions. Activating logic rules for reasoning involves two problems: variable binding and relation activating, and this is a first step to solve them jointly. Our model achieves an average error rate of 0.7% on bAbI-20, a widely-used synthetic reasoning benchmark, using less than 1k training samples and no supporting facts. © 2018 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2018.06.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049856582&doi=10.1016%2fj.neunet.2018.06.012&partnerID=40&md5=47e0384962823cd19727841570859356,2018,2021-07-20 15:48:26,2021-07-20 15:48:26
LVIKJ2AS,journalArticle,2021,"Li, S.; Lajoie, S.P.; Zheng, J.; Wu, H.; Cheng, H.",Automated detection of cognitive engagement to inform the art of staying engaged in problem-solving,"In the present paper, we used supervised machine learning algorithms to predict students' cognitive engagement states from their facial behaviors as 61 students solved a clinical reasoning problem in an intelligent tutoring system. We also examined how high and low performers differed in cognitive engagement levels when performing surface and deep learning behaviors. We found that students' facial behaviors were powerful predictors of their cognitive engagement states. In particular, we found that the SVM (Support Vector Machine) model demonstrated excellent capacity for distinguishing engaged and less engaged states when 17 informative facial features were added into the model. In addition, the results suggested that high performers did not differ significantly in the general level of cognitive engagement with low performers. There was also no difference in cognitive engagement levels between high and low performers when they performed shallow learning behaviors. However, high performers showed a significantly higher level of cognitive engagement than low performers when conducting deep learning behaviors. This study advances our understanding of how students regulate their engagement to succeed in problem-solving. This study also has significant methodological implications for the automated measurement of cognitive engagement. © 2020 Elsevier Ltd",Computers and Education,10.1016/j.compedu.2020.104114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098594114&doi=10.1016%2fj.compedu.2020.104114&partnerID=40&md5=31f941ab39260123234f0fcf93e82393,2021,2021-07-20 15:48:27,2021-07-20 15:48:27
8MT47N5Y,journalArticle,2021,"Mehri, R.; Haarslev, V.; Chinaei, H.",A machine learning approach for optimizing heuristic decision-making in Web Ontology Language reasoners,"Description logics (DLs) are formalisms for representing knowledge bases of application domains. The Web Ontology Language (OWL) is a syntactic variant of a very expressive DL. OWL reasoners can infer implied information from OWL ontologies. The performance of OWL reasoners can be severely affected by situations that require decision-making over many alternatives. Such a nondeterministic behavior is often controlled by heuristics that are based on insufficient information. This article proposes a novel OWL reasoning approach that applies machine learning (ML) to implement pragmatic and optimal decision-making strategies in such situations. Disjunctions occurring in ontologies are one source of nondeterministic actions in reasoners. We propose two ML-based approaches to reduce the nondeterminism caused by dealing with disjunctions. The first approach is restricted to propositional DL while the second one can deal with standard DL. Both approaches speed up our ML-based reasoner by up to two orders of magnitude in comparison to the non-ML reasoner. Another source of nondeterministic actions is the order in which tableau rules should be applied. On average, our ML-based approach achieves a speedup of two orders of magnitude when compared to the most expensive rule ordering of the non-ML reasoner. © 2020 Wiley Periodicals LLC.",Computational Intelligence,10.1111/coin.12404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092509786&doi=10.1111%2fcoin.12404&partnerID=40&md5=34ce5e4b928e3f69c439db0695d9c946,2021,2021-07-20 15:48:27,2021-07-20 15:48:27
TUVZGCHD,journalArticle,2017,"Goldberg, Y.",Neural Network Methods for Natural Language Processing,"Neural networks are a family of powerful machine learning models. This book focuses on the application of neural network models to natural language data. The first half of the book (Parts I and II) covers the basics of supervised machine learning and feed-forward neural networks, the basics of working with machine learning over language data, and the use of vector-based rather than symbolic representations for words. It also covers the computation-graph abstraction, which allows to easily define and train arbitrary neural networks, and is the basis behind the design of contemporary neural network software libraries. The second part of the book (Parts III and IV) introduces more specialized neural network architectures, including 1D convolutional neural networks, recurrent neural networks, conditioned-generation models, and attention-based models. These architectures and techniques are the driving force behind state-of-the-art algorithms for machine translation, syntactic parsing, and many other applications. Finally, we also discuss tree-shaped networks, structured prediction, and the prospects of multi-task learning. Copyright © 2017 by Morgan & Claypool.",Synthesis Lectures on Human Language Technologies,10.2200/S00762ED1V01Y201703HLT037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033223087&doi=10.2200%2fS00762ED1V01Y201703HLT037&partnerID=40&md5=73db6b5659c49b4a23d3c1d45e5ced9b,2017,2021-07-20 15:48:27,2021-07-20 15:48:27
8B9CH47V,journalArticle,2021,"He, M.; Zhang, L.",Machine learning and symbolic regression investigation on stability of MXene materials,"Materials stability is a fundamental parameter that should be considered in almost all materials researches. In this manuscript, we employ machine learning techniques and symbolic regression to investigate material stabilities, focusing on the An+1Bn-type prototypical MXenes. Based on a small dataset, the machine learning algorithms including Random forest, KNN, Logistic regression, SVM and GaussianNB are investigated to evaluate the MXene stabilities, with the SVM algorithm achieving the best accuracy for the classification purpose. More importantly, the symbolic regression is verified to be a viable method to identify proper descriptors and construct new descriptors that correlate with the MXene material stability. This study demonstrates the viability of the machine learning and symbolic regression methods to classify materials and describe materials stability. © 2021 Elsevier B.V.",Computational Materials Science,10.1016/j.commatsci.2021.110578,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105601070&doi=10.1016%2fj.commatsci.2021.110578&partnerID=40&md5=e24af7c49c27c7f49c7b015a760b8701,2021,2021-07-20 15:48:27,2021-07-20 15:48:27
APAYH7C2,journalArticle,2019,"Dreossi, T.; Donzé, A.; Seshia, S.A.",Compositional Falsification of Cyber-Physical Systems with Machine Learning Components,"Cyber-physical systems (CPS), such as automotive systems, are starting to include sophisticated machine learning (ML) components. Their correctness, therefore, depends on properties of the inner ML modules. While learning algorithms aim to generalize from examples, they are only as good as the examples provided, and recent efforts have shown that they can produce inconsistent output under small adversarial perturbations. This raises the question: can the output from learning components lead to a failure of the entire CPS? In this work, we address this question by formulating it as a problem of falsifying signal temporal logic specifications for CPS with ML components. We propose a compositional falsification framework where a temporal logic falsifier and a machine learning analyzer cooperate with the aim of finding falsifying executions of the considered model. The efficacy of the proposed technique is shown on an automatic emergency braking system model with a perception component based on deep neural networks. © 2019, Springer Nature B.V.",Journal of Automated Reasoning,10.1007/s10817-018-09509-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060349868&doi=10.1007%2fs10817-018-09509-5&partnerID=40&md5=f87cb216796cb6bb61e71f71664c8f18,2019,2021-07-20 15:48:27,2021-07-20 15:48:27
NQI9B797,journalArticle,2020,"Wang, X.; Xu, Z.; Gou, X.",A novel plausible reasoning based on intuitionistic fuzzy propositional logic and its application in decision making,"Automatic reasoning based on propositional logic is considered as an important tool in machine learning, and intuitionistic fuzzy sets have turned out to deal with vague and uncertain information effectively in real world. In this paper, a novel plausible reasoning based on intuitionistic fuzzy propositional logic is proposed. On the basis of it, the categories of intuitionistic fuzzy logic proposition (IFLP) formula are discussed both considering the true degree and the false degree at the same time. Some basic operational laws and inference rules of IFLPs on the basis of closely-reasoned scientific proofs are put out. Then, we develop two classification methods of IFLPs, i.e., truth table and figure of equivalence, respectively. After that, the reasoning theory of IFLPs is introduced and three reasoning methods are further established including direct proof method, additional premise proof method and reduction to absurdity method, respectively. Finally, a case study about strategy initiatives of HBIS GROUP on Supply-side Structural Reform is presented and some discussions are provided to validate the proposed methods. As a result, the proposed methods based on the plausible reasoning offer sound structure and can improve the efficiency of logic programming. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Fuzzy Optimization and Decision Making,10.1007/s10700-020-09319-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082800130&doi=10.1007%2fs10700-020-09319-8&partnerID=40&md5=91c048c896368b8db9418e1594708278,2020,2021-07-20 15:48:27,2021-07-20 15:48:27
2N43RWSY,journalArticle,2019,"Li, J.; Zhang, G.; Yu, L.; Meng, T.",Research and Design on Cognitive Computing Framework for Predicting Judicial Decisions,"This paper aims to provide a cognitive computing framework to meet the challenges of semantic understanding, knowledge learning and judicial reasoning in the Chinese legal domain. In our framework, legal factors are first represented in a formal way; secondly, legal factors are extracted, and concepts and their relations are augmented with a combination of rule-based and deep learning methods; thirdly, a predication model is generated and trained to make judicial decisions. When a fact description is brought into the model, the probability of judicial decisions will be given automatically. Two elementary results are obtained: I. Our method can effectively predict the decisions for divorce cases with different expression styles, and offers better performance than traditional methods like Support Vector Machine (SVM); II. Our machine learning predicting results can be easily understood by general public as applied induction rules are given. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Signal Processing Systems,10.1007/s11265-018-1429-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058237823&doi=10.1007%2fs11265-018-1429-9&partnerID=40&md5=2fa5e5285532e9674e4183edf208ab06,2019,2021-07-20 15:48:27,2021-07-20 15:48:27
YJAE26I7,journalArticle,2012,"Jiang, J.; Zhang, J.",A review and prospect of readable machine proofs for geometry theorems,"After half a century research, the mechanical theorem proving in geometries has become an active research topic in the automated reasoning field. This review involves three approaches on automated generating readable machine proofs for geometry theorems which include search methods, coordinate-free methods, and formal logic methods. Some critical issues about these approaches are also discussed. Furthermore, the authors propose three further research directions for the readable machine proofs for geometry theorems, including geometry inequalities, intelligent geometry softwares and machine learning. © 2012 Institute of Systems Science, Academy of Mathematics and Systems Science, CAS and Springer-Verlag Berlin Heidelberg.",Journal of Systems Science and Complexity,10.1007/s11424-012-2048-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865544085&doi=10.1007%2fs11424-012-2048-3&partnerID=40&md5=904cbef1ff529be5a866f52ffa2a13c4,2012,2021-07-20 15:48:27,2021-07-20 15:48:27
DPR3HWPZ,journalArticle,2021,"Giri, S.J.; Dutta, P.; Halani, P.; Saha, S.","MultiPredGO: Deep Multi-Modal Protein Function Prediction by Amalgamating Protein Structure, Sequence, and Interaction Information","Protein is an essential macro-nutrient for perceiving a wide range of biochemical activities and biological regulations in living cells. In this work, we have presented a novel multi-modal approach, named MultiPredGO, for predicting protein functions by utilizing two different kinds of information, namely protein sequence and the protein secondary structure. Here, our contributions are threefold; firstly, along with the protein sequence, we learn the feature representation from the protein structure. Secondly, we develop two different deep learning models after considering the characteristics of the underlying data patterns of the protein sequence and protein 3D structures. Finally, along with these two modalities, we have also utilized protein interaction information for expediting the efficiency of the proposed model in predicting the protein functions. For extracting features from different modalities, we have utilized various variations of the convolutional neural network. As the protein function classes are dependent on each other, we have used a neuro-symbolic hierarchical classification model, which resembles the structure of Gene Ontology (GO), for effectively predicting the dependent protein functions. Finally, to validate the goodness of our proposed method (MultiPredGO), we have compared our results with various uni-modal along with two well-known multi-modal protein function prediction approaches, namely, INGA and DeepGO. Results show that the overall performance of the proposed approach in terms of accuracy, F-measure, precision, and recall metrics are better than those by the state-of-the-art methods. MultiPredGO attains an average 13.05% and 30.87% improvements over the best existing comparing approach (DeepGO) for cellular component and molecular functions, respectively. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2020.3022806,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105851526&doi=10.1109%2fJBHI.2020.3022806&partnerID=40&md5=975f3316296ef8db1858e2f79f040a13,2021,2021-07-20 15:48:27,2021-07-20 15:48:27
WQ7KRNXL,journalArticle,2011,"Polkowski, L.","Reasoning with Rough Inclusions: Granular Computing, Granular Logics, Perception Calculus, Cognitive and MAS Reasoning","Rough mereology allows for a plethora of applications in various reasoning schemes due to universality of its primitive predicate of a part to a degree. We have already stressed that by its nature, rough mereology is especially suited to reasoning with collective concepts like geometric figures or solids, or, concepts learned by machine learning methods, i.e., with collective concepts. Those applications are presented in Ch. 8 and Ch. 9. In this chapter, we begin this discussion with a formal approach to the problem of granulation of knowledge and then we examine rough mereological logics: from our results in Ch. 6 it follows that representing implication with a rough inclusion μ leads to logics which extend and generalize fuzzy logics. As an application, we propose a formal rendering of the idea of perception calculus, due to Zadeh [67]. We apply rough mereological schemes to reasoning by multi-agent (MAS) systems, and finally we present a rough mereological variant of cognitive reasoning in neural-like systems.",Intelligent Systems Reference Library,10.1007/978-3-642-22279-5_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885585030&doi=10.1007%2f978-3-642-22279-5_7&partnerID=40&md5=221e2f59d06c09311f92b2904d9fe5d9,2011,2021-07-20 15:48:27,2021-07-20 15:48:27
5PDBVUWW,journalArticle,2021,"Evans, R.; Bošnjak, M.; Buesing, L.; Ellis, K.; Pfau, D.; Kohli, P.; Sergot, M.",Making sense of raw input,"How should a machine intelligence perform unsupervised structure discovery over streams of sensory input? One approach to this problem is to cast it as an apperception task [1]. Here, the task is to construct an explicit interpretable theory that both explains the sensory sequence and also satisfies a set of unity conditions, designed to ensure that the constituents of the theory are connected in a relational structure. However, the original formulation of the apperception task had one fundamental limitation: it assumed the raw sensory input had already been parsed using a set of discrete categories, so that all the system had to do was receive this already-digested symbolic input, and make sense of it. But what if we don't have access to pre-parsed input? What if our sensory sequence is raw unprocessed information? The central contribution of this paper is a neuro-symbolic framework for distilling interpretable theories out of streams of raw, unprocessed sensory experience. First, we extend the definition of the apperception task to include ambiguous (but still symbolic) input: sequences of sets of disjunctions. Next, we use a neural network to map raw sensory input to disjunctive input. Our binary neural network is encoded as a logic program, so the weights of the network and the rules of the theory can be solved jointly as a single SAT problem. This way, we are able to jointly learn how to perceive (mapping raw sensory information to concepts) and apperceive (combining concepts into declarative rules). © 2021 The Author(s)",Artificial Intelligence,10.1016/j.artint.2021.103521,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105271962&doi=10.1016%2fj.artint.2021.103521&partnerID=40&md5=dc2cad187bdce0ca218e86e5c2765137,2021,2021-07-20 15:48:27,2021-07-20 15:48:27
JN4IZETG,journalArticle,2021,"Škrlj, B.; Martinc, M.; Lavrač, N.; Pollak, S.",autoBOT: evolving neuro-symbolic representations for explainable low resource text classification,"Learning from texts has been widely adopted throughout industry and science. While state-of-the-art neural language models have shown very promising results for text classification, they are expensive to (pre-)train, require large amounts of data and tuning of hundreds of millions or more parameters. This paper explores how automatically evolved text representations can serve as a basis for explainable, low-resource branch of models with competitive performance that are subject to automated hyperparameter tuning. We present autoBOT (automatic Bags-Of-Tokens), an autoML approach suitable for low resource learning scenarios, where both the hardware and the amount of data required for training are limited. The proposed approach consists of an evolutionary algorithm that jointly optimizes various sparse representations of a given text (including word, subword, POS tag, keyword-based, knowledge graph-based and relational features) and two types of document embeddings (non-sparse representations). The key idea of autoBOT is that, instead of evolving at the learner level, evolution is conducted at the representation level. The proposed method offers competitive classification performance on fourteen real-world classification tasks when compared against a competitive autoML approach that evolves ensemble models, as well as state-of-the-art neural language models such as BERT and RoBERTa. Moreover, the approach is explainable, as the importance of the parts of the input space is part of the final solution yielded by the proposed optimization procedure, offering potential for meta-transfer learning. © 2021, The Author(s).",Machine Learning,10.1007/s10994-021-05968-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104608374&doi=10.1007%2fs10994-021-05968-x&partnerID=40&md5=59b5fd0f7b23614c0337e650f6050d25,2021,2021-07-20 15:48:27,2021-07-20 15:48:27
NFK585BK,journalArticle,2021,"Gupta, R.; Srivastava, D.; Sahu, M.; Tiwari, S.; Ambasta, R.K.; Kumar, P.",Artificial intelligence to deep learning: machine intelligence approach for drug discovery,"Drug designing and development is an important area of research for pharmaceutical companies and chemical scientists. However, low efficacy, off-target delivery, time consumption, and high cost impose a hurdle and challenges that impact drug design and discovery. Further, complex and big data from genomics, proteomics, microarray data, and clinical trials also impose an obstacle in the drug discovery pipeline. Artificial intelligence and machine learning technology play a crucial role in drug discovery and development. In other words, artificial neural networks and deep learning algorithms have modernized the area. Machine learning and deep learning algorithms have been implemented in several drug discovery processes such as peptide synthesis, structure-based virtual screening, ligand-based virtual screening, toxicity prediction, drug monitoring and release, pharmacophore modeling, quantitative structure–activity relationship, drug repositioning, polypharmacology, and physiochemical activity. Evidence from the past strengthens the implementation of artificial intelligence and deep learning in this field. Moreover, novel data mining, curation, and management techniques provided critical support to recently developed modeling algorithms. In summary, artificial intelligence and deep learning advancements provide an excellent opportunity for rational drug design and discovery process, which will eventually impact mankind. Graphic abstract: The primary concern associated with drug design and development is time consumption and production cost. Further, inefficiency, inaccurate target delivery, and inappropriate dosage are other hurdles that inhibit the process of drug delivery and development. With advancements in technology, computer-aided drug design integrating artificial intelligence algorithms can eliminate the challenges and hurdles of traditional drug design and development. Artificial intelligence is referred to as superset comprising machine learning, whereas machine learning comprises supervised learning, unsupervised learning, and reinforcement learning. Further, deep learning, a subset of machine learning, has been extensively implemented in drug design and development. The artificial neural network, deep neural network, support vector machines, classification and regression, generative adversarial networks, symbolic learning, and meta-learning are examples of the algorithms applied to the drug design and discovery process. Artificial intelligence has been applied to different areas of drug design and development process, such as from peptide synthesis to molecule design, virtual screening to molecular docking, quantitative structure–activity relationship to drug repositioning, protein misfolding to protein–protein interactions, and molecular pathway identification to polypharmacology. Artificial intelligence principles have been applied to the classification of active and inactive, monitoring drug release, pre-clinical and clinical development, primary and secondary drug screening, biomarker development, pharmaceutical manufacturing, bioactivity identification and physiochemical properties, prediction of toxicity, and identification of mode of action. [Figure not available: see fulltext.] © 2021, The Author(s), under exclusive licence to Springer Nature Switzerland AG.",Molecular Diversity,10.1007/s11030-021-10217-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105946110&doi=10.1007%2fs11030-021-10217-3&partnerID=40&md5=67f665893791474380ba232e1060f689,2021,2021-07-20 15:48:27,2021-07-20 15:48:27
IADBKVDB,journalArticle,2020,"Maddalena, L.; Gori, M.; Pal, S.K.",Pattern recognition and beyond: Alfredo Petrosino's scientific results,"We summarize the main scientific contributions of our friend and colleague Alfredo Petrosino, full professor in computer science at the University of Naples Parthenope, Italy. They mainly cover topics in high-performance computing, neural network models, soft and granular computing, computer vision, and machine learning. We also highlight how most of his research activity lays the foundation for biometry and its applications. © 2020 Elsevier B.V.",Pattern Recognition Letters,10.1016/j.patrec.2020.07.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088802598&doi=10.1016%2fj.patrec.2020.07.032&partnerID=40&md5=1933fd892c78c6b8392b68b63b7b3aa5,2020,2021-07-20 15:48:28,2021-07-20 15:48:28
Q4LPN6I8,journalArticle,2017,"Vasuki, A.; Govindaraju, S.",Deep Neural Networks for Image Classification,"This chapter gives an insight into Deep Learning Neural Networks and their application to Image Classification / Pattern Recognition. The principle of Convolutional Neural Networks will be described and an in-depth study of the algorithms for image classification will be made. In artificial intelligence, machine learning plays a key role. The algorithm learns when exposed to new data or environment. Object / Pattern Recognition is an integral part of machine learning and image classification is an integral part of such algorithms. The Human Visual System efficiently classifies known objects and also learns easily when exposed to new objects. This capability is being developed in Artificial Neural Networks and there are several types of such networks with increasing capabilities in solving problems. Neural networks themselves have evolved from evolutionary computing techniques that try to simulate the behavior of the human brain in reasoning, recognition and learning. Deep neural networks have powerful architectures with the capability to learn and there are training algorithms that make the networks adapt themselves in machine learning. The networks extract the features from the object and these are used for classification. The chapter concludes with a brief overview of some of the applications / case studies already published in the literature. © 2017 The authors and IOS Press. All rights reserved.",Advances in Parallel Computing,10.3233/978-1-61499-822-8-27,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046335585&doi=10.3233%2f978-1-61499-822-8-27&partnerID=40&md5=05d44a1b15f4f8f5579461af1e22b230,2017,2021-07-20 15:48:28,2021-07-20 15:48:28
KSDE58VV,journalArticle,2020,"Fenzl, F.; Rieke, R.; Chevalier, Y.; Dominik, A.; Kotenko, I.",Continuous fields: Enhanced in-vehicle anomaly detection using machine learning models,"The attack surface of a modern vehicle increases with its connectivity. A strategy to prevent attacks or at least to identify such attacks and to mitigate their effects is therefore imperative. The detection of indications for intrusive behavior in an in-vehicle network is an important aspect of a holistic security concept. The structure of the payload of in-vehicle messages with respect to the encoded sensor values is in general confidential. Therefore, most researchers consider the structure of the in-vehicle messages to be bit- or byte-fields. However, this may hide anomalies which are characterized by correlations between sensor values transferred by the in-vehicle messages. In this work, we evaluate the influence of accuracy of the model of the payload structure with respect to the actual sensor values on the results of different intrusion detection methods. In particular, we analyze if an improved alignment is helpful to detect anomalies introduced by stealthy intrusions. In order to cover conceptually different modeling and reasoning techniques, we adapted a deep learning approach as well as a characteristic functions based intrusion detection approach to utilize such message streams. An important aspect is that the explainability of the results is better compared to deep learning systems. We further developed a set of test vectors based on log files of a vehicle enriched by different intrusions. In particular, we included simulations of stealthy intrusions which mask certain sensor values within the respective messages. The effectiveness of the developed methods is demonstrated by various experiments. © 2020 The Authors",Simulation Modelling Practice and Theory,10.1016/j.simpat.2020.102143,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087525067&doi=10.1016%2fj.simpat.2020.102143&partnerID=40&md5=1b73b8cb7e51338b99fa25b9979b3d2c,2020,2021-07-20 15:48:28,2021-07-20 15:48:28
EKX2PU85,journalArticle,2018,"Muggleton, S.H.; Schmid, U.; Zeller, C.; Tamaddoni-Nezhad, A.; Besold, T.",Ultra-Strong Machine Learning: comprehensibility of programs learned with ILP,"During the 1980s Michie defined Machine Learning in terms of two orthogonal axes of performance: predictive accuracy and comprehensibility of generated hypotheses. Since predictive accuracy was readily measurable and comprehensibility not so, later definitions in the 1990s, such as Mitchell’s, tended to use a one-dimensional approach to Machine Learning based solely on predictive accuracy, ultimately favouring statistical over symbolic Machine Learning approaches. In this paper we provide a definition of comprehensibility of hypotheses which can be estimated using human participant trials. We present two sets of experiments testing human comprehensibility of logic programs. In the first experiment we test human comprehensibility with and without predicate invention. Results indicate comprehensibility is affected not only by the complexity of the presented program but also by the existence of anonymous predicate symbols. In the second experiment we directly test whether any state-of-the-art ILP systems are ultra-strong learners in Michie’s sense, and select the Metagol system for use in humans trials. Results show participants were not able to learn the relational concept on their own from a set of examples but they were able to apply the relational definition provided by the ILP system correctly. This implies the existence of a class of relational concepts which are hard to acquire for humans, though easy to understand given an abstract explanation. We believe improved understanding of this class could have potential relevance to contexts involving human learning, teaching and verbal interaction. © 2018, The Author(s).",Machine Learning,10.1007/s10994-018-5707-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045625860&doi=10.1007%2fs10994-018-5707-3&partnerID=40&md5=563e54b43de3b0f9a6ae87b1fdca0f7e,2018,2021-07-20 15:48:28,2021-07-20 15:48:28
A8RPZG9Y,journalArticle,2019,"Tangiuchi, T.; Mochihashi, D.; Nagai, T.; Uchida, S.; Inoue, N.; Kobayashi, I.; Nakamura, T.; Hagiwara, Y.; Iwahashi, N.; Inamura, T.",Survey on frontiers of language and robotics,"The understanding and acquisition of a language in a real-world environment is an important task for future robotics services. Natural language processing and cognitive robotics have both been focusing on the problem for decades using machine learning. However, many problems remain unsolved despite significant progress in machine learning (such as deep learning and probabilistic generative models) during the past decade. The remaining problems have not been systematically surveyed and organized, as most of them are highly interdisciplinary challenges for language and robotics. This study conducts a survey on the frontier of the intersection of the research fields of language and robotics, ranging from logic probabilistic programming to designing a competition to evaluate language understanding systems. We focus on cognitive developmental robots that can learn a language from interaction with their environment and unsupervised learning methods that enable robots to learn a language without hand-crafted training data. © 2019, © 2019 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",Advanced Robotics,10.1080/01691864.2019.1632223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068064287&doi=10.1080%2f01691864.2019.1632223&partnerID=40&md5=fb9809699eafb5cf9f5835a69d53fd47,2019,2021-07-20 15:48:28,2021-07-20 15:48:28
Q7873LYV,journalArticle,2020,"Lecue, F.",On the role of knowledge graphs in explainable AI,"The current hype of Artificial Intelligence (AI) mostly refers to the success of machine learning and its sub-domain of deep learning. However, AI is also about other areas, such as Knowledge Representation and Reasoning, or Distributed AI, i.e., areas that need to be combined to reach the level of intelligence initially envisioned in the 1950s. Explainable AI (XAI) now refers to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. This paper reviews XAI not only from a Machine Learning perspective, but also from the other AI research areas, such as AI Planning or Constraint Satisfaction and Search. We expose the XAI challenges of AI fields, their existing approaches, limitations and opportunities for Knowledge Graphs and their underlying technologies. © 2020-IOS Press and the authors. All rights reserved.",Semantic Web,10.3233/SW-190374,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079051021&doi=10.3233%2fSW-190374&partnerID=40&md5=cfa870c330299d6eed0ec936cc29e7c2,2020,2021-07-20 15:48:28,2021-07-20 15:48:28
4288AB9K,journalArticle,2016,"Hu, H.; Pang, L.; Shi, Z.",Image matting in the perception granular deep learning,"In the past decade, proposed by Geoffrey Hinton, deep learning has been proved its powerful ability in processing data from lower level to higher level and gradually composes more and more semantic concepts by unsupervised feature learning for single modalities (e.g., text, images or audio). Usually a multi scale pyramid structure is applied in a layered deep learning neural network. But how to design a multi scale pyramid structure is still an open problem. At the same time, granular computing (GrC) has been an active topic of research in machine learning and computer vision. In this paper, inspired by the original insight of granular computing proposed by Zadeh, a generalized image-matting approach is defined in the framework of a novel Granular Deep Learning(GDL), in which the information similarity, proximity and functionality are very important for feature learning. We show that layered deep learning can be formally represented as a framework of a granular system defined by fuzzy logic. In this way, the pyramids or hierarchical structure of a layered deep learning neural network can be easily designed in such a granular system, i.e., the convolution pyramids or hierarchical convolutional factor analysis in the deep learning can be viewed as special cases of granular computing. The experiments show the effectiveness of our approach in the task of foreground and background separating. © 2016 Elsevier B.V. All rights reserved.",Knowledge-Based Systems,10.1016/j.knosys.2016.03.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964241002&doi=10.1016%2fj.knosys.2016.03.018&partnerID=40&md5=07e3e6b2178ac57c0a93f708e205b36a,2016,2021-07-20 15:48:28,2021-07-20 15:48:28
Y6V929T2,journalArticle,2018,"Lőrincz, A.; Csákvári, M.; Fóthi, Á.; Milacski, Z.Á.; Sárkány, A.; Tősér, Z.",Towards reasoning based representations: Deep Consistence Seeking Machine,"Machine learning is making substantial progress in diverse applications. The success is mostly due to advances in deep learning. However, deep learning can make mistakes and its generalization abilities to new tasks are questionable. We ask when and how one can combine network outputs, when (i) details of the observations are evaluated by learned deep components and (ii) facts and rules are available. The Deep Consistence Seeking (DCS) machine seeks for consistent and deterministic event descriptions and improves the representation accordingly. The machine has an anomaly detection component that may trigger coherence seeking. Coherence seeking resolves conflicts between computational modules by preferring components with higher scores. We illustrate that context can help in correcting recognitions and in deriving training samples for self-training. We put these concepts into a general framework of cognition, by distinguishing creativity, rule extraction, verification, and symbol grounding. We demonstrate our approach in a driving scenario. © 2017 Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2017.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034101865&doi=10.1016%2fj.cogsys.2017.08.004&partnerID=40&md5=d549bca6dc992df23220746d15992106,2018,2021-07-20 15:48:28,2021-07-20 15:48:28
94Q97J7C,journalArticle,2021,"Piantadosi, S.T.",The Computational Origin of Representation,"Each of our theories of mental representation provides some insight into how the mind works. However, these insights often seem incompatible, as the debates between symbolic, dynamical, emergentist, sub-symbolic, and grounded approaches to cognition attest. Mental representations—whatever they are—must share many features with each of our theories of representation, and yet there are few hypotheses about how a synthesis could be possible. Here, I develop a theory of the underpinnings of symbolic cognition that shows how sub-symbolic dynamics may give rise to higher-level cognitive representations of structures, systems of knowledge, and algorithmic processes. This theory implements a version of conceptual role semantics by positing an internal universal representation language in which learners may create mental models to capture dynamics they observe in the world. The theory formalizes one account of how truly novel conceptual content may arise, allowing us to explain how even elementary logical and computational operations may be learned from a more primitive basis. I provide an implementation that learns to represent a variety of structures, including logic, number, kinship trees, regular languages, context-free languages, domains of theories like magnetism, dominance hierarchies, list structures, quantification, and computational primitives like repetition, reversal, and recursion. This account is based on simple discrete dynamical processes that could be implemented in a variety of different physical or biological systems. In particular, I describe how the required dynamics can be directly implemented in a connectionist framework. The resulting theory provides an “assembly language” for cognition, where high-level theories of symbolic computation can be implemented in simple dynamics that themselves could be encoded in biologically plausible systems. © 2020, Springer Nature B.V.",Minds and Machines,10.1007/s11023-020-09540-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094971194&doi=10.1007%2fs11023-020-09540-9&partnerID=40&md5=acf104d268f0f57433dcf2ae3a9ee58b,2021,2021-07-20 15:48:28,2021-07-20 15:48:28
FC8WCW5P,journalArticle,2020,"Louati, A.",A hybridization of deep learning techniques to predict and control traffic disturbances,"Predicting traffic disturbances is a challenging problem in urban cities. Emergency vehicles (EV) is one of the biggest disturbances that affect traffic fluidity. The goal of this paper is to provide a machine learning application to deal with emergency cases in traffic networks. Particularly, we investigate the use of deep learning techniques coupled with Artificial Immune System to tackle the issue of EV guidance at signalized intersections. To accomplish this goal, we develop a traffic signal control system capable to estimate traffic status, guide EV to reach their destinations while assuming better traffic condition, control traffic signals, and adapt to new disturbances. For traffic forecasting, the suggested system inherits the advantages of convolutional neural networks, classification, and long short term memory. To control traffic signals, the suggested system uses the immune memory algorithm. To enhance and adapt control decisions to traffic disturbances, the suggested system uses a continuous learning approach assumed by an adapted reinforcement learning algorithm. Assessments using well-known algorithms from the literature are detailed in this work. The benchmarking algorithms are the preemptive longest queue first matching weight matrix system, the pre-emptive immune memory algorithm inspired case-based reasoning, and the preemptive optimized stage based fixed time algorithm. Experiments show a competitive performance of the suggested system compared to benchmarking algorithms. © 2020, Springer Nature B.V.",Artificial Intelligence Review,10.1007/s10462-020-09831-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083457229&doi=10.1007%2fs10462-020-09831-8&partnerID=40&md5=78f2b5eb190c71f366a105b3d717b0e6,2020,2021-07-20 15:48:28,2021-07-20 15:48:28
N6H2XHZF,journalArticle,2012,"Ontañón, S.; Dellunde, P.; Godo, L.; Plaza, E.",A defeasible reasoning model of inductive concept learning from examples and communication,"This paper introduces a logical model of inductive generalization, and specifically of the machine learning task of inductive concept learning (ICL). We argue that some inductive processes, like ICL, can be seen as a form of defeasible reasoning. We define a consequence relation characterizing which hypotheses can be induced from given sets of examples, and study its properties, showing they correspond to a rather well-behaved non-monotonic logic. We will also show that with the addition of a preference relation on inductive theories we can characterize the inductive bias of ICL algorithms. The second part of the paper shows how this logical characterization of inductive generalization can be integrated with another form of non-monotonic reasoning (argumentation), to define a model of multiagent ICL. This integration allows two or more agents to learn, in a consistent way, both from induction and from arguments used in the communication between them. We show that the inductive theories achieved by multiagent induction plus argumentation are sound, i.e. they are precisely the same as the inductive theories built by a single agent with all data. © 2012 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2012.08.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866507808&doi=10.1016%2fj.artint.2012.08.006&partnerID=40&md5=f87c6118cb125a4a37ffe085014c6916,2012,2021-07-20 15:48:28,2021-07-20 15:48:28
HNVJJ6WV,journalArticle,2021,"Wang, S.; Gong, X.; Song, M.; Fei, C.Y.; Quaadgras, S.; Peng, J.; Zou, P.; Chen, J.; Zhang, W.; Jiao, R.J.",Smart dispatching and optimal elevator group control through real-time occupancy-aware deep learning of usage patterns,"Passengers spend much time on elevator journeys in high-rise buildings every day, in which unnecessary stops caused by lack of cab capacity take up a certain proportion of the journey time. This study proposes real-time occupancy-aware smart dispatching to avoid pick-up failure by introducing the occupancy information that reflects elevator capacity into the optimization model, thus improving dispatching performance. Occupancy awareness is firstly implemented with deep learning-based object detection to provide estimated capacity. Traffic pattern recognition is implemented with time series analysis and fuzzy logic. Case-based reasoning is applied to recognize the current usage pattern and to deploy specific dispatching strategies. A prioritized A* search model is built to solve dispatching optimization with occupancy information. Discrete event simulation is conducted with Simio and MATLAB to validate the proposed dispatching model. © 2021 Elsevier Ltd",Advanced Engineering Informatics,10.1016/j.aei.2021.101286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103965768&doi=10.1016%2fj.aei.2021.101286&partnerID=40&md5=06c45302a9ccc49108020cd6d3b7b75d,2021,2021-07-20 15:48:28,2021-07-20 15:48:28
P9YQ6EFR,journalArticle,2021,"Athiraja, A.; Vijayakumar, P.",Banana disease diagnosis using computer vision and machine learning methods,"As it is observed that the banana production is plagued by numerous disease conditions and inflicting large loss to the poor farmers. By using modern technology of image processing and soft computing techniques, these may be known at the sooner stage and appropriate precautions may be taken to avoid more injury and thus increase in healthy production. In this research work used identified the banana diseases in sooner stage. Through the pre-processing technique, image is input to urge standardization and soft coring filter is completed to get rid of the noise. Then colour, shape and texture feature are completed for feature extraction, followed by classification techniques. During these classification techniques, two algorithms are used, that’s the Adaptive Neuro-Fuzzy Inference System and case-based reasoning. Then fuzzy logic is used for making the decision. The proposed system analysis was done using the Receiver Operating Characteristics (ROC) curve. The analysis shows Adaptive Neuro-Fuzzy Inference System is best than the case-based reasoning algorithm. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-020-02273-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087651905&doi=10.1007%2fs12652-020-02273-8&partnerID=40&md5=ab141577bef7d3a9ced8964b07876f2c,2021,2021-07-20 15:48:28,2021-07-20 15:48:28
QK6KLI3G,journalArticle,2013,"Windridge, D.; Felsberg, M.; Shaukat, A.",A framework for hierarchical perception-action learning utilizing fuzzy reasoning,"Perception-action (P-A) learning is an approach to cognitive system building that seeks to reduce the complexity associated with conventional environment-representation/action-planning approaches. Instead, actions are directly mapped onto the perceptual transitions that they bring about, eliminating the need for intermediate representation and significantly reducing training requirements.We here set out a very general learning framework for cognitive systems in which online learning of the P-A mapping may be conducted within a symbolic processing context, so that complex contextual reasoning can influence the P-A mapping. In utilizing a variational calculus approach to define a suitable objective function, the P-A mapping can be treated as an online learning problem via gradient descent using partial derivatives. Our central theoretical result is to demonstrate top-down modulation of low-level perceptual confidences via the Jacobian of the higher levels of a subsumptive P-A hierarchy. Thus, the separation of the Jacobian as a multiplying factor between levels within the objective function naturally enables the integration of abstract symbolic manipulation in the form of fuzzy deductive logic into the P-A mapping learning. We experimentally demonstrate that the resulting framework achieves significantly better accuracy than using P-A learning without top-down modulation. We also demonstrate that it permits novel forms of context-dependent multilevel P-A mapping, applying the mechanism in the context of an intelligent driver assistance system. © 2012 IEEE.",IEEE Transactions on Cybernetics,10.1109/TSMCB.2012.2202109,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890431227&doi=10.1109%2fTSMCB.2012.2202109&partnerID=40&md5=77b46679c16bb633e723b4686c5779d8,2013,2021-07-20 15:48:28,2021-07-20 15:48:28
M8Z5J8KA,journalArticle,2019,"Karimipour, H.; Dehghantanha, A.; Parizi, R.M.; Choo, K.-K.R.; Leung, H.",A Deep and Scalable Unsupervised Machine Learning System for Cyber-Attack Detection in Large-Scale Smart Grids,"Smart grid technology increases reliability, security, and efficiency of the electrical grids. However, its strong dependencies on digital communication technology bring up new vulnerabilities that need to be considered for efficient and reliable power distribution. In this paper, an unsupervised anomaly detection based on statistical correlation between measurements is proposed. The goal is to design a scalable anomaly detection engine suitable for large-scale smart grids, which can differentiate an actual fault from a disturbance and an intelligent cyber-attack. The proposed method applies feature extraction utilizing symbolic dynamic filtering (SDF) to reduce computational burden while discovering causal interactions between the subsystems. The simulation results on IEEE 39, 118, and 2848 bus systems verify the performance of the proposed method under different operation conditions. The results show an accuracy of 99%, true positive rate of 98%, and false positive rate of less than 2% © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2920326,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069003434&doi=10.1109%2fACCESS.2019.2920326&partnerID=40&md5=e1913c4f01bb3df79661f20c9516c6a0,2019,2021-07-20 15:48:28,2021-07-20 15:48:28
7SDMQNPP,journalArticle,2021,"Singh, G.; Yow, K.-C.",These do not Look like Those: An Interpretable Deep Learning Model for Image Recognition,"Interpretation of the reasoning process of a prediction made by a deep learning model is always desired. However, when it comes to the predictions of a deep learning model that directly impacts on the lives of people then the interpretation becomes a necessity. In this paper, we introduce a deep learning model: negative-positive prototypical part network (NP-ProtoPNet). This model attempts to imitate human reasoning for image recognition while comparing the parts of a test image with the corresponding parts of the images from known classes. We demonstrate our model on the dataset of chest X-ray images of Covid-19 patients, pneumonia patients and normal people. The accuracy and precision that our model receives is on par with the best performing non-interpretable deep learning models. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3064838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102624946&doi=10.1109%2fACCESS.2021.3064838&partnerID=40&md5=71543b665de2b265164bd1f20775a67a,2021,2021-07-20 15:48:29,2021-07-20 15:48:29
M36TNPXK,journalArticle,2021,"Cozman, F.G.; Munhoz, H.N.",Some thoughts on knowledge-enhanced machine learning,"How can we employ theoretical insights and practical tools from knowledge representation and reasoning to enhance machine learning, and when is it worthwhile to do so? This paper is based on an invited talk delivered at ECSQARU2019 around this question. It emphasizes the knowledge representation and reasoning side of knowledge-enhanced machine learning, looking at a few case studies: the finite model theory of probabilistic languages, the generation of explanations for embeddings, and an “explainable” version of the Winograd Challenge. © 2021 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2021.06.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109146391&doi=10.1016%2fj.ijar.2021.06.003&partnerID=40&md5=fd187a6fb7b604ab8b39c82f94a80793,2021,2021-07-20 15:48:29,2021-07-20 15:48:29
6Z9JP7A2,journalArticle,2016,"Hao, W.; Yeung, D.-Y.",Towards Bayesian Deep Learning: A Framework and Some Existing Methods,"While perception tasks such as visual object recognition and text understanding play an important role in human intelligence, subsequent tasks that involve inference, reasoning, and planning require an even higher level of intelligence. The past few years have seen major advances in many perception tasks using deep learning models. For higher-level inference, however, probabilistic graphical models with their Bayesian nature are still more powerful and flexible. To achieve integrated intelligence that involves both perception and inference, it is naturally desirable to tightly integrate deep learning and Bayesian models within a principled probabilistic framework, which we call Bayesian deep learning. In this unified framework, the perception of text or images using deep learning can boost the performance of higher-level inference and in return, the feedback from the inference process is able to enhance the perception of text or images. This paper proposes a general framework for Bayesian deep learning and reviews its recent applications on recommender systems, topic models, and control. In this paper, we also discuss the relationship and differences between Bayesian deep learning and other related topics such as the Bayesian treatment of neural networks. © 1989-2012 IEEE.",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2016.2606428,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027053369&doi=10.1109%2fTKDE.2016.2606428&partnerID=40&md5=2298795d82efbe8c50a7599aad18c609,2016,2021-07-20 15:48:29,2021-07-20 15:48:29
NIJX7GV3,journalArticle,2019,"Zhang, J.; Wang, Y.; Molino, P.; Li, L.; Ebert, D.S.",Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models,"Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied. © 2018 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2018.2864499,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051755222&doi=10.1109%2fTVCG.2018.2864499&partnerID=40&md5=5638a86cbc08953f2793e4a6252057d8,2019,2021-07-20 15:48:29,2021-07-20 15:48:29
DFVAQCYK,journalArticle,2015,"Hüllermeier, E.",Does machine learning need fuzzy logic?,"This article is a short position paper in which the author outlines his (necessarily subjective) perception of current research in fuzzy machine learning, that is, the use of formal concepts and mathematical tools from fuzzy sets and fuzzy logic in the field of machine learning. The paper starts with a critical appraisal of previous contributions to fuzzy machine learning and ends with a suggestion of some directions for future work. © 2015 Elsevier B.V. All rights reserved.",Fuzzy Sets and Systems,10.1016/j.fss.2015.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945489768&doi=10.1016%2fj.fss.2015.09.001&partnerID=40&md5=b1842c1a461d20de00951b4effc44cbd,2015,2021-07-20 15:48:29,2021-07-20 15:48:29
W9ZTSUJL,journalArticle,2021,"Racharak, T.",On Approximation of Concept Similarity Measure in Description Logic ELH with Pre-Trained Word Embedding,"Data-driven and knowledge-driven methods are two mainstream techniques in the pursuit of developing artificial intelligence systems. While data-driven methods seek to develop a decision model from observations in the real world, they are difficult to provide an explanation for the results in human terms. On the other hand, knowledge-driven methods that employ symbolic reasoning based on formal semantics of a knowledge-base are thus more interpretable and explainable, while lacking an ability to deal with incomplete modeling of the structured knowledge-bases. This work aims to tackle these issues on ontology similarity by proposing a general framework that combines the strengths of both approaches for measuring semantic similarity of concepts in a description logic (DL) ontology. More specifically, a neuro-symbolic integrated framework is defined to exploit the pre-trained word embeddings with semantic definitions in an ontology to yield an explainable degree of concept similarity. To demonstrate its applicability, we develop a concrete similarity measure sf sim_ϵ conforming to the proposed framework and also introduce an efficient algorithm that can extract an explanation for why such a degree is indicated. The correctness is shown by analyzing theoretical properties that it guarantees to preserve and also by performing an empirical evaluation with a medical ontology SNOMED CT and a medical pre-trained embedding BioWordVec. The results show that our proposed method remains both interpretability and explainability while achieving comparable performance, relative to the state-of-the-art approaches in the data and knowledge-driven methods. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3073730,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104631138&doi=10.1109%2fACCESS.2021.3073730&partnerID=40&md5=529c9a386b7a809ea18740d154d07878,2021,2021-07-20 15:48:29,2021-07-20 15:48:29
5EEQ5ZAD,journalArticle,2011,"Chapin, N.; Szymanski, B.; Bringsjord, S.; Schimanski, B.",A bottom-up complement to the logic-based top-down approach to the story arrangement test,"Psychometric AI is a type of AI distinguished by the pursuit of intelligent systems able to excel on psychometrically validated human-level tests of cognitive abilities. We seek to build a system that solves a specific sub-test within Psychometric AI: the story arrangment test. Items in this test confront the test-taker with a set of jumbed snapshots (whether diagrammatic or otherwise) which must be ordered to tell a coherent story. We propose a dual-process system that combines bottom-up non- or sub-symbolic processing (e.g. neural network-based modelling) with top-down symbolic processing (e.g. deductive reasoning over declarative information represented as formulae in a logical system) for solving these tests of cognitive ability. The top-down process provides the benefits of a traceable proof, but requires a large amount of pre-existing knowledge. The bottom-up technique sacrifices provability and certainty on some problems for speed, but always yields some level of an answer to a given problem. This demonstrates a natural marriage between the two: the bottom-up approach seems especially powerful when used as a form of pre-processing in conjunction with a logic-based approach, because the latter approach would only need to consider a small number of possible orderings of snapshots. © 2011 Taylor & Francis.",Journal of Experimental and Theoretical Artificial Intelligence,10.1080/0952813X.2010.502313,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960791745&doi=10.1080%2f0952813X.2010.502313&partnerID=40&md5=db04f10d1680508d046bf49ce7ee636b,2011,2021-07-20 15:48:29,2021-07-20 15:48:29
B47SSE6P,journalArticle,2019,"Chaturvedi, I.; Satapathy, R.; Cavallari, S.; Cambria, E.",Fuzzy commonsense reasoning for multimodal sentiment analysis,"The majority of user-generated content posted online is in the form of text, images and videos but also physiological signals in games. AffectiveSpace is a vector space of affective commonsense available for English text but not for other languages nor other modalities such as electrocardiogram signals. We overcome this limitation by using deep learning to extract features from each modality and then projecting them to a common AffectiveSpace that has been clustered into different emotions. Because, in the real world, individuals tend to have partial or mixed sentiments about an opinion target, we use a fuzzy logic classifier to predict the degree of a particular emotion in AffectiveSpace. The combined model of deep convolutional neural networks and fuzzy logic is termed Convolutional Fuzzy Sentiment Classifier. Lastly, because the computational complexity of a fuzzy classifier is exponential with respect to the number of features, we project features to a four dimensional emotion space in order to speed up the classification performance. © 2019",Pattern Recognition Letters,10.1016/j.patrec.2019.04.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065463296&doi=10.1016%2fj.patrec.2019.04.024&partnerID=40&md5=e809ca1e1a16b6df5b2a4aa1287fff8d,2019,2021-07-20 15:48:29,2021-07-20 15:48:29
6MJP3R6E,journalArticle,2013,"López, B.",Case-based reasoning: A concise introductionz,"Case-based reasoning is a methodology with a long tradition in artificial intelligence that brings together reasoning and machine learning techniques to solve problems based on past experiences or cases. Given a problem to be solved, reasoning involves the use of methods to retrieve similar past cases in order to reuse their solution for the problem at hand. Once the problem has been solved, learning methods can be applied to improve the knowledge based on past experiences. In spite of being a broad methodology applied in industry and services, case-based reasoning has often been forgotten in both artificial intelligence and machine learning books. The aim of this book is to present a concise introduction to case-based reasoning providing the essential building blocks for the design of case-based reasoning systems, as well as to bring together the main research lines in this field to encourage students to solve current CBR challenges. Table of Contents: Introduction/The Case-Base/Reasoning and Decision Making/Learning/Formal Aspects/Summary and Beyond. © 2013 by Morgan & Claypool.",Synthesis Lectures on Artificial Intelligence and Machine Learning,10.2200/S00490ED1V01Y201303AIM020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877625434&doi=10.2200%2fS00490ED1V01Y201303AIM020&partnerID=40&md5=f399f1a0b9cf36afb0924098de39a504,2013,2021-07-20 15:48:29,2021-07-20 15:48:29
8MXUIQCN,journalArticle,2017,"Wang, C.; Gong, L.; Yu, Q.; Li, X.; Xie, Y.; Zhou, X.",DLAU: A scalable deep learning accelerator unit on FPGA,"As the emerging field of machine learning, deep learning shows excellent ability in solving complex learning problems. However, the size of the networks becomes increasingly large scale due to the demands of the practical applications, which poses significant challenge to construct a high performance implementations of deep learning neural networks. In order to improve the performance as well as to maintain the low power cost, in this paper we design deep learning accelerator unit (DLAU), which is a scalable accelerator architecture for large-scale deep learning networks using field-programmable gate array (FPGA) as the hardware prototype. The DLAU accelerator employs three pipelined processing units to improve the throughput and utilizes tile techniques to explore locality for deep learning applications. Experimental results on the state-of-the-art Xilinx FPGA board demonstrate that the DLAU accelerator is able to achieve up to 36.1 × speedup comparing to the Intel Core2 processors, with the power consumption at 234 mW. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2016.2587683,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013628943&doi=10.1109%2fTCAD.2016.2587683&partnerID=40&md5=8bda909f74760d69cca8b9cfc118f64d,2017,2021-07-20 15:48:29,2021-07-20 15:48:29
NB9ATMIZ,journalArticle,2020,"Niu, J.; Tang, Y.; Sun, Z.; Zhang, W.",Inter-Patient ECG Classification with Symbolic Representations and Multi-Perspective Convolutional Neural Networks,"This paper presents a novel deep learning framework for the inter-patient electrocardiogram (ECG) heartbeat classification. A symbolization approach especially designed for ECG is introduced, which can jointly represent the morphology and rhythm of the heartbeat and alleviate the influence of inter-patient variation through baseline correction. The symbolic representation of the heartbeat is used by a multi-perspective convolutional neural network (MPCNN) to learn features automatically and classify the heartbeat. We evaluate our method for the detection of the supraventricular ectopic beat (SVEB) and ventricular ectopic beat (VEB) on MIT-BIH arrhythmia dataset. Compared with the state-of-the-art methods based on manual features or deep learning models, our method shows superior performance: the overall accuracy of 96.4%, F1 scores for SVEB and VEB of 76.6% and 89.7%, respectively. The ablation study on our method validates the effectiveness of the proposed symbolization approach and joint representation architecture, which can help the deep learning model to learn more general features and improve the ability of generalization for unseen patients. Because our method achieves a competitive inter-patient heartbeat classification performance without complex handcrafted features or the intervention of the human expert, it can also be adjusted to handle various other tasks relative to ECG classification. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2019.2942938,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084720737&doi=10.1109%2fJBHI.2019.2942938&partnerID=40&md5=2304ad1ed3044a01249177fb5860f30f,2020,2021-07-20 15:48:29,2021-07-20 15:48:29
FID3CQC2,journalArticle,2021,"Bride, H.; Cai, C.-H.; Dong, J.; Dong, J.S.; Hóu, Z.; Mirjalili, S.; Sun, J.",Silas: A high-performance machine learning foundation for logical reasoning and verification,"This paper introduces a new high-performance machine learning tool named Silas, which is built to provide a more transparent, dependable and efficient data analytics service. We discuss the machine learning aspects of Silas and demonstrate the advantage of Silas in its predictive and computational performance. We show that several customised algorithms in Silas yield better predictions in a significantly shorter time compared to the state-of-the-art. Another focus of Silas is on providing a formal foundation of decision trees to support logical analysis and verification of learned prediction models. We illustrate the potential capabilities of the fusion of machine learning and logical reasoning by showcasing applications in three directions: formal verification of the prediction model against user specifications, training correct-by-construction models, and explaining the decision-making of predictions. © 2021 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2021.114806,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103686889&doi=10.1016%2fj.eswa.2021.114806&partnerID=40&md5=00e3baa1c4a988e3dda622f3c3764a4c,2021,2021-07-20 15:48:29,2021-07-20 15:48:29
FQ78YXTY,journalArticle,2020,"Kelly, M.A.; Arora, N.; West, R.L.; Reitter, D.",Holographic Declarative Memory: Distributional Semantics as the Architecture of Memory,"We demonstrate that the key components of cognitive architectures (declarative and procedural memory) and their key capabilities (learning, memory retrieval, probability judgment, and utility estimation) can be implemented as algebraic operations on vectors and tensors in a high-dimensional space using a distributional semantics model. High-dimensional vector spaces underlie the success of modern machine learning techniques based on deep learning. However, while neural networks have an impressive ability to process data to find patterns, they do not typically model high-level cognition, and it is often unclear how they work. Symbolic cognitive architectures can capture the complexities of high-level cognition and provide human-readable, explainable models, but scale poorly to naturalistic, non-symbolic, or big data. Vector-symbolic architectures, where symbols are represented as vectors, bridge the gap between the two approaches. We posit that cognitive architectures, if implemented in a vector-space model, represent a useful, explanatory model of the internal representations of otherwise opaque neural architectures. Our proposed model, Holographic Declarative Memory (HDM), is a vector-space model based on distributional semantics. HDM accounts for primacy and recency effects in free recall, the fan effect in recognition, probability judgments, and human performance on an iterated decision task. HDM provides a flexible, scalable alternative to symbolic cognitive architectures at a level of description that bridges symbolic, quantum, and neural models of cognition. © 2020 Cognitive Science Society, Inc",Cognitive Science,10.1111/cogs.12904,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095401513&doi=10.1111%2fcogs.12904&partnerID=40&md5=fc6197e721487682e27359e97464b85e,2020,2021-07-20 15:48:29,2021-07-20 15:48:29
86M4DFR4,journalArticle,2020,"Ali, D.; Frimpong, S.","Artificial intelligence, machine learning and process automation: existing knowledge frontier and way forward for mining sector","Machine learning and artificial intelligence are the two fields of computer science dealing with the innovative idea of inducing smartness and intelligence in machines and automating complex tasks and operations through modern learning algorithms. While the rest of the operational fields have been diligent in developing new technologies, the mining industry has been lacking when it comes to applying these innovative methodologies to achieve operation autonomy with intelligence. However, this trend is beginning to change with a few researchers adopting the fields of machine learning and artificial intelligence to improve the existing technologies. This study was an attempt to review and analyze all the recent automation related work in every sector of the mining industry including mineral prospecting and exploration, mine planning, equipment selection, underground and surface equipment operation, drilling and blasting, mineral processing, etc., for establishing the existing frontiers of technological advancement. Shortcomings and challenges were identified within the current research work. Recommendations were provided to progress the existing technology by implementing deep learning, machine learning, and artificial intelligence for smart and intelligence-based evolution in the mining sector. With all of this innovative development and implementation of smart automation systems, the foundation for the mine of the future could be built, thus creating efficient, effective, and safer machines with sustainable mining operations. © 2020, Springer Nature B.V.",Artificial Intelligence Review,10.1007/s10462-020-09841-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083888978&doi=10.1007%2fs10462-020-09841-6&partnerID=40&md5=88eb24b7dee80422ffb5e16e6f18b9fa,2020,2021-07-20 15:48:29,2021-07-20 15:48:29
G7A2MTFU,journalArticle,2019,"Khemlani, S.; Johnson-Laird, P.N.",Why Machines Don’t (yet) Reason Like People,"AI has never come to grips with how human beings reason in daily life. Many automated theorem-proving technologies exist, but they cannot serve as a foundation for automated reasoning systems. In this paper, we trace their limitations back to two historical developments in AI: the motivation to establish automated theorem-provers for systems of mathematical logic, and the formulation of nonmonotonic systems of reasoning. We then describe why human reasoning cannot be simulated by current machine reasoning or deep learning methodologies. People can generate inferences on their own instead of just evaluating them. They use strategies and fallible shortcuts when they reason. The discovery of an inconsistency does not result in an explosion of inferences—instead, it often prompts reasoners to abandon a premise. And the connectives they use in natural language have different meanings than those in classical logic. Only recently have cognitive scientists begun to implement automated reasoning systems that reflect these human patterns of reasoning. A key constraint of these recent implementations is that they compute, not proofs or truth values, but possibilities. © 2019, This is a U.S. government work and not under copyright protection in the U.S.; foreign copyright protection may apply.",KI - Kunstliche Intelligenz,10.1007/s13218-019-00599-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074617318&doi=10.1007%2fs13218-019-00599-w&partnerID=40&md5=a6f38ebad9483391866b21845ea15e4f,2019,2021-07-20 15:48:30,2021-07-20 15:48:30
H42NNAJX,journalArticle,2020,"Nebiu, D.; Kamberaj, H.",Symbolic Information Flow Measurement (SIFM): A software for measurement of information flow using symbolic analysis,Symbolic Information Flow Measurement software is used to compute the information flow between different components of a dynamical system or different dynamical systems using symbolic transfer entropy. The time series represents the time evolution trajectory of a dynamical system. We introduce a method to perform a symbolic analysis of the time series based on the coarse-graining using a machine learning approach and computation of the embedding parameters. Information flow is measured in terms of the local and average symbolic transfer entropies. We also introduce a new measure of mutual information based on the symbolic analysis. © 2020,SoftwareX,10.1016/j.softx.2020.100470,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083002119&doi=10.1016%2fj.softx.2020.100470&partnerID=40&md5=cd138e9f10fd5eae9a5b5599d5c1d86b,2020,2021-07-20 15:48:30,2021-07-20 15:48:30
JQNQNRGA,journalArticle,2019,"Leeuwenberg, A.; Moens, M.-F.",A survey on temporal reasoning for temporal information extraction from text,"Time is deeply woven into how people perceive, and communicate about the world. Almost unconsciously, we provide our language utterances with temporal cues, like verb tenses, and we can hardly produce sentences without such cues. Extracting temporal cues from text, and constructing a global temporal view about the order of described events is a major challenge of automatic natural language understanding. Temporal reasoning, the process of combining different temporal cues into a coherent temporal view, plays a central role in temporal information extraction. This article presents a comprehensive survey of the research from the past decades on temporal reasoning for automatic temporal information extraction from text, providing a case study on how combining symbolic reasoning with machine learning-based information extraction systems can improve performance. It gives a clear overview of the used methodologies for temporal reasoning, and explains how temporal reasoning can be, and has been successfully integrated into temporal information extraction systems. Based on the distillation of existing work, this survey also suggests currently unexplored research areas. We argue that the level of temporal reasoning that current systems use is still incomplete for the full task of temporal information extraction, and that a deeper understanding of how the various types of temporal information can be integrated into temporal reasoning is required to drive future research in this area. © 2019 AI Access Foundation. All rights reserved.",Journal of Artificial Intelligence Research,10.1613/jair.1.11727,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075483377&doi=10.1613%2fjair.1.11727&partnerID=40&md5=1db8d20cce9d93aa12e06ec9ad74540f,2019,2021-07-20 15:48:30,2021-07-20 15:48:30
YBBHMIUM,journalArticle,2020,"Yang, C.; Moi, S.; Hou, M.; Chuang, L.; Lin, Y.",Applications of Deep Learning and Fuzzy Systems to Detect Cancer Mortality in Next-Generation Genomic Data,"In the era of advanced precision medicine, next-generation genomic data are crucial to achieve breakthroughs in cancer medicine. Effective cancer mortality risk estimation for genomic data associated with cancer remains a vital challenge. The combination of machine learning algorithms and conventional survival analysis can advance the detection of high-risk missense mutation variants and candidate genes associated with cancer mortality in next-generation genomic data. In this study, a fuzzy logic system combined with machine learning algorithms and conventional survival analysis named FuzzyDeepCoxPH was proposed to identify high-risk missense mutation variants and candidate genes highly associated with cancer mortality. DL-derived abstracted weights and Cox proportional hazards (CoxPH) ratios were used to develop four model-based risk scores to consider the factor importance associated with risk stratification, time-varying effects, and individual and interaction effects among features. Fuzzy rules based on a fuzzy logic system were designed to integrate these considerations by merging four model-based risk scores to develop advanced risk estimation. The clinical features and next-generation sequencing of deoxyribonucleic acid and ribonucleic acid genomic data were used to evaluate FuzzyDeepCoxPH performance. The results indicated that FuzzyDeepCoxPH can effectively distinguish high-risk variants and candidate genes related to cancer mortality. In FuzzyDeepCoxPH, the fuzzy logic system was applied to combine DL-based and CoxPH-based models to provide a comprehensive cancer mortality risk estimation for cancer medicine. IEEE",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2020.3028909,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101441448&doi=10.1109%2fTFUZZ.2020.3028909&partnerID=40&md5=a662a33018350913518b1a53db7d1d13,2020,2021-07-20 15:48:30,2021-07-20 15:48:30
IQU9Q8N2,journalArticle,2020,"Sacco, A.; Flocco, M.; Esposito, F.; Marchetto, G.",An architecture for adaptive task planning in support of IoT-based machine learning applications for disaster scenarios,"The proliferation of the Internet of Things (IoT) in conjunction with edge computing has recently opened up several possibilities for several new applications. Typical examples are Unmanned Aerial Vehicles (UAV) that are deployed for rapid disaster response, photogrammetry, surveillance, and environmental monitoring. To support the flourishing development of Machine Learning assisted applications across all these networked applications, a common challenge is the provision of a persistent service, i.e., a service capable of consistently maintaining a high level of performance, facing possible failures. To address these service resilient challenges, we propose APRON, an edge solution for distributed and adaptive task planning management in a network of IoT devices, e.g., drones. Exploiting Jackson's network model, our architecture applies a novel planning strategy to better support control and monitoring operations while the states of the network evolve. To demonstrate the functionalities of our architecture, we also implemented a deep-learning based audio-recognition application using the APRON NorthBound interface, to detect human voices in challenged networks. The application's logic uses Transfer Learning to improve the audio classification accuracy and the runtime of the UAV-based rescue operations. © 2020 Elsevier B.V.",Computer Communications,10.1016/j.comcom.2020.07.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088030926&doi=10.1016%2fj.comcom.2020.07.011&partnerID=40&md5=86f36b355c178f76082a60511356a978,2020,2021-07-20 15:48:30,2021-07-20 15:48:30
CDBXMQNU,journalArticle,2021,"Moscato, P.; Sun, H.; Haque, M.N.",Analytic Continued Fractions for Regression: A Memetic Algorithm Approach,We present an approach for regression problems that employs analytic continued fractions as a novel representation. Comparative computational results using a memetic algorithm are reported in this work. Our experiments included fifteen other different machine learning approaches including five genetic programming methods for symbolic regression and ten machine learning methods. The comparison on training and test generalization was performed using 94 datasets of the Penn State Machine Learning Benchmark. The statistical tests showed that the generalization results using analytic continued fractions provide a powerful and interesting new alternative in the quest for compact and interpretable mathematical models for artificial intelligence. © 2021 Elsevier Ltd,Expert Systems with Applications,10.1016/j.eswa.2021.115018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105057778&doi=10.1016%2fj.eswa.2021.115018&partnerID=40&md5=84add55f8d0e4ecf6f185f0459b23e00,2021,2021-07-20 15:48:30,2021-07-20 15:48:30
UHPX4UN3,journalArticle,2021,"Krishnamurthy, P.; Sarmadi, A.; Khorrami, F.",Explainable classification by learning human-readable sentences in feature subsets,"We propose a new methodology (Sentences in Feature Subsets, i.e., SiFS) to mine human-readable decision rules from empirical data sets. Unlike opaque classifiers obtained using deep learning, the proposed methodology derives decision rules that are compact and comprised of Boolean logic sentences involving subsets of features in the input data. For this purpose, we develop a new classifier model defined in terms of sets of inequalities among selected features in the input data. To empirically derive suitable inequalities from training data, our approach combines a differentiable representation of sets of Boolean logic sentences, gradient-based optimization of coefficients in the inequalities, a genetic-based algorithm for selection of the subsets of features, and a “goodness” model of sentences to prune and down-select sentences. We present results on synthetic and real-world benchmark datasets to demonstrate efficacy of SiFS in deriving human-readable decision rules. It is seen that SiFS achieves comparable accuracies to the best among various other classification algorithms (accuracies of 95% to 100% on several datasets, F1 scores between 0.95 and 1.0), reasonable computation times (training times of a few seconds for considered datasets), and compact human-readable decision rules (between 1 to 10 sentences of 3 words or less for considered datasets). © 2021 Elsevier Inc.",Information Sciences,10.1016/j.ins.2021.02.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102427389&doi=10.1016%2fj.ins.2021.02.031&partnerID=40&md5=6cf9b1b59d37a1d0b5e7c3100f11a2df,2021,2021-07-20 15:48:30,2021-07-20 15:48:30
JG7WSN9B,journalArticle,2020,"Bołtuć, P.; Boltuc, M.",BICA for AGI,"BICAs for AI have been happening for decades, realized within multiple cognitive architectures. What is a BICA for AGI? It requires AI to go beyond the limitations of predicative human language, and of predicative logic based on it; also, above human reportable consciousness, to the subconscious/non-conscious level of human (and machine) mind; and then still beyond it. Within machine cognition this is the sub-symbolic level. It has to gauge gestalts, or patterns, directly from the processes, which goes beyond human-level observational capacities or human-understandable language, even beyond the language of human-readable mathematics (Boltuc, 2018). It requires versatile life-long learning (Siegelmann, 2018). Through complex stochastic processes, it needs to confabulate by creative permutations of multifarious gestalts and to select those with useful applications (Thaler, 1997). This is computing at the edge of chaos (Goertzel, 2006). © 2019 Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2019.09.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073815581&doi=10.1016%2fj.cogsys.2019.09.020&partnerID=40&md5=a8d49eee1ad08550b6efe89ad6e1b94c,2020,2021-07-20 15:48:30,2021-07-20 15:48:30
Z9LQSC4L,journalArticle,2011,"Leandro, C.; Pita, H.; Monteiro, L.",Symbolic knowledge extraction from trained neural networks governed by Łukasiewicz logics,"This work describes a methodology to extract symbolic rules from trained neural networks. In our approach, patterns on the network are codified using formulas on a Łukasiewicz logic. For this we take advantage of the fact that every connective in this multi-valued logic can be evaluated by a neuron in an artificial network having, by activation function the identity truncated to zero and one. This fact simplifies symbolic rule extraction and allows the easy injection of formulas into a network architecture. We trained this type of neural network using a back-propagation algorithm based on Levenderg-Marquardt algorithm, where in each learning iteration, we restricted the knowledge dissemination in the network structure. This makes the descriptive power of produced neural networks similar to the descriptive power of Łukasiewicz logic language, minimizing the information loss on the translation between connectionist and symbolic structures. To avoid redundance on the generated network, the method simplifies them in a pruning phase, using the ""Optimal Brain Surgeon"" algorithm. We tested this method on the task of finding the formula used on the generation of a given truth table. For real data tests, we selected the Mushrooms data set, available on the UCI Machine Learning Repository. © 2011 Springer-Verlag Berlin Heidelberg.",Studies in Computational Intelligence,10.1007/978-3-642-20206-3_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954551066&doi=10.1007%2f978-3-642-20206-3_3&partnerID=40&md5=22f44da1c9521a754c2671153708e250,2011,2021-07-20 15:48:30,2021-07-20 15:48:30
BYLHAZZD,journalArticle,2018,"El Hatri, C.; Boumhidi, J.",Fuzzy deep learning based urban traffic incident detection,"Traffic incident detection (TID) is an important part of any modern traffic control because it offers an opportunity to maximise road system performance. For the complexity and the nonlinear characteristics of traffic incidents, this paper proposes a novel fuzzy deep learning based TID method which considers the spatial and temporal correlations of traffic flow inherently. Parameters of the deep network are initialized using a Stacked Auto-Encoder (SAE) model following a layer by layer pre-training procedure. To conduct the fine tuning step, the back-propagation algorithm is used to precisely adjust the parameters in the deep network. Fuzzy logic is employed to control the learning parameters where the objective is to reduce the possibility of overshooting during the learning process, increase the convergence speed and minimize the error. To find the best architecture of the deep network, we used a separate validation set to evaluate different architectures generated randomly based on the Mean Squared Error (MSE). Simulation results show that the proposed incident detection method has many advantages such as higher detection rate and lower false alarm rate. © 2017",Cognitive Systems Research,10.1016/j.cogsys.2017.12.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038826829&doi=10.1016%2fj.cogsys.2017.12.002&partnerID=40&md5=a21ff71b4c8c6848764a73f5b17cdbe3,2018,2021-07-20 15:48:30,2021-07-20 15:48:30
ABBRPB2W,journalArticle,2015,"Khasanvis, S.; Li, M.; Rahman, M.; Salehi-Fashami, M.; Biswas, A.K.; Atulasimha, J.; Bandyopadhyay, S.; Moritz, C.A.",Self-Similar Magneto-Electric Nanocircuit Technology for Probabilistic Inference Engines,"Probabilistic graphical models are powerful mathematical formalisms for machine learning and reasoning under uncertainty that are widely used for cognitive computing. However, they cannot be employed efficiently for large problems (with variables in the order of 100K or larger) on conventional systems, due to inefficiencies resulting from layers of abstraction and separation of logic and memory in CMOS implementations. In this paper, we present a magnetoelectric probabilistic technology framework for implementing probabilistic reasoning functions. The technology leverages straintronic magneto-tunneling junction (S-MTJ) devices in a novel mixed-signal circuit framework for direct computations on probabilities while enabling in-memory computations with persistence. Initial evaluations of the Bayesian likelihood estimation operation occurring during Bayesian Network inference indicate up to 127x lower area, 214x lower active power, and 70x lower latency compared to an equivalent 45-nm CMOS Boolean implementation. © 2002-2012 IEEE.",IEEE Transactions on Nanotechnology,10.1109/TNANO.2015.2439618,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947764734&doi=10.1109%2fTNANO.2015.2439618&partnerID=40&md5=2dd702088a12efe1bde1e15332697b13,2015,2021-07-20 15:48:30,2021-07-20 15:48:30
ZRVVSYJS,journalArticle,2013,"Meyer, R.; O'Keefe, S.",A fuzzy binary neural network for interpretable classifications,"Classification is probably the most frequently encountered problem in machine learning (ML). The most successful ML techniques like multi-layer perceptrons or support vector machines constitute very complex systems and the underlying reasoning processes of a classification decision are most often incomprehensible. We propose a classification system based on a hybridization of binary correlation matrix memories and fuzzy logic that yields interpretable solutions to classification tasks. A binary correlation matrix memory is a simple single-layered network consisting of a matrix with binary weights with easy to understand dynamics. Fuzzy logic has proven to be a suitable framework for reasoning under uncertainty and modelling human language concepts. The usage of binary correlation matrix memories and of fuzzy logic facilitates interpretability. Two fuzzy recall algorithms carry out the classification. The first one resembles fuzzy inference, uses fuzzy operators, and can directly be translated into a fuzzy ruleset in human language. The second recall algorithm is based on a well known classification technique, that is fuzzy K-nearest neighbour classification. The proposed classifier is benchmarked on six different data sets and compared to other systems, that is, a multi-layer perceptron, a support vector machine, an adaptive neuro-fuzzy inference system, and fuzzy and standard K-nearest neighbour classification. Besides its advantage of being interpretable, the proposed system shows strong performance on most of the data sets. © 2013 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2013.05.030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884157689&doi=10.1016%2fj.neucom.2013.05.030&partnerID=40&md5=6b5b9d8ecf3a1abba7a60d0e90cc3d78,2013,2021-07-20 15:48:30,2021-07-20 15:48:30
P53Y4CWV,journalArticle,2021,"Dash, T.; Srinivasan, A.; Vig, L.",Incorporating symbolic domain knowledge into graph neural networks,"Our interest is in scientific problems with the following characteristics: (1) Data are naturally represented as graphs; (2) The amount of data available is typically small; and (3) There is significant domain-knowledge, usually expressed in some symbolic form (rules, taxonomies, constraints and the like). These kinds of problems have been addressed effectively in the past by symbolic machine learning methods like Inductive Logic Programming (ILP), by virtue of 2 important characteristics: (a) The use of a representation language that easily captures the relation encoded in graph-structured data, and (b) The inclusion of prior information encoded as domain-specific relations, that can alleviate problems of data scarcity, and construct new relations. Recent advances have seen the emergence of deep neural networks specifically developed for graph-structured data (Graph-based Neural Networks, or GNNs). While GNNs have been shown to be able to handle graph-structured data, less has been done to investigate the inclusion of domain-knowledge. Here we investigate this aspect of GNNs empirically by employing an operation we term vertex-enrichment and denote the corresponding GNNs as VEGNNs. Using over 70 real-world datasets and substantial amounts of symbolic domain-knowledge, we examine the result of vertex-enrichment across 5 different variants of GNNs. Our results provide support for the following: (a) Inclusion of domain-knowledge by vertex-enrichment can significantly improve the performance of a GNN. That is, the performance of VEGNNs is significantly better than GNNs across all GNN variants; (b) The inclusion of domain-specific relations constructed using ILP improves the performance of VEGNNs, across all GNN variants. Taken together, the results provide evidence that it is possible to incorporate symbolic domain knowledge into a GNN, and that ILP can play an important role in providing high-level relationships that are not easily discovered by a GNN. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",Machine Learning,10.1007/s10994-021-05966-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107808586&doi=10.1007%2fs10994-021-05966-z&partnerID=40&md5=40a0ff763e0663db0dd1e1fcb59206ae,2021,2021-07-20 15:48:30,2021-07-20 15:48:30
3TAY8JB3,journalArticle,2014,"Perlovsky, L.; Shevchenko, O.",Dynamic logic machine learning for cybersecurity,,Advances in Information Security,10.1007/978-3-319-10374-7_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927588779&doi=10.1007%2f978-3-319-10374-7_6&partnerID=40&md5=f9ee24a4b7e9056a10a410bbe690730e,2014,2021-07-20 15:48:30,2021-07-20 15:48:30
MBTKMGYY,journalArticle,2020,"Nam, C.; Lee, S.; Lee, J.; Cheong, S.H.; Kim, D.H.; Kim, C.; Kim, I.; Park, S.-K.",A Software Architecture for Service Robots Manipulating Objects in Human Environments,"This paper presents a software architecture for robots providing manipulation services autonomously in human environments. In an unstructured human environment, a service robot often needs to perform tasks even without human intervention and prior knowledge about tasks and environments. For autonomous execution of tasks, varied processes are necessary such as perceiving environments, representing knowledge, reasoning with the knowledge, and planning for task and motion. While developing each of the processes is important, integrating them into a working system for deployment is also important as a robotic system can bring tangible outcomes when it works in real world. However, such an architecture has been rarely realized in the literature owing to the difficulties of a full integration, deployment, understanding high-level goals without human interventions. In this work, we suggest a software architecture that integrates the components necessary to perform tasks by a real robot without human intervention. We show our architecture composed of deep learning based perception, symbolic reasoning, AI task planning, and geometric motion planning. We implement a deep neural network that produces information about the environment, which are then stored in a knowledge base. We implement a reasoner that processes the knowledge to use the result for task planning. We show our implementation of the symbolic task planner that generates a sequence of motion predicates. We implement an interface that computes geometric information necessary for motion planning to execute the symbolic task plans. We describe the deployment of the architecture through the result of lab tests and a public demonstration. The architecture is developed based on Robot Operating System (ROS) so compatible with any robot that is capable of object manipulation and mobile navigation running in ROS. We deploy the architecture to two different robot platforms to show the compatibility. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3003991,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090248501&doi=10.1109%2fACCESS.2020.3003991&partnerID=40&md5=3823ec5cc0b6add243d7233f7c27a94f,2020,2021-07-20 15:48:30,2021-07-20 15:48:30
QTZ3D9XY,journalArticle,2021,"Elavarasan, D.; Durai Raj Vincent, P.M.",Fuzzy deep learning-based crop yield prediction model for sustainable agronomical frameworks,"The evolution in science and innovation has to lead to an immense volume of information from various agricultural fields to be accumulated in the public domain. As a result, an objective arises from the investigation of the accessible information and incorporating them with processes like foreseeing crop yield, plant diseases examination, crops enhancement, etc. Machine learning has grown with tremendous processing methods to conceive new innovations in the multi-disciplinary agricultural sector. In experimenting with machine learning models, there exist certain limitations like improvident nonlinear mapping between the raw data and crop yield values. Hence, deep learning models are comprehensively used to extricate critical crop parameters for prediction. Foreseeing the crop yield depending on climate, soil and water parameters has been a potential research subject. This paper proposes a hybrid deep learning-based crop yield prediction system using deep belief network (DBN) and fuzzy neural networks system (FNN). DBN is a combination of statistics and probability with neural networks. Though DBN performs better for nonlinear systems, the algorithm alone cannot provide satisfactory results in terms of robustness, model accuracy and learning speed, which is predominantly due to gradient diffusion. Hence, a DBN along with FNN has been proposed to overcome the nonlinearity and gradient diffusion problems. The proposed model initially performs an efficient pre-training technique by DBN for enhanced model development and feature vector generation. This characteristic feature vector is fed as an input to the FNN for further processing. The superiority of the proposed fuzzy neural network-based deep belief network is analyzed by comparing it with other deep learning algorithms. The proposed model efficiently predicts the results outperforming the other models by preserving the original data distribution with an accuracy of 92%. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-021-05950-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104257384&doi=10.1007%2fs00521-021-05950-7&partnerID=40&md5=313cae8723305d96e413a4956253ce49,2021,2021-07-20 15:48:30,2021-07-20 15:48:30
Q9QRT7Q7,journalArticle,2021,"Kondaka, L.S.; Thenmozhi, M.; Vijayakumar, K.; Kohli, R.",An intensive healthcare monitoring paradigm by using IoT based machine learning strategies,"Internet of Things (IoT) in association with cloud technologies are the raising stars in information technology industry, which provides a lot of innovative gadgets to several industries to automate their needs as well as monitoring the events properly without any human interventions. The most common and emerging needs now-a-days is the development of new gadgets to support healthcare industry to rectify the medical flaws and save the human life. In one side the booming technologies such as Internet of Things and Cloud platforms are available and the other hand a drastic need to introduce an intelligent gadget for medical oriented needs to save one’s life in critical situations. This paper aims to create a bridge between the two and introduce a new device to combine healthcare with recent technological developments. The proposed approach introduces a new algorithm called iCloud Assisted Intensive Deep Learning (iCAIDL), which provides support to healthcare medium as well as patients by means of applying the intelligent cloud system along with machine learning strategies and this proposed algorithm is derived from the base of deep learning norms. An iCloud Assisted Intensive Deep Learning algorithm initially begins with the flow of collecting the existing health records from the data repository and train the system with deep learning principles. Once the data training phase ends the proposed algorithm begins to get the live data from patient and this data is assumed as a testing data, which will be processed by using intensive deep learning principles and store the resulting summary into the Cloud repository by means of enabling Internet of Things feature in association with the proposed algorithm called iCAIDL. This is transparent in both the state of users like doctors as well as the patients to monitor the health records in intelligent manner. A Smart Medical Gadget is designed to collect the health record from patients and maintain it into the medical repository for testing phase, in which it collects the patient heart rate, pressure level, blood flow in intensive manner. The logic of IoT is connected with the machine learning process such as: the data accumulated from the smart Medical Gadget needs to be send to the Server end for processing, here the processing is mentioned as the machine learning based processing, in which the received data is considered to be the testing data and the results are emulated accordingly. Once the results are emulated that also will be coming for training part for the upcoming testing data. So, that the data coming from the Medical Gadget is considered to be the testing data and once the processing is done it will be considered to be the training data for further medical summaries. The performance evaluations of the proposed approach is estimated based on the following metrics such as data transfer ratio from Smart medical Gadget to the server end, storage accuracy and the communication efficiency. Empirical results are attained using simulation, in which it produces a drastical improvement of healthcare parameters by merge the proposed algorithm called iCloud Assisted Intensive Deep Learning. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-021-11111-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108139345&doi=10.1007%2fs11042-021-11111-8&partnerID=40&md5=50922eee41a64113dbd8a1f2c1ce5570,2021,2021-07-20 15:48:30,2021-07-20 15:48:30
BC9VZ9N8,journalArticle,2013,"Rincón, J.M.D.; Santofimia, M.J.; Nebel, J.-C.",Common-sense reasoning for human action recognition,"This paper presents a novel method that leverages reasoning capabilities in a computer vision system dedicated to human action recognition. The proposed methodology is decomposed into two stages. First, a machine learning based algorithm - known as bag of words - gives a first estimate of action classification from video sequences, by performing an image feature analysis. Those results are afterward passed to a common-sense reasoning system, which analyses, selects and corrects the initial estimation yielded by the machine learning algorithm. This second stage resorts to the knowledge implicit in the rationality that motivates human behaviour. Experiments are performed in realistic conditions, where poor recognition rates by the machine learning techniques are significantly improved by the second stage in which common-sense knowledge and reasoning capabilities have been leveraged. This demonstrates the value of integrating common-sense capabilities into a computer vision pipeline. © 2012 Elsevier B.V. All rights reserved.",Pattern Recognition Letters,10.1016/j.patrec.2012.10.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885075445&doi=10.1016%2fj.patrec.2012.10.020&partnerID=40&md5=124e285a0f3bc2426101e63f1f5d3371,2013,2021-07-20 15:48:30,2021-07-20 15:48:30
SR6NMLH8,journalArticle,2020,"Bettini, C.; Civitarese, G.; Giancane, D.; Presotto, R.",ProCAVIAR: Hybrid Data-Driven and Probabilistic Knowledge-Based Activity Recognition,"The recognition of physical activities using sensors on mobile devices has been mainly addressed with supervised and semi-supervised learning. The state-of-the-art methods are mainly based on the analysis of the user's movement patterns that emerge from inertial sensors data. While the literature on this topic is quite mature, existing approaches are still not adequate to discriminate activities characterized by similar physical movements. The context that surrounds the user (e.g., semantic location) could be used as additional information to significantly extend the set of recognizable activities. Since collecting a comprehensive training set with activities performed in every possible context condition is too costly, if possible at all, existing works proposed knowledge-based reasoning over ontological representation of context data to refine the predictions obtained from machine learning. A problem with this approach is the rigidity of the underlying logic formalism that cannot capture the intrinsic uncertainty of the relationships between activities and context. In this work, we propose a novel activity recognition method that combines semi-supervised learning and probabilistic ontological reasoning. We model the relationships between activities and context as a combination of soft and hard ontological axioms. For each activity, we use a probabilistic ontology to compute its compatibility with the current context conditions. The output of probabilistic semantic reasoning is combined with the output of a machine learning classifier based on inertial sensor data to obtain the most likely activity performed by the user. The evaluation of our system on a dataset with 13 types of activities performed by 26 subjects shows that our probabilistic framework outperforms both a pure machine learning approach and previous hybrid approaches based on classic ontological reasoning. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3015091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090277228&doi=10.1109%2fACCESS.2020.3015091&partnerID=40&md5=de7117e5124ae459e0dffb55670300b2,2020,2021-07-20 15:48:31,2021-07-20 15:48:31
BHTPX39Q,journalArticle,2020,"Jain, R.; Jain, N.; Gupta, Y.; Chugh, T.; Chugh, T.; Hemanth, D.J.",A Modified Fuzzy Logic Relation-Based Approach for Electricity Consumption Forecasting in India,"Prediction of electricity demand is made using Load forecasting technique to meet the ever-growing demand. In this paper, future electricity demand forecasted for the whole state of Uttar Pradesh (India), using the dataset collected from the Central Electricity Authority. This dataset consists of electricity demand for the whole state of UP for every 15-min block. Different models were used to forecast future demand. XGradientBoost (XGBoost), a machine learning algorithm was used to forecast demand first. Further forecasting was performed using deep learning models such as Long Short-Term Memory (LSTM) using neural networks as they are considered to be more efficient and accurate than XGBoost. Fuzzy time series (FTS) models were considered to incorporate trend and seasonality present in our dataset. From various FTS models, the best mean absolute percentage error achieved (MAPE) was 2.34%. A new method KmFuzz is proposed in this paper that uses modified K-Means clustering for finding an optimal number of partitions on which fuzzy logic is applied. Fuzzy sets are obtained by applying fuzzification on the dataset and the total number of sets generated are equal to the number of optimal partitions. Then the weighted average method is used for defuzzification and forecasting the next hour demand using the demand data of previous hours with MAPE of 1.94%, thus improving the accuracy further. © 2019, Taiwan Fuzzy Systems Association.",International Journal of Fuzzy Systems,10.1007/s40815-019-00704-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069633133&doi=10.1007%2fs40815-019-00704-z&partnerID=40&md5=1ac367be0a2d23805a841c0000be7988,2020,2021-07-20 15:48:31,2021-07-20 15:48:31
PU8QQXLC,journalArticle,2021,"Chang, X.; Wu, J.; Sun, H.; Wang, G.; Feng, Z.; Bao, X.",Understanding and predicting short-term passenger flow of station-free shared bike: A spatiotemporal deep learning approach,,IEEE Intelligent Transportation Systems Magazine,10.1109/MITS.2021.3049362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101427189&doi=10.1109%2fMITS.2021.3049362&partnerID=40&md5=09cb67bf60185964055511c5166a48ab,2021,2021-07-20 15:48:31,2021-07-20 15:48:31
47SLCMFA,journalArticle,2020,"Qian, P.; Liu, Z.; He, Q.; Zimmermann, R.; Wang, X.",Towards Automated Reentrancy Detection for Smart Contracts Based on Sequential Models,"In the last decade, smart contract security issues lead to tremendous losses, which has attracted increasing public attention both in industry and in academia. Researchers have embarked on efforts with logic rules, symbolic analysis, and formal analysis to achieve encouraging results in smart contract vulnerability detection tasks. However, the existing detection tools are far from satisfactory. In this paper, we attempt to utilize the deep learning-based approach, namely bidirectional long-short term memory with attention mechanism (BLSTM-ATT), aiming to precisely detect reentrancy bugs. Furthermore, we propose contract snippet representations for smart contracts, which contributes to capturing essential semantic information and control flow dependencies. Our extensive experimental studies on over 42,000 real-world smart contracts show that our proposed model and contract snippet representations significantly outperform state-of-the-art methods. In addition, this work proves that it is practical to apply deep learning-based technology on smart contract vulnerability detection, which is able to promote future research towards this area. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2969429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079788399&doi=10.1109%2fACCESS.2020.2969429&partnerID=40&md5=45532dfcbcd73d67396d001ed1729e63,2020,2021-07-20 15:48:31,2021-07-20 15:48:31
CL6MK6AE,journalArticle,2011,"Haikonen, P.O.A.",XCR-1: An Experimental Cognitive Robot Based on an Associative Neural Architecture,"The experimental cognitive robot XCR-1 is a small three-wheel robot with gripper hands, multiple sensory modalities, and self-talk. The robot XCR-1 is designed for studies and experiments with a new paradigm for cognitive computation, namely an associative neural processing style that inherently and seamlessly combines sub-symbolic and symbolic computation. This operation is realized by using associative neurons and neuron groups organized according to the Haikonen Cognitive Architecture. Recently, there have been many efforts toward machine consciousness, and the Haikonen Cognitive Architecture is one attempt in that direction. Human consciousness is characterized by subjective inner experience that is related to qualia. Accordingly, it can be proposed that true conscious machines should also have some kind of inner experience and qualia. On the other hand, it has been argued that qualia are direct and cannot be artificially realized in symbolic systems. In order to facilitate qualia-related practical investigations, the robot XCR-1 utilizes direct perception processes, with dedicated hardware and without symbolic pre-programmed algorithms. The robot XCR-1 does not utilize microprocessors or programs of any kind. Natural language is one manifestation of symbolic processing. The robot XCR-1 is designed also for experiments with simple speech and the basic grounding of the meaning of words. The experiments with the robot XCR-1 could be greatly enhanced if dedicated associative neuron group chips were available. © 2011 Springer Science+Business Media, LLC.",Cognitive Computation,10.1007/s12559-011-9100-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957635678&doi=10.1007%2fs12559-011-9100-9&partnerID=40&md5=80f96fd81f4eceb2df78b55e64c931eb,2011,2021-07-20 15:48:31,2021-07-20 15:48:31
LKV55ABP,journalArticle,2011,"Biba, M.; Xhafa, F.; Esposito, F.; Ferilli, S.",Stochastic simulation and modelling of metabolic networks in a machine learning framework,"Metabolomics is increasingly becoming an important field. The fundamental task in this area is to measure and interpret complex time and condition dependent parameters such as the activity or flux of metabolites in cells, their concentration, tissues elements and other biosamples. The careful study of all these elements has led to important insights in the functioning of metabolism. Recently, however, there is a growing interest towards an integrated approach to studying biological systems. This is the main goal in Systems Biology where a combined investigation of several components of a biological system is thought to produce a thorough understanding of such systems. Biological circuits are complex to model and simulate and many efforts are being made to develop models that can handle their intrinsic complexity. A significant part of biological networks still remains unknown even though recent technological developments allow simultaneous acquisition of many metabolite measurements. Metabolic networks are not only structurally complex but behave also in a stochastic fashion. Therefore, it is necessary to express structure and handle uncertainty to construct complete dynamics of these networks. In this paper we describe how stochastic modeling and simulation can be performed in a symbolic-statistical machine learning (ML) framework. We show that symbolic ML deal with structural and relational complexity while statistical ML provides principled approaches to uncertainty modeling. Learning is used to analyze traces of biochemical reactions and model the dynamicity through parameter learning, while inference is used to produce stochastic simulation of the network. © 2011 Elsevier B.V. All rights reserved.",Simulation Modelling Practice and Theory,10.1016/j.simpat.2011.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958075443&doi=10.1016%2fj.simpat.2011.05.002&partnerID=40&md5=2c1029676efcd60bf9f169aa4a246c5e,2011,2021-07-20 15:48:31,2021-07-20 15:48:31
ET57NWY3,journalArticle,2021,"Köktürk-Güzel, B.E.; Beyhan, S.",Symbolic Regression Based Extreme Learning Machine Models for System Identification,"Reproducible machine learning models with less number of parameters and fast optimization are preferred in embedded system design for the applications of artificial intelligence. Due to implementation advantages, symbolic regression with genetic programming has been used for modeling data. In addition, extreme learning machines have been designed with acceptable performances in virtue of random learning strategy. In this paper, symbolic regression featured extreme learning machine models are proposed for the system identification. The symbolic regression layer with mathematical operators and basis functions has been randomly constructed instead of genetic programming whereas the output weighting parameters are optimized via least-squares optimization as in extreme learning machines. Consequently; implementable, efficient and easy designed models are constructed for future applications. Comparative results of the proposed and literature models present that proposed models provided smaller mean-squared errors and minimum-descriptive length performances. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Neural Processing Letters,10.1007/s11063-021-10465-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102939756&doi=10.1007%2fs11063-021-10465-2&partnerID=40&md5=6944b40db09dc0adad036b6674e118c4,2021,2021-07-20 15:48:31,2021-07-20 15:48:31
Q5DASCAC,journalArticle,2019,"Nguyen, T.-L.; Kavuri, S.; Lee, M.",A multimodal convolutional neuro-fuzzy network for emotion understanding of movie clips,"Multimodal emotion understanding enables AI systems to interpret human emotions. With accelerated video surge, emotion understanding remains challenging due to inherent data ambiguity and diversity of video content. Although deep learning has made a considerable progress in big data feature learning, they are viewed as deterministic models used in a “black-box” manner which does not have capabilities to represent inherent ambiguities with data. Since the possibility theory of fuzzy logic focuses on knowledge representation and reasoning under uncertainty, we intend to incorporate the concepts of fuzzy logic into deep learning framework. This paper presents a novel convolutional neuro-fuzzy network, which is an integration of convolutional neural networks in fuzzy logic domain to extract high-level emotion features from text, audio, and visual modalities. The feature sets extracted by fuzzy convolutional layers are compared with those of convolutional neural networks at the same level using t-distributed Stochastic Neighbor Embedding. This paper demonstrates a multimodal emotion understanding framework with an adaptive neural fuzzy inference system that can generate new rules to classify emotions. For emotion understanding of movie clips, we concatenate audio, visual, and text features extracted using the proposed convolutional neuro-fuzzy network to train adaptive neural fuzzy inference system. In this paper, we go one step further to explain how deep learning arrives at a conclusion that can guide us to an interpretable AI. To identify which visual/text/audio aspects are important for emotion understanding, we use direct linear non-Gaussian additive model to explain the relevance in terms of causal relationships between features of deep hidden layers. The critical features extracted are input to the proposed multimodal framework to achieve higher accuracy. © 2019 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2019.06.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068521737&doi=10.1016%2fj.neunet.2019.06.010&partnerID=40&md5=6877fe50695bc699097c38c79499fd3e,2019,2021-07-20 15:48:31,2021-07-20 15:48:31
8INQPZF7,journalArticle,2019,"Liu, J.; Zhang, X.; Li, Y.; Wang, J.; Kim, H.-J.",Deep learning-based reasoning with multi-ontology for IoT applications,"In the era of mobile big data, data driven intelligent Internet of Things (IoT) applications are becoming widespread, and knowledge-based reasoning is one of the essential tasks of these applications. While most knowledge-based reasoning work is conducted with knowledge graph, ontology-based reasoning method can inherently achieve higher level intelligence by leveraging both explicit and tacit knowledge in specific domains, and its performance is determined by precise refinement of the inference rules. However, most ontology-based reasoning work concentrates on semantic reasoning in a single ontology, and fail to utilize association of multiple ontologies in various domains to extend reasoning capacity. This is even the case for the IoT applications where knowledge from multiple domains needs to be utilized. To overcome this issue, we propose a deep learning-based method to associate multiple ontology rule bases, thereby discover new inference rules. In our method, we first use a regression tree model to determine the threshold value for parameters in inference rules that constitute the ontology rule base, avoiding the influence of uncertainty factors on knowledge reasoning results. Then, a two-way GRU (Gated Recurrent Unit) neural network with attention mechanism is used to discover semantic relations among the rule bases of ontologies. Therefore, the association of multiple ontology rule bases is realized, and the rule base of knowledge reasoning is expanded by acquiring some unspecified rules. To the best our knowledge, this work is the first one to leverage deep learning in reasoning with multiple ontologies. In order to verify the effectiveness of our method, we apply it in a real traffic safety monitoring application by relating rule bases of a vehicle ontology and a traffic management ontology, and achieve effective knowledge reasoning. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2937353,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077961533&doi=10.1109%2fACCESS.2019.2937353&partnerID=40&md5=d7c3e6b475f82b548ca443a53ccda160,2019,2021-07-20 15:48:31,2021-07-20 15:48:31
5ZPANC8A,journalArticle,2020,"Xie, S.R.; Kotlarz, P.; Hennig, R.G.; Nino, J.C.",Machine learning of octahedral tilting in oxide perovskites by symbolic classification with compressed sensing,"The steady growth of online materials databases, coupled with efforts in materials informatics, has invited the reexamination of existing empirical models through the lens of modern machine learning techniques. Inspired by recent efforts to improve on the Goldschmidt tolerance factor for perovskite formation, we apply the symbolic regression to the problem of predicting octahedral tilting. In addition to its impact on the crystal structure, octahedral tilting is related to functional properties, including dielectric permittivity, ferroelectricity, magnetic properties, and metal–insulator transitions. By relating a selection of physical parameters (e.g., atomic radii, electronegativity) with mathematical operations (e.g., addition, exponentiation), we identify an analytical equation that correctly predicts the octahedral tilting classification for 49 perovskite oxides in a dataset of 60 materials. Using the same training dataset, we additionally fit and compare seven models generated by other common machine learning methods. Despite the increased complexity afforded by support vector machines, decision trees/random forests, and artificial neural networks, we find that our equation outperforms the other models as well as the original tolerance factor in predicting octahedral tilting. © 2020 Elsevier B.V.",Computational Materials Science,10.1016/j.commatsci.2020.109690,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082793843&doi=10.1016%2fj.commatsci.2020.109690&partnerID=40&md5=cf2c261d2ee22334c3f1f629978e9829,2020,2021-07-20 15:48:31,2021-07-20 15:48:31
UFXC578Y,journalArticle,2020,"Eldafrawy, M.; Boutros, A.; Yazdanshenas, S.; Betz, V.",FPGA Logic Block Architectures for Efficient Deep Learning Inference,"Reducing the precision of deep neural network (DNN) inference accelerators can yield large efficiency gains with little or no accuracy degradation compared to half or single precision floating-point by enabling more multiplication operations per unit area. A wide range of precisions fall on the pareto-optimal curve of hardware efficiency vs. accuracy with no single precision dominating, making the variable precision capabilities of FPGAs very valuable. We propose three types of logic block architectural enhancements and fully evaluate a total of six architectures that improve the area efficiency of multiplications and additions implemented in the soft fabric. Increasing the LUT fracturability and adding two adders to the ALM (4-bit Adder Double Chain architecture) leads to a 1.5× area reduction for arithmetic heavy machine learning (ML) kernels, while increasing their speed. In addition, this architecture also reduces the logic area of general applications by 6%, while increasing the critical path delay by only 1%. However, our highest impact option, which adds a 9-bit shadow multiplier to the logic clusters, reduces the area and critical path delay of ML kernels by 2.4× and 1.2×, respectively. These large gains come at a cost of 15% logic area increase for general applications. © 2020 ACM.",ACM Transactions on Reconfigurable Technology and Systems,10.1145/3393668,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091027276&doi=10.1145%2f3393668&partnerID=40&md5=86414b60f10a5708e97250aa20f0f007,2020,2021-07-20 15:48:31,2021-07-20 15:48:31
7SGVPUQ2,journalArticle,2014,"Hu, H.; Pang, L.; Tian, D.; Shi, Z.",Perception granular computing in visual haze-free task,"In the past decade, granular computing (GrC) has been an active topic of research in machine learning and computer vision. However, the granularity division is itself an open and complex problem. Deep learning, at the same time, has been proposed by Geoffrey Hinton, which simulates the hierarchical structure of human brain, processes data from lower level to higher level and gradually composes more and more semantic concepts. The information similarity, proximity and functionality constitute the key points in the original insight of granular computing proposed by Zadeh. Many GrC researches are based on the equivalence relation or the more general tolerance relation, either of which can be described by some distance functions. The information similarity and proximity depended on the samples distribution can be easily described by the fuzzy logic. From this point of view, GrC can be considered as a set of fuzzy logical formulas, which is geometrically defined as a layered framework in a multi-scale granular system. The necessity of such kind multi-scale layered granular system can be supported by the columnar organization of the neocortex. So the granular system proposed in this paper can be viewed as a new explanation of deep learning that simulates the hierarchical structure of human brain. In view of this, a novel learning approach, which combines fuzzy logical designing with machine learning, is proposed in this paper to construct a GrC system to explore a novel direction for deep learning. Unlike those previous works on the theoretical framework of GrC, our granular system is abstracted from brain science and information science, so it can be used to guide the research of image processing and pattern recognition. Finally, we take the task of haze-free as an example to demonstrate that our multi-scale GrC has high ability to increase the texture information entropy and improve the effect of haze-removing. © 2013 Published by Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2013.11.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890486569&doi=10.1016%2fj.eswa.2013.11.006&partnerID=40&md5=e3854fae3c4745ee147d0e0b37d3c80d,2014,2021-07-20 15:48:31,2021-07-20 15:48:31
JIAW6SJJ,journalArticle,2013,"Huang, M.-J.; Chiang, H.-K.; Wu, P.-F.; Hsieh, Y.-J.",A multi-strategy machine learning student modeling for intelligent tutoring systems: Based on blackboard approach,"Purpose: This study aims to propose a blackboard approach using multistrategy machine learning student modeling techniques to learn the properties of students' inconsistent behaviors during their learning process. Design/methodology/approach: These multistrategy machine learning student modeling techniques include inductive reasoning (similarity-based learning), deductive reasoning (explanation-based learning), and analogical reasoning (case-based reasoning). Findings: According to the properties of students' inconsistent behaviors, the ITS (intelligent tutoring system) may then adopt appropriate methods, such as intensifying teaching and practicing, to prevent their inconsistent behaviors from reoccurring. Originality/value: This research sets the learning object on a single student. After the inferences are accumulated from a group of students, what kinds of students tend to have inconsistent behaviors or under what conditions the behaviors happened for most students can be learned. © Emerald Group Publishing Limited.",Library Hi Tech,10.1108/07378831311329059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879948442&doi=10.1108%2f07378831311329059&partnerID=40&md5=1c5bb4998863afe396bb9cd68968ba88,2013,2021-07-20 15:48:31,2021-07-20 15:48:31
H45KKPLF,journalArticle,2016,"Cvetković, B.; Janko, V.; Romero, A.E.; Kafalı, Ö.; Stathis, K.; Luštrek, M.",Activity Recognition for Diabetic Patients Using a Smartphone,"Diabetes is a disease that has to be managed through appropriate lifestyle. Technology can help with this, particularly when it is designed so that it does not impose an additional burden on the patient. This paper presents an approach that combines machine-learning and symbolic reasoning to recognise high-level lifestyle activities using sensor data obtained primarily from the patient’s smartphone. We compare five methods for machine-learning which differ in the amount of manually labelled data by the user, to investigate the trade-off between the labelling effort and recognition accuracy. In an evaluation on real-life data, the highest accuracy of 83.4 % was achieved by the MCAT method, which is capable of gradually adapting to each user. © 2016, Springer Science+Business Media New York.",Journal of Medical Systems,10.1007/s10916-016-0598-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84990842257&doi=10.1007%2fs10916-016-0598-y&partnerID=40&md5=4f5ffc855828fe41f310311ae3dc1585,2016,2021-07-20 15:48:31,2021-07-20 15:48:31
96RM683E,journalArticle,2020,"Wang, L.; Zhang, L.; Jiang, J.",Duplicate Question Detection with Deep Learning in Stack Overflow,"Stack Overflow is a popular Community-based Question Answer (CQA) website focused on software programming and has attracted more and more users in recent years. However, duplicate questions frequently appear in Stack Overflow and they are manually marked by the users with high reputation. Automatic duplicate question detection alleviates labor and effort for users with high reputation. Although existing approaches extract textual features to automatically detect duplicate questions, these approaches are limited since semantic information could be lost. To tackle this problem, we explore the use of powerful deep learning techniques, including Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN) and Long Short-Term Memory (LSTM), to detect duplicate questions in Stack Overflow. In addition, we use Word2Vec to obtain the vector representations of words. They can fully capture semantic information at document-level and word-level respectively. Therefore, we construct three deep learning approaches WV-CNN, WV-RNN and WV-LSTM, which are based on Word2Vec, CNN, RNN and LSTM, to detect duplicate questions in Stack Overflow. Evaluation results show that WV-CNN and WV-LSTM have made significant improvements over four baseline approaches (i.e., DupPredictor, Dupe, DupPredictorRep-T, and DupeRep) and three deep learning approaches (i.e., DQ-CNN, DQ-RNN, and DQ-LSTM) in terms of recall-rate@5, recall-rate@10 and recall-rate@20. Furthermore, the experimental results indicate that our approaches WV-CNN, WV-RNN, and WV-LSTM outperform four machine learning approaches based on Support Vector Machine, Logic Regression, Random Forest and eXtreme Gradient Boosting in terms of recall-rate@5, recall-rate@10 and recall-rate@20. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2968391,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079666852&doi=10.1109%2fACCESS.2020.2968391&partnerID=40&md5=cc09d30f64d158dbcedb3b42e4f5eb3b,2020,2021-07-20 15:48:31,2021-07-20 15:48:31
M3WPKU55,journalArticle,2021,"Kawamoto, Y.",An epistemic approach to the formal specification of statistical machine learning,"We propose an epistemic approach to formalizing statistical properties of machine learning. Specifically, we introduce a formal model for supervised learning based on a Kripke model where each possible world corresponds to a possible dataset and modal operators are interpreted as transformation and testing on datasets. Then, we formalize various notions of the classification performance, robustness, and fairness of statistical classifiers by using our extension of statistical epistemic logic. In this formalization, we show relationships among properties of classifiers, and relevance between classification performance and robustness. As far as we know, this is the first work that uses epistemic models and logical formulas to express statistical properties of machine learning, and would be a starting point to develop theories of formal specification of machine learning. © 2020, The Author(s).",Software and Systems Modeling,10.1007/s10270-020-00825-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091169688&doi=10.1007%2fs10270-020-00825-2&partnerID=40&md5=fa16e2583e0096f1d86113915526b1b2,2021,2021-07-20 15:48:32,2021-07-20 15:48:32
6G8P9YYB,journalArticle,2011,"Hüllermeier, E.",Fuzzy machine learning and data mining,"The development of methods for machine learning and data mining has attracted increasing attention in the fuzzy set community in recent years. The aim of this paper is to assess the relevance of fuzzy set theory and fuzzy logic for these fields, highlighting potential contributions without concealing alleged limitations and shortcomings of current research. To this end, some typical applications of fuzzy logic will be reviewed, followed by a more systematic discussion of possible benefits of fuzzy methods. © 2011 John Wiley & Sons, Inc.",Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,10.1002/widm.34,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873126615&doi=10.1002%2fwidm.34&partnerID=40&md5=e6ef97bd711694070a003e65ec342396,2011,2021-07-20 15:48:32,2021-07-20 15:48:32
Q2RU93A3,journalArticle,2018,"Despotovic, V.; Walter, O.; Haeb-Umbach, R.",Machine learning techniques for semantic analysis of dysarthric speech: An experimental study,"We present an experimental comparison of seven state-of-the-art machine learning algorithms for the task of semantic analysis of spoken input, with a special emphasis on applications for dysarthric speech. Dysarthria is a motor speech disorder, which is characterized by poor articulation of phonemes. In order to cater for these non-canonical phoneme realizations, we employed an unsupervised learning approach to estimate the acoustic models for speech recognition, which does not require a literal transcription of the training data. Even for the subsequent task of semantic analysis, only weak supervision is employed, whereby the training utterance is accompanied by a semantic label only, rather than a literal transcription. Results on two databases, one of them containing dysarthric speech, are presented showing that Markov logic networks and conditional random fields substantially outperform other machine learning approaches. Markov logic networks have proved to be especially robust to recognition errors, which are caused by imprecise articulation in dysarthric speech. © 2018 Elsevier B.V.",Speech Communication,10.1016/j.specom.2018.04.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046031303&doi=10.1016%2fj.specom.2018.04.005&partnerID=40&md5=c4fbe96f27950db6094bf01a11544d72,2018,2021-07-20 15:48:32,2021-07-20 15:48:32
YBUAUWEY,journalArticle,2019,"Muggleton, S.H.; Hocquette, C.",Machine Discovery of Comprehensible Strategies for Simple Games Using Meta-interpretive Learning,"Recently, world-class human players have been outperformed in a number of complex two-person games (Go, Chess, Checkers) by Deep Reinforcement Learning systems. However, the data efficiency of the learning systems is unclear given that they appear to require far more training games to achieve such performance than any human player might experience in a lifetime. In addition, the resulting learned strategies are not in a form which can be communicated to human players. This contrasts to earlier research in Behavioural Cloning in which single-agent skills were machine learned in a symbolic language, facilitating their being taught to human beings. In this paper, we consider Machine Discovery of human-comprehensible strategies for simple two-person games (Noughts-and-Crosses and Hexapawn). One advantage of considering simple games is that there is a tractable approach to calculating minimax regret. We use these games to compare Cumulative Minimax Regret for variants of both standard and deep reinforcement learning against two variants of a new Meta-interpretive Learning system called MIGO. In our experiments, tested variants of both normal and deep reinforcement learning have consistently worse performance (higher cumulative minimax regret) than both variants of MIGO on Noughts-and-Crosses and Hexapawn. In addition, MIGO’s learned rules are relatively easy to comprehend, and are demonstrated to achieve significant transfer learning in both directions between Noughts-and-Crosses and Hexapawn. © 2019, The Author(s).",New Generation Computing,10.1007/s00354-019-00054-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065030104&doi=10.1007%2fs00354-019-00054-2&partnerID=40&md5=d40c517598f829ac81d0bf29d641f7f0,2019,2021-07-20 15:48:32,2021-07-20 15:48:32
F42A3BYC,journalArticle,2019,"Chen, B.; Wan, J.; Lan, Y.; Imran, M.; Li, D.; Guizani, N.",Improving cognitive ability of edge intelligent IIoT through machine learning,"Computer-integrated manufacturing is a notable feature of Industry 4.0. Integrating machine learning (ML) into edge intelligent Industrial Internet of Things (IIoT) is a key enabling technology to achieve intelligent IIoT. To realize novel intelligent applications of edge-enhanced IIoT, ML methods are proposed to improve the cognitive ability of edge intelligent IIoT in this article. First, an ML-enabled framework of the cognitive IIoT is proposed. Second, the ML methods are presented to enhance the cognitive ability of IIoT including the ML model of IIoT, data-driven learning and reasoning, and coordination with cognitive methods. Finally, with a focus on the reconfigurable production line, a scenario-aware dynamic adaptive planning (DAP) with deep reinforcement learning (DRL) was conducted. The experimental results show that the DRL-based dynamic adaptive planning (DRL-based DAP) had good performance in an observable IIoT environment. The main purpose of this work is to point out the effects of ML-based optimization methods on the analysis of industrial IoT from the macroscopic view. © 1986-2012 IEEE.",IEEE Network,10.1109/MNET.001.1800505,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073325061&doi=10.1109%2fMNET.001.1800505&partnerID=40&md5=f0c612cb3e964404f16fa0229d6e08a7,2019,2021-07-20 15:48:32,2021-07-20 15:48:32
VXKL79DR,journalArticle,2020,"Corbat, L.; Nauval, M.; Henriet, J.; Lapayre, J.-C.",A fusion method based on Deep Learning and Case-Based Reasoning which improves the resulting medical image segmentations,"The fusion of multiple segmentations of different biological structures is inevitable in the case where each structure has been segmented individually for performance reasons. However, when aggregating these structures for a final segmentation, conflicting pixels may appear. These conflicts can be solved by artificial intelligence techniques. Our system, integrated into the SAIAD project, carries out the fusion of deformed kidneys and nephroblastoma segmentations using the combination of Deep Learning and Case-Based Reasoning. The performances of our method were evaluated on 9 patients affected by nephroblastoma, and compared with other AI and non-AI methods adapted from the literature. The results demonstrate its effectiveness in resolving the conflicting pixels and its ability to improve the resulting segmentations. © 2020 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2020.113200,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077952118&doi=10.1016%2fj.eswa.2020.113200&partnerID=40&md5=0566fc0cc56f887c3f932c4dcfce2204,2020,2021-07-20 15:48:32,2021-07-20 15:48:32
J9DS9WQH,journalArticle,2021,"Zheng, W.; Liu, X.; Ni, X.; Yin, L.; Yang, B.",Improving Visual Reasoning through Semantic Representation (December 2020),"In visual reasoning, the achievement of deep learning significantly improved the accuracy of results. Image features are primarily used as input to get answers. However, the image features are too redundant to learn accurate characterizations within a limited complexity and time. While in the process of human reasoning, abstract description of an image is usually to avoid irrelevant details. Inspired by this, a higher-level representation named semantic representation is introduced. In this paper, a detailed visual reasoning model is proposed. This new model contains an image understanding model based on semantic representation, feature extraction and process model refined with watershed and u-distance method, a feature vector learning model using pyramidal pooling and residual network, and a question understanding model combining problem embedding coding method and machine translation decoding method. The feature vector could better represent the whole image instead of overly focused on specific characteristics. The model using semantic representation as input verifies that more accurate results can be obtained by introducing a high-level semantic representation. The result also shows that it is feasible and effective to introduce high-level and abstract forms of knowledge representation into deep learning tasks. This study lays a theoretical and experimental foundation for introducing different levels of knowledge representation into deep learning in the future. CCBY",IEEE Access,10.1109/ACCESS.2021.3074937,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104572196&doi=10.1109%2fACCESS.2021.3074937&partnerID=40&md5=396ef61ab58e0f23155760404b05d0d2,2021,2021-07-20 15:48:32,2021-07-20 15:48:32
44WHSCJI,journalArticle,2021,"Derner, E.; Kubalik, J.; Babuska, R.",Selecting Informative Data Samples for Model Learning through Symbolic Regression,"Continual model learning for nonlinear dynamic systems, such as autonomous robots, presents several challenges. First, it tends to be computationally expensive as the amount of data collected by the robot quickly grows in time. Second, the model accuracy is impaired when data from repetitive motions prevail in the training set and outweigh scarcer samples that also capture interesting properties of the system. It is not known in advance which samples will be useful for model learning. Therefore, effective methods need to be employed to select informative training samples from the continuous data stream collected by the robot. Existing literature does not give any guidelines as to which of the available sample-selection methods are suitable for such a task. In this paper, we compare five sample-selection methods, including a novel method using the model prediction error. We integrate these methods into a model learning framework based on symbolic regression, which allows for learning accurate models in the form of analytic equations. Unlike the currently popular data-hungry deep learning methods, symbolic regression is able to build models even from very small training data sets. We demonstrate the approach on two real robots: the TurtleBot mobile robot and the Parrot Bebop drone. The results show that an accurate model can be constructed even from training sets as small as 24 samples. Informed sample-selection techniques based on prediction error and model variance clearly outperform uninformed methods, such as sequential or random selection. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3052130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099724321&doi=10.1109%2fACCESS.2021.3052130&partnerID=40&md5=0cb7c7754655bff638a92d13d5cd5f07,2021,2021-07-20 15:48:32,2021-07-20 15:48:32
EH3BMBMR,journalArticle,2017,"Endert, A.; Ribarsky, W.; Turkay, C.; Wong, B.L.W.; Nabney, I.; Blanco, I.D.; Rossi, F.",The State of the Art in Integrating Machine Learning into Visual Analytics,"Visual analytics systems combine machine learning or other analytic techniques with interactive data visualization to promote sensemaking and analytical reasoning. It is through such techniques that people can make sense of large, complex data. While progress has been made, the tactful combination of machine learning and data visualization is still under-explored. This state-of-the-art report presents a summary of the progress that has been made by highlighting and synthesizing select research advances. Further, it presents opportunities and challenges to enhance the synergy between machine learning and visual analytics for impactful future research directions. © 2017 The Authors Computer Graphics Forum © 2017 The Eurographics Association and John Wiley & Sons Ltd.",Computer Graphics Forum,10.1111/cgf.13092,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016273988&doi=10.1111%2fcgf.13092&partnerID=40&md5=ed35cfe47deb19b88fa4d8f5f07e8fa0,2017,2021-07-20 15:48:32,2021-07-20 15:48:32
QJAQWKUS,journalArticle,2021,"Luria, D.M.; Vemuri, R.",Logic Encryption for Resource Constrained Designs,"Logic Encryption is a hardware security technique that protects integrated circuit designs that are fabricated at untrusted pure play foundries from being pirated or maliciously modified. In the technique, logic gates are added to the design that are driven by an added key input bus, such that the correct behavior of the circuit is recovered with only the exact correct key input pattern. However, the power, performance, and area (PPA) cost of implementing logic encryption has often been ignored in the literature in favor of increasing the level of security provided. This has proved to be a significant hurdle in transitioning the method to use in commercial-grade designs and a systematic methodology of constraining the cost of logic encryption is needed. In this paper, we propose a generalized Constraint-Directed Logic Encryption (CDLE) methodology. In CDLE, the potential design space of encrypted versions of a circuit is searched to apply logic encryption under PPA constraints. Two example CDLE methods are proposed. The first is a concurrent tree search method which uses commercial tools to sample designs for their PPA cost and determine the optimal encryption strategy. In this method, PPA cost is accurately analyzed at the cost of heavy runtime. The second is a machine learning approach which estimates the PPA cost to predict the optimal encryption strategy. The machine learning model developed in this work is limited, but the results are promising as a direction for study in logic encryption. Detailed experimental results evaluating both methods are presented. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3059163,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100853105&doi=10.1109%2fACCESS.2021.3059163&partnerID=40&md5=ac394862ec3218445caed7c3948b2807,2021,2021-07-20 15:48:32,2021-07-20 15:48:32
M3TG8SKF,journalArticle,2018,"Lima, R.; Espinasse, B.; Freitas, F.",OntoILPER: an ontology- and inductive logic programming-based system to extract entities and relations from text,"Named entity recognition (NER) and relation extraction (RE) are two important subtasks in information extraction (IE). Most of the current learning methods for NER and RE rely on supervised machine learning techniques with more accurate results for NER than RE. This paper presents OntoILPER a system for extracting entity and relation instances from unstructured texts using ontology and inductive logic programming, a symbolic machine learning technique. OntoILPER uses the domain ontology and takes advantage of a higher expressive relational hypothesis space for representing examples whose structure is relevant to IE. It induces extraction rules that subsume examples of entities and relation instances from a specific graph-based model of sentence representation. Furthermore, OntoILPER enables the exploitation of the domain ontology and further background knowledge in the form of relational features. To evaluate OntoILPER, several experiments over the TREC corpus for both NER and RE tasks were conducted and the yielded results demonstrate its effectiveness in both tasks. This paper also provides a comparative assessment among OntoILPER and other NER and RE systems, showing that OntoILPER is very competitive on NER and outperforms the selected systems on RE. © 2017, Springer-Verlag London Ltd.",Knowledge and Information Systems,10.1007/s10115-017-1108-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030849258&doi=10.1007%2fs10115-017-1108-3&partnerID=40&md5=547a569fba2b1e4253593f5230953d91,2018,2021-07-20 15:48:32,2021-07-20 15:48:32
32KWJPQX,journalArticle,2015,"Chen, C.L.P.; Zhang, C.-Y.; Chen, L.; Gan, M.",Fuzzy Restricted Boltzmann Machine for the Enhancement of Deep Learning,"In recent years, deep learning caves out a research wave in machine learning. With outstanding performance, more and more applications of deep learning in pattern recognition, image recognition, speech recognition, and video processing have been developed. Restricted Boltzmann machine (RBM) plays an important role in current deep learning techniques, as most of existing deep networks are based on or related to it. For regular RBM, the relationships between visible units and hidden units are restricted to be constants. This restriction will certainly downgrade the representation capability of the RBM. To avoid this flaw and enhance deep learning capability, the fuzzy restricted Boltzmann machine (FRBM) and its learning algorithm are proposed in this paper, in which the parameters governing the model are replaced by fuzzy numbers. This way, the original RBM becomes a special case in the FRBM, when there is no fuzziness in the FRBM model. In the process of learning FRBM, the fuzzy free energy function is defuzzified before the probability is defined. The experimental results based on bar-and-stripe benchmark inpainting and MNIST handwritten digits classification problems show that the representation capability of FRBM model is significantly better than the traditional RBM. Additionally, the FRBM also reveals better robustness property compared with RBM when the training data are contaminated by noises. © 2015 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2015.2406889,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959506269&doi=10.1109%2fTFUZZ.2015.2406889&partnerID=40&md5=0278e2b5a468834929c4b644bacced00,2015,2021-07-20 15:48:32,2021-07-20 15:48:32
MLIW5XJN,journalArticle,2017,"Angelopoulos, N.; Cussens, J.",Distributional logic programming for Bayesian knowledge representation,"We present a formalism for combining logic programming and its flavour of nondeterminism with probabilistic reasoning. In particular, we focus on representing prior knowledge for Bayesian inference. Distributional logic programming (Dlp), is considered in the context of a class of generative probabilistic languages. A characterisation based on probabilistic paths which can play a central role in clausal probabilistic reasoning is presented. We illustrate how the characterisation can be utilised to clarify derived distributions with regards to mixing the logical and probabilistic constituents of generative languages. We use this operational characterisation to define a class of programs that exhibit probabilistic determinism. We show how Dlp can be used to define generative priors over statistical model spaces. For example, a single program can generate all possible Bayesian networks having N nodes while at the same time it defines a prior that penalises networks with large families. Two classes of statistical models are considered: Bayesian networks and classification and regression trees. Finally we discuss: (1) a Metropolis–Hastings algorithm that can take advantage of the defined priors and the probabilistic choice points in the prior programs and (2) its application to real-world machine learning tasks. © 2016 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2016.08.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983546588&doi=10.1016%2fj.ijar.2016.08.004&partnerID=40&md5=268fb1b72b44236f9edb479d8e5aa4b7,2017,2021-07-20 15:48:32,2021-07-20 15:48:32
KDZ6AWYV,journalArticle,2020,"Gehrmann, S.; Strobelt, H.; Kruger, R.; Pfister, H.; Rush, A.M.",Visual Interaction with Deep Learning Models through Collaborative Semantic Inference,"Automation of tasks can have critical consequences when humans lose agency over decision processes. Deep learning models are particularly susceptible since current black-box approaches lack explainable reasoning. We argue that both the visual interface and model structure of deep learning systems need to take into account interaction design. We propose a framework of collaborative semantic inference (CSI) for the co-design of interactions and models to enable visual collaboration between humans and algorithms. The approach exposes the intermediate reasoning process of models which allows semantic interactions with the visual metaphors of a problem, which means that a user can both understand and control parts of the model reasoning process. We demonstrate the feasibility of CSI with a co-designed case study of a document summarization system. © 1995-2012 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2019.2934595,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075638456&doi=10.1109%2fTVCG.2019.2934595&partnerID=40&md5=9cd3c347b36186d413c6a55ad41613c3,2020,2021-07-20 15:48:33,2021-07-20 15:48:33
4RM8QMSE,journalArticle,2020,"Nassar, M.; Salah, K.; ur Rehman, M.H.; Svetinovic, D.",Blockchain for explainable and trustworthy artificial intelligence,"The increasing computational power and proliferation of big data are now empowering Artificial Intelligence (AI) to achieve massive adoption and applicability in many fields. The lack of explanation when it comes to the decisions made by today's AI algorithms is a major drawback in critical decision-making systems. For example, deep learning does not offer control or reasoning over its internal processes or outputs. More importantly, current black-box AI implementations are subject to bias and adversarial attacks that may poison the learning or the inference processes. Explainable AI (XAI) is a new trend of AI algorithms that provide explanations of their AI decisions. In this paper, we propose a framework for achieving a more trustworthy and XAI by leveraging features of blockchain, smart contracts, trusted oracles, and decentralized storage. We specify a framework for complex AI systems in which the decision outcomes are reached based on decentralized consensuses of multiple AI and XAI predictors. The paper discusses how our proposed framework can be utilized in key application areas with practical use cases. This article is categorized under: Technologies > Machine Learning Technologies > Computer Architectures for Data Mining Fundamental Concepts of Data and Knowledge > Key Design Issues in Data Mining. © 2019 Wiley Periodicals, Inc.",Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,10.1002/widm.1340,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074413095&doi=10.1002%2fwidm.1340&partnerID=40&md5=b206ab6e1410a016f85c06d8832bf43d,2020,2021-07-20 15:48:33,2021-07-20 15:48:33
8FCXTF65,journalArticle,2017,"Zhou, R.; Feng, J.; Chang, H.; Zhou, Y.",Fuzzification of attribute information granules and its formal reasoning model,"The model of attribute information granule based on the method of attribute theory and the problem of fuzzification of attribute information granules are discussed. The concept of information granule given by Zadeh can be explained with an attribute and its qualitative mapping operator. Finally, this study also discusses the reasoning of granulation form of attribute fuzzification information which is based on the fuzzy Petri net, with the good formalisation structure of fuzzy Petri net, as well as its asynchronous, concurrency, uncertainty, and other characteristics which is similar to those of human cognitive activities, enabling the fuzzy Petri net to express the basic characteristics of a cognitive system in the form of computing attribute granules. The results of this study can provide a reference for the establishment of the granular logic model in the uncertain problems and a new interpretation framework for revealing the inherent laws of knowledge uncertainty’s change with knowledge granularity, and also provides another new possible approach to the establishment of the machine learning model of fuzzy Petri net. © Academic Press. All rights reserved.",CAAI Transactions on Intelligence Technology,10.1049/trit.2017.0014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081690365&doi=10.1049%2ftrit.2017.0014&partnerID=40&md5=8b7ac3cab0708918183e2506b60df780,2017,2021-07-20 15:48:33,2021-07-20 15:48:33
4XBWWFM7,journalArticle,2020,"Kaur, S.; Singla, J.; Nkenyereye, L.; Jha, S.; Prashar, D.; Joshi, G.P.; El-Sappagh, S.; Islam, M.S.; Riazul Islam, S.M.",Medical Diagnostic Systems Using Artificial Intelligence (AI) Algorithms: Principles and Perspectives,"Disease diagnosis is the identification of an health issue, disease, disorder, or other condition that a person may have. Disease diagnoses could be sometimes very easy tasks, while others may be a bit trickier. There are large data sets available; however, there is a limitation of tools that can accurately determine the patterns and make predictions. The traditional methods which are used to diagnose a disease are manual and error-prone. Usage of Artificial Intelligence (AI) predictive techniques enables auto diagnosis and reduces detection errors compared to exclusive human expertise. In this paper, we have reviewed the current literature for the last 10 years, from January 2009 to December 2019. The study considered eight most frequently used databases, in which a total of 105 articles were found. A detailed analysis of those articles was conducted in order to classify most used AI techniques for medical diagnostic systems. We further discuss various diseases along with corresponding techniques of AI, including Fuzzy Logic, Machine Learning, and Deep Learning. This research paper aims to reveal some important insights into current and previous different AI techniques in the medical field used in today's medical research, particularly in heart disease prediction, brain disease, prostate, liver disease, and kidney disease. Finally, the paper also provides some avenues for future research on AI-based diagnostics systems based on a set of open problems and challenges. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3042273,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097951867&doi=10.1109%2fACCESS.2020.3042273&partnerID=40&md5=f3edd019abfda4aa34626f80b1f21f28,2020,2021-07-20 15:48:33,2021-07-20 15:48:33
I9IHFH6N,journalArticle,2020,"Thibodeau, P.H.; Blonder, A.; Flusberg, S.J.",A connectionist account of the relational shift and context sensitivity in the development of generalisation,"Similarity-based generalisation is fundamental to human cognition, and the ability to draw analogies based on relational similarities between superficially different domains is crucial for reasoning and inference. Learning to base generalisation on shared relations rather than (or in the face of) shared perceptual features has been identified as an important developmental milestone. However, recent research has highlighted the context-sensitivity of generalisation: children and adults use perceptual similarity to make inferences in some cases and relational similarity in others, a finding that suggests people track the predictive validity of different types of inferences. Here we demonstrate that this pattern of behaviour naturally emerges over the course of development in a domain-general statistical learning model that employs distributed, sub-symbolic representations. We suggest that this model offers a parsimonious account of the development of context-sensitive, similarity-based generalisation and may provide several advantages over other popular structured or symbolic approaches to modelling relational inference. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",Connection Science,10.1080/09540091.2020.1728519,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079377998&doi=10.1080%2f09540091.2020.1728519&partnerID=40&md5=bb948bdd2c4714b316a668cae6e72f95,2020,2021-07-20 15:48:33,2021-07-20 15:48:33
CDYCTGLD,journalArticle,2021,"Alfeo, A.L.; Cimino, M.G.C.A.; Vaglini, G.",Degradation stage classification via interpretable feature learning,"Predictive maintenance (PdM) advocates for the usage of machine learning technologies to monitor asset's health conditions and plan maintenance activities accordingly. However, according to the specific degradation process, some health-related measures (e.g. temperature) may be not informative enough to reliably assess the health stage. Moreover, each measure needs to be properly treated to extract the information linked to the health stage. Those issues are usually addressed by performing a manual feature engineering, which results in high management cost and poor generalization capability of those approaches. In this work, we address this issue by coupling a health stage classifier with a feature learning mechanism. With feature learning, minimally processed data are automatically transformed into informative features. Many effective feature learning approaches are based on deep learning. With those, the features are obtained as a non-linear combination of the inputs, thus it is difficult to understand the input's contribution to the classification outcome and so the reasoning behind the model. Still, these insights are increasingly required to interpret the results and assess the reliability of the model. In this regard, we propose a feature learning approach able to (i) effectively extract high-quality features by processing different input signals, and (ii) provide useful insights about the most informative domain transformations (e.g. Fourier transform or probability density function) of the input signals (e.g. vibration or temperature). The effectiveness of the proposed approach is tested with publicly available real-world datasets about bearings' progressive deterioration and compared with the traditional feature engineering approach. © 2021 The Society of Manufacturing Engineers",Journal of Manufacturing Systems,10.1016/j.jmsy.2021.05.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105746772&doi=10.1016%2fj.jmsy.2021.05.003&partnerID=40&md5=90b9adba2c1b063aaccbd14ca77a0e15,2021,2021-07-20 15:48:33,2021-07-20 15:48:33
WZPCLZV6,journalArticle,2021,"Wang, Q.; Alexander, W.; Pegg, J.; Qu, H.; Chen, M.",HypoML: Visual analysis for hypothesis-based evaluation of machine learning models,"In this paper, we present a visual analytics tool for enabling hypothesis-based evaluation of machine learning (ML) models. We describe a novel ML-testing framework that combines the traditional statistical hypothesis testing (commonly used in empirical research) with logical reasoning about the conclusions of multiple hypotheses. The framework defines a controlled configuration for testing a number of hypotheses as to whether and how some extra information about a 'concept' or 'feature' may benefit or hinder an ML model. Because reasoning multiple hypotheses is not always straightforward, we provide HypoML as a visual analysis tool, with which, the multi-thread testing results are first transformed to analytical results using statistical and logical inferences, and then to a visual representation for rapid observation of the conclusions and the logical flow between the testing results and hypotheses. We have applied HypoML to a number of hypothesized concepts, demonstrating the intuitive and explainable nature of the visual analysis. © 1995-2012 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2020.3030449,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100417185&doi=10.1109%2fTVCG.2020.3030449&partnerID=40&md5=6cf2b3bbee8be8105934a237a238075e,2021,2021-07-20 15:48:33,2021-07-20 15:48:33
TZQTNAVD,journalArticle,2021,"Ly, A.O.; Akhloufi, M.",Learning to Drive by Imitation: An Overview of Deep Behavior Cloning Methods,"There is currently a huge interest around autonomous vehicles from both industry and academia. This is mainly due to recent advances in machine learning and deep learning, allowing the development of promising methods for autonomous driving. The gap toward full autonomy is incrementally being reduced with essentially three main existing approaches. First, Modular systems that combine a pipeline of methods with each solving one specific sub-task of driving. Second, Direct Perception techniques that directly estimate affordances (car orientation, distances between lane borders, etc) used to compute control commands through a simple logic. Finally, end-to-end frameworks that automatically map raw sensor data to actuation values. The objective of this paper is to review some recent works focusing on end-to-end deep learning models for lane stable driving, as well as some publicly available real world datasets and open-source simulators that enable the development and evaluation of such methods. © 2016 IEEE.",IEEE Transactions on Intelligent Vehicles,10.1109/TIV.2020.3002505,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086725080&doi=10.1109%2fTIV.2020.3002505&partnerID=40&md5=0a81fffa3effa049f174a27c4444e015,2021,2021-07-20 15:48:33,2021-07-20 15:48:33
LJ6KTV3S,journalArticle,2020,"Smiti, A.; Elouedi, Z.",Dynamic maintenance case base using knowledge discovery techniques for case based reasoning systems,"The achievement of a Case Based Reasoning (CBR) system is strongly related to the quality of case data and the rapidity of the retrieval process that depends on the quantity of the cases. This quality can diminish especially when the number of cases gets outsized. To guarantee this quality, maintenance the case base becomes essentially. Much existing maintenance CBR approaches focus on the performance of the CBR or the study of the case base (CB) competence. Even though the two points are directly related, there is a few research on using strategies at both points at the same time. Furthermore, the proposed methods are not dynamic, they are not suitable for the frequently change in learning process. In this paper, we propose maintenance CBR method based on well-organized machine learning techniques, in the process of improving the competence and the performance of the CB and can handle incremental cases which evolve over time. We support our approach with empirical evaluation using different benchmark data sets to show the effectiveness of our method. © 2019 Elsevier B.V.",Theoretical Computer Science,10.1016/j.tcs.2019.06.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069548008&doi=10.1016%2fj.tcs.2019.06.026&partnerID=40&md5=36058eb16d2c9ef02b0239e7eab0f7c3,2020,2021-07-20 15:48:33,2021-07-20 15:48:33
CGLBI7VX,journalArticle,2019,"Nápoles, G.; Vanhoenshoven, F.; Vanhoof, K.","Short-term cognitive networks, flexible reasoning and nonsynaptic learning","While the machine learning literature dedicated to fully automated reasoning algorithms is abundant, the number of methods enabling the inference process on the basis of previously defined knowledge structures is scanter. Fuzzy Cognitive Maps (FCMs) are recurrent neural networks that can be exploited towards this goal because of their flexibility to handle external knowledge. However, FCMs suffer from a number of issues that range from the limited prediction horizon to the absence of theoretically sound learning algorithms able to produce accurate predictions. In this paper we propose a neural system named Short-term Cognitive Networks that tackle some of these limitations. In our model, used for regression and pattern completion, weights are not constricted and may have a causal nature or not. As a second contribution, we present a nonsynaptic learning algorithm to improve the network performance without modifying the previously defined weight matrix. Besides, we derive a stop condition to prevent the algorithm from iterating without significantly decreasing the global simulation error. © 2019 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2019.03.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063943662&doi=10.1016%2fj.neunet.2019.03.012&partnerID=40&md5=e2fc39c366bb6790ab00aabc75ce215b,2019,2021-07-20 15:48:33,2021-07-20 15:48:33
UGTAYGSI,journalArticle,2021,"Enni, S.A.; Herrie, M.B.",Turning biases into hypotheses through method: A logic of scientific discovery for machine learning,"Machine learning (ML) systems have shown great potential for performing or supporting inferential reasoning through analyzing large data sets, thereby potentially facilitating more informed decision-making. However, a hindrance to such use of ML systems is that the predictive models created through ML are often complex, opaque, and poorly understood, even if the programs “learning” the models are simple, transparent, and well understood. ML models become difficult to trust, since lay-people, specialists, and even researchers have difficulties gauging the reasonableness, correctness, and reliability of the inferences performed. In this article, we argue that bridging this gap in the understanding of ML models and their reasonableness requires a focus on developing an improved methodology for their creation. This process has been likened to “alchemy” and criticized for involving a large degree of “black art,” owing to its reliance on poorly understood “best practices”. We soften this critique and argue that the seeming arbitrariness often is the result of a lack of explicit hypothesizing stemming from an empiricist and myopic focus on optimizing for predictive performance rather than from an occult or mystical process. We present some of the problems resulting from the excessive focus on optimizing generalization performance at the cost of hypothesizing about the selection of data and biases. We suggest embedding ML in a general logic of scientific discovery similar to the one presented by Charles Sanders Peirce, and present a recontextualized version of Peirce’s scientific hypothesis adjusted to ML. © The Author(s) 2021.",Big Data and Society,10.1177/20539517211020775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106973377&doi=10.1177%2f20539517211020775&partnerID=40&md5=401bd2ca39fb9f9ad723093b66c28adf,2021,2021-07-20 15:48:33,2021-07-20 15:48:33
Q7RZGZ48,journalArticle,2018,"López-Sánchez, D.; Herrero, J.R.; Arrieta, A.G.; Corchado, J.M.",Hybridizing metric learning and case-based reasoning for adaptable clickbait detection,"The term clickbait is usually used to name web contents which are specifically designed to maximize advertisement monetization, often at the expense of quality and exactitude. The rapid proliferation of this type of content has motivated researchers to develop automatic detection methods, to effectively block clickbaits in different application domains. In this paper, we introduce a novel clickbait detection method. Our approach leverages state-of-the-art techniques from the fields of deep learning and metric learning, integrating them into the Case-Based Reasoning methodology. This provides the model with the ability to learn-over-time, adapting to different users’ criteria. Our experimental results also evidence that the proposed approach outperforms previous clickbait detection methods by a large margin. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.",Applied Intelligence,10.1007/s10489-017-1109-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040791890&doi=10.1007%2fs10489-017-1109-7&partnerID=40&md5=183e854d657d892f20d9053e0a4d6dd5,2018,2021-07-20 15:48:33,2021-07-20 15:48:33
EGI547RA,journalArticle,2019,"Honda, H.; Hagiwara, M.",Question Answering Systems with Deep Learning-Based Symbolic Processing,"The authors propose methods to learn symbolic processing with deep learning and to build question answering systems by means of learned models. Symbolic processing, performed by the Prolog processing systems which execute unification, resolution, and list operations, is learned by a combination of deep learning models, Neural Machine Translation (NMT) and Word2Vec training. To our knowledge, the implementation of a Prolog-like processing system using deep learning is a new experiment that has not been conducted in the past. The results of their experiments revealed that the proposed methods are superior to the conventional methods because symbolic processing (1) has rich representations, (2) can interpret inputs even if they include unknown symbols, and (3) can be learned with a small amount of training data. In particular (2), handling of unknown data, which is a major task in artificial intelligence research, is solved using Word2Vec. Furthermore, question answering systems can be built from knowledge bases written in Prolog with learned symbolic processing, which, with conventional methods, is extremely difficult to accomplish. Their proposed systems can not only answer questions through powerful inferences by utilizing facts that harbor unknown data but also have the potential to build knowledge bases from a large amount of data, including unknown data, on the Web. The proposed systems are a completely new trial, there is no state-of-the-art methods in the sense of 'newest'. Therefore, to evaluate their efficiency, they are compared with the most traditional and robust system i.e., the Prolog system. This is new research that encompasses the subjects of conventional artificial intelligence and neural network, and their systems have higher potential to build applications such as FAQ chatbots, decision support systems and energy-efficient estimation using a large amount of information on the Web. Mining hidden information through these applications will provide great value. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2948081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078352116&doi=10.1109%2fACCESS.2019.2948081&partnerID=40&md5=dd4f6f6b24f6bb01178c87f12ce8cb5e,2019,2021-07-20 15:48:33,2021-07-20 15:48:33
J2VN49U9,journalArticle,2018,"Kalsi, S.; Kaur, H.; Chang, V.",DNA Cryptography and Deep Learning using Genetic Algorithm with NW algorithm for Key Generation,"Cryptography is not only a science of applying complex mathematics and logic to design strong methods to hide data called as encryption, but also to retrieve the original data back, called decryption. The purpose of cryptography is to transmit a message between a sender and receiver such that an eavesdropper is unable to comprehend it. To accomplish this, not only we need a strong algorithm, but a strong key and a strong concept for encryption and decryption process. We have introduced a concept of DNA Deep Learning Cryptography which is defined as a technique of concealing data in terms of DNA sequence and deep learning. In the cryptographic technique, each alphabet of a letter is converted into a different combination of the four bases, namely; Adenine (A), Cytosine (C), Guanine (G) and Thymine (T), which make up the human deoxyribonucleic acid (DNA). Actual implementations with the DNA don’t exceed laboratory level and are expensive. To bring DNA computing on a digital level, easy and effective algorithms are proposed in this paper. In proposed work we have introduced firstly, a method and its implementation for key generation based on the theory of natural selection using Genetic Algorithm with Needleman-Wunsch (NW) algorithm and Secondly, a method for implementation of encryption and decryption based on DNA computing using biological operations Transcription, Translation, DNA Sequencing and Deep Learning. © 2017, Springer Science+Business Media, LLC.",Journal of Medical Systems,10.1007/s10916-017-0851-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037640800&doi=10.1007%2fs10916-017-0851-z&partnerID=40&md5=0783471a077f6469a8716d777656a5f7,2018,2021-07-20 15:48:33,2021-07-20 15:48:33
7FV4X2GA,journalArticle,2013,"Lisi, F.A.; Straccia, U.",A logic-based computational method for the automated induction of fuzzy ontology axioms,"Fuzzy Description Logics (DLs) are logics that allow to deal with structured vague knowledge. Although a relatively important amount of work has been carried out in the last years concerning the use of fuzzy DLs as ontology languages, the problem of automatically managing the evolution of fuzzy ontologies has received very little attention so far. We describe here a logic-based computational method for the automated induction of fuzzy ontology axioms which follows the machine learning approach of Inductive Logic Programming. The potential usefulness of the method is illustrated by means of an example taken from the tourism application domain.",Fundamenta Informaticae,10.3233/FI-2013-846,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879546019&doi=10.3233%2fFI-2013-846&partnerID=40&md5=bcc965fbcb37bc3c71c829a1782bbe9b,2013,2021-07-20 15:48:33,2021-07-20 15:48:33
28AU6BRU,journalArticle,2013,"Hruschka Jr., E.R.; Nicoletti, M.C.",Roles played by Bayesian networks in machine learning: An empirical investigation,"Bayesian networks (BN) and Bayesian classifiers (BC) are traditional probabilistic techniques that have been successfully used by various machine learning methods to help solving a variety of problems in many different domains. BNs (and BCs) can be considered a probabilistic graphical language suitable for inducing models from data aiming at knowledge representation and reasoning about data domains. The main goal of this chapter is the empirical investigation of a few roles played by BCs in machine learning related processes namely (i) data pre-processing (feature selection and imputation), (ii) learning and (iii) postprocessing (rule generation). By doing so the chapter contributes with organizing, specifying and discussing the many different ways Bayes-based concepts can successfully be employed in automatic learning. © Springer-Verlag Berlin Heidelberg 2013.","Smart Innovation, Systems and Technologies",10.1007/978-3-642-28699-5_5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879285849&doi=10.1007%2f978-3-642-28699-5_5&partnerID=40&md5=3278bc628978f9aa604a450aa98228b4,2013,2021-07-20 15:48:33,2021-07-20 15:48:33
BNUU5UXR,journalArticle,2017,"Kodagoda, N.; Pontis, S.; Simmie, D.; Attfield, S.; Wong, B.L.W.; Blandford, A.; Hankin, C.",Using Machine Learning to Infer Reasoning Provenance from User Interaction Log Data,"The reconstruction of analysts' reasoning processes (reasoning provenance) during complex sensemaking tasks can support reflection and decision making. One potential approach to such reconstruction is to automatically infer reasoning from low-level user interaction logs. We explore a novel method for doing this using machine learning. Two user studies were conducted in which participants performed similar intelligence analysis tasks. In one study, participants used a standard web browser and word processor; in the other, they used a system called INVISQUE (Interactive Visual Search and Query Environment). Interaction logs were manually coded for cognitive actions based on captured think-aloud protocol and posttask interviews based on Klein, Phillips, Rall, and Pelusos's data/frame model of sensemaking as a conceptual framework. This analysis was then used to train an interaction frame mapper, which employed multiple machine learning models to learn relationships between the interaction logs and the codings. Our results show that, for one study at least, classification accuracy was significantly better than chance and compared reasonably to a reported manual provenance reconstruction method. We discuss our results in terms of variations in feature sets from the two studies and what this means for the development of the method for provenance capture and the evaluation of sensemaking systems. © 2016, Human Factors and Ergonomics Society.",Journal of Cognitive Engineering and Decision Making,10.1177/1555343416672782,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011579228&doi=10.1177%2f1555343416672782&partnerID=40&md5=ae4fb83c666f66415f0ccd34c622fcb5,2017,2021-07-20 15:48:34,2021-07-20 15:48:34
6QNPYEA5,journalArticle,2017,"Urovi, V.; del Toro, O.J.; Dubosson, F.; Torres, A.R.; Schumacher, M.I.",COMPOSE: Using temporal patterns for interpreting wearable sensor data with computer interpretable guidelines,"This paper describes a novel temporal logic-based framework for reasoning with continuous data collected from wearable sensors. The work is motivated by the Metabolic Syndrome, a cluster of conditions which are linked to obesity and unhealthy lifestyle. We assume that, by interpreting the physiological parameters of continuous monitoring, we can identify which patients have a higher risk of Metabolic Syndrome. We define temporal patterns for reasoning with continuous data and specify the coordination mechanisms for combining different sets of clinical guidelines that relate to this condition. The proposed solution is tested with data provided by twenty subjects, which used sensors for four days of continuous monitoring. The results are compared to the gold standard. The novelty of the framework stands in extending a temporal logic formalism, namely the Event Calculus, with temporal patterns. These patterns are helpful to specify the rules for reasoning with continuous data and in combining new knowledge into one consistent outcome that is tailored to the patient's profile. The overall approach opens new possibilities for delivering patient-tailored interventions and educational material before the patients present the symptoms of the disease. © 2016 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2016.11.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006810282&doi=10.1016%2fj.compbiomed.2016.11.015&partnerID=40&md5=4d66abadc1f46b6dedf27833fd2060ff,2017,2021-07-20 15:48:34,2021-07-20 15:48:34
ZW6KGA63,journalArticle,2018,"Priore, P.; Ponte, B.; Puente, J.; Gómez, A.",Learning-based scheduling of flexible manufacturing systems using ensemble methods,"Dispatching rules are commonly applied to schedule jobs in Flexible Manufacturing Systems (FMSs). However, the suitability of these rules relies heavily on the state of the system; hence, there is no single rule that always outperforms the others. In this scenario, machine learning techniques, such as support vector machines (SVMs), inductive learning-based decision trees (DTs), backpropagation neural networks (BPNs), and case based-reasoning (CBR), offer a powerful approach for dynamic scheduling, as they help managers identify the most appropriate rule in each moment. Nonetheless, different machine learning algorithms may provide different recommendations. In this research, we take the analysis one step further by employing ensemble methods, which are designed to select the most reliable recommendations over time. Specifically, we compare the behaviour of the bagging, boosting, and stacking methods. Building on the aforementioned machine learning algorithms, our results reveal that ensemble methods enhance the dynamic performance of the FMS. Through a simulation study, we show that this new approach results in an improvement of key performance metrics (namely, mean tardiness and mean flow time) over existing dispatching rules and the individual use of each machine learning algorithm. © 2018 Elsevier Ltd",Computers and Industrial Engineering,10.1016/j.cie.2018.09.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054163976&doi=10.1016%2fj.cie.2018.09.034&partnerID=40&md5=f91bd0e079bbfe2179eb5d3c91ae1782,2018,2021-07-20 15:48:34,2021-07-20 15:48:34
N74Z86UI,journalArticle,2014,"Bottou, L.",From machine learning to machine reasoning: An essay,"A plausible definition of ""reasoning"" could be ""algebraically manipulating previously acquired knowledge in order to answer a new question"". This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text. This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated ""all-purpose"" inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up. © 2013 The Author(s).",Machine Learning,10.1007/s10994-013-5335-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894902026&doi=10.1007%2fs10994-013-5335-x&partnerID=40&md5=294e90e9a10ef0743770ebe073caba74,2014,2021-07-20 15:48:34,2021-07-20 15:48:34
7F87LCWU,journalArticle,2020,"Fayek, H.M.; Johnson, J.",Temporal Reasoning via Audio Question Answering,"Multimodal question answering tasks can be used as proxy tasks to study systems that can perceive and reason about the world. Answering questions about different types of input modalities stresses different aspects of reasoning such as visual reasoning, reading comprehension, story understanding, or navigation. In this article, we use the task of Audio Question Answering (AQA) to study the temporal reasoning abilities of machine learning models. To this end, we introduce the Diagnostic Audio Question Answering (DAQA) dataset comprising audio sequences of natural sound events and programmatically generated questions and answers that probe various aspects of temporal reasoning. We adapt several recent state-of-the-art methods for visual question answering to the AQA task, and use DAQA to demonstrate that they perform poorly on questions that require in-depth temporal reasoning. Finally, we propose a new model, Multiple Auxiliary Controllers for Linear Modulation (MALiMo) that extends the recent Feature-wise Linear Modulation (FiLM) model and significantly improves its temporal reasoning capabilities. We envisage DAQA to foster research on AQA and temporal reasoning and MALiMo a step towards models for AQA. © 2014 IEEE.",IEEE/ACM Transactions on Audio Speech and Language Processing,10.1109/TASLP.2020.3010650,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090773916&doi=10.1109%2fTASLP.2020.3010650&partnerID=40&md5=a9fac5776fa9b6e35d9b78ea25d23c22,2020,2021-07-20 15:48:34,2021-07-20 15:48:34
P32QRIYJ,journalArticle,2021,"Mishra, K.N.; Pandey, S.C.",Fraud Prediction in Smart Societies Using Logistic Regression and k-fold Machine Learning Techniques,"The credit/debit card deceit detection is an enormously difficult task. However, it is a well known problem of our cloud based mobile internet society and it must be solved by technocrats in the welfare of societal mental harassments. The main problem in executing credit/debit card fraud detection technique is the availability of limited amount of fraud related data like transaction amount, transaction date, transaction time, address, and vendor category code related to the frauds. It is the truth of mobile internet world that there are billions of potential places and e-commerce websites where a credit/debit card can be used by fraudulent people for online transactions and payments which make it exceedingly thorny to trace the pattern of frauds. Moreover, the problem of fraud detection in cloud— Internet of Things (IoT) based smart societies has numerous constraints like continuous change in the behavior of normal and fraudulent persons, the fraudulent people try to develop and use new method for executing frauds, and very little availability of frauds related bench mark data sets. In this research article, the authors have presented logistic regression based k-fold machine learning technique (MLT) for fraud detection and prevention in cloud-IoT based smart societal environment. The k-fold method creates multiple folds of bank transactions related data before implementing logistic regression and MLT. The logistic regression performs logic based regression analysis and the intelligent machine learning approach performs registration, classification, clustering, dimensionality reduction, deep learning, training, and reinforcement learning steps on the received bank transactions data. The implementation of proposed methodology and its further analysis using intelligent machine learning tools like ROC (Receiver Operating Characteristic) curve, confusion matrix, mean-recall score value, and precision recall curves for European banks day-to-day transactions related bench mark data set reveal that the proposed methodology is efficient, accurate, and reliable for detecting frauds in cloud-IoT based smart societal environment. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Wireless Personal Communications,10.1007/s11277-021-08283-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101816961&doi=10.1007%2fs11277-021-08283-9&partnerID=40&md5=77b6349cc6716f632a600917da1ad46d,2021,2021-07-20 15:48:34,2021-07-20 15:48:34
7Q5JRVK2,journalArticle,2017,"Diligenti, M.; Gori, M.; Saccà, C.",Semantic-based regularization for learning and inference,"This paper proposes a unified approach to learning from constraints, which integrates the ability of classical machine learning techniques to learn from continuous feature-based representations with the ability of reasoning using higher-level semantic knowledge typical of Statistical Relational Learning. Learning tasks are modeled in the general framework of multi-objective optimization, where a set of constraints must be satisfied in addition to the traditional smoothness regularization term. The constraints translate First Order Logic formulas, which can express learning-from-example supervisions and general prior knowledge about the environment by using fuzzy logic. By enforcing the constraints also on the test set, this paper presents a natural extension of the framework to perform collective classification. Interestingly, the theory holds for both the case of data represented by feature vectors and the case of data simply expressed by pattern identifiers, thus extending classic kernel machines and graph regularization, respectively. This paper also proposes a probabilistic interpretation of the proposed learning scheme, and highlights intriguing connections with probabilistic approaches like Markov Logic Networks. Experimental results on classic benchmarks provide clear evidence of the remarkable improvements that are obtained with respect to related approaches. © 2015 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2015.08.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940937276&doi=10.1016%2fj.artint.2015.08.011&partnerID=40&md5=3333e89077342ba6c46a593809d11eb2,2017,2021-07-20 15:48:34,2021-07-20 15:48:34
L33Q7QJ5,journalArticle,2020,"Mukhtaruzzaman, M.; Atiquzzaman, M.",Clustering in vehicular ad hoc network: Algorithms and challenges,"Clustering is an important concept in vehicular ad hoc network (VANET) where several vehicles join to form a group based on common features. Mobility-based clustering strategies are the most common in VANET clustering; however, machine learning and fuzzy logic algorithms are also the basis of many VANET clustering algorithms. Some VANET clustering algorithms integrate machine learning and fuzzy logic algorithms to make the cluster more stable and efficient. Network mobility (NEMO) and multi-hop-based strategies are also used for VANET clustering. Mobility and some other clustering strategies are presented in the existing literature reviews; however, extensive study of intelligence-based, mobility-based, and multi-hop-based strategies still missing in the VANET clustering reviews. In this paper, we presented a classification of intelligence-based clustering algorithms, mobility-based algorithms, and multi-hop-based algorithms with an analysis on the mobility metrics, evaluation criteria, challenges, and future directions of machine learning, fuzzy logic, mobility, NEMO, and multi-hop clustering algorithms. © 2020",Computers and Electrical Engineering,10.1016/j.compeleceng.2020.106851,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092229046&doi=10.1016%2fj.compeleceng.2020.106851&partnerID=40&md5=dd14a385f7b96cfc84df2cea0fa3edc0,2020,2021-07-20 15:48:34,2021-07-20 15:48:34
HRGZGEVN,journalArticle,2020,"LaBorde, M.L.; Rogers, A.C.; Dowling, J.P.",Finding broken gates in quantum circuits: exploiting hybrid machine learning,"Current implementations of quantum logic gates can be highly faulty and introduce errors. In order to correct these errors, it is necessary to first identify the faulty gates. We demonstrate a procedure to diagnose where gate faults occur in a circuit by using a hybridized quantum-and-classical K-Nearest-Neighbors (KNN) machine-learning technique. We accomplish this task using a diagnostic circuit and selected input qubits to obtain the fidelity between a set of output states and reference states. The outcomes of the circuit can then be stored to be used for a classical KNN algorithm. We numerically demonstrate an ability to locate a faulty gate in circuits with over 30 gates and up to nine qubits with over 90% accuracy. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Quantum Information Processing,10.1007/s11128-020-02729-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087420929&doi=10.1007%2fs11128-020-02729-y&partnerID=40&md5=6ffaea148121a8d02b00afd27f18e4e0,2020,2021-07-20 15:48:34,2021-07-20 15:48:34
9AMIEFEX,journalArticle,2013,"Rybinski, H.; Ryzko, D.; Wiech, P.W.",Learning of defaults by agents in a distributed multi-agent system environment,"The paper introduces a novel approach to machine learning in a multi-agents system. A distributed version of Inductive Logic Programming is used, which allows agents to construct new rules based on knowledge and examples, which are available to different memebrs of the system. The learning process is performed in two phases - first locally by each agent and then on the global level while reasoning. © Springer-Verlag Berlin Heidelberg 2013.","Smart Innovation, Systems and Technologies",10.1007/978-3-642-28699-5_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879304696&doi=10.1007%2f978-3-642-28699-5_8&partnerID=40&md5=f18c9d8863cef0425ebef00b3ba5529e,2013,2021-07-20 15:48:34,2021-07-20 15:48:34
5DGXDZ77,journalArticle,2020,"Fan, F.; Wang, G.",Fuzzy logic interpretation of quadratic networks,"Over past several years, deep learning has achieved huge successes in various applications. However, such a data-driven approach is often criticized for lack of interpretability. Recently, we proposed artificial quadratic neural networks consisting of quadratic neurons in potentially many layers. In cellular level, a quadratic function is used to replace the inner product in a traditional neuron, and then undergoes a nonlinear activation. With a single quadratic neuron, any fuzzy logic operation, such as XOR, can be implemented. In this sense, any deep network constructed with quadratic neurons can be interpreted as a deep fuzzy logic system. Since traditional neural networks and quadratic counterparts can represent each other and fuzzy logic operations are naturally implemented in quadratic neural networks, it is plausible to explain how a deep neural network works with a quadratic network as the system model. In this paper, we generalize and categorize fuzzy logic operations implementable with individual quadratic neurons, and then perform statistical/information-theoretic analyses of exemplary quadratic neural networks. © 2019 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2019.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073032808&doi=10.1016%2fj.neucom.2019.09.001&partnerID=40&md5=eca123700491bf1a0dcb8c6ecb61d608,2020,2021-07-20 15:48:34,2021-07-20 15:48:34
J9TUSXR6,journalArticle,2019,"Lamy, J.-B.; Sekar, B.; Guezennec, G.; Bouaud, J.; Séroussi, B.",Explainable artificial intelligence for breast cancer: A visual case-based reasoning approach,"Case-Based Reasoning (CBR) is a form of analogical reasoning in which the solution for a (new) query case is determined using a database of previous known cases with their solutions. Cases similar to the query are retrieved from the database, and then their solutions are adapted to the query. In medicine, a case usually corresponds to a patient and the problem consists of classifying the patient in a class of diagnostic or therapy. Compared to “black box” algorithms such as deep learning, the responses of CBR systems can be justified easily using the similar cases as examples. However, this possibility is often under-exploited and the explanations provided by most CBR systems are limited to the display of the similar cases. In this paper, we propose a CBR method that can be both executed automatically as an algorithm and presented visually in a user interface for providing visual explanations or for visual reasoning. After retrieving similar cases, a visual interface displays quantitative and qualitative similarities between the query and the similar cases, so as one can easily classify the query through visual reasoning, in a fully explainable manner. It combines a quantitative approach (visualized by a scatter plot based on Multidimensional Scaling in polar coordinates, preserving distances involving the query) and a qualitative approach (set visualization using rainbow boxes). We applied this method to breast cancer management. We showed on three public datasets that our qualitative method has a classification accuracy comparable to k-Nearest Neighbors algorithms, but is better explainable. We also tested the proposed interface during a small user study. Finally, we apply the proposed approach to a real dataset in breast cancer. Medical experts found the visual approach interesting as it explains why cases are similar through the visualization of shared patient characteristics. © 2019 The Authors",Artificial Intelligence in Medicine,10.1016/j.artmed.2019.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060492700&doi=10.1016%2fj.artmed.2019.01.001&partnerID=40&md5=f0ad3acc1cee4f2d1f5412ba1aed9e92,2019,2021-07-20 15:48:34,2021-07-20 15:48:34
NJSX8NPA,journalArticle,2019,"Khan, M.J.; Hayat, H.; Awan, I.",Hybrid case-base maintenance approach for modeling large scale case-based reasoning systems,"Case-based reasoning (CBR) is a nature inspired paradigm of machine learning capable to continuously learn from the past experience. Each newly solved problem and its corresponding solution is retained in its central knowledge repository called case-base. Withρ the regular use of the CBR system, the case-base cardinality keeps on growing. It results into performance bottleneck as the number of comparisons of each new problem with the existing problems also increases with the case-base growth. To address this performance bottleneck, different case-base maintenance (CBM) strategies are used so that the growth of the case-base is controlled without compromising on the utility of knowledge maintained in the case-base. This research work presents a hybrid case-base maintenance approach which equally utilizes the benefits of case addition as well as case deletion strategies to maintain the case-base in online and offline modes respectively. The proposed maintenance method has been evaluated using a simulated model of autonomic forest fire application and its performance has been compared with the existing approaches on a large case-base of the simulated case study. © 2019, The Author(s).",Human-centric Computing and Information Sciences,10.1186/s13673-019-0171-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063066175&doi=10.1186%2fs13673-019-0171-z&partnerID=40&md5=4a423a3cc229371ebaa6eb6d2efd0d88,2019,2021-07-20 15:48:34,2021-07-20 15:48:34
X9Z84HMG,journalArticle,2021,"Mukhopadhyay, S.; Leung, H.",Blind Identification of Sparse Systems Using Symbolic Dynamics Encoding,"The unique properties of chaotic signals have led to their application in improving blind system identification performance. However, the role of chaos in blind identification of a sparse system has not been investigated. In this letter, we apply symbolic dynamics to encode a random signal to reap the benefits of chaos in improving blind identification of a sparse Moving Average (MA) system. We derive an estimation technique using the encoded signal by training a machine learning model that mimics a chaotic map. The novelty of our work is to exploit the merits of chaos in improving blind estimation performance of sparse systems at low signal-to-noise (SNR) ratio. The estimation error of our method is close to the minimum mean square error of the nonblind method for sparse system estimation and works well for a short data sequence. © 1997-2012 IEEE.",IEEE Communications Letters,10.1109/LCOMM.2021.3053151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099729595&doi=10.1109%2fLCOMM.2021.3053151&partnerID=40&md5=19707f87f3b74a564219c3988fe39383,2021,2021-07-20 15:48:34,2021-07-20 15:48:34
AZXPAGR7,journalArticle,2012,"Emele, C.D.; Norman, T.J.; Şensoy, M.; Parsons, S.",Learning strategies for task delegation in norm-governed environments,"How do I choose whom to delegate a task to? This is an important question for an autonomous agent collaborating with others to solve a problem. Were similar proposals accepted from similar agents in similar circumstances? What arguments were most convincing? What are the costs incurred in putting certain arguments forward? Can I exploit domain knowledge to improve the outcome of delegation decisions? In this paper, we present an agent decision-making mechanism where models of other agents are refined through evidence from past dialogues and domain knowledge, and where these models are used to guide future delegation decisions. Our approach combines ontological reasoning, argumentation and machine learning in a novel way, which exploits decision theory for guiding argumentation strategies. Using our approach, intelligent agents can autonomously reason about the restrictions (e. g., policies/norms) that others are operating with, and make informed decisions about whom to delegate a task to. In a set of experiments, we demonstrate the utility of this novel combination of techniques. Our empirical evaluation shows that decision-theory, machine learning and ontology reasoning techniques can significantly improve dialogical outcomes. © 2012 The Author(s).",Autonomous Agents and Multi-Agent Systems,10.1007/s10458-012-9194-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861481400&doi=10.1007%2fs10458-012-9194-9&partnerID=40&md5=abbf3d67bf6eb4b8bba5503533109cc8,2012,2021-07-20 15:48:34,2021-07-20 15:48:34
K5VREYQ8,journalArticle,2020,"Hernandez-Matamoros, A.; Fujita, H.; Nakano-Miyatake, M.; Perez-Meana, H.; Escamilla-Hernandez, E.",Scheme fuzzy approach to classify skin tonalities through geographic distribution,"One of the most significant current discussions in Computer Science is the skin recognition. Many papers have studied the skin detection using different techniques, artificial vision, machine learning, deep learning among others. Despite its long success, the skin recognition has several problems in use. However, there has been little discussion about generate a skin tonalities classification. The aim of this paper is to propose a system to skin tonalities through geographic distribution based on clustering algorithms, pattern recognition and fuzzy logic. This distribution gives us the opportunity to classify the skin tonalities. We can study each skin tonality for any applications as medical diagnosis, security. In the first stage, we use the RGB color model to training the system. Then, we tested the system with different color models. We use color model with the best result to propose geographic distribution based skin tonalities. The results show that is possible to generate a skin tonalities classification. The proposed system is using to skin recognition, showing interesting results under controlled and no controlled conditions. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-019-01400-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070730600&doi=10.1007%2fs12652-019-01400-4&partnerID=40&md5=83097fe1083a13d69bc3f5d24420c3e9,2020,2021-07-20 15:48:34,2021-07-20 15:48:34
YY3243BS,journalArticle,2017,"Kunze, L.; Beetz, M.",Envisioning the qualitative effects of robot manipulation actions using simulation-based projections,"Autonomous robots that are to perform complex everyday tasks such as making pancakes have to understand how the effects of an action depend on the way the action is executed. Within Artificial Intelligence, classical planning reasons about whether actions are executable, but makes the assumption that the actions will succeed (with some probability). In this work, we have designed, implemented, and analyzed a framework that allows us to envision the physical effects of robot manipulation actions. We consider envisioning to be a qualitative reasoning method that reasons about actions and their effects based on simulation-based projections. Thereby it allows a robot to infer what could happen when it performs a task in a certain way. This is achieved by translating a qualitative physics problem into a parameterized simulation problem; performing a detailed physics-based simulation of a robot plan; logging the state evolution into appropriate data structures; and then translating these sub-symbolic data structures into interval-based first-order symbolic, qualitative representations, called timelines. The result of the envisioning is a set of detailed narratives represented by timelines which are then used to infer answers to qualitative reasoning problems. By envisioning the outcome of actions before committing to them, a robot is able to reason about physical phenomena and can therefore prevent itself from ending up in unwanted situations. Using this approach, robots can perform manipulation tasks more efficiently, robustly, and flexibly, and they can even successfully accomplish previously unknown variations of tasks. © 2014 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2014.12.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921450087&doi=10.1016%2fj.artint.2014.12.004&partnerID=40&md5=7e2b100acc6d29c12a584d38e1efb2cb,2017,2021-07-20 15:48:34,2021-07-20 15:48:34
WIRWVTPG,journalArticle,2020,"Varghese, E.; Thampi, S.M.; Berretti, S.",A Psychologically Inspired Fuzzy Cognitive Deep Learning Framework to Predict Crowd Behavior,"In an intelligent surveillance system, detecting and predicting diverse collective crowd behaviors has emerged as a challenging problem for efficient crowd management. In real-world scenarios, potential disasters and hazards can be averted by considering crowd psychology for predicting crowd behaviors. This paper proposes an approach that exploits the psychological and cognitive aspects of human behavior in determining nine diverse crowd behaviors. The proposed approach is a combination of two cognitive deep learning frameworks and a psychological fuzzy computational model that utilizes OCC theory of emotions, OCEAN five-factor model of personality and visual attention for detecting crowd behaviors. Experiments are performed on different datasets and the results prove that our approach is successful in detecting and predicting crowd behavior in confronting situations and also outperforms the state-of-the-art methods. In particular, considering psychological aspects and cognition in determining crowd behavior is beneficial for rectifying the semantic ambiguity in identifying crowd behaviors. IEEE",IEEE Transactions on Affective Computing,10.1109/TAFFC.2020.2987021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083694464&doi=10.1109%2fTAFFC.2020.2987021&partnerID=40&md5=2821d0b1eb523715b23db732328b2399,2020,2021-07-20 15:48:34,2021-07-20 15:48:34
5P2Z4DHH,journalArticle,2018,"Guo, D.; Zhong, M.; Ji, H.; Liu, Y.; Yang, R.",A hybrid feature model and deep learning based fault diagnosis for unmanned aerial vehicle sensors,"Fault diagnosis plays an important role in guaranteeing system safety and reliability for unmanned aerial vehicles (UAVs). In this study, a hybrid feature model and deep learning based fault diagnosis for UAV sensors is proposed. The residual signals of different sensor faults, including global positioning system (GPS), inertial measurement unit (IMU), air data system (ADS), were collected. This paper used short time fourier transform (STFT) to transform the residual signal to the corresponding time-frequency map. Then, a convolutional neural network (CNN) was used to extract the feature of the map and the fault diagnosis of the UAV sensors was implemented. Finally, the performance of the proposed methodology is evaluated through flight experiments of the UAV. From the visualization, the sensor faults information can be extracted by CNN and the fault diagnosis logic between the residuals and the health status can be constructed successfully. © 2018 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2018.08.046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053723961&doi=10.1016%2fj.neucom.2018.08.046&partnerID=40&md5=bb08f11d75ec8b2d810320c5548afca0,2018,2021-07-20 15:48:34,2021-07-20 15:48:34
E55R8LH9,journalArticle,2020,"González Rodríguez, G.; Gonzalez-Cava, J.M.; Méndez Pérez, J.A.",An intelligent decision support system for production planning based on machine learning,"This paper presents a new methodology to solve a Closed-Loop Supply Chain (CLSC) management problem through a decision-making system based on fuzzy logic built on machine learning. The system will provide decisions to operate a production plant integrated in a CLSC to meet the production goals with the presence of uncertainties. One of the main contributions of the proposal is the ability to reject the effects that the imbalances in the rest of the chain have on the inventories of raw materials and finished products. For this, an intelligent algorithm will be in charge of the supervision of the plant operation and task-reprogramming to ensure the achievement of the process goals. Fuzzy logic and machine learning techniques are combined to design the tool. The method was tested on an industrial hospital laundry with satisfactory results, thus highlighting the potential of this proposal for its incorporation into the Industry 4.0 framework. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Intelligent Manufacturing,10.1007/s10845-019-01510-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074834155&doi=10.1007%2fs10845-019-01510-y&partnerID=40&md5=19aa04e07ce67066094fda3a9f42639e,2020,2021-07-20 15:48:35,2021-07-20 15:48:35
Q6IGFUXE,journalArticle,2021,"Chen, C.; Li, K.; Zou, X.; Cheng, Z.; Wei, W.; Tian, Q.; Zeng, Z.",Hierarchical Semantic Graph Reasoning for Train Component Detection,"Recently, deep learning-based approaches have achieved superior performance on object detection applications. However, object detection for industrial scenarios, where the objects may also have some structures and the structured patterns are normally presented in a hierarchical way, is not well investigated yet. In this work, we propose a novel deep learning-based method, hierarchical graphical reasoning (HGR), which utilizes the hierarchical structures of trains for train component detection. HGR contains multiple graphical reasoning branches, each of which is utilized to conduct graphical reasoning for one cluster of train components based on their sizes. In each branch, the visual appearances and structures of train components are considered jointly with our proposed novel densely connected dual-gated recurrent units (Dense-DGRUs). To the best of our knowledge, HGR is the first kind of framework that explores hierarchical structures among objects for object detection. We have collected a data set of 1130 images captured from moving trains, in which 17 334 train components are manually annotated with bounding boxes. Based on this data set, we carry out extensive experiments that have demonstrated our proposed HGR outperforms the existing state-of-the-art baselines significantly. The data set and the source code can be downloaded online at https://github.com/ChengZY/HGR. IEEE",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2021.3057792,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103783230&doi=10.1109%2fTNNLS.2021.3057792&partnerID=40&md5=c4c7e3c5e2658df60bdd411f2dceecea,2021,2021-07-20 15:48:35,2021-07-20 15:48:35
B7D2DTG3,journalArticle,2019,"Al-Asaly, M.S.; Hassan, M.M.; Alsanad, A.",A cognitive/intelligent resource provisioning for cloud computing services: opportunities and challenges,"In cloud computing, resources could be provisioned in a dynamic way on demand for cloud services. Cloud providers seek to realize effective SLA execution mechanisms for avoiding SLA violations by provisioning the resources or applications and timely interacting to environmental changes and failures. Sufficient resource provisioning to cloud’s services relies on the requirements of the workloads to achieve a high performance for quality of service. Therefore, deciding the suitable amount of cloud’s resources for these services to achieve is one of the main works in cloud computing. During the runtime of services, the amount of cloud’s resources can be specified and provisioned based on the actual workloads changes. Determining the correct amount of cloud’s resources needed for running the services on clouds is not easy task, and it depends on the existing workloads of services. Consequently, it is required to predict the future workloads for dynamic provisioning of resources in order to meet the changes in workloads and demands of services in cloud computing environments. In this paper, we study the possibility of using a cognitive/intelligent approach for cloud resource provisioning which is a combination of the autonomic computing concept, deep learning technique and fuzzy logic control. Deep learning technique is a state-of-the-art in the machine learning field. It achieved promising results in many other fields like image classification and speech recognition. For these reasons, deep learning is proposed in this work to tackle the workload prediction in cloud computing. Additionally, we also propose to use a fuzzy logic-based method in order to make a decision in the case of uncertainty of the workload prediction. We study various exiting works on autonomic cloud resource provisioning and show that there is still an opportunity to improve the current methods. We also present the challenges that may exist on this domain. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-019-04061-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066029238&doi=10.1007%2fs00500-019-04061-9&partnerID=40&md5=049e49d578b6908b22ccb743d2f30e66,2019,2021-07-20 15:48:35,2021-07-20 15:48:35
R3KAQ6JV,journalArticle,2021,"Guo, Y.; Zhang, B.; Sun, Y.; Jiang, K.; Wu, K.",Machine learning based feature selection and knowledge reasoning for CBR system under big data,"Under big data, large number of features as well as their complex data types makes traditional feature selection and knowledge reasoning in CBR system not adapt to new condition. To solve these problems, first, this paper proposes Weighted Relative Probability Change of Solution Parameters (WRPCSP) algorithm to execute feature selection. Then, this paper integrates Bayesian network (BN) with CBR system for knowledge reasoning. Based on probability calculation and reasoning, WRPCSP algorithm together with BN allows the proposed CBR system to well work under big data. In addition, to overcome the efficiency problem caused by large number of features, this paper also proposes Group-Outside (GO) algorithm to assign the computing task of big data for parallel data processing. GO algorithm can make the computing capacity of Hadoop fully utilized to gain the least time costing for parallel data processing. Finally, lots of experiments are performed to validate the proposed method. © 2020",Pattern Recognition,10.1016/j.patcog.2020.107805,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098751098&doi=10.1016%2fj.patcog.2020.107805&partnerID=40&md5=1b1f21745e0f29c200d3f1ff162bf3f9,2021,2021-07-20 15:48:35,2021-07-20 15:48:35
FEXZ3FTL,journalArticle,2021,"Ramoa, A.; Condeço, J.; Fdez-Riverola, F.; Lourenço, A.","HaemoKBS: A knowledge-based system for real-time, continuous categorisation of adverse reactions in blood recipients","This work introduces HaemoKBS, a novel Haemovigilance decision support system for adverse reactions in blood recipients. Machine learning inference and rule-based reasoning were applied to build the underlying decision support models, namely to automatically extract evidence from different types of data included in hospital notifications and incorporate a priori expert knowledge. The ultimate aim is to dynamically learn and improve the reasoning abilities of the system and thus, be able to provide educated recommendations to hospital notifiers along with understandable explanations on the acquired knowledge. Experiments over the records of the Portuguese National Haemovigilance System from the last 10 years demonstrate the practical usefulness of HaemoKBS, which will contribute to a better depiction of the adverse reactions and to flag any incomplete notification enforcing data quality. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.04.101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086458649&doi=10.1016%2fj.neucom.2020.04.101&partnerID=40&md5=8df9265de7b5d3bedd95d4af6dc0f6dd,2021,2021-07-20 15:48:35,2021-07-20 15:48:35
LWBRNSMB,journalArticle,2020,"Belle, V.; De Raedt, L.",Semiring programming: A semantic framework for generalized sum product problems,"To solve hard problems, AI relies on a variety of disciplines such as logic, probabilistic reasoning, machine learning and mathematical programming. Although it is widely accepted that solving real-world problems requires an integration amongst these, contemporary representation methodologies offer little support for this. In an attempt to alleviate this situation, we position and motivate a new declarative programming framework in this paper. We focus on the semantical foundations in service of providing abstractions of well-known problems such as SAT, Bayesian inference, generative models, learning and convex optimization. Programs are understood in terms of first-order logic structures with semiring labels, which allows us to freely combine and integrate problems from different AI disciplines and represent non-standard problems over unbounded domains. Thus, the main thrust of this paper is to view such well-known problems through a unified lens in the hope that appropriate solver strategies (exact, approximate, portfolio or hybrid) may emerge that tackle real-world problems in a principled way. © 2020 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2020.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089937553&doi=10.1016%2fj.ijar.2020.08.001&partnerID=40&md5=efc1a8b4b625bdd2f02a47556ac5534a,2020,2021-07-20 15:48:35,2021-07-20 15:48:35
3YG74NCW,journalArticle,2021,"Zheng, W.; Yan, L.; Gou, C.; Wang, F.-Y.",KM4: Visual reasoning via Knowledge Embedding Memory Model with Mutual Modulation,"Visual reasoning is a special kind of visual question answering, which is essentially multi-step and compositional, and also requires intensive text-visual interaction. The most important and challenging problem of visual reasoning is to design an effective and robust visual reasoning model. To this end, there are two challenges to overcome. The first is that textual and visual information must be jointly considered to make accurate inferences about reasoning. The second is that existing deep learning-based works are often too specific to a particular task. To address these issues, we propose a knowledge memory embedding model with mutual modulation for visual reasoning. This approach learns not only knowledge-based embeddings derived from key–value memory network to make the full and joint of textual and visual information, but also exploits the prior knowledge to improve the performance with knowledge-based representation learning for applying other general reasoning tasks. Experimental results on four benchmarks show that the proposed approach significantly improves performance compared with other state-of-the-art methods, guarantees the robustness with our model. Most importantly, we apply our model to four reasoning tasks, and experimentally show that our model effectively supports relational reasoning and improves performance in several tasks and datasets. © 2020 Elsevier B.V.",Information Fusion,10.1016/j.inffus.2020.10.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092745169&doi=10.1016%2fj.inffus.2020.10.007&partnerID=40&md5=fa4153a6694d6e1c2a6e9a44a75ed731,2021,2021-07-20 15:48:35,2021-07-20 15:48:35
JNLYI6BL,journalArticle,2012,"Kumar, Y.J.; Salim, N.; Raza, B.",Cross-document structural relationship identification using supervised machine learning,"Multi document analysis has been a field of interest for decades and is still being actively researched until today. One example of such analysis could be for the task of multi document summarization which is meant to represent the concise description of the original documents. In this paper, we will focus on some special properties that multi document articles hold, specifically news articles. Information across news articles reporting on the same story are often related. Cross-document structure theory (CST) gives several relationships between pairs of sentences from different documents. Among them, we focus on four relations namely ""Identity"", ""Overlap"", ""Subsumption"", and ""Description"". Our aim is to automatically identify these CST relationships. We applied three machine learning techniques, i.e. SVM, neural network and our proposed case-based reasoning (CBR) model. Comparison between these techniques shows that the proposed CBR model yields better results. © 2012 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2012.06.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864771051&doi=10.1016%2fj.asoc.2012.06.017&partnerID=40&md5=dab67b68c87ad29fa6c5de6374229f2a,2012,2021-07-20 15:48:35,2021-07-20 15:48:35
6R2XFBJL,journalArticle,2020,"Chen, S.",Exploration of artistic creation of Chinese ink style painting based on deep learning framework and convolutional neural network model,"For the purpose of applying information technology to the creation of ink style painting, the algorithm of ink painting rendering based on the deep learning framework and convolutional neural network model is designed and improved. Firstly, the ink style rendering program is written in Python. Secondly, VGG under Caffe architecture and Illustration 2Vec models are transplanted to TensorFlow architecture, and the image is rendered in ink style based on deep learning framework and convolutional neural network model. Finally, based on Node.js, the server-side program for image ink style rendering is built. Among them, Express is adopted as the Web-side framework, and the front-end page effect is completed. The results show that the ink rendering logic program is applicable, and the expected purpose is achieved. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-019-03985-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080989861&doi=10.1007%2fs00500-019-03985-6&partnerID=40&md5=fa92a6fd90cee4b33520f7d9217f2639,2020,2021-07-20 15:48:35,2021-07-20 15:48:35
UFEDPFJX,journalArticle,2018,"Cho, Y.; Yoon, J.; Lee, S.",Using social network analysis and gradient boosting to develop a soccer win–lose prediction model,"We present the conceptual framework of a soccer win–lose prediction system (SWLPS) focused on passing distribution data (which is a representative characteristic of soccer) using social network analysis (SNA) and gradient boosting (GB). The general purpose of soccer predictions is to help the field supervisor design a strategy to win subsequent games using the derived information to improve and expand the coaching process. To implement and evaluate the proposed SWLPS, actual network indicators and predicted network indicators are generated using passing distribution data and SNA. The win–lose prediction is conducted using the GB machine learning technique. The performance of the SWLPS is analyzed through comparison with various machine learning techniques (i.e., support vector machine (SVM), neural network (NN), decision tree (DT), case-based reasoning (CBR), and logistic regression (LR)). The experimental results and analyses demonstrate that the network indicators generated through SNA can represent soccer team performance and that an accurate win–lose prediction system can be developed using GB technique. © 2018 Elsevier Ltd",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2018.04.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046805969&doi=10.1016%2fj.engappai.2018.04.010&partnerID=40&md5=c559cf225d04819bc97ef4047af72e5d,2018,2021-07-20 15:48:35,2021-07-20 15:48:35
SJ38TLUH,journalArticle,2012,"Lattner, A.D.; Cervone, G.",Ensemble modeling of transport and dispersion simulations guided by machine learning hypotheses generation,"In this article an approach is presented where machine learning classifiers are used to drive an ensemble modeling method of multiple atmospheric transport and dispersion simulations. The goal is to achieve a higher spread of the results with a lower number of ensemble simulations. Symbolic machine learning algorithms are used to define choices for the variation of meteorological input data, model parameters, model physics, based on their combined effects on the final dispersion calculations (i.e., construction of ensembles). The methodology uses an iterative approach with the aim to identify ensemble members leading to a more balanced distribution of results.The methodology is tested using real meteorological data from Istanbul, Turkey, simulating atmospheric releases along the Bosphorus channel. In an extensive evaluation, different settings of the approach are compared in a series of experiments. The results indicate that the desired effect of more balanced results of the ensemble members can be achieved by the approach. © 2012 Elsevier Ltd.",Computers and Geosciences,10.1016/j.cageo.2012.01.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866151975&doi=10.1016%2fj.cageo.2012.01.017&partnerID=40&md5=f83f9606670ac9efcb9e554d2dc24cfb,2012,2021-07-20 15:48:35,2021-07-20 15:48:35
2PNFHS7L,journalArticle,2021,"Liu, W.; Tang, J.; Liang, X.; Cai, Q.",Heterogeneous graph reasoning for knowledge-grounded medical dialogue system,"Beyond the common difficulties faced in task-oriented dialogue system, medical dialogue has recently attracted increasing attention due to its huge application potential while posing more challenges in reasoning over medical domain knowledge and logic. Existing works resort to neural language models for dialogue embedding and neglect the explicit logical reasoning, leading to poor explainable and generalization ability. In this work, we propose an explainable Heterogeneous Graph Reasoning (HGR) model to unify the relational dialogue context understanding and entity-correlation reasoning into a heterogeneous graph structure. HGR encodes entity context according to the corresponding utterance and deduces next response after fusing the underlying medical knowledge with entity context by attentional graph propagation. To push forward the future research on expert-sensitive task-oriented dialogue system, we first release a large-scale Medical Dialogue Consultant benchmark (MDG-C) with 16 Gastrointestinal diseases for evaluating consultant capability and a Medical Dialogue Diagnosis benchmark (MDG-D) with 6 diseases for measuring diagnosis capability of models, respectively. Extensive experiments on both MDG-C and MDG-D benchmarks demonstrate the superiority of our HGR over state-of-the-art knowledge grounded approaches in general fields of medical dialogue system. © 2021 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2021.02.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102968935&doi=10.1016%2fj.neucom.2021.02.021&partnerID=40&md5=6add2adafb53cee8074009061704a920,2021,2021-07-20 15:48:35,2021-07-20 15:48:35
A6RER5LE,journalArticle,2019,"Vasquez-Morales, G.R.; Martinez-Monterrubio, S.M.; Moreno-Ger, P.; Recio-Garcia, J.A.",Explainable Prediction of Chronic Renal Disease in the Colombian Population Using Neural Networks and Case-Based Reasoning,"This paper presents a neural network-based classifier to predict whether a person is at risk of developing chronic kidney disease (CKD). The model is trained with the demographic data and medical care information of two population groups: on the one hand, people diagnosed with CKD in Colombia during 2018, and on the other, a sample of people without a diagnosis of this disease. Once the model is trained and evaluation metrics for classification algorithms are applied, the model achieves 95% accuracy in the test data set, making its application for disease prognosis feasible. However, despite the demonstrated efficiency of the neural networks to predict CKD, this machine-learning paradigm is opaque to the expert regarding the explanation of the outcome. Current research on eXplainable AI proposes the use of twin systems, where a black-box machine-learning method is complemented by another white-box method that provides explanations about the predicted values. Case-Based Reasoning (CBR) has proved to be an ideal complement as this paradigm is able to find explanatory cases for an explanation-by-example justification of a neural network's prediction. In this paper, we apply and validate a NN-CBR twin system for the explanation of CKD predictions. As a result of this research, 3,494,516 people were identified as being at risk of developing CKD in Colombia, or 7% of the total population. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2948430,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078361922&doi=10.1109%2fACCESS.2019.2948430&partnerID=40&md5=09de5383da31106131ee2fa2664e558d,2019,2021-07-20 15:48:35,2021-07-20 15:48:35
E3NY8RZF,journalArticle,2019,"Torres, V.F.; Torres, F.S.",Resilient training of neural network classifiers with approximate computing techniques for hardware-optimised implementations,"As Machine Learning applications increase the demand for optimised implementations in both embedded and high-end processing platforms, the industry and research community have been responding with different approaches to implement these solutions. This work presents approximations to arithmetic operations and mathematical functions that, associated with a customised adaptive artificial neural networks training method, based on RMSProp, provide reliable and efficient implementations of classifiers. The proposed solution does not rely on mixed operations with higher precision or complex rounding methods that are commonly applied. The intention of this work is not to find the optimal simplifications for specific deep learning problems but to present an optimised framework that can be used as reliably as one implemented with precise operations, standard training algorithms and the same network structures and hyper-parameters. By simplifying the 'half-precision' floating point format and approximating exponentiation and square root operations, the authors' work drastically reduces the field programmable gate array implementation complexity (e.g. −43 and −57% in two of the component resources). The reciprocal square root approximation is so simple it could be implemented only with combination logic. In a full software implementation for a mixed-precision platform, only two of the approximations compensate the processing overhead of precision conversions. © The Institution of Engineering and Technology 2019.",IET Computers and Digital Techniques,10.1049/iet-cdt.2019.0036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074284660&doi=10.1049%2fiet-cdt.2019.0036&partnerID=40&md5=c3ca6e729f6ef07dff90478e8f3a3c01,2019,2021-07-20 15:48:36,2021-07-20 15:48:36
ZZAHFFRU,journalArticle,2019,"Hu, R.; Zhao, H.-M.; Xu, H.",A big data intelligence analysis expression method based on machine learning,"A dynamic intelligence expression method is presented in this paper, which uses big data analysis to represent the intelligence to be taken from Web. In this method, reasoning methods are used to create new ideas which can be added to field intelligence systems in favor of big data analysis. This is used for the generalization of the well-known analysis to implement rule based generalization. The method plans to produce a learning model which best take offs the class members of a marked rule base. The object categories are given by an interface which is represented by the standards of a mathematical method. The category is defined by the formula. In our big data method, the learned artificial intelligence model is represented by models and it is consisted of a best condition of expressions of a given category. We show that this feature gives scholar choices to get ideas into the application field. Furthermore, the expression according to models adds additional value to the function and enables to answer questions, which big data function method cannot. The big data expression of the models can be explained by scholar. The reasoning logic can be added to the existing artificial intelligence expression method. Additionally, the reasoning logic obtaining method can be used repeatedly. In each procedure, new ideas from the search step can be added to the reasoning rule sets to enhance the comprehensive characteristics of the presented reasoning methods. © 2017, Springer Science+Business Media, LLC, part of Springer Nature.",Cluster Computing,10.1007/s10586-017-1578-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039710022&doi=10.1007%2fs10586-017-1578-9&partnerID=40&md5=88c40148aad42105435361f150362159,2019,2021-07-20 15:48:36,2021-07-20 15:48:36
HF8M5WU5,journalArticle,2017,"Teney, D.; Wu, Q.; Van Den Hengel, A.",Visual Question Answering: A Tutorial,"The task of visual question answering (VQA) is receiving increasing interest from researchers in both the computer vision and natural language processing fields. Tremendous advances have been seen in the field of computer vision due to the success of deep learning, in particular on low- and midlevel tasks, such as image segmentation or object recognition. These advances have fueled researchers' confidence for tackling more complex tasks that combine vision with language and high-level reasoning. VQA is a prime example of this trend. This article presents the ongoing work in the field and the current approaches to VQA based on deep learning. VQA constitutes a test for deep visual understanding and a benchmark for general artificial intelligence (AI). While the field of VQA has seen recent successes, it remains a largely unsolved task. © 1991-2012 IEEE.",IEEE Signal Processing Magazine,10.1109/MSP.2017.2739826,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040308877&doi=10.1109%2fMSP.2017.2739826&partnerID=40&md5=545f4dc20a2485bbdfa7cb8641a9e4eb,2017,2021-07-20 15:48:36,2021-07-20 15:48:36
UN2ELMEG,journalArticle,2011,"Prentzas, J.; Hatzilygeroudis, I.",Neurules-A Type of Neuro-symbolic Rules: An Overview,"Neurules are a kind of integrated rules integrating neurocomputing and production rules. Each neurule is represented as an adaline unit. Thus, the corresponding neurule base consists of a number of autonomous adaline units (neurules). Due to this fact, a modular and natural knowledge base is constructed, in contrast to existing connectionist knowledge bases. In this paper, we present an overview of our main work involving neurules. We focus on aspects concerning construction of neurules, efficient updates of neurule bases, neurule-based inference and combination of neurules with case-based reasoning. Neurules may be constructed from either symbolic rule bases or empirical data in the form of training examples. Due to the fact that the source knowledge of neurules may change with time, efficient updates of corresponding neurule bases to reflect such changes are performed. Furthermore, the neurule-based inference mechanism is interactive and more efficient than the inference mechanism used in connectionist expert systems. Finally, neurules can be naturally combined with case-based reasoning to provide a more effective representation scheme that exploits multiple knowledge sources and provides enhanced reasoning capabilities. © Springer-Verlag Berlin Heidelberg 2011.","Smart Innovation, Systems and Technologies",10.1007/978-3-642-19618-8_9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875611088&doi=10.1007%2f978-3-642-19618-8_9&partnerID=40&md5=28cfe0939e764527c00dabf882416579,2011,2021-07-20 15:48:36,2021-07-20 15:48:36
9HJAZC4J,journalArticle,2018,"Fernandez-Labrador, C.; Perez-Yus, A.; Lopez-Nicolas, G.; Guerrero, J.J.",Layouts from panoramic images with geometry and deep learning,"In this letter, we propose a novel procedure for three-dimensional layout recovery of indoor scenes from single 360 circ panoramic images. With such images, all scene is seen at once, allowing us to recover closed geometries. Our method combines strategically the accuracy provided by geometric reasoning (lines and vanishing points) with the higher level of data abstraction and pattern recognition achieved by deep learning techniques (edge and normal maps). Thus, we extract structural corners from which we generate layout hypotheses of the room assuming Manhattan world. The best layout model is selected, achieving good performance on both simple rooms (box-type), and complex shaped rooms (with more than four walls). Experiments of the proposed approach are conducted within two public datasets, SUN360 and Stanford (2D-3D-S) demonstrating the advantages of estimating layouts by combining geometry and deep learning and the effectiveness of our proposal with respect to the state of the art. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2018.2850532,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051626057&doi=10.1109%2fLRA.2018.2850532&partnerID=40&md5=232017668bc849c720bdd5db2b25bf0f,2018,2021-07-20 15:48:36,2021-07-20 15:48:36
DAUD7PW2,journalArticle,2021,"Liu, X.; Lu, H.; Nayak, A.",A Spam Transformer Model for SMS Spam Detection,"In this paper, we aim to explore the possibility of the Transformer model in detecting the spam Short Message Service (SMS) messages by proposing a modified Transformer model that is designed for detecting SMS spam messages. The evaluation of our proposed spam Transformer is performed on SMS Spam Collection v.1 dataset and UtkMl&#x2019;s Twitter Spam Detection Competition dataset, with the benchmark of multiple established machine learning classifiers and state-of-the-art SMS spam detection approaches. In comparison to all other candidates, our experiments on SMS spam detection show that the proposed modified spam Transformer has the optimal results on the accuracy, recall, and F1-Score with the values of 98.92%, 0.9451, and 0.9613, respectively. Besides, the proposed model also achieves good performance on the UtkMl&#x2019;s Twitter dataset, which indicates a promising possibility of adapting the model to other similar problems. CCBYNCND",IEEE Access,10.1109/ACCESS.2021.3081479,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107221706&doi=10.1109%2fACCESS.2021.3081479&partnerID=40&md5=b86aa2909e96e2171ba2c43f986dc11a,2021,2021-07-20 15:48:36,2021-07-20 15:48:36
YS67B433,journalArticle,2020,"Ren, W.; Ma, O.; Ji, H.; Liu, X.",Human Posture Recognition Using a Hybrid of Fuzzy Logic and Machine Learning Approaches,"An autonomous assistive robot needs to recognize the body-limb posture of the person being assisted while he/she is lying in a bed to provide care services such as helping change the posture of the person or carrying him/her from the bed to a wheelchair. This paper presents a data-efficient classification of human postures when lying in a bed using a hybrid fuzzy logic and machine learning approach. The classifier was trained using a relatively small dataset containing 19,800 annotated depth images collected using Kinect from 32 test subjects lying in bed. An overall accuracy of 97.1% was achieved on the dataset. Furthermore, the image dataset including depth and red-green-blue (RGB) images, is available to the research community with the publication of this paper, with the hope that it can benefit other researchers. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3011697,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089345413&doi=10.1109%2fACCESS.2020.3011697&partnerID=40&md5=46baee59b82c35d0ce1d788dfe1e1a67,2020,2021-07-20 15:48:36,2021-07-20 15:48:36
RFAHZZH8,journalArticle,2020,"Zhao, J.; Liu, X.; He, S.; Sun, S.",Probabilistic inference of Bayesian neural networks with generalized expectation propagation,"Deep learning plays an important role in the field of machine learning. However, deterministic methods such as neural networks cannot capture the model uncertainty. Bayesian neural network (BNN) are recently under consideration since Bayesian models provide a theoretical framework to infer model uncertainty. Since it is often difficult to find an analytical solution for BNNs, an effective and efficient approximate inference method is very important for model training and prediction. The generalized version of expectation propagation (GEP) was recently proposed and considered a powerful approximate inference method, which is based on the minimization of Kullback–Leibler (KL) divergence of the true posterior and the approximate distributions. In this paper, we further instantiate the GEP to provide an effective and efficient approximate inference method for BNNs. We assess this method on BNNs including fully connected neural networks and convolutional neural networks on multiple benchmark datasets and show a better performance than some state-of-the-art approximate inference methods. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.06.060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088021650&doi=10.1016%2fj.neucom.2020.06.060&partnerID=40&md5=57a0f7e026c35d1cb85f48404a166fb6,2020,2021-07-20 15:48:36,2021-07-20 15:48:36
SX2W3B6B,journalArticle,2019,"McIntosh, A.; Hassan, S.; Hindle, A.",What can Android mobile app developers do about the energy consumption of machine learning?,"Machine learning is a popular method of learning functions from data to represent and to classify sensor inputs, multimedia, emails, and calendar events. Smartphone applications have been integrating more and more intelligence in the form of machine learning. Machine learning functionality now appears on most smartphones as voice recognition, spell checking, word disambiguation, face recognition, translation, spatial reasoning, and even natural language summarization. Excited app developers who want to use machine learning on mobile devices face one serious constraint that they did not face on desktop computers or cloud virtual machines: the end-user’s mobile device has limited battery life, thus computationally intensive tasks can harm end users’ phone availability by draining batteries of their stored energy. Currently, there are few guidelines for developers who want to employ machine learning on mobile devices yet are concerned about software energy consumption of their applications. In this paper, we combine empirical measurements of different machine learning algorithm implementations with complexity theory to provide concrete and theoretically grounded recommendations to developers who want to employ machine learning on smartphones. We conclude that some implementations of algorithms, such as J48, MLP, and SMO, do generally perform better than others in terms of energy consumption and accuracy, and that energy consumption is well-correlated to algorithmic complexity. However, to achieve optimal results a developer must consider their specific application as many factors — dataset size, number of data attributes, whether the model will require updating, etc. — affect which machine learning algorithm and implementation will provide the best results. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Empirical Software Engineering,10.1007/s10664-018-9629-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047981983&doi=10.1007%2fs10664-018-9629-2&partnerID=40&md5=ac8bac9682f550c15be749aae7429b97,2019,2021-07-20 15:48:36,2021-07-20 15:48:36
AT28KRG8,journalArticle,2020,"Mirzakhanov, V.E.",Value of fuzzy logic for data mining and machine learning: A case study,"In this paper, a case study on the role of fuzzy logic (FL) in data mining and machine learning is carried out. It is outlined that, in order to draw more attention of data-mining and machine-learning communities to FL, studies on FL could be more focused not on the activities that fuzzy methods can perform better but rather on the activities that fuzzy methods can perform and the non-fuzzy ones can't. Such approach takes us away from discussing quantitative differences between fuzzy and non-fuzzy methods to discussing qualitative differences, which are possibly more favorable objects of scientific curiosity. Following the outlined suggestion, a novel speed-up technique is proposed in this paper to support association rule mining (ARM). The proposed technique is a clustering-based one and provides fusion of clustering and ARM. The catchy feature of this technique is that it works well if applied in fuzzy ARM and doesn't work well if applied in non-fuzzy ARM. The proposed technique is put through experimental verification involving several real-world datasets, and the results substantiate its effectiveness. © 2020 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2020.113781,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089080243&doi=10.1016%2fj.eswa.2020.113781&partnerID=40&md5=5d11696efea22f6bcc583041c6694bb6,2020,2021-07-20 15:48:36,2021-07-20 15:48:36
YAYYLU6C,journalArticle,2020,"Ma, C.; Mohammadzadeh, A.; Turabieh, H.; Mafarja, M.; Band, S.S.; Mosavi, A.",Optimal Type-3 Fuzzy System for Solving Singular Multi-Pantograph Equations,In this study a new machine learning technique is presented to solve singular multi-pantograph differential equations (SMDEs). A new optimized type-3 fuzzy logic system (T3-FLS) by unscented Kalman filter (UKF) is proposed for solution estimation. The convergence and stability of presented algorithm are ensured by the suggested Lyapunov analysis. By two SMDEs the effectiveness and applicability of the suggested method is demonstrated. The statistical analysis show that the suggested method results in accurate and robust performance and the estimated solution is well converged to the exact solution. The proposed algorithm is simple and can be applied on various SMDEs with variable coefficients. © 2013 IEEE.,IEEE Access,10.1109/ACCESS.2020.3044548,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098563867&doi=10.1109%2fACCESS.2020.3044548&partnerID=40&md5=ac36366d4062a9d66bbf78247c64fe93,2020,2021-07-20 15:48:36,2021-07-20 15:48:36
W5QEYKDB,journalArticle,2021,"Louati, A.; Louati, H.; Li, Z.",Deep learning and case-based reasoning for predictive and adaptive traffic emergency management,"An efficient traffic signal control system (TSCS) should not only be reactive to the current traffic but also be predictive by anticipating future traffic disturbances. In this study, we investigate the potential of using convolution neural network (CNN) in detecting emergency cases and forecasting events that can interrupt the traffic flow. Case-based reasoning (CBR) is then exploited to react to detected and forecasted events. We further develop an adapted Reinforcement Leaning (RL) algorithm in building and enhancing the case bases. The proposed system inherits the advantages of CNN, CBR, and RL, which allow detection, prediction, control, evaluation, and learning in a unified framework. To assess the proposed TSCS, we compare our approach with a set of state-of-art algorithms (e.g., multi-agent preemptive case-based reasoning algorithm and multi-agent preemptive longest queue first—maximal weight matching). The proposed TSCS outperforms the benchmarking algorithms through experiments in various traffic scenarios. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Supercomputing,10.1007/s11227-020-03435-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091885884&doi=10.1007%2fs11227-020-03435-3&partnerID=40&md5=72ecc99ea4dba3dc3293229bb85aac67,2021,2021-07-20 15:48:36,2021-07-20 15:48:36
ZC3UYALA,journalArticle,2016,"Hentech, R.; Jenhani, I.; Elouedi, Z.",Possibilistic AIRS induction from uncertain data,"This paper presents a new approach in machine learning, especially, in supervised classification and reasoning under uncertainty. For many classification problems, uncertainty is often inherent in modeling applications and should be treated carefully and not rejected to make better decisions. Artificial immune recognition system (AIRS) is a well-known classifier that has provided good results with certain data. However, this method is not able to cope with uncertainty. To overcome this limitation, we propose a new classification approach combining the AIRS and possibility theory. The new approach is allowing to deal with uncertain attribute and also class values of training instances. The uncertainty is expressed via possibility distributions. Experimentations on real datasets from the U.C.I machine learning repository show good performances of the proposed approach. © 2015, Springer-Verlag Berlin Heidelberg.",Soft Computing,10.1007/s00500-015-1627-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952976875&doi=10.1007%2fs00500-015-1627-3&partnerID=40&md5=7b660f4595e0dd6503c072284fb2afe8,2016,2021-07-20 15:48:36,2021-07-20 15:48:36
6PBV4L54,journalArticle,2015,"Chen, S.-M.; Hsin, W.-C.",Weighted Fuzzy Interpolative Reasoning Based on the Slopes of Fuzzy Sets and Particle Swarm Optimization Techniques,"In this paper, we propose a new weighted fuzzy interpolative reasoning method for sparse fuzzy rule-based systems based on the slopes of fuzzy sets. We also propose a particle swarm optimization (PSO)-based weights-learning algorithm to automatically learn the optimal weights of the antecedent variables of fuzzy rules for weighted fuzzy interpolative reasoning. We apply the proposed weighted fuzzy interpolative reasoning method using the proposed PSO-based weights-learning algorithm to deal with the computer activity prediction problem, the multivariate regression problems, and the time series prediction problems. The experimental results show that the proposed weighted fuzzy interpolative reasoning method using the proposed PSO-based weights-learning algorithm outperforms the existing methods for dealing with the computer activity prediction problem, the multivariate regression problems, and the time series prediction problems. © 2013 IEEE.",IEEE Transactions on Cybernetics,10.1109/TCYB.2014.2347956,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933047118&doi=10.1109%2fTCYB.2014.2347956&partnerID=40&md5=c95ecd975db8e5cc72ecb7fb069f7ef7,2015,2021-07-20 15:48:36,2021-07-20 15:48:36
UEXNC74X,journalArticle,2017,"Fan, J.; Chow, T.",Deep learning based matrix completion,"Previous matrix completion methods are generally based on linear and shallow models where the given incomplete matrices are of low-rank and the data are assumed to be generated by linear latent variable models. In this paper, we first propose a novel method called AutoEncoder based matrix completion (AEMC). The main idea of AEMC is to utilize the partially observed data to learn and construct a nonlinear latent variable model in the form of AutoEncoder. The hidden layer of the AutoEncoder has much fewer units than the visible layers do. Meanwhile, the unknown entries of the data are recovered to fit the nonlinear latent variable model. Based on AEMC, we further propose a deep learning based matrix completion (DLMC) method. In DLMC, AEMC is used as a pre-training step for both the missing entries and network parameters; the hidden layer of AEMC is then used to learn stacked AutoEncoders (SAEs) with greedy layer-wise training; finally, fine-tuning is carried out on the deep network formed by AEMC and SAEs to obtain the missing entries of the data and the parameters of the network. In addition, we also provide out-of-sample extensions for AEMC and DLMC to recover online incomplete data. AEMC and DLMC are compared with state-of-the-art methods in the tasks of synthetic matrix completion, image inpainting, and collaborative filtering. The experimental results verify the effectiveness and superiority of the proposed methods. © 2017 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2017.05.074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020903809&doi=10.1016%2fj.neucom.2017.05.074&partnerID=40&md5=5878a36bf16ed71e6d7ea469534613ea,2017,2021-07-20 15:48:37,2021-07-20 15:48:37
ZHA5CY6A,journalArticle,2018,"Ontañón, S.; Shokoufandeh, A.",Refinement operators for directed labeled graphs with applications to instance-based learning,"This paper presents a collection of refinement operators for directed labeled graphs (DLGs), and a family of distance and similarity measures based on them. We build upon previous work on refinement operators for other representations such as feature terms and description logic models. Specifically, we present eight refinement operators for DLGs, which will allow for the adaptation of three similarity measures to DLGs: the anti-unification-based, Sλ, the property-based, Sπ, and the weighted property-based, Swπ, similarities. We evaluate the resulting measures empirically, comparing them to existing similarity measures for structured data in the context of instance-based machine learning. © 2018 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2018.08.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053324162&doi=10.1016%2fj.knosys.2018.08.006&partnerID=40&md5=1f4ffc631abd2dfb7d0b98e4f34e3fc1,2018,2021-07-20 15:48:37,2021-07-20 15:48:37
WXL6C38K,journalArticle,2018,"Srajer, F.; Kukelova, Z.; Fitzgibbon, A.",A benchmark of selected algorithmic differentiation tools on some problems in computer vision and machine learning,"Algorithmic differentiation (AD) allows exact computation of derivatives given only an implementation of an objective function. Although many AD tools are available, a proper and efficient implementation of AD methods is not straightforward. The existing tools are often too different to allow for a general test suite. In this paper, we compare 15 ways of computing derivatives including 11 automatic differentiation tools implementing various methods and written in various languages (C++, F#, MATLAB, Julia and Python), 2 symbolic differentiation tools, finite differences and hand-derived computation. We look at three objective functions from computer vision and machine learning. These objectives are for the most part simple, in the sense that no iterative loops are involved, and conditional statements are encapsulated in functions such as abs or logsumexp. However, it is important for the success of AD that such ‘simple’ objective functions are handled efficiently, as so many problems in computer vision and machine learning are of this form. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",Optimization Methods and Software,10.1080/10556788.2018.1435651,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042907340&doi=10.1080%2f10556788.2018.1435651&partnerID=40&md5=4bef21d3af755dc6a9b5bdd109135ad6,2018,2021-07-20 15:48:37,2021-07-20 15:48:37
JS6XTEN8,journalArticle,2020,"Buckner, C.",Understanding adversarial examples requires a theory of artefacts for deep learning,"Deep neural networks are currently the most widespread and successful technology in artificial intelligence. However, these systems exhibit bewildering new vulnerabilities: most notably a susceptibility to adversarial examples. Here, I review recent empirical research on adversarial examples that suggests that deep neural networks may be detecting in them features that are predictively useful, though inscrutable to humans. To understand the implications of this research, we should contend with some older philosophical puzzles about scientific reasoning, helping us to determine whether these features are reliable targets of scientific investigation or just the distinctive processing artefacts of deep neural networks. © 2020, Springer Nature Limited.",Nature Machine Intelligence,10.1038/s42256-020-00266-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096456531&doi=10.1038%2fs42256-020-00266-y&partnerID=40&md5=544109111be87b4b710889e07ee9e7b8,2020,2021-07-20 15:48:37,2021-07-20 15:48:37
TADKJC3H,journalArticle,2021,"Ardakani, A.; Ardakani, A.; Gross, W.J.",Training Binarized Neural Networks using Ternary Multipliers,"Deep learning offers the promise of intelligent devices that are able to perceive, reason and take intuitive actions. The rising adoption of deep learning techniques has motivated researchers and developers to seek low-cost and high-speed software/hardware solutions for deployment on smart devices. In recent years work has focused on reducing the complexity of inference in deep neural networks, however, a method enabling low-complexity on-chip learning is still missing. In this work, we introduce a gradient estimation method that performs back-propagation with ternary (2-bit) quantized gradients. Our method replaces all full-precision multipliers (i.e., 16-bit fixed-point multipliers) in neural network training with ternary operators while maintaining a comparable accuracy. Furthermore, we propose a new stochastic computing-based neural network (SC-based NN) which employs a new stochastic representation (i.e., dynamic sign-magnitude stochastic sequence). Our proposed SC-based NN produces state-of-the-art results while using shorter sequence lengths (i.e., sequence length of 16) compared to its SC-based counterparts. IEEE",IEEE Design and Test,10.1109/MDAT.2021.3063356,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102259492&doi=10.1109%2fMDAT.2021.3063356&partnerID=40&md5=0a49a6652051d0f570c7051d0c3433d5,2021,2021-07-20 15:48:37,2021-07-20 15:48:37
SSFQQ8XR,journalArticle,2021,"Khan, M.J.; Khan, C.",Performance evaluation of fuzzy clustered case-based reasoning,"Case-based reasoning (CBR) is a nature-inspired machine learning technique. It solves a new problem using the existing similar problems with their solutions stored in central repository known as case-base. It results in continuous growth of the case-base enhancing the problem solving capability of the system but at the same time compromising the performance. First performance challenge is continuous growth of the case-base. Second performance challenge is to handle the performance bottleneck without compromising the relevance of cases with their neighbourhood to solve new problems. Different approaches have been introduced in literature to address this performance challenge. In this work, a knowledge-base maintenance approach using fuzzy clustering has been presented which takes care of the performance bottleneck and does not compromise the problem solving capability of CBR. Performance of the proposed approach has been evaluated on different case-bases and results have been compared with the conventional CBR approach. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",Journal of Experimental and Theoretical Artificial Intelligence,10.1080/0952813X.2020.1744194,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082942629&doi=10.1080%2f0952813X.2020.1744194&partnerID=40&md5=4e85762f609527e27114b3a33a7cf8f9,2021,2021-07-20 15:48:37,2021-07-20 15:48:37
57NJKR3Q,journalArticle,2018,"Houeland, T.G.; Aamodt, A.",A learning system based on lazy metareasoning,"Metareasoning has been widely studied in the literature, with a wide variety of algorithms and partially overlapping methodological approaches. However, these methods are typically either not targeted toward practical machine learning systems or alternatively are focused on achieving the best possible performance for a particular domain, with extensive human tuning and research, and vast computing resources. In this paper, our goal is to create systems that perform sustained autonomous learning, with automatically determined domain-specific optimizations for any given domain, and without requiring human assistance. We present Alma, a metareasoning architecture that creates and selects reasoning methods based on empirically observed performance. This is achieved by using lazy learning at the metalevel, and automatically training and combining reasoning methods at run-time. In experiments across diverse data sets, we demonstrate the ability of Alma to successfully reason about learner performance in different domains and achieve a better overall result than any of the individual reasoning methods, even with limited computing time available. © 2017, Springer-Verlag GmbH Germany.",Progress in Artificial Intelligence,10.1007/s13748-017-0138-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056120994&doi=10.1007%2fs13748-017-0138-0&partnerID=40&md5=75b79eac1b12b3320289c48f183eb2c3,2018,2021-07-20 15:48:37,2021-07-20 15:48:37
ZMR83WWL,journalArticle,2019,"Malathi, D.; Logesh, R.; Subramaniyaswamy, V.; Vijayakumar, V.; Sangaiah, A.K.",Hybrid Reasoning-based Privacy-Aware Disease Prediction Support System,"Recent developments in Information and Communication Technologies (ICT) and online healthcare services have created a huge volume of health data. With the advancements in machine learning approaches, the research on Disease Prediction Support System (DPSS) has attracted many researchers globally. In this article, we present a hybrid reasoning-based methodology on predicting diseases. The combinatorial advantage of Fuzzy sety theory, k-nearest neighbor and case-based reasoning helps to yield enhanced prediction results. Though DPSS facilitates promising healthcare services, data security and privacy are still crucial challenging issues to be addressed. The DPSS is extended as a Privacy-Aware Disease Prediction Support System (PDPSS) using Paillier Homomorphic Encryption to preserve patients’ sensitive information from unauthorized user access. The proposed prediction model is evaluated with the statistical evaluation metrics, and the experimental results reveal the improved performance of PDPSS in enhanced prediction accuracy and better security. © 2018 Elsevier Ltd",Computers and Electrical Engineering,10.1016/j.compeleceng.2018.11.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056580402&doi=10.1016%2fj.compeleceng.2018.11.009&partnerID=40&md5=f55b378268a788b4cef1322a89466fb3,2019,2021-07-20 15:48:37,2021-07-20 15:48:37
4K274QZ9,journalArticle,2020,"Kommenda, M.; Burlacu, B.; Kronberger, G.; Affenzeller, M.",Parameter identification for symbolic regression using nonlinear least squares,"In this paper we analyze the effects of using nonlinear least squares for parameter identification of symbolic regression models and integrate it as local search mechanism in tree-based genetic programming. We employ the Levenberg–Marquardt algorithm for parameter optimization and calculate gradients via automatic differentiation. We provide examples where the parameter identification succeeds and fails and highlight its computational overhead. Using an extensive suite of symbolic regression benchmark problems we demonstrate the increased performance when incorporating nonlinear least squares within genetic programming. Our results are compared with recently published results obtained by several genetic programming variants and state of the art machine learning algorithms. Genetic programming with nonlinear least squares performs among the best on the defined benchmark suite and the local search can be easily integrated in different genetic programming algorithms as long as only differentiable functions are used within the models. © 2019, The Author(s).",Genetic Programming and Evolvable Machines,10.1007/s10710-019-09371-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076628069&doi=10.1007%2fs10710-019-09371-3&partnerID=40&md5=6e7b9ae9dcdf5f4134a0619d63acb2bd,2020,2021-07-20 15:48:37,2021-07-20 15:48:37
ED9VWN2P,journalArticle,2021,"Pintelas, E.; Liaskos, M.; Livieris, I.E.; Kotsiantis, S.; Pintelas, P.",A novel explainable image classification framework: case study on skin cancer and plant disease prediction,"An explainable/interpretable machine learning model is able to make reasoning about its predictions in understandable terms to humans. These properties are essential in order to trust model’s predictions, especially when these decisions affect critical aspects such as health, rights, security, and educational issues. Image classification is an area in machine learning and computer vision in which convolutional neural networks have flourished since they have shown remarkable performance in such problems. However, these models suffer in terms of transparency, interpretability and explainability, considered as black box models. This work proposes a novel explainable image classification framework applying it on skin cancer and plant diseases prediction problems. This framework combines segmentation and clustering techniques aiming to extract texture features from various subregions of the input image. Then, a feature filtering and cleaning procedure is applied on these extracted features in order to ensure that the proposed model will be also reliable and trustful, while these final extracted features are utilized for training an intrinsic linear white box prediction model. Finally, a hierarchy-based tree approach was created, in order to provide a meaningful interpretation of the model’s decision behavior. The experimental results have shown that the model’s explanations are clearly understandable, reliable, and trustful. Furthermore, regarding the prediction accuracy, the proposed model manages to achieve almost equal performance score (1–2% difference on average) comparing to the state-of-the-art black box convolutional image classification models. Such performance is considered noticeably good since the proposed classifier is an explainable intrinsic white box model. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-021-06141-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107463085&doi=10.1007%2fs00521-021-06141-0&partnerID=40&md5=2c79a2dfbdf9e0434c3433b1d4f6f209,2021,2021-07-20 15:48:37,2021-07-20 15:48:37
LXVVU9IG,journalArticle,2019,"Makni, B.; Hendler, J.",Deep learning for noise-tolerant RDFS reasoning,"Since the 2001 envisioning of the Semantic Web (SW) (Scientific American 284(5) (2001) 34-43), the main research focus in SW reasoning has been on the soundness and completeness of reasoners. While these reasoners assume the veracity of input data, the reality is that the Web of data is inherently noisy. Although there has been recent work on noise-tolerant reasoning, it has focused on type inference rather than full RDFS reasoning. Even though RDFS closure generation can be seen as a Knowledge Graph (KG) completion problem, the problem setting is different - making KG embedding techniques that were designed for link prediction not suitable for RDFS reasoning. This paper documents a novel approach that extends noise-tolerance in the SW to full RDFS reasoning. Our embedding technique - that is tailored for RDFS reasoning - consists of layering RDF graphs and encoding them in the form of 3D adjacency matrices where each layer layout forms a graph word. Each input graph and its entailments are then represented as sequences of graph words, and RDFS inference can be formulated as translation of these graph words sequences, achieved through neural machine translation. Our evaluation on LUBM1 synthetic dataset shows 97 % validation accuracy and 87.76 % on a subset of DBpedia while demonstrating a noise-tolerance unavailable with rule-based reasoners. © 2019 - IOS Press and the authors. All rights reserved.",Semantic Web,10.3233/SW-180363,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072370517&doi=10.3233%2fSW-180363&partnerID=40&md5=b3a71ad3e5347c72d3ad65d01bb1e67a,2019,2021-07-20 15:48:37,2021-07-20 15:48:37
Q7NAWFG3,journalArticle,2012,"Ioannides, C.; Eder, K.I.",Coverage-directed test generation automated by machine learning - A review,"The increasing complexity and size of digital designs, in conjunction with the lack of a potent verification methodology that can effectively cope with this trend, continue to inspire engineers and academics in seeking ways to further automate design verification. In an effort to increase performance and to decrease engineering effort, research has turned to artificial intelligence (AI) techniques for effective solutions. The generation of tests for simulation-based verification can be guided by machine-learning techniques. In fact, recent advances demonstrate that embedding machine-learning (ML) techniques into a coverage-directed test generation (CDG) framework can effectively automate the test generation process, making it more effective and less error-prone. This article reviews some of the most promising approaches in this field, aiming to evaluate the approaches and to further stimulate more directed research in this area. © 2012 ACM.",ACM Transactions on Design Automation of Electronic Systems,10.1145/2071356.2071363,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857846062&doi=10.1145%2f2071356.2071363&partnerID=40&md5=c9812111b3cb6cbb3e5ecc5941f6bcd5,2012,2021-07-20 15:48:37,2021-07-20 15:48:37
J9SKSF6N,journalArticle,2019,"Huang, Z.; England, M.; Wilson, D.J.; Bridge, J.; Davenport, J.H.; Paulson, L.C.",Using Machine Learning to Improve Cylindrical Algebraic Decomposition,"Cylindrical Algebraic Decomposition (CAD) is a key tool in computational algebraic geometry, best known as a procedure to enable Quantifier Elimination over real-closed fields. However, it has a worst case complexity doubly exponential in the size of the input, which is often encountered in practice. It has been observed that for many problems a change in algorithm settings or problem formulation can cause huge differences in runtime costs, changing problem instances from intractable to easy. A number of heuristics have been developed to help with such choices, but the complicated nature of the geometric relationships involved means these are imperfect and can sometimes make poor choices. We investigate the use of machine learning (specifically support vector machines) to make such choices instead. Machine learning is the process of fitting a computer model to a complex function based on properties learned from measured data. In this paper we apply it in two case studies: the first to select between heuristics for choosing a CAD variable ordering; the second to identify when a CAD problem instance would benefit from Gröbner Basis preconditioning. These appear to be the first such applications of machine learning to Symbolic Computation. We demonstrate in both cases that the machine learned choice outperforms human developed heuristics. © 2019, The Author(s).",Mathematics in Computer Science,10.1007/s11786-019-00394-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069154868&doi=10.1007%2fs11786-019-00394-8&partnerID=40&md5=2bc2462332cec8bd54365a3c6e10c3fb,2019,2021-07-20 15:48:37,2021-07-20 15:48:37
HKSWXY29,journalArticle,2018,"Plappert, M.; Mandery, C.; Asfour, T.",Learning a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks,"Linking human whole-body motion and natural language is of great interest for the generation of semantic representations of observed human behaviors as well as for the generation of robot behaviors based on natural language input. While there has been a large body of research in this area, most approaches that exist today require a symbolic representation of motions (e.g. in the form of motion primitives), which have to be defined a-priori or require complex segmentation algorithms. In contrast, recent advances in the field of neural networks and especially deep learning have demonstrated that sub-symbolic representations that can be learned end-to-end usually outperform more traditional approaches, for applications such as machine translation. In this paper we propose a generative model that learns a bidirectional mapping between human whole-body motion and natural language using deep recurrent neural networks (RNNs) and sequence-to-sequence learning. Our approach does not require any segmentation or manual feature engineering and learns a distributed representation, which is shared for all motions and descriptions. We evaluate our approach on 2 846 human whole-body motions and 6 187 natural language descriptions thereof from the KIT Motion-Language Dataset. Our results clearly demonstrate the effectiveness of the proposed model: We show that our model generates a wide variety of realistic motions only from descriptions thereof in form of a single sentence. Conversely, our model is also capable of generating correct and detailed natural language descriptions from human motions. © 2018 Elsevier B.V.",Robotics and Autonomous Systems,10.1016/j.robot.2018.07.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052875451&doi=10.1016%2fj.robot.2018.07.006&partnerID=40&md5=9edc44fcf708e88fc2a8a74deba0296b,2018,2021-07-20 15:48:37,2021-07-20 15:48:37
Y5DEU66R,journalArticle,2019,"Dai, X.; Zhao, X.; Jin, P.; Cai, X.; Zhang, H.; Yang, C.; Li, B.",Opera-oriented character relations extraction for role interaction and behaviour Understanding: a deep learning approach,"There are a great number of complex relations among different characters in an opera. Retrieving such relations is crucial for performers and audience to accurately understand the features and behaviour of roles. Aiming to automatically extract relations among characters in an opera, in this paper we propose an effective method that can extract character relations from opera scripts. Firstly, we construct a uniform reasoning framework for opera scripts. Based on this model, we propose a deep syntax-parsing method to detect character relations from opera scripts. After that, we propose a new deep learning approach called SL-Bi-LSTM-CRF to extract the objects involved in character relations. The proposed SL-Bi-LSTM-CRF algorithm is a sentence-level relation extraction algorithm based on the Bi-directional LSTM with a CRF layer. With this mechanism, we are able to get a detailed description for character relations. We conduct experiments on a real dataset of opera scripts. The experimental results in terms of precision, recall, and F-score suggest the effectiveness of our proposal. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",Behaviour and Information Technology,10.1080/0144929X.2019.1584246,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061991420&doi=10.1080%2f0144929X.2019.1584246&partnerID=40&md5=11bbf1c511988eb74e99678a9026bcfb,2019,2021-07-20 15:48:38,2021-07-20 15:48:38
IMUWQCJG,journalArticle,2019,"Amaral, R.P.F.; Ribeiro, M.V.; de Aguiar, E.P.",Type-1 and singleton fuzzy logic system trained by a fast scaled conjugate gradient methods for dealing with binary classification problems,"This work introduces the type-1 and singleton fuzzy logic system trained by scaled conjugate gradient method and its usage for binary classification problems. Aiming to improve the performance of the training procedure, we propose the multiplication of the Hessian matrix by the directional vector using the so-called differential operator R·, which results in another proposal for training the aforementioned fuzzy logic system. In order to evaluate the proposals, performance analyses based on well-known data sets provided by UCI Machine Learning Repository and Knowledge Extraction based on Evolutionary Learning Repository together with well-established metrics are detailed. The numerical results show that the proposals achieve improvements in comparison with others gradient based training methods applied to the type-1 fuzzy logic system present in the literature. These improvements regard to the fast convergence speed under the constraint over the number of epochs during the training phase. © 2019",Neurocomputing,10.1016/j.neucom.2019.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065533996&doi=10.1016%2fj.neucom.2019.05.002&partnerID=40&md5=6e7c1a10dcfe9bc03b88e9bee58054bd,2019,2021-07-20 15:48:38,2021-07-20 15:48:38
HIJL2VU6,journalArticle,2019,"Cropper, A.; Muggleton, S.H.",Learning efficient logic programs,"When machine learning programs from data, we ideally want to learn efficient rather than inefficient programs. However, existing inductive logic programming (ILP) techniques cannot distinguish between the efficiencies of programs, such as permutation sort (n!) and merge sort O(nlogn). To address this limitation, we introduce Metaopt, an ILP system which iteratively learns lower cost logic programs, each time further restricting the hypothesis space. We prove that given sufficiently large numbers of examples, Metaopt converges on minimal cost programs, and our experiments show that in practice only small numbers of examples are needed. To learn minimal time-complexity programs, including non-deterministic programs, we introduce a cost function called tree cost which measures the size of the SLD-tree searched when a program is given a goal. Our experiments on programming puzzles, robot strategies, and real-world string transformation problems show that Metaopt learns minimal cost programs. To our knowledge, Metaopt is the first machine learning approach that, given sufficient numbers of training examples, is guaranteed to learn minimal cost logic programs, including minimal time-complexity programs. © 2018, The Author(s).",Machine Learning,10.1007/s10994-018-5712-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045856425&doi=10.1007%2fs10994-018-5712-6&partnerID=40&md5=a476b40df794375ab07b5999e70bdade,2019,2021-07-20 15:48:38,2021-07-20 15:48:38
PPIJBUP8,journalArticle,2020,"White, G.; Clarke, S.",Urban Intelligence with Deep Edges,"With the increased accuracy available from state of the art deep learning models and new embedded devices at the edge of the network capable of running and updating these models there is potential for urban intelligence at the edge of the network. The physical proximity of these edge devices will allow for intelligent reasoning one hop away from data generation. This will allow a range of modern urban reasoning applications that require reduced latency and jitter such as remote surgery, vehicle collision detection and augmented reality. The traffic flow from IoT devices to the cloud will also be reduced as with the increased accuracy from deep learning models only a subset of the data will need to be reported after a first pass analysis. However, the training time of deep learning models can be long, taking weeks on multiple desktop GPUs for large datasets. In this paper we show how transfer learning can be used to update the last layers of pre-trained models at the edge of the network, dramatically reducing the training time and allowing the model to perform new tasks without data ever having to be sent to the cloud. This will also improve the users' privacy, which is a key requirement for urban intelligence applications with the introduction of GDPR. We compare our approach to alternative IoT urban intelligence architectures such as cloud-based architectures and deep learning algorithms trained only on local data. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2963912,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078528624&doi=10.1109%2fACCESS.2020.2963912&partnerID=40&md5=e82484535e4625c25650fbf36fcaa288,2020,2021-07-20 15:48:38,2021-07-20 15:48:38
YEYDA8HI,journalArticle,2019,"Iliou, T.; Konstantopoulou, G.; Ntekouli, M.; Lymperopoulou, C.; Assimakopoulos, K.; Galiatsatos, D.; Anastassopoulos, G.",ILIOU machine learning preprocessing method for depression type prediction,"The main objective of this study was to find a data preprocessing method to boost the prediction performance of the machine learning algorithms in datasets of mental patients. Specifically, the machine learning methods must have almost excellent classification results in patients with depression, in order to achieve the sooner the possible the appropriate treatment. In this paper, we establish ILIOU data preprocessing method for Depression type detection. The performance of ILIOU data preprocessing method and principal component analysis preprocessing method was evaluated using the tenfold cross validation method assessing seven machine learning classification algorithms, nearest-neighbour classifier (IB1), C4.5 algorithm implementation (J48), random forest, multilayer perceptron (MLP), support vector machine (SMO), JRIP and fuzzy logic (FURIA), respectively. The classification results are presented and compared analytically. The experimental results reveal that the transformed dataset with new features after ILIOU preprocessing method implementation to the original dataset achieved 100% classification–prediction performance of the classification algorithms. So ILIOU data preprocessing method can be used for significantly boost classification algorithms performance in similar datasets and can be used for depression type prediction. © 2017, Springer-Verlag GmbH Germany.",Evolving Systems,10.1007/s12530-017-9205-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065170434&doi=10.1007%2fs12530-017-9205-9&partnerID=40&md5=a2dd710abb8817deee59705893192076,2019,2021-07-20 15:48:38,2021-07-20 15:48:38
8I89SD4K,journalArticle,2020,"Ul Islam, R.; Hossain, M.S.; Andersson, K.",A deep learning inspired belief rule-based expert system,"Recent technological advancements in the area of the Internet of Things (IoT) and cloud services, enable the generation of large amounts of raw data. However, the accurate prediction by using this data is considered as challenging for machine learning methods. Deep Learning (DL) methods are widely used to process large amounts of data because they need less preprocessing than traditional machine learning methods. Various types of uncertainty associated with large amounts of raw data hinder the prediction accuracy. Belief Rule-Based Expert Systems (BRBES) are widely used to handle uncertain data. However, due to their incapability of integrating associative memory within the inference procedures, they demonstrate poor accuracy of prediction when large amounts of data is considered. Therefore, we propose the integration of an associative memory based DL method within the BRBES inference procedures, allowing to discover accurate data patterns and hence, the improvement of prediction under uncertainty. To demonstrate the applicability of the proposed method, which is named BRB-DL, it has been fine tuned against two datasets, one in the area of air pollution and the other in the area of power generation. The reliability of the proposed BRB-DL method, has also been compared with other DL methods such as Long-Short Term Memory and Deep Neural Network, and BRBES by taking into account of the air quality dataset from Beijing city and the power generation dataset of a combined cycle power plant. BRB-DL outperforms the above-mentioned methods in terms of prediction accuracy. For example, the Mean Square Error value of BRB-DL is 4.12 whereas for Long-Short Term Memory, Deep Neural Network, Fuzzy Deep Neural Network, Adaptive Neuro Fuzzy Inference System and BRBES it is 18.66, 28.49, 17.05, 16.37 and 38.15 for combined cycle power plant respectively, which are significantly higher. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",IEEE Access,10.1109/ACCESS.2020.3031438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102834557&doi=10.1109%2fACCESS.2020.3031438&partnerID=40&md5=ef7afa89137816308f10eb1bbecfbfbe,2020,2021-07-20 15:48:38,2021-07-20 15:48:38
MNNI7L8R,journalArticle,2016,"Chellappa, R.",The changing fortunes of pattern recognition and computer vision,"As someone who had been attending conferences on pattern recognition and computer vision since 1978, I have watched with interest the ups and downs of pattern recognition and computer vision areas and how they have been presented at various conferences such as PRIP, CVPR, ECCV, ICCV, ACCV, ICPR, IJCAI, AAAI, NIPS, ICASSP and ICIP. Given the recent successes of deep learning networks, it appears that the scale is tilting towards pattern recognition as is commonly understood. A good number of papers in recent vision conferences seem to push data through one or other deep learning networks and report improvements over state of the art. While one cannot argue against the remarkable (magical?) performance improvements obtained by deep learning network-based approaches, I worry that five years from now, most students in computer vision will only be aware of software that implements some deep learning network. After all, 2-D based detection and recognition problems for which the deep learning networks have shown their mettle are only a subset of the computer vision field. While enjoying the ride, I would like to caution that understanding of scene and image formation, invariants, interaction between light and matter, human vision, 3D recovery, and emerging concepts like common sense reasoning are too important to ignore for the long-term viability of the computer vision field. It will be a dream come true if we manage to integrate these computer vision concepts into deep learning networks so that more robust performance can be obtained with less data and cheaper computers. © 2016 Elsevier B.V.",Image and Vision Computing,10.1016/j.imavis.2016.04.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965064343&doi=10.1016%2fj.imavis.2016.04.005&partnerID=40&md5=61d0867311bd86e3178b46c6517c5ca9,2016,2021-07-20 15:48:38,2021-07-20 15:48:38
E9QRNUBD,journalArticle,2019,"Couso, I.; Borgelt, C.; Hullermeier, E.; Kruse, R.",Fuzzy sets in data analysis: From statistical foundations to machine learning,"Basic ideas and formal concepts from fuzzy sets and fuzzy logic have been used successfully in various branches of science and engineering. This paper elaborates on the use of fuzzy sets in the broad field of data analysis and statistical sciences, including modern manifestations such as data mining and machine learning. In the fuzzy logic community, this branch of research has recently gained in importance, especially due to the emergence of data science as a new scientific discipline, and the increasing relevance of machine learning as a key methodology of modern artificial intelligence. This development has been accompanied by an internal shift from largely knowledge-based to strongly data-driven fuzzy modeling and systems design. Reflecting on the historical dimension and evolution of the area, we discuss the role of fuzzy logic in data analysis and related fields, highlight existing contributions of fuzzy sets in these fields, and outline interesting directions for future work. © 2005-2012 IEEE.",IEEE Computational Intelligence Magazine,10.1109/MCI.2018.2881642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060024760&doi=10.1109%2fMCI.2018.2881642&partnerID=40&md5=6ec75e137f0fed53d6fb882182688cd1,2019,2021-07-20 15:48:38,2021-07-20 15:48:38
Y63DYXIK,journalArticle,2011,"Chen, Z.; Aghakhani, S.; Man, J.; Dick, S.",ANCFIS: A neurofuzzy architecture employing complex fuzzy sets,"Complex fuzzy sets (CFSs) are an extension of type-1 fuzzy sets in which the membership of an object to the set is a value from the unit disc of the complex plane. Although there has been considerable progress made in determining the properties of CFSs and complex fuzzy logic, there has yet to be any practical application of this concept. We present the adaptive neurocomplex-fuzzy-inferential system (ANCFIS), which is the first neurofuzzy system architecture to implement complex fuzzy rules (and, in particular, the signature property of rule interference). We have applied this neurofuzzy system to the domain of time-series forecasting, which is an important machine-learning problem. We find that ANCFIS performs well in one synthetic and five real-world forecasting problems and is also very parsimonious. Experimental comparisons show that ANCFIS is comparable with existing approaches on our five datasets. This work demonstrates the utility of complex fuzzy logic on real-world problems. © 2010 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2010.2096469,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953666016&doi=10.1109%2fTFUZZ.2010.2096469&partnerID=40&md5=82aa147b1788db6d8b31aa65bdd76821,2011,2021-07-20 15:48:38,2021-07-20 15:48:38
GYKW9BLA,journalArticle,2019,"Faust, K.; Bala, S.; van Ommeren, R.; Portante, A.; Al Qawahmed, R.; Djuric, U.; Diamandis, P.",Intelligent feature engineering and ontological mapping of brain tumour histomorphologies by deep learning,"Deep learning is an emerging transformative tool in diagnostic medicine, yet limited access and the interpretability of learned parameters hinders widespread adoption. Here we have generated a diverse repository of 838,644 histopathologic images and used them to optimize and discretize learned representations into 512-dimensional feature vectors. Importantly, we show that individual machine-engineered features correlate with salient human-derived morphologic constructs and ontological relationships. Deciphering the overlap between human and machine reasoning may aid in eliminating biases and improving automation and accountability for artificial intelligence-assisted medicine. © 2019, The Author(s), under exclusive licence to Springer Nature Limited.",Nature Machine Intelligence,10.1038/s42256-019-0068-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081268713&doi=10.1038%2fs42256-019-0068-6&partnerID=40&md5=253f259cb481a91b222608ed12b97649,2019,2021-07-20 15:48:38,2021-07-20 15:48:38
VSSHXGIM,journalArticle,2013,"Galitsky, B.",Exhaustive simulation of consecutive mental states of human agents,"We develop a generic software component for computing consecutive plausible mental states of human agents. The simulation approach to reasoning about mental world is introduced that is based on exhaustive search through the space of available behaviors. This approach to reasoning is implemented as a logic program in a natural language multiagent mental simulator NL-MAMS, which yields the totality of possible mental states few steps in advance, given an arbitrary initial mental state of participating agents. Due to an extensive vocabulary of formally represented mental attitudes, communicative actions and accumulated library of behaviors, NL-MAMS is capable of yielding much richer set of sequences of mental state than a conventional system of reasoning about beliefs, desires and intentions would deliver. Also, NL-MAMS functions in domain-independent manner, outperforming machine learning-based systems for predicting behaviors of human agents in broad domains where training sets are limited. We evaluate the correctness, coverage and maximum complexity of the NL-MAMS and discuss its integration with other reasoning components and its application domains. The proposed component is intended to be integrated into eBay human behavior simulation system, predicting behavior of buyers and sellers in normal and conflict situations. Also, NL-MAMS can be a part of any software system where modeling of human users is necessary, such as a personalized assistant, a tutoring or decision support system, advisor, recommender and conflict resolver. © 2013 Elsevier B.V. All rights reserved.",Knowledge-Based Systems,10.1016/j.knosys.2012.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875278403&doi=10.1016%2fj.knosys.2012.11.001&partnerID=40&md5=ae1eb07625384814ece6d32af8cc4bd1,2013,2021-07-20 15:48:38,2021-07-20 15:48:38
7MGKKE7Q,journalArticle,2019,"Lima, R.; Espinasse, B.; Freitas, F.",A logic-based relational learning approach to relation extraction: The OntoILPER system,"Relation Extraction (RE), the task of detecting and characterizing semantic relations between entities in text, has gained much importance in the last two decades, mainly in the biomedical domain. Many papers have been published on Relation Extraction using supervised machine learning techniques. Most of these techniques rely on statistical methods, such as feature-based and tree-kernels-based methods. Such statistical learning techniques are usually based on a propositional hypothesis space for representing examples, i.e., they employ an attribute–value representation of features. This kind of representation has some drawbacks, particularly in the extraction of complex relations which demand more contextual information about the involving instances, i.e., it is not able to effectively capture structural information from parse trees without loss of information. In this work, we present OntoILPER, a logic-based relational learning approach to Relation Extraction that uses Inductive Logic Programming for generating extraction models in the form of symbolic extraction rules. OntoILPER takes profit of a rich relational representation of examples, which can alleviate the aforementioned drawbacks. The proposed relational approach seems to be more suitable for Relation Extraction than statistical ones for several reasons that we argue. Moreover, OntoILPER uses a domain ontology that guides the background knowledge generation process and is used for storing the extracted relation instances. The induced extraction rules were evaluated on three protein–protein interaction datasets from the biomedical domain. The performance of OntoILPER extraction models was compared with other state-of-the-art RE systems. The encouraging results seem to demonstrate the effectiveness of the proposed solution. © 2018 Elsevier Ltd",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2018.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057496358&doi=10.1016%2fj.engappai.2018.11.001&partnerID=40&md5=46ddd00d613ed9d46426d943543fafd9,2019,2021-07-20 15:48:38,2021-07-20 15:48:38
B5Y83LBQ,journalArticle,2021,"Folino, F.; Folino, G.; Guarascio, M.; Pisani, F.S.; Pontieri, L.",On learning effective ensembles of deep neural networks for intrusion detection,"Classification-oriented Machine Learning methods are a precious tool, in modern Intrusion Detection Systems (IDSs), for discriminating between suspected intrusion attacks and normal behaviors. Many recent proposals in this field leveraged Deep Neural Network (DNN) methods, capable of learning effective hierarchical data representations automatically. However, many of these solutions were validated on data featuring stationary distributions and/or large amounts of training examples. By contrast, in real IDS applications different kinds of attack tend to occur over time, and only a small fraction of the data instances is labeled (usually with far fewer examples of attacks than of normal behavior). A novel ensemble-based Deep Learning framework is proposed here that tries to face the challenging issues above. Basically, the non-stationary nature of IDS log data is faced by maintaining an ensemble consisting of a number of specialized base DNN classifiers, trained on disjoint chunks of the data instances’ stream, plus a combiner model (reasoning on both the base classifiers predictions and original instance features). In order to learn deep base classifiers effectively from small training samples, an ad-hoc shared DNN architecture is adopted, featuring a combination of dropout capabilities, skip-connections, along with a cost-sensitive loss (for dealing with unbalanced data). Tests results, conducted on two benchmark IDS datasets and involving several competitors, confirmed the effectiveness of our proposal (in terms of both classification accuracy and robustness to data scarcity), and allowed us to evaluate different ensemble combination schemes. © 2021 Elsevier B.V.",Information Fusion,10.1016/j.inffus.2021.02.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101425216&doi=10.1016%2fj.inffus.2021.02.007&partnerID=40&md5=d5aa1b96cff318d1800b41ddd4787da9,2021,2021-07-20 15:48:38,2021-07-20 15:48:38
ECHDXEJG,journalArticle,2018,"Zuallaert, J.; Godin, F.; Kim, M.; Soete, A.; Saeys, Y.; De Neve, W.",Splicerover: Interpretable convolutional neural networks for improved splice site prediction,"Motivation: During the last decade, improvements in high-throughput sequencing have generated a wealth of genomic data. Functionally interpreting these sequences and finding the biological signals that are hallmarks of gene function and regulation is currently mostly done using automated genome annotation platforms, which mainly rely on integrated machine learning frameworks to identify different functional sites of interest, including splice sites. Splicing is an essential step in the gene regulation process, and the correct identification of splice sites is a major cornerstone in a genome annotation system. Results: In this paper, we present SpliceRover, a predictive deep learning approach that outperforms the state-of-the-art in splice site prediction. SpliceRover uses convolutional neural networks (CNNs), which have been shown to obtain cutting edge performance on a wide variety of prediction tasks. We adapted this approach to deal with genomic sequence inputs, and show it consistently outperforms already existing approaches, with relative improvements in prediction effectiveness of up to 80.9% when measured in terms of false discovery rate. However, a major criticism of CNNs concerns their ‘black box’ nature, as mechanisms to obtain insight into their reasoning processes are limited. To facilitate interpretability of the SpliceRover models, we introduce an approach to visualize the biologically relevant information learnt. We show that our visualization approach is able to recover features known to be important for splice site prediction (binding motifs around the splice site, presence of polypyrimidine tracts and branch points), as well as reveal new features (e.g. several types of exclusion patterns near splice sites). © The Author(s) 2018. Published by Oxford University Press. All rights reserved.",Bioinformatics,10.1093/bioinformatics/bty497,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058444187&doi=10.1093%2fbioinformatics%2fbty497&partnerID=40&md5=7766b05bd9215f05d5bd403e51b3446b,2018,2021-07-20 15:48:38,2021-07-20 15:48:38
Y7XQEZMH,journalArticle,2021,"Bu, L.; Liang, Y.; Xie, Z.; Qian, H.; Hu, Y.-Q.; Yu, Y.; Chen, X.; Li, X.",Machine learning steered symbolic execution framework for complex software code,"During program traversing, symbolic execution collects pathconditions and feeds them to a constraint solver to obtain feasiblesolutions. However, complex path conditions, like nonlinearconstraints, which widely appear in programs, are hard to be handledefficiently by the existing solvers. In this paper, we adapt theclassical symbolic execution framework with a machine learningapproach for constraint satisfaction. The approach samples andlearns from different solutions to identify potentially feasiblearea. This sampling-learning style solving can be applied indifferent class of complex problems easily. Therefore, incorporatingthis approach, our framework, MLBSE, supports the symbolicexecution of not only simple linear path conditions, but alsononlinear arithmetic operations, and even black-box function callsof library methods. Meanwhile, thanks to the theoretical foundationof the machine learning based approach, when the solver fails tosolve a path condition, we can have an estimation of the confidencein the satisfiability (ECS) of the problem to give users insightsabout how the problem is analyzed and whether they could ultimatelyfind a solution. We implement MLBSE on the basis of SymbolicPath Finder (SPF) into a fully automatic Java symbolic executionengine. Users can feed their code to MLBSE directly, which isvery convenient to use. To evaluate its performance, 22 real caseprograms are used as the benchmarks for MLBSE to generate testcases, which involve a total number of 1042 methods that are full ofnonlinear operations, floating-point arithmetic as well as nativemethod calls. Experiment results show that the coverage achieved byMLBSE is much higher than the state-of-the-art tools. © 2021, British Computer Society.",Formal Aspects of Computing,10.1007/s00165-021-00538-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106508716&doi=10.1007%2fs00165-021-00538-3&partnerID=40&md5=f27f87a74e093a0563b6a02a05cfc129,2021,2021-07-20 15:48:38,2021-07-20 15:48:38
LS69UH8F,journalArticle,2017,"Otaibi, J.A.; Safi, Z.; Hassaine, A.; Islam, F.; Jaoua, A.",Machine Learning and Conceptual Reasoning for Inconsistency Detection,"This paper focuses on detecting inconsistencies within text corpora. It is a very interesting area with many applications. Most existing methods deal with this problem using complicated textual analysis, which is known for not being accurate enough. We propose a new methodology that consists of two steps, the first one being a machine learning step that performs multilevel text categorization. The second one applies conceptual reasoning on the predicted categories in order to detect inconsistencies. This paper has been validated on a set of Islamic advisory opinions (also known as fatwas). This domain is gaining a large interest with users continuously checking the authenticity and relevance of such content. The results show that our method is very accurate and can complement existing methods using the linguistic analysis. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2016.2642402,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018484520&doi=10.1109%2fACCESS.2016.2642402&partnerID=40&md5=01f5cae922b149b00e8bf26d723b8cef,2017,2021-07-20 15:48:38,2021-07-20 15:48:38
CI546TXE,journalArticle,2014,"Alama, J.; Heskes, T.; Kühlwein, D.; Tsivtsivadze, E.; Urban, J.",Premise selection for mathematics by corpus analysis and Kernel methods,"Smart premise selection is essential when using automated reasoning as a tool for large-theory formal proof development. This work develops learning-based premise selection in two ways. First, a fine-grained dependency analysis of existing high-level formal mathematical proofs is used to build a large knowledge base of proof dependencies, providing precise data for ATP-based re-verification and for training premise selection algorithms. Second, a new machine learning algorithm for premise selection based on kernel methods is proposed and implemented. To evaluate the impact of both techniques, a benchmark consisting of 2078 large-theory mathematical problems is constructed, extending the older MPTP Challenge benchmark. The combined effect of the techniques results in a 50 % improvement on the benchmark over the state-of-the-art Vampire/SInE system for automated reasoning in large theories. © 2013 Springer Science+Business Media Dordrecht.",Journal of Automated Reasoning,10.1007/s10817-013-9286-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84895060542&doi=10.1007%2fs10817-013-9286-5&partnerID=40&md5=3adaa7ada1f14515117bc48912c6ba50,2014,2021-07-20 15:48:39,2021-07-20 15:48:39
6Q2DW2ZL,journalArticle,2014,"Jamshidi, Y.; Nezamabadi-Pour, H.",Rule inducing by fuzzy lattice reasoning classifier based on metric distances (FLRC-MD),"Recently much more attention has been paid to the applications of lattice theory in different fields. Fuzzy lattice reasoning (FLR) was described lately as a lattice data domain extension of fuzzy-ARTMAP based on a lattice inclusion measure function. In this work, we develop a fuzzy lattice reasoning classifier using various distance metrics. As a consequence, the new algorithm named FLRC-MD shows better classification results and more generalization and it will lead to generate fewer induced rules. To assess the effectiveness of the proposed model, twenty benchmark data sets are tested. The results are compared favorably with those from a number of state-of-the-art machine learning techniques published in the literature. Results obtained confirm the effectiveness of the proposed method. © 2014 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2014.08.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907318796&doi=10.1016%2fj.asoc.2014.08.016&partnerID=40&md5=d51cca632852ae3e4361172bc4041c88,2014,2021-07-20 15:48:39,2021-07-20 15:48:39
LK4RL8K4,journalArticle,2018,"Toor, A.S.; Wechsler, H.",Biometrics and forensics integration using deep multi-modal semantic alignment and joint embedding,"This paper proposes collaborative and context-aware visual question answering (C2VQA) for multi-modal information channels integration, and details its particular mapping and realization for biometrics forensic integration (BFI) using Show and Tell like architectures. C2VQA, which expands on Visual Query Answering (VQA) and the Visual Turing Test (VTT), engages deep semantic alignment and joint embedding using deep learning (DL) for image analysis, vector space as skip-grams and long-term dependencies as gated recurrent networks for context prediction, and multi-strategy learning including conformal prediction for control and meta-reasoning. C2VQA would engage in purposeful dialog to address and correct for misinformation and uncertainty and considers behavior to model realistic VQA problems characteristic of open rather than closed set VQA. © 2017 Elsevier B.V.",Pattern Recognition Letters,10.1016/j.patrec.2017.02.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013630552&doi=10.1016%2fj.patrec.2017.02.012&partnerID=40&md5=56bd27a8cf4e71d58aa87fab0af40b7d,2018,2021-07-20 15:48:39,2021-07-20 15:48:39
2MDIKLK7,journalArticle,2017,"Guesgen, H.W.; Marsland, S.",Modelling spatial and temporal context to support activity recognition,"With the dawn of mobile computing and the Internet of Things, context-aware computing has gained increasing attention. The idea is to not only consider explicit inputs when computing an output, but also to consider contextual factors such as location, time, and environmental data. Context-aware computing is not restricted to mobile computing and the Internet of Things, but is also beneficial in other areas. The area we focus on in this chapter is activity recognition in smart environments. In the last ten years or so, a number of approaches have been developed that aim to recognise activities in smart environments such as smart homes or smart offices, ranging from logic-based approaches to probabilistic machine learning approaches. These approaches still have deficits and only work under certain assumptions. We argue in this chapter that using context information can lead to an improvement in activity recognition. To prove our point, we focus on spatial and temporal context information and introduce a number of methods to reason about this type of information. © 2017 The authors and IOS Press.",Frontiers in Artificial Intelligence and Applications,10.3233/978-1-61499-804-4-35,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027314077&doi=10.3233%2f978-1-61499-804-4-35&partnerID=40&md5=98e51bc55bf876d65bce7ff9c4b339b3,2017,2021-07-20 15:48:39,2021-07-20 15:48:39
V6GFPJBU,journalArticle,2021,"Cheng, S.-T.; Hsu, C.-W.; Horng, G.-J.; Jiang, C.-R.",Video reasoning for conflict events through feature extraction,"The rapid growth of multimedia data and the improvement of deep learning technology has allowed high-accuracy models to be trained for various fields. Video tools such as video classification, temporal action detection, and video summary are now available for the understanding of videos. In daily life, many social events start with a small conflict event. If conflicts and the subsequent dangers can be learned about from a video, we can prevent social incidents from occurring early on. This research presents a video and audio reasoning network that infers possible conflict events through video and audio features. To make the respective model more generalizable to other tasks, we have also added a predictive network to predict the risk of conflict events. We use multitasking to render the characteristics of movies and voices more generalizable to other similar tasks. We also propose several methods to integrate video features and audio features, improving the reasoning performance of the model. There’s a model we proposed is called the video and audio reasoning Network (VARN) which is more accurate than other models. Compared with RandomNet, it achieves a 2.9 times greater accuracy. © 2021, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Supercomputing,10.1007/s11227-020-03514-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098696134&doi=10.1007%2fs11227-020-03514-5&partnerID=40&md5=ea56b5c1fbcc2a235b610c46cbc67af6,2021,2021-07-20 15:48:39,2021-07-20 15:48:39
TZPURNF3,journalArticle,2012,"Rettinger, A.; Lösch, U.; Tresp, V.; D'Amato, C.; Fanizzi, N.",Mining the semantic web: Statistical learning for next generation knowledge bases,"In the SemanticWeb vision of theWorldWideWeb, content will not only be accessible to humans but will also be available in machine interpretable form as ontological knowledge bases. Ontological knowledge bases enable formal querying and reasoning and, consequently, a main research focus has been the investigation of how deductive reasoning can be utilized in ontological representations to enable more advanced applications. However, purely logic methods have not yet proven to be very effective for several reasons: First, there still is the unsolved problem of scalability of reasoning to Web scale. Second, logical reasoning has problems with uncertain information, which is abundant on SemanticWeb data due to its distributed and heterogeneous nature. Third, the construction of ontological knowledge bases suitable for advanced reasoning techniques is complex, which ultimately results in a lack of such expressive real-world data sets with large amounts of instance data. From another perspective, the more expressive structured representations open up new opportunities for data mining, knowledge extraction and machine learning techniques. If moving towards the idea that part of the knowledge already lies in the data, inductive methods appear promising, in particular since inductive methods can inherently handle noisy, inconsistent, uncertain and missing data. While there has been broad coverage of inducing concept structures from less structured sources (text, Web pages), like in ontology learning, given the problems mentioned above, we focus on new methods for dealing with Semantic Web knowledge bases, relying on statistical inference on their standard representations. We argue that machine learning research has to offer a wide variety of methods applicable to different expressivity levels of SemanticWeb knowledge bases: ranging from weakly expressive but widely available knowledge bases in RDF to highly expressive first-order knowledge bases, this paper surveys statistical approaches to mining the Semantic Web. We specifically cover similarity and distance-based methods, kernel machines, multivariate prediction models, relational graphical models and first-order probabilistic learning approaches and discuss their applicability to Semantic Web representations. Finally, we present selected experimentswhich were conducted on SemanticWebmining tasks for some of the algorithms presented before. This is intended to show the breadth and general potential of this exiting new research and application area for data mining. © The Author(s) 2012.",Data Mining and Knowledge Discovery,10.1007/s10618-012-0253-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870679926&doi=10.1007%2fs10618-012-0253-2&partnerID=40&md5=57b77f00f808fa74152d2e71b6e46930,2012,2021-07-20 15:48:39,2021-07-20 15:48:39
NWKGBV36,journalArticle,2018,"Sykes, E.R.",Reasoning about ideal interruptible moments: A soft computing implementation of an interruption classifier in free-form task environments,"Current trends in society and technology make the concept of interruption a central human computer interaction problem. In this work, a novel soft computing implementation for an Interruption Classifier was designed, developed and evaluated that draws from a user model and real-time observations of the user's actions as s/he works on computer-based tasks to determine ideal times to interact with the user. This research is timely as the number of interruptions people experience daily has grown considerably over the last decade. Thus, systems are needed to manage interruptions by reasoning about ideal timings of interactions. This research shows: (1) the classifier incorporates a user model in its’ reasoning process. Most of the research in this area has focused on task-based contextual information when designing systems that reason about interruptions; (2) the classifier performed at 96% accuracy in experimental test scenarios and significantly outperformed other comparable systems; (3) the classifier is implemented using an advanced machine learning technology—an Adaptive Neural-Fuzzy Inference System—this is unique since all other systems use Bayesian Networks or other machine learning tools; (4) the classifier does not require any direct user involvement—in other systems, users must provide interruption annotations while reviewing video sessions so the system can learn; and (5) a promising direction for reasoning about interruptions for free-form tasks–this is largely an unsolved problem. © 2018",International Journal of Human Computer Studies,10.1016/j.ijhcs.2018.06.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051663528&doi=10.1016%2fj.ijhcs.2018.06.005&partnerID=40&md5=ada9734099cc2ff3e7e3ed337e9c05da,2018,2021-07-20 15:48:39,2021-07-20 15:48:39
UBS58BBV,journalArticle,2021,"Deng, Z.; Liu, H.",Geometry-attentive relational reasoning for robust facial landmark detection,"In this paper, we propose a geometry-attentive relational reasoning approach to investigate the problem of robust facial landmark detection, especially when faces were captured in wild conditions. Unlike existing methods which usually cannot explicitly exploit the geometric relationship among different landmarks, our approach aims to reason about the intrinsic geometry-aware relations among landmarks for feature enhancement. To achieve this, we carefully develop an interpretable and plug-and-play module to reinforce the discriminativeness and uniqueness of feature maps, which typically operates on all possible pairs on the immediate inter-landmark heat maps. Among these pairing maps, our model learns to infer the meaningful relational clues in the transformed feature space on condition of holistic facial shape prior. For permutation order invariance, we pool these features as a single aggregated relational feature. To further improve the performance, we simply equip the proposed module inside the backbone hourglass networks. The experimental results on the standard benchmarking datasets indicate the effectiveness of our proposed approach. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.06.126,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091536217&doi=10.1016%2fj.neucom.2020.06.126&partnerID=40&md5=f9ec882b20f8de403020b268373473c7,2021,2021-07-20 15:48:39,2021-07-20 15:48:39
U3SRTFRB,journalArticle,2020,"Hadj-Mabrouk, H.",Case-based reasoning for safety assessment of critical software,"The commissioning of a new guided or automated rail transport system requires an in-depth analysis of all the methods, techniques, procedures, regulations and safety standards to ensure that the risk level of the future system does not present any danger likely to jeopardize the safety of travelers. Among these numerous safety methods implemented to guarantee safety at the system, automation, hardware and software level, there is a method called 'Software Errors and Effects Analysis (SEEA)' whose objective is to determine the nature and the severity of the consequences of software failures, to propose measures to detect errors and finally to improve the robustness of the software. In order to strengthen and rationalize this SEEA method, we have agreed to use machine learning techniques and in particular Case-Based Reasoning (CBR) in order to assist the certification experts in their difficult task of assessing completeness and the consistency of safety of critical software equipment. The main objective consists, from a set of data in the form of accident scenarios or incidents experienced on rail transport systems (experience feedback), to exploit by automatic learning this mass of data to stimulate the imagination of certification experts and assist them in their crucial task of researching scenarios of potential accidents not taken into account during the design phase of new critical software. The originality of the tool developed lies not only in its ability to model, capitalize, sustain and disseminate SEEA expertise, but it represents the first research on the application of CBR to SEEA. In fact, in the field of rail transport, there are currently no software tools for assisting SEEAs based on machine learning techniques and in particular based on CBR. © 2020-IOS Press and the authors. All rights reserved.",Intelligent Decision Technologies,10.3233/IDT-200016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099583839&doi=10.3233%2fIDT-200016&partnerID=40&md5=483d31c33f4fe894637d0033a2fab9ab,2020,2021-07-20 15:48:39,2021-07-20 15:48:39
GWAQ7FNX,journalArticle,2018,"Zheng, P.; Lee, B.C.",Hound: Causal Learning for Datacenter-scale Straggler Diagnosis,"Stragglers are exceptionally slow tasks within a job that delay its completion. Stragglers, which are uncommon within a single job, are pervasive in datacenters with many jobs. We present Hound, a statistical machine learning framework that infers the causes of stragglers from traces of datacenter-scale jobs. Hound is designed to achieve several objectives: datacenter-scale diagnosis, unbiased inference, interpretable models, and computational efficiency. We demonstrate Hound's capabilities for a production trace from Google's warehouse-scale datacenters and two Spark traces from Amazon EC2 clusters. © 2018 ACM.",Performance Evaluation Review,10.1145/3219617.3219641,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084185272&doi=10.1145%2f3219617.3219641&partnerID=40&md5=b7ae3b2485275c8023df164b8e8f2512,2018,2021-07-20 15:48:39,2021-07-20 15:48:39
G4HD5S9E,journalArticle,2016,"Robson, B.","Studies in using a universal exchange and inference language for evidence based medicine. Semi-automated learning and reasoning for PICO methodology, systematic review, and environmental epidemiology","The Q-UEL language of XML-like tags and the associated software applications are providing a valuable toolkit for Evidence Based Medicine (EBM). In this paper the already existing applications, data bases, and tags are brought together with new ones. The particular Q-UEL embodiment used here is the BioIngine. The main challenge is one of bringing together the methods of symbolic reasoning and calculative probabilistic inference that underlie EBM and medical decision making. Some space is taken to review this background. The unification is greatly facilitated by Q-UEL's roots in the notation and algebra of Dirac, and by extending Q-UEL into the Wolfram programming environment. Further, the overall problem of integration is also a relatively simple one because of the nature of Q-UEL as a language for interoperability in healthcare and biomedicine, while the notion of workflow is facilitated because of the EBM best practice known as PICO. What remains difficult is achieving a high degree of overall automation because of a well-known difficulty in capturing human expertise in computers: the Feigenbaum bottleneck. © 2016 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2016.10.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995486980&doi=10.1016%2fj.compbiomed.2016.10.009&partnerID=40&md5=97b7ee75cc621f20a413624799af1ac1,2016,2021-07-20 15:48:39,2021-07-20 15:48:39
RJ8WN6JX,journalArticle,2020,"Hui, W.; Yu, L.",The uncertainty and explainability in object recognition,"In the object recognition task, due to changes in size, colour, illumination, position, viewing angle, and environmental background, great uncertainty is caused. The invariant recognition capability required to adapt to such diverse uncertainties has also become one of the most challenging goals of artificial intelligence, as only biometric systems can do this. The approach to dealing with uncertainty in the field of pattern recognition is nothing more than training with a large number of samples and powerful machine learning algorithms, and finally generating a classifier. Among them, knowledge representation has not only been ignored, but has even been considered redundant. But research from cognitive psychology and experimental psychology suggests that humans do not use such a huge mechanism. In contrast, we use more similar methods of symbolic artificial intelligence, such as representation, induction, reasoning, interpretation, and constraint propagation, to deal with uncertainty in object recognition. In this paper, the skeleton tree is used as the basic means to form the formal representation of the topological features and geometric features of the object, and based on the generalisation framework, the knowledge extraction of a small number of similar representations is used to form a generalised explicit representation of the knowledge about the object category. This generalised form representation can cope with uncertainties such as size, colour, shape, etc., and is significantly superior to current mainstream machine learning methods in terms of explainability and computational cost. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.",Journal of Experimental and Theoretical Artificial Intelligence,10.1080/0952813X.2020.1785021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087923951&doi=10.1080%2f0952813X.2020.1785021&partnerID=40&md5=4a2b9ca690a1772de1ba8c262c6f3cf7,2020,2021-07-20 15:48:39,2021-07-20 15:48:39
M7ZG4WZM,journalArticle,2017,"Sánchez-Ruiz, A.A.; Miranda, M.",A machine learning approach to predict the winner in StarCraft based on influence maps,"Real-Time Strategy games are very popular test beds for Artificial Intelligence (AI) researchers because they provide complex and controlled environments on which to test different AI techniques. In this paper we play the role of an external observer that tries to predict who is going to win from the events occurring during the game. In order to predict the outcome of the game, we model the game states using influence maps. Influence maps are numerical matrices representing the influence of each player's army in the map, and they are useful for different types of spatial reasoning. We test different machine learning techniques on two different datasets of StarCraft games. The first dataset contains 4-player games in which the players are controlled by the internal game AI. The second dataset contains 2-player human games from specialized websites. We analyze the similarities and differences between both datasets and the performance of each algorithm. Finally, we perform a small experiment with expert players and conclude that our system reaches a level of precision similar to the human judges, although human judges base their predictions on a much more complex and abstract set of game features. © 2016 Elsevier B.V.",Entertainment Computing,10.1016/j.entcom.2016.11.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002728568&doi=10.1016%2fj.entcom.2016.11.005&partnerID=40&md5=d70e6fa2a20877bf1a80cb86fc1d5af5,2017,2021-07-20 15:48:39,2021-07-20 15:48:39
AP4S9FHA,journalArticle,2019,"Chinchali, S.P.; Livingston, S.C.; Chen, M.; Pavone, M.",Multi-objective optimal control for proactive decision making with temporal logic models,"The operation of today’s robots entails interactions with humans, e.g., in autonomous driving amidst human-driven vehicles. To effectively do so, robots must proactively decode the intent of humans and concurrently leverage this knowledge for safe, cooperative task satisfaction: a problem we refer to as proactive decision making. However, simultaneous intent decoding and robotic control requires reasoning over several possible human behavioral models, resulting in high-dimensional state trajectories. In this paper, we address the proactive decision-making problem using a novel combination of formal methods, control, and data mining techniques. First, we distill high-dimensional state trajectories of human–robot interaction into concise, symbolic behavioral summaries that can be learned from data. Second, we leverage formal methods to model high-level agent goals, safe interaction, and information-seeking behavior with temporal logic formulas. Finally, we design a novel decision-making scheme that maintains a belief distribution over models of human behavior, and proactively plans informative actions. After showing several desirable theoretical properties, we apply our framework to a dataset of humans driving in crowded merging scenarios. For it, temporal logic models are generated and used to synthesize control strategies using tree-based value iteration and deep reinforcement learning. In addition, we illustrate how data-driven models of human responses to informative robot probes, such as from generative models such as conditional variational autoencoders, can be clustered with formal specifications. Results from simulated self-driving car scenarios demonstrate that data-driven strategies enable safe interaction, correct model identification, and significant dimensionality reduction. © The Author(s) 2019.",International Journal of Robotics Research,10.1177/0278364919868290,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071509132&doi=10.1177%2f0278364919868290&partnerID=40&md5=9f980da7dcdd39c39f139452d05de06b,2019,2021-07-20 15:48:39,2021-07-20 15:48:39
D98FGVN5,journalArticle,2020,"Panigrahi, S.; Behera, H.S.",A study on leading machine learning techniques for high order fuzzy time series forecasting,"Fuzzy time series forecasting (FTSF) methods avoid the basic assumptions of traditional time series forecasting (TSF) methods. The FTSF methods consist of four stages namely determination of effective length of interval, fuzzification of crisp time series data, modeling of fuzzy logical relationships (FLRs) and defuzzification. All the four stages play a vital role in achieving better forecasting accuracy. This paper addresses two key issues such as modeling FLRs and determination of effective length of interval. Three leading machine learning (ML) techniques, namely deep belief network (DBN), long short-term memory (LSTM) and support vector machine (SVM) are first time used for modeling the FLRs. Additionally, a modified average-based method is proposed to estimate the effective length of interval. The proposed FTSF-DBN, FTSF-LSTM and FTSF-SVM methods are being compared with three papers from the literature along with four crisp TSF methods using multilayer perceptron (MLP), LSTM, DBN and SVM. A total of fourteen time series datasets (Sun Spot, Lynx, Mumps and 11 TAIEX time series datasets i.e. 2000–2010) are considered for comparative performance analysis. Results revealed the statistical superiority of FTSF-SVM method and proposed improved average-based method based on the popular Friedman and Nemenyi hypothesis test. It is also observed that the proposed FTSF methods provide statistical superior performance than their crisp TSF counterparts. © 2019 Elsevier Ltd",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2019.103245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072385696&doi=10.1016%2fj.engappai.2019.103245&partnerID=40&md5=f2dfae15392c7d7412ff5434bd5c88bd,2020,2021-07-20 15:48:39,2021-07-20 15:48:39
WI9SL3MG,journalArticle,2014,"Velik, R.",A brain-inspired multimodal data mining approach for human activity recognition in elderly homes,"Human activity recognition is a prerequisite for many innovative applications including elderly activity monitoring and support in order to enable elderly people to live longer independently in their own homes. Over the past decade, a diversity of different activity recognition approaches has been developed from which the majority focuses on the processing of data from one sensor modality only (e.g., vision). Nonetheless, a merging of data from multiple disparate sources has the potential of offering more accurate, robust, descriptive, intuitive, and meaningful results due to the availability of complementary and partially redundant information. This article (1) gives a review of existing multimodal approaches for elderly activity recognition in home settings, (2) introduces a powerful activity recognition model based on brain-inspired multimodal data mining methods, (3) employs this model for the purpose of daily activity recognition in a home setting using a publically available real world dataset, and (4) quantitatively compares the obtained results with state of the art multimodal activity recognition methods including hidden Markov models, conditional random fields, decision trees, a Bayes approach, and a context lattice. © 2014 - IOS Press and the authors. All rights reserved.",Journal of Ambient Intelligence and Smart Environments,10.3233/AIS-140266,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941095891&doi=10.3233%2fAIS-140266&partnerID=40&md5=616103647b279df15ddab53a67d784b0,2014,2021-07-20 15:48:39,2021-07-20 15:48:39
ETE3ZTWR,journalArticle,2020,"Wang, Q.; Hao, Y.",ALSTM: An attention-based long short-term memory framework for knowledge base reasoning,"Knowledge Graphs (KGs) have been applied to various application scenarios including Web searching, Q&A, recommendation system, natural language processing and so on. However, the vast majority of Knowledge Bases (KBs) are incomplete, necessitating a demand for KB completion (KBC). Methods of KBC used in the mainstream current knowledge base include the latent factor model, the random walk model and recent popular methods based on reinforcement learning, which performs well in their respective areas of expertise. Recurrent neural network (RNN) and its variants model temporal data by remembering information for long periods, however, whether they also have the ability to use the information they have already remembered to achieve complex reasoning in the knowledge graph. In this paper, we produce a novel framework (ALSTM) based on the Attention mechanism and Long Short-Term Memory (LSTM), which associates structure learning with parameter learning of first-order logical rules in an end-to-end differentiable neural networks model. In this framework, we designed a memory system and employed a multi-head dot product attention (MHDPA) to interact and update the memories embedded in the memory system for reasoning purposes. This is also consistent with the process of human cognition and reasoning, looking for enlightenment for the future in historical memory. In addition, we explored the use of inductive bias in deep learning to facilitate learning of entities, relations, and rules. Experiments establish the efficiency and effectiveness of our model and show that our method achieves better performance in tasks which include fact prediction and link prediction than baseline models on several benchmark datasets such as WN18RR, FB15K-237 and NELL-995. © 2020 Elsevier Ltd",Neurocomputing,10.1016/j.neucom.2020.02.065,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081931785&doi=10.1016%2fj.neucom.2020.02.065&partnerID=40&md5=1cbeb4e9a1aa7b61a3f9f20e5f3cd26e,2020,2021-07-20 15:48:40,2021-07-20 15:48:40
FELSBHY4,journalArticle,2020,"Yan, S.; Xie, Y.; Wu, F.; Smith, J.S.; Lu, W.; Zhang, B.",Image captioning via hierarchical attention mechanism and policy gradient optimization,"Automatically generating the descriptions of an image, i.e., image captioning, is an important and fundamental topic in artificial intelligence, which bridges the gap between computer vision and natural language processing. Based on the successful deep learning models, especially the CNN model and Long Short Term Memories (LSTMs) with attention mechanism, we propose a hierarchical attention model by utilizing both of the global CNN features and the local object features for more effective feature representation and reasoning in image captioning. The generative adversarial network (GAN), together with a reinforcement learning (RL) algorithm, is applied to solve the exposure bias problem in RNN-based supervised training for language problems. In addition, through the automatic measurement of the consistency between the generated caption and the image content by the discriminator in the GAN framework and RL optimization, we make the finally generated sentences more accurate and natural. Comprehensive experiments show the improved performance of the hierarchical attention mechanism and the effectiveness of our RL-based optimization method. Our model achieves state-of-the-art results on several important metrics in the MSCOCO dataset, using only greedy inference. © 2019",Signal Processing,10.1016/j.sigpro.2019.107329,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073026284&doi=10.1016%2fj.sigpro.2019.107329&partnerID=40&md5=2c3ccd0dc8202ff26aadf51d7310ce18,2020,2021-07-20 15:48:40,2021-07-20 15:48:40
FC8PKM8Q,journalArticle,2021,"Bagheri Khoulenjani, N.; Saniee Abadeh, M.; Sarbazi-Azad, S.; Jaddi, N.S.",Cancer miRNA biomarkers classification using a new representation algorithm and evolutionary deep learning,"The diagnosis of cancer is presently undergoing a change of paradigm for the diagnostic panel using molecular biomarkers. MicroRNA (miRNA) is one of the most important genomic datasets presenting the genome sequences. Since several studies have shown the relationship between miRNAs and cancers, data mining and machine learning methods can be incorporated to extract a large amount of knowledge from cancer genomic datasets. However, previous research works on the identification of cancers from miRNAs have made it possible to diagnose cancer, and the accuracy of some classes is not quite satisfactory. Therefore, this research is aimed at promoting a super-class (meta-label) approach and deep learning in a three-phase method to diagnose cancers from miRNAs. The steps in the first phase of the proposed method, named Representation learning, are partitioning data into super-classes, meta-data creation and super-classes classification. This phase helps data to be split into some subsets to improve classification accuracy. In other words, the first phase groups labels based on the separability of classes into a meta-label, and then a multi-label learner is built to predict these meta-labels. In the second phase, a feature selection to reduce the dimensions of the problem is applied to each super-class to help to focus the attention of an induction algorithm in those features that are more important to predict the target concept. In the third phase of the proposed method, an evolutionary deep neural network for the classification of labels in each super-class is performed. The last two phases are done separately for each subset in which five super-classes and subsequently five deep neural networks are trained. The experimental results reveal that the proposed method achieved more efficient results than 19 recent machine learning methods. Despite the fact that evaluating the dataset which consists of 29 types of cancers provides a more complicated situation for the convolutional neural network to be learned, the performance of the method is noticeably better than other existing methods. The other success which can be considered here is a significant reduction in running time comparing to other methods. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-020-05366-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092501283&doi=10.1007%2fs00500-020-05366-w&partnerID=40&md5=7fd9acff59bc7d0605fb02a2f74d38ac,2021,2021-07-20 15:48:40,2021-07-20 15:48:40
QFCCXVHG,journalArticle,2021,"Lim, S.; Prade, H.; Richard, G.",Classifying and completing word analogies by machine learning,"Analogical proportions are statements of the form ‘a is to b as c is to d’, formally denoted a:b::c:d. They are the basis of analogical reasoning which is often considered as an essential ingredient of human intelligence. For this reason, recognizing analogies in natural language has long been a research focus within the Natural Language Processing (NLP) community. With the emergence of word embedding models, a lot of progress has been made in NLP, essentially assuming that a word analogy like man:king::woman:queen is an instance of a parallelogram within the underlying vector space. In this paper, we depart from this assumption to adopt a machine learning approach, i.e., learning a substitute of the parallelogram model. To achieve our goal, we first review the formal modeling of analogical proportions, highlighting the properties which are useful from a machine learning perspective. For instance, the postulates supposed to govern such proportions entail that when a:b::c:d holds, then seven permutations of a,b,c,d still constitute valid analogies. From a machine learning perspective, this provides guidelines to build training sets of positive and negative examples. Taking into account these properties for augmenting the set of positive and negative examples, we first implement word analogy classifiers using various machine learning techniques, then we approximate by regression an analogy completion function, i.e., a way to compute the missing word when we have the three other ones. Using a GloVe embedding, classifiers show very high accuracy when recognizing analogies, improving state of the art on word analogy classification. Also, the regression processes usually lead to much more successful analogy completion than the ones derived from the parallelogram assumption. © 2021",International Journal of Approximate Reasoning,10.1016/j.ijar.2021.02.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100806437&doi=10.1016%2fj.ijar.2021.02.002&partnerID=40&md5=ef270977dcac9a665e311f90f0c452e7,2021,2021-07-20 15:48:40,2021-07-20 15:48:40
7QUCC4LF,journalArticle,2019,"Stockman, M.; Dwivedi, D.; Gentz, R.; Peisert, S.",Detecting control system misbehavior by fingerprinting programmable logic controller functionality,"In recent years, attacks such as the Stuxnet malware have demonstrated that cyberattacks against control systems cause extensive damage. These attacks can result in physical damage to the networked systems under their control. In this paper, we discuss our approach for detecting such attacks by distinguishing between programs running on a programmable logic controller (PLC) without having to monitor communications. Using power signatures generated by an attached, high-frequency power measurement device, we can identify what a PLC is doing and when an attack may have altered what the PLC should be doing. To accomplish this, we generated labeled data for testing our methods and applied feature engineering techniques and machine learning models. The results demonstrate that Random Forests and Convolutional Neural Networks classify programs with up to 98% accuracy for major program differences and 84% accuracy for minor differences. Our results can be used for both online and offline applications. © 2019 Elsevier B.V.",International Journal of Critical Infrastructure Protection,10.1016/j.ijcip.2019.100306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070316658&doi=10.1016%2fj.ijcip.2019.100306&partnerID=40&md5=7217c7522f0c10e820e5cd5f183dc4e1,2019,2021-07-20 15:48:40,2021-07-20 15:48:40
KQKN3SHS,journalArticle,2019,"Almalaq, A.; Zhang, J.J.",Evolutionary Deep Learning-Based Energy Consumption Prediction for Buildings,"Today's energy resources are closer to consumers due to sustainable energy and advanced technology. To that end, ensuring a precise prediction of energy consumption at the buildings' level is vital and significant to manage the consumed energy efficiently using a robust predictive model. Growing concern about reducing the energy consumption of buildings makes it necessary to predict the future energy consumption precisely using an optimizable predictive model. Most of the previously proposed methods for energy consumption prediction are conventional prediction methods that are normally designed based on the developer's knowledge about the hyper-parameters. However, the time lag inputs and the network's hyper-parameters of learning methods need to be adjusted to have a more accurate prediction. This paper proposes a novel hybrid prediction approach based on the evolutionary deep learning (DL) method that is combining genetic algorithm with long short-term memory and optimizing its objective function with time window lags and the network's hidden neurons. The performance of the presented optimization predictive model is investigated using public building datasets of residential and commercial buildings for very short-term prediction, and the results indicate that the evolutionary DL models have better performance than conventional and regular prediction models. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2887023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058895059&doi=10.1109%2fACCESS.2018.2887023&partnerID=40&md5=95370cf7e3e8a8c178d6c1cd8b6ca963,2019,2021-07-20 15:48:40,2021-07-20 15:48:40
ME6YR92R,journalArticle,2021,"Kasnesis, P.; Chatzigeorgiou, C.; Patrikakis, C.Z.; Rangoussi, M.",Modality-wise relational reasoning for one-shot sensor-based activity recognition,"Deep learning concepts have been successfully transferred from the computer vision task to that of wearable human activity recognition (HAR) over the last few years. However, deep learning models require a large volume of annotated samples to be efficiently trained, while adding new activities results in training the whole network from scratch. In this paper, we study the use of one-shot learning techniques based on high-level features extracted by deep neural networks that rely on convolutional layers. Using these feature vectors as input we measure the similarity of two activities by computing their Euclidean distance, cosine similarity or applying self-attention to perceive the relations between the signals. We evaluate four different one-shot learning approaches using two publicly available HAR datasets, by keeping out of the training set several activity classes. Our results demonstrate that the model relying on modality-wise relational reasoning surpasses the other three, achieving 94.8% and 84.41% one-shot accuracy on UCL and PAMAP2 dataset respectively, while we demonstrate the model's sensitivity on fusing sensor modalities and provide explainable attention maps to display the modality-wise similarities. © 2021 Elsevier B.V.",Pattern Recognition Letters,10.1016/j.patrec.2021.03.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103294943&doi=10.1016%2fj.patrec.2021.03.003&partnerID=40&md5=88b7d3cbd498e9923076606308d77fd9,2021,2021-07-20 15:48:40,2021-07-20 15:48:40
UMI9HNEY,journalArticle,2019,"Motepe, S.; Hasan, A.N.; Stopforth, R.",Improving Load Forecasting Process for a Power Distribution Network Using Hybrid AI and Deep Learning Algorithms,"Load forecasting is useful for various applications, including maintenance planning. The study of load forecasting using recent state-of-the-art hybrid artificial intelligence (AI) and deep learning (DL) techniques is limited in South Africa (SA) and South African power distribution networks. This paper proposes a novel hybrid AI and DL South African distribution network load forecasting system. The system comprises of modules that handle the collection of the loading data from the field, analysis of data integrity using fuzzy logic, data preprocessing, consolidation of the loading and the temperature data, and load forecasting. The load forecasting results are then used to inform maintenance planning. The load forecasting is conducted using a hybrid AI/DL load forecasting module. A novel comparative study of recent state-of-the-art AI techniques is also presented to determine the best technique to deploy in this module when forecasting South African power redistributing customers' loads. The impact of the inclusion of weather parameters and loading data clean up on the load forecasting performance of a hybrid AI technique, optimally pruned extreme learning machines (OP-ELM), and a deep learning technique, long short-term memory (LSTM), is also investigated. These techniques are compared with each other and also with a commonly used powerful hybrid AI technique, adaptive neuro-fuzzy inference system (ANFIS). LSTM was found to achieve higher load forecasting accuracies than ANFIS and OP-ELM in forecasting the two distribution customers' loads in this paper. Only the LSTM models' performance improved with the inclusion of temperature in their development. © 2019 IEEE.",IEEE Access,10.1109/ACCESS.2019.2923796,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068640058&doi=10.1109%2fACCESS.2019.2923796&partnerID=40&md5=9bbdc546bc5233dd0dd139eac2c2a5a5,2019,2021-07-20 15:48:40,2021-07-20 15:48:40
S8J4ZJ49,journalArticle,2014,"Olsher, D.","Semantically-based priors and nuanced knowledge core for Big Data, Social AI, and language understanding","Noise-resistant and nuanced, COGBASE makes 10 million pieces of commonsense data and a host of novel reasoning algorithms available via a family of semantically-driven prior probability distributions.Machine learning, Big Data, natural language understanding/processing, and social AI can draw on COGBASE to determine lexical semantics, infer goals and interests, simulate emotion and affect, calculate document gists and topic models, and link commonsense knowledge to domain models and social, spatial, cultural, and psychological data.COGBASE is especially ideal for social Big Data, which tends to involve highly implicit contexts, cognitive artifacts, difficult-to-parse texts, and deep domain knowledge dependencies. © 2014 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2014.05.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906098361&doi=10.1016%2fj.neunet.2014.05.022&partnerID=40&md5=45b8090cf9f747df4c5387382e737404,2014,2021-07-20 15:48:40,2021-07-20 15:48:40
XK7N7E5X,journalArticle,2019,"Žegklitz, J.; Pošík, P.",Symbolic regression in dynamic scenarios with gradually changing targets,"Symbolic regression is a machine learning task: given a training dataset with features and targets, find a symbolic function that best predicts the target given the features. This paper concentrates on dynamic regression tasks, i.e. tasks where the goal changes during the model fitting process. Our study is motivated by dynamic regression tasks originating in the domain of reinforcement learning: we study four dynamic symbolic regression problems related to well-known reinforcement learning benchmarks, with data generated from the standard Value Iteration algorithm. We first show that in these problems the target function changes gradually, with no abrupt changes. Even these gradual changes, however, are a challenge to traditional Genetic Programming-based Symbolic Regression algorithms because they rely only on expression manipulation and selection. To address this challenge, we present an enhancement to such algorithms suitable for dynamic scenarios with gradual changes, namely the recently introduced type of leaf nodes called Linear Combination of Features. This type of leaf node, aided by the error backpropagation technique known from artificial neural networks, enables the algorithm to better fit the data by utilizing the error gradient to its advantage rather than searching blindly using only the fitness values. This setup is compared with a baseline of the core algorithm without any of our improvements and also with a classic evolutionary dynamic optimization technique: hypermutation. The results show that the proposed modifications greatly improve the algorithm ability to track a gradually changing target. © 2019 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2019.105621,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068820262&doi=10.1016%2fj.asoc.2019.105621&partnerID=40&md5=79527ec4f68ad29593e594efeffa23b1,2019,2021-07-20 15:48:40,2021-07-20 15:48:40
WM45UBTN,journalArticle,2020,"Huang, D.; Fuhg, J.N.; Weißenfels, C.; Wriggers, P.",A machine learning based plasticity model using proper orthogonal decomposition,"Data-driven material models have many advantages over classical numerical approaches, such as the direct utilization of experimental data and the possibility to improve performance of predictions when additional data is available. One approach to develop a data-driven material model is to use machine learning tools. These can be trained offline to fit an observed material behaviour and then be applied in online applications. However, learning and predicting history dependent material models, such as plasticity, is still challenging. In this work, a machine learning based material modelling framework is proposed for both elasticity and plasticity. The machine learning based hyperelasticity model is developed with the Feed forward Neural Network (FNN) directly whereas the machine learning based plasticity model is developed by using of a novel method called Proper Orthogonal Decomposition Feed forward Neural Network (PODFNN). In order to account for the loading history, the accumulated absolute strain is proposed to be the history variable of the plasticity model. Additionally, the strain–stress sequence data for plasticity is collected from different loading–unloading paths based on the concept of sequence for plasticity. By means of the POD, the multi-dimensional stress sequence is decoupled leading to independent one dimensional coefficient sequences. In this case, the neural network with multiple output is replaced by multiple independent neural networks each possessing a one-dimensional output, which leads to less training time and better training performance. To apply the machine learning based material model in finite element analysis, the tangent matrix is derived by the automatic symbolic differentiation tool AceGen. The effectiveness and generalization of the presented models are investigated by a series of numerical examples using both 2D and 3D finite element analysis. © 2020 Elsevier B.V.",Computer Methods in Applied Mechanics and Engineering,10.1016/j.cma.2020.113008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082671315&doi=10.1016%2fj.cma.2020.113008&partnerID=40&md5=6392b8727e6e53eb8673ab758d4c56ae,2020,2021-07-20 15:48:40,2021-07-20 15:48:40
8UBLGFBX,journalArticle,2019,"Al-Hmouz, R.; Pedrycz, W.; Balamash, A.; Morfeq, A.",Logic-driven autoencoders,"Autoencoders are computing architectures encountered in various schemes of deep learning and realizing an efficient way of representing data in a compact way by forming a set of features. In this study, a concept, architecture, and algorithmic developments of logic-driven autoencoders are presented. In such structures, encoding and the decoding processes realized at the consecutive layers of the autoencoder are completed with the aid of some fuzzy logic operators (namely, OR, AND, NOT operations) and the ensuing encoding and decoding processing is carried out with the aid of fuzzy logic processing. The optimization of the autoencoder is completed through a gradient-based learning. The transparent knowledge representation delivered by autoencoders is facilitated by the involvement of logic processing, which implies that the encoding mechanism comes with the generalization abilities delivered by OR neurons while the specialization mechanism is achieved by the AND-like neurons forming the decoding layer. A series of illustrative examples is also presented. © 2019 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2019.104874,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070212527&doi=10.1016%2fj.knosys.2019.104874&partnerID=40&md5=d23a07c789b979b939e4c2b1b880b18a,2019,2021-07-20 15:48:40,2021-07-20 15:48:40
TI86647N,journalArticle,2021,"Chary, M.; Boyer, E.W.; Burns, M.M.",Diagnosis of Acute Poisoning using explainable artificial intelligence,"Introduction: Medical toxicology is the clinical specialty that treats the toxic effects of substances, for example, an overdose, a medication error, or a scorpion sting. The volume of toxicological knowledge and research has, as with other medical specialties, outstripped the ability of the individual clinician to entirely master and stay current with it. The application of machine learning/artificial intelligence (ML/AI) techniques to medical toxicology is challenging because initial treatment decisions are often based on a few pieces of textual data and rely heavily on experience and prior knowledge. ML/AI techniques, moreover, often do not represent knowledge in a way that is transparent for the physician, raising barriers to usability. Logic-based systems are more transparent approaches, but often generalize poorly and require expert curation to implement and maintain. Methods: We constructed a probabilistic logic network to model how a toxicologist recognizes a toxidrome, using only physical exam findings. Our approach transparently mimics the knowledge representation and decision-making of practicing clinicians. We created a library of 300 synthetic cases of varying clinical complexity. Each case contained 5 physical exam findings drawn from a mixture of 1 or 2 toxidromes. We used this library to evaluate the performance of our probabilistic logic network, dubbed Tak, against 2 medical toxicologists, a decision tree model, as well as its ability to recover the actual diagnosis. Results: The inter-rater reliability between Tak and the consensus of human raters was κ = 0.8432 for straightforward cases, 0.4396 for moderately complex cases, and 0.3331 for challenging cases. The inter-rater reliability between the decision tree classifier and the consensus of human raters was, κ = 0.2522 for straightforward cases, 0.1963 for moderately complex cases and 0.0331 for challenging cases. Conclusions: The software, dubbed Tak, performs comparably to humans on straightforward cases and intermediate difficulty cases, but is outperformed by humans on challenging clinical cases. Tak outperforms a decision tree classifier at all levels of difficulty. Our results are a proof-of-concept that, in a restricted domain, probabilistic logic networks can perform medical reasoning comparably to humans. © 2021 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2021.104469,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107300043&doi=10.1016%2fj.compbiomed.2021.104469&partnerID=40&md5=5cb19376d1981f68c9797e13251c033d,2021,2021-07-20 15:48:41,2021-07-20 15:48:41
BHHRLTYN,journalArticle,2011,"Dodig-Crnkovic, G.","Significance of models of computation, from turing model to natural computation","The increased interactivity and connectivity of computational devices along with the spreading of computational tools and computational thinking across the fields, has changed our understanding of the nature of computing. In the course of this development computing models have been extended from the initial abstract symbol manipulating mechanisms of stand-alone, discrete sequential machines, to the models of natural computing in the physical world, generally concurrent asynchronous processes capable of modelling living systems, their informational structures and dynamics on both symbolic and sub-symbolic information processing levels. Present account of models of computation highlights several topics of importance for the development of new understanding of computing and its role: natural computation and the relationship between the model and physical implementation, interactivity as fundamental for computational modelling of concurrent information processing systems such as living organisms and their networks, and the new developments in logic needed to support this generalized framework. Computing understood as information processing is closely related to natural sciences; it helps us recognize connections between sciences, and provides a unified approach for modeling and simulating of both living and non-living systems. © Springer Science+Business Media B.V. 2011.",Minds and Machines,10.1007/s11023-011-9235-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959541227&doi=10.1007%2fs11023-011-9235-1&partnerID=40&md5=45e543febfe4fb73c47ee2543f846541,2011,2021-07-20 15:48:41,2021-07-20 15:48:41
5ZKZKZH8,journalArticle,2016,"Kate, R.J.",Using dynamic time warping distances as features for improved time series classification,"Dynamic time warping (DTW) has proven itself to be an exceptionally strong distance measure for time series. DTW in combination with one-nearest neighbor, one of the simplest machine learning methods, has been difficult to convincingly outperform on the time series classification task. In this paper, we present a simple technique for time series classification that exploits DTW’s strength on this task. But instead of directly using DTW as a distance measure to find nearest neighbors, the technique uses DTW to create new features which are then given to a standard machine learning method. We experimentally show that our technique improves over one-nearest neighbor DTW on 31 out of 47 UCR time series benchmark datasets. In addition, this method can be easily extended to be used in combination with other methods. In particular, we show that when combined with the symbolic aggregate approximation (SAX) method, it improves over it on 37 out of 47 UCR datasets. Thus the proposed method also provides a mechanism to combine distance-based methods like DTW with feature-based methods like SAX. We also show that combining the proposed classifiers through ensembles further improves the performance on time series classification. © 2015, The Author(s).",Data Mining and Knowledge Discovery,10.1007/s10618-015-0418-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958052891&doi=10.1007%2fs10618-015-0418-x&partnerID=40&md5=ad0655a73612a3154bb252cd41bab3d9,2016,2021-07-20 15:48:41,2021-07-20 15:48:41
B2ZYRVRM,journalArticle,2020,"Zhang, L.; He, W.; Morkved, O.; Zhao, V.; Littman, M.L.; Lu, S.; Ur, B.",Trace2TAP: Synthesizing Trigger-Action Programs from Traces of Behavior,"Two common approaches for automating IoT smart spaces are having users write rules using trigger-action programming (TAP) or training machine learning models based on observed actions. In this paper, we unite these approaches. We introduce and evaluate Trace2TAP, a novel method for automatically synthesizing TAP rules from traces (time-stamped logs of sensor readings and manual actuations of devices). We present a novel algorithm that uses symbolic reasoning and SAT-solving to synthesize TAP rules from traces. Compared to prior approaches, our algorithm synthesizes generalizable rules more comprehensively and fully handles nuances like out-of-order events. Trace2TAP also iteratively proposes modified TAP rules when users manually revert automations. We implemented our approach on Samsung SmartThings. Through formative deployments in ten offices, we developed a clustering/ranking system and visualization interface to intelligibly present the synthesized rules to users. We evaluated Trace2TAP through a field study in seven additional offices. Participants frequently selected rules ranked highly by our clustering/ranking system. Participants varied in their automation priorities, and they sometimes chose rules that would seem less desirable by traditional metrics like precision and recall. Trace2TAP supports these differing priorities by comprehensively synthesizing TAP rules and bringing humans into the loop during automation. © 2020 Owner/Author.","Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies",10.1145/3411838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092435676&doi=10.1145%2f3411838&partnerID=40&md5=b95c8d5171bb8976d4bf2b1aa946503d,2020,2021-07-20 15:48:41,2021-07-20 15:48:41
SKTJX4GI,journalArticle,2020,"Mathisen, B.M.; Aamodt, A.; Bach, K.; Langseth, H.",Learning similarity measures from data,"Defining similarity measures is a requirement for some machine learning methods. One such method is case-based reasoning (CBR) where the similarity measure is used to retrieve the stored case or a set of cases most similar to the query case. Describing a similarity measure analytically is challenging, even for domain experts working with CBR experts. However, datasets are typically gathered as part of constructing a CBR or machine learning system. These datasets are assumed to contain the features that correctly identify the solution from the problem features; thus, they may also contain the knowledge to construct or learn such a similarity measure. The main motivation for this work is to automate the construction of similarity measures using machine learning. Additionally, we would like to do this while keeping training time as low as possible. Working toward this, our objective is to investigate how to apply machine learning to effectively learn a similarity measure. Such a learned similarity measure could be used for CBR systems, but also for clustering data in semi-supervised learning, or one-shot learning tasks. Recent work has advanced toward this goal which relies on either very long training times or manually modeling parts of the similarity measure. We created a framework to help us analyze the current methods for learning similarity measures. This analysis resulted in two novel similarity measure designs: The first design uses a pre-trained classifier as basis for a similarity measure, and the second design uses as little modeling as possible while learning the similarity measure from data and keeping training time low. Both similarity measures were evaluated on 14 different datasets. The evaluation shows that using a classifier as basis for a similarity measure gives state-of-the-art performance. Finally, the evaluation shows that our fully data-driven similarity measure design outperforms state-of-the-art methods while keeping training time low. © 2019, The Author(s).",Progress in Artificial Intelligence,10.1007/s13748-019-00201-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074730982&doi=10.1007%2fs13748-019-00201-2&partnerID=40&md5=378f18593449a1e13e5b19a7b144a87c,2020,2021-07-20 15:48:41,2021-07-20 15:48:41
D6GWTMCQ,journalArticle,2021,"Mirsky, R.; Keren, S.; Geib, C.",Introduction to Symbolic Plan and Goal Recognition Reuth Mirsky,"Plan recognition, activity recognition, and goal recognition all involve making inferences about other actors based on observations of their interactions with the environment and other agents. This synergistic area of research combines, unites, and makes use of techniques and research from a wide range of areas including user modeling, machine vision, automated planning, intelligent user interfaces, human-computer interaction, autonomous and multi-agent systems, natural language understanding, and machine learning. It plays a crucial role in a wide variety of applications including assistive technology, software assistants, computer and network security, human-robot collaboration, natural language processing, video games, and many more. This wide range of applications and disciplines has produced a wealth of ideas, models, tools, and results in the recognition literature. However, it has also contributed to fragmentation in the field, with researchers publishing relevant results in a wide spectrum of journals and conferences. This book seeks to address this fragmentation by providing a high-level introduction and historical overview of the plan and goal recognition literature. It provides a description of the core elements that comprise these recognition problems and practical advice for modeling them. In particular, we define and distinguish the different recognition tasks. We formalize the major approaches to modeling these problems using a single motivating example. Finally, we describe a number of state-of-the-art systems and their extensions, future challenges, and some potential applications. Table of Contents: Preface / Acknowledgments / Introduction / Defining a Recognition Problem / Implicit vs. Explicit Representation of Knowledge / Improving a Recognizer / Future Directions / Bibliography / Authors' Biographies Copyright © 2021 by Morgan & Claypool.",Synthesis Lectures on Artificial Intelligence and Machine Learning,10.2200/S01062ED1V01Y202012AIM047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100068841&doi=10.2200%2fS01062ED1V01Y202012AIM047&partnerID=40&md5=ac6cfbd4605faaf2364ae17a016712f6,2021,2021-07-20 15:48:41,2021-07-20 15:48:41
RQ8SSQUN,journalArticle,2012,"Dědek, J.; Vojtáš, P.; Vomlelová, M.",Fuzzy ILP Classification of web reports after linguistic text mining,"In this paper we study the problem of classification of textual web reports. We are specifically focused on situations in which structured information extracted from the reports is used for classification. We present an experimental classification system based on usage of third party linguistic analyzers, our previous work on web information extraction, and fuzzy inductive logic programming (fuzzy ILP). A detailed study of the so-called 'Fuzzy ILP Classifier' is the main contribution of the paper. The study includes formal models, prototype implementation, extensive evaluation experiments and comparison of the classifier with other alternatives like decision trees, support vector machines, neural networks, etc. © 2011 Elsevier Ltd. All rights reserved.",Information Processing and Management,10.1016/j.ipm.2011.02.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859164254&doi=10.1016%2fj.ipm.2011.02.008&partnerID=40&md5=d440fba239f48ea9fd51f1b5153b7408,2012,2021-07-20 15:48:41,2021-07-20 15:48:41
I2FPD8T4,journalArticle,2016,"Wang, H.; Blanton, R.D.S.",Ensemble reduction via logic minimization,"An ensemble of machine learning classifiers usually improves generalization performance and is useful for many applications. However, the extra memory storage and computational cost incurred from the combined models often limits their potential applications. In this article, we propose a new ensemble reduction method called CANOPY that significantly reduces memory storage and computations. CANOPY uses a technique from logic minimization for digital circuits to select and combine particular classification models from an initial pool in the form of a Boolean function, through which the reduced ensemble performs classification. Experiments on 20 UCI datasets demonstrate that CANOPY either outperforms or is very competitive with the initial ensemble and one state-of-the-art ensemble reduction method in terms of generalization error, and is superior to all existing reduction methods surveyed for identifying the smallest numbers of models in the reduced ensembles. © 2016 ACM.",ACM Transactions on Design Automation of Electronic Systems,10.1145/2897515,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974577919&doi=10.1145%2f2897515&partnerID=40&md5=27cbd3561f820262bb9e0327b415b223,2016,2021-07-20 15:48:41,2021-07-20 15:48:41
MQAZ3KLB,journalArticle,2020,"Khalek, N.A.; Hamouda, W.","From Cognitive to Intelligent Secondary Cooperative Networks for the Future Internet: Design, Advances, and Challenges","Cognitive Radio (CR) technology was first introduced to solve the problem of radio spectrum under-utilization. A cognitive radio network consists of smart radio devices that have the ability to sense radio environment variables and take actions accordingly. To realize their full potential and to become fully cognitive, the CR nodes need to be equipped with learning and reasoning capabilities. Machine learning has been one of the enabling vehicles for intelligent CR networks. Inspired by the cognition cycle of a CR node, over the past years there has been an ever growing interest in using machine learning techniques to enhance the performance of CR networks. In this article, an overview of the various learning techniques currently used in the literature of CR networks is given. We focus on feature classification and clustering algorithms, and their application in cooperative CR networks. We outline the steps to establishing a learning-based cooperative secondary network, highlighting factors that impact detection performance. Additionally, current state-of-the-art learning-based applications in Cognitive Internet of Things (CIoT) are presented. Finally, the key challenges and future directions of intelligent cognitive networks are discussed. IEEE",IEEE Network,10.1109/MNET.011.2000504,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096017088&doi=10.1109%2fMNET.011.2000504&partnerID=40&md5=7f61c41f5082a95ce8fb02f890e762b8,2020,2021-07-20 15:48:41,2021-07-20 15:48:41
66UQX9LW,journalArticle,2017,"Tam, G.K.L.; Kothari, V.; Chen, M.",An Analysis of Machine- and Human-Analytics in Classification,"In this work, we present a study that traces the technical and cognitive processes in two visual analytics applications to a common theoretic model of soft knowledge that may be added into a visual analytics process for constructing a decision-tree model. Both case studies involved the development of classification models based on the 'bag of features' approach. Both compared a visual analytics approach using parallel coordinates with a machine-learning approach using information theory. Both found that the visual analytics approach had some advantages over the machine learning approach, especially when sparse datasets were used as the ground truth. We examine various possible factors that may have contributed to such advantages, and collect empirical evidence for supporting the observation and reasoning of these factors. We propose an information-theoretic model as a common theoretic basis to explain the phenomena exhibited in these two case studies. Together we provide interconnected empirical and theoretical evidence to support the usefulness of visual analytics. © 2016 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2016.2598829,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999287019&doi=10.1109%2fTVCG.2016.2598829&partnerID=40&md5=151cb0469f99c768864997bce90e6c8e,2017,2021-07-20 15:48:41,2021-07-20 15:48:41
G5P9LXN5,journalArticle,2013,"Maier, F.; Ma, Y.; Hitzler, P.",Paraconsistent OWL and related logics,"The Web Ontology Language OWL is currently the most prominent formalism for representing ontologies in Semantic Web applications. OWL is based on description logics, and automated reasoners are used to infer knowledge implicitly present in OWL ontologies. However, because typical description logics obey the classical principle of explosion, reasoning over inconsistent ontologies is impossible in OWL. This is so despite the fact that inconsistencies are bound to occur in many realistic cases, e.g., when multiple ontologies are merged or when ontologies are created by machine learning or data mining tools. In this paper, we present four-valued paraconsistent description logics which can reason over inconsistencies. We focus on logics corresponding to OWL DL and its profiles. We present the logic SROIQ4, showing that it is both sound relative to classical SROIQ and that its embedding into SROIQ is consequence preserving. We also examine paraconsistent varieties of EL++, DL-Lite, and Horn-DLs. The general framework described here has the distinct advantage of allowing classical reasoners to draw sound but nontrivial conclusions from even inconsistent knowledge bases. Truth-value gaps and gluts can also be selectively eliminated from models (by inserting additional axioms into knowledge bases). If gaps but not gluts are eliminated, additional classical conclusions can be drawn without affecting paraconsistency. © 2012-IOS Press and the authors. All rights reserved.",Semantic Web,10.3233/sw-2012-0066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890268194&doi=10.3233%2fsw-2012-0066&partnerID=40&md5=46930fddc5746535a29a44abd80490ac,2013,2021-07-20 15:48:41,2021-07-20 15:48:41
5Z8CLFZV,journalArticle,2014,"Stein, G.; Gonzalez, A.J.",Learning in context: enhancing machine learning with context-based reasoning,"This article describes how an experiment to train an agent to perform a task, which had originally failed, was made successful by incorporating a contextual structure that decomposed the tasks into contexts through Context-based Reasoning. The task involved a simulation of a crane that was used by a human operator to move boxes from arbitrary locations throughout a wide area to a designated drop off location in the environment. Initial attempts to teach an agent how to perform the task through observation in a context-free manner yielded poor performance. However, when the task to be learned was decomposed into separate contexts and the agents learned each context independently, the performance improved significantly. The paper describes the process that enabled the improvements achieved and discusses the tests and results that demonstrated the improvement. © 2014, Springer Science+Business Media New York.",Applied Intelligence,10.1007/s10489-014-0550-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918829113&doi=10.1007%2fs10489-014-0550-0&partnerID=40&md5=3ea2290040c29ab064596da7a906bc10,2014,2021-07-20 15:48:41,2021-07-20 15:48:41
PDXCAKK7,journalArticle,2014,"Wang, L.; Liu, Z.; Chen, C.L.P.; Zhang, Y.",Interval type-2 fuzzy weighted support vector machine learning for energy efficient biped walking,"An interval type-2 fuzzy weighted support vector machine (IT2FW-SVM) is proposed to address the problem of high energy consumption for biped walking robots. Different from the traditional machine learning method of 'copy learning', the proposed IT2FW-SVM obtains lower energy cost and larger zero moment point (ZMP) stability margin using a novel strategy of 'selective learning', which is similar to human selections based on experience. To handle the uncertainty of the experience, the learning weights in the IT2FW-SVM are deduced using an interval type-2 fuzzy logic system (IT2FLS), which is an extension of the previous weighted SVM. Simulation studies show that the existing biped walking which generates the original walking samples is improved remarkably in terms of both energy efficiency and biped dynamic balance using the proposed IT2FW-SVM. © 2013 Springer Science+Business Media New York.",Applied Intelligence,10.1007/s10489-013-0472-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897021938&doi=10.1007%2fs10489-013-0472-2&partnerID=40&md5=07e02d504c18f495bdce5fb11d415a09,2014,2021-07-20 15:48:41,2021-07-20 15:48:41
JQIKCMV2,journalArticle,2021,"Abeyrathna, K.D.; Granmo, O.-C.; Goodwin, M.",Extending the Tsetlin Machine with Integer-Weighted Clauses for Increased Interpretability,"Building models that are both interpretable and accurate is an unresolved challenge for many pattern recognition problems. In general, rule-based and linear models lack accuracy, while deep learning interpretability is based on rough approximations of the underlying inference. However, recently, the rule-based Tsetlin Machines (TMs) have obtained competitive performance in terms of accuracy, memory footprint, and inference speed on diverse benchmarks (image classification, regression, natural language understanding, and game-playing). TMs construct rules using human-interpretable conjunctive clauses in propositional logic. These, in turn, are combined linearly to solve complex pattern recognition tasks. This paper addresses the accuracy-interpretability challenge in machine learning by introducing a TM with integer weighted clauses - the Integer Weighted TM (IWTM). The intent is to increase TM interpretability by reducing the number of clauses required for competitive performance. The IWTM achieves this by weighting the clauses so that a single clause can replace multiple duplicates. Since each TM clause is formed adaptively by a Tsetlin Automata (TA) team, identifying effective weights becomes a challenging online learning problem. We solve this problem by extending each team of TA with another kind of automaton: the stochastic searching on the line (SSL) automaton. We evaluate the performance of the new scheme empirically using five datasets, along with a study of interpretability. On average, IWTM uses 6.5 times fewer literals than the vanilla TM and 120 times fewer literals than a TM with real-valued weights. Furthermore, in terms of average memory usage and F1-Score, IWTM outperforms simple Multi-Layered Artificial Neural Networks, Decision Trees, Support Vector Machines, K-Nearest Neighbor, Random Forest, Gradient Boosted Trees (XGBoost), Explainable Boosting Machines (EBMs), as well as the standard and real-value weighted TMs. IWTM finally outperforms Neural Additive Models on Fraud Detection and StructureBoost on CA-58 in terms of Area Under Curve, while performing competitively on COMPAS. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3049569,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099201234&doi=10.1109%2fACCESS.2021.3049569&partnerID=40&md5=5322e5ba8ef699b8165123d956114d20,2021,2021-07-20 15:48:42,2021-07-20 15:48:42
GDPJCNFX,journalArticle,2011,"Atighetchi, M.; Benyo, B.; Gosain, A.; MacIntyre, R.; Pal, P.; Travers, V.; Zinky, J.",Transparent insertion of custom logic in HTTP(S) streams using PbProxy,"Cost and testing considerations limit the acceptance and deployment of technologies that make information exchanges more secure, reliable, semantically understandable, and self-improving. PbProxy is a flexible proxy that enables transparent insertion of custom logic into HTTP and HTTPS interactions. It has successfully been used to facilitate behavior-based prevention of phishing attacks, machine learning of Web service procedures, and Web browsing over disruption-tolerant networks by injecting custom logic into existing applications and communication streams. PbProxy encapsulates common functionality into a proxy base and supports customizable plugins to foster code reuse. © 2011 IEEE.",IEEE Internet Computing,10.1109/MIC.2010.103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955552295&doi=10.1109%2fMIC.2010.103&partnerID=40&md5=87ea5b0bbc2d851e9a7e259352a93bdc,2011,2021-07-20 15:48:42,2021-07-20 15:48:42
WIM3YU49,journalArticle,2020,"Fazel Zarandi, M.H.; Sadat Asl, A.A.; Sotudian, S.; Castillo, O.",A state of the art review of intelligent scheduling,"Intelligent scheduling covers various tools and techniques for successfully and efficiently solving the scheduling problems. In this paper, we provide a survey of intelligent scheduling systems by categorizing them into five major techniques containing fuzzy logic, expert systems, machine learning, stochastic local search optimization algorithms and constraint programming. We also review the application case studies of these techniques. © 2018, Springer Nature B.V.",Artificial Intelligence Review,10.1007/s10462-018-9667-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056868663&doi=10.1007%2fs10462-018-9667-6&partnerID=40&md5=be2991a7f2eb3dba655a804466669b88,2020,2021-07-20 15:48:42,2021-07-20 15:48:42
U9TXV924,journalArticle,2021,"Pathak, S.; Lu, C.; Nagaraj, S.B.; van Putten, M.; Seifert, C.",STQS: Interpretable multi-modal Spatial-Temporal-seQuential model for automatic Sleep scoring,"Sleep scoring is an important step for the detection of sleep disorders and usually performed by visual analysis. Since manual sleep scoring is time consuming, machine-learning based approaches have been proposed. Though efficient, these algorithms are black-box in nature and difficult to interpret by clinicians. In this paper, we propose a deep learning architecture for multi-modal sleep scoring, investigate the model's decision making process, and compare the model's reasoning with the annotation guidelines in the AASM manual. Our architecture, called STQS, uses convolutional neural networks (CNN) to automatically extract spatio-temporal features from 3 modalities (EEG, EOG and EMG), a bidirectional long short-term memory (Bi-LSTM) to extract sequential information, and residual connections to combine spatio-temporal and sequential features. We evaluated our model on two large datasets, obtaining an accuracy of 85% and 77% and a macro F1 score of 79% and 73% on SHHS and an in-house dataset, respectively. We further quantify the contribution of various architectural components and conclude that adding LSTM layers improves performance over a spatio-temporal CNN, while adding residual connections does not. Our interpretability results show that the output of the model is well aligned with AASM guidelines, and therefore, the model's decisions correspond to domain knowledge. We also compare multi-modal models and single-channel models and suggest that future research should focus on improving multi-modal models. © 2021 The Author(s)",Artificial Intelligence in Medicine,10.1016/j.artmed.2021.102038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102305568&doi=10.1016%2fj.artmed.2021.102038&partnerID=40&md5=3c1d894f17de63a54c8ed18d4d4550b6,2021,2021-07-20 15:48:42,2021-07-20 15:48:42
QJ4AF8EJ,journalArticle,2020,"Martinez-Gil, J.; Chaves-Gonzalez, J.M.",A novel method based on symbolic regression for interpretable semantic similarity measurement,"The problem of automatically measuring the degree of semantic similarity between textual expressions is a challenge that consists of calculating the degree of likeness between two text fragments that have none or few features in common according to human judgment. In recent times, several machine learning methods have been able to establish a new state-of-the-art regarding the accuracy, but none or little attention has been paid to their interpretability, i.e. the extent to which an end-user could be able to understand the cause of the output from these approaches. Although such solutions based on symbolic regression already exist in the field of clustering, we propose here a new approach which is being able to reach high levels of interpretability without sacrificing accuracy in the context of semantic textual similarity. After a complete empirical evaluation using several benchmark datasets, it is shown that our approach yields promising results in a wide range of scenarios. © 2020 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2020.113663,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087275641&doi=10.1016%2fj.eswa.2020.113663&partnerID=40&md5=6471ee0af40219ec2cf132803f6e8d8d,2020,2021-07-20 15:48:42,2021-07-20 15:48:42
2PZIW8P2,journalArticle,2017,"Versino, D.; Tonda, A.; Bronkhorst, C.A.",Data driven modeling of plastic deformation,"In this paper the application of machine learning techniques for the development of constitutive material models is being investigated. A flow stress model, for strain rates ranging from 10−4 to 1012 (quasi-static to highly dynamic), and temperatures ranging from room temperature to over 1000 K, is obtained by beginning directly with experimental stress–strain data for Copper. An incrementally objective and fully implicit time integration scheme is employed to integrate the hypo-elastic constitutive model, which is then implemented into a finite element code for evaluation. Accuracy and performance of the flow stress models derived from symbolic regression are assessed by comparison to Taylor anvil impact data. The results obtained with the free-form constitutive material model are compared to well-established strength models such as the Preston–Tonks–Wallace (PTW) model and the Mechanical Threshold Stress (MTS) model. Preliminary results show candidate free-form models comparing well with data in regions of stress–strain space with sufficient experimental data, pointing to a potential means for both rapid prototyping in future model development, as well as the use of machine learning in capturing more data as a guide for more advanced model development. © 2017 Elsevier B.V.",Computer Methods in Applied Mechanics and Engineering,10.1016/j.cma.2017.02.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014514126&doi=10.1016%2fj.cma.2017.02.016&partnerID=40&md5=f8679d3bf75737b7114121a5ac82a80e,2017,2021-07-20 15:48:42,2021-07-20 15:48:42
85WXY5JP,journalArticle,2013,"Podgorelec, V.; Šprogar, M.; Pohorec, S.",Evolutionary design of decision trees,"Decision tree (DT) is one of the most popular symbolic machine learning approaches to classification with a wide range of applications. Decision trees are especially attractive in data mining. It has an intuitive representation and is, therefore, easy to understand and interpret, also by nontechnical experts. The most important and critical aspect of DTs is the process of their construction. Several induction algorithms exist that use the recursive top-down principle to divide training objects into subgroups based on different statistical measures in order to achieve homogeneous subgroups. Although being robust and fast, generally providing good results, their deterministic and heuristic nature can lead to suboptimal solutions. Therefore, alternative approaches have developed which try to overcome the drawbacks of classical induction. One of the most viable approaches seems to be the use of evolutionary algorithms, which can produce better DTs as they are searching for globally optimal solutions, evaluating potential solutions with regard to different criteria. We review the process of evolutionary design of DTs, providing the description of the most common approaches as well as referring to recognized specializations. The overall process is first explained and later demonstrated in a step-by-step case study using a dataset from the University of California, Irvine (UCI) machine learning repository. © 2012 John Wiley & Sons, Inc.",Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery,10.1002/widm.1079,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879535331&doi=10.1002%2fwidm.1079&partnerID=40&md5=785662a3d0e299b4a6132980ff6c9d96,2013,2021-07-20 15:48:42,2021-07-20 15:48:42
XL6MBMQQ,journalArticle,2020,"Kushik, N.; Yevtushenko, N.; Evtushenko, T.",Novel machine learning technique for predicting teaching strategy effectiveness,"In this paper, we present an approach for evaluating and predicting the student's level of proficiency when using a certain teaching strategy. This problem remains a hot topic, especially nowadays when information technologies are highly integrated into the educational process. Such a problem is essential for those institutions that rely on e-learning strategies as various techniques for the same teaching activities and disciplines are now available online. In order to effectively predict the quality of this type of (electronic) educational process we suggest to use one of the well known machine learning techniques. In particular, a proposed approach relies on using logic circuits/networks for such prediction. Given an electronic service providing a teaching strategy, the mathematical model of logic circuits is used for evaluating the student's level of proficiency. Given two (or more) logic circuits that predict the student's educational proficiency using different electronic services (teaching strategies), we also propose a method for synthesizing the resulting logic circuit that predicts the effectiveness of the teaching process when two given strategies are combined. The proposed technique can be effectively used in the educational management when the best (online) teaching strategy should be chosen based on student's goals, individual features, needs and preferences. As an example of the technique proposed in the paper, we consider an educational process of teaching foreign languages at one of Russian universities. Preliminary experimental results demonstrate the expected scalability and applicability of the proposed approach. © 2016 Elsevier Ltd",International Journal of Information Management,10.1016/j.ijinfomgt.2016.02.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056746718&doi=10.1016%2fj.ijinfomgt.2016.02.006&partnerID=40&md5=33b9bfb316a509045aab830d85ef12dd,2020,2021-07-20 15:48:42,2021-07-20 15:48:42
QV9CPFGX,journalArticle,2014,"Grozea, C.; Popescu, M.",Can machine learning learn a decision oracle for NP problems? A test on SAT,"This note describes our experiments aiming to empirically test the ability of machine learning models to act as decision oracles for NP problems. Focusing on satisfiability testing problems, we have generated random 3-SAT instances and found out that the correct branch prediction accuracy reached levels in excess of 99%. The branching in a simple backtracking-based SAT solver has been reduced in more than 90% of the tested cases, and the average number of branching steps has reduced to between 1/5 and 1/3 of the one without the machine learning model. The percentage of SAT instances where the machine learned heuristic-enhanced algorithm solved SAT in a single pass reached levels of 80-90%, depending on the set of features used.",Fundamenta Informaticae,10.3233/FI-2014-1024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900402398&doi=10.3233%2fFI-2014-1024&partnerID=40&md5=292ca8e0bd24f3d7d6eefcbe9ceb4a71,2014,2021-07-20 15:48:42,2021-07-20 15:48:42
6GZPGNHG,journalArticle,2021,"Chennupati, G.; Santhi, N.; Romero, P.; Eidenbenz, S.",Machine learning-enabled scalable performance prediction of scientific codes,"Hardware architectures become increasingly complex as the compute capabilities grow to exascale. We present the Analytical Memory Model with Pipelines (AMMP) of the Performance Prediction Toolkit (PPT). PPT-AMMP takes high-level source code and hardware architecture parameters as input and predicts runtime of that code on the target hardware platform, which is defined in the input parameters. PPT-AMMP transforms the code to an (architecture-independent) intermediate representation, then (i) analyzes the basic block structure of the code, (ii) processes architecture-independent virtual memory access patterns that it uses to build memory reuse distance distribution models for each basic block, and (iii) runs detailed basic-block level simulations to determine hardware pipeline usage. PPT-AMMP uses machine learning and regression techniques to build the prediction models based on small instances of the input code, then integrates into a higher-order discrete-event simulation model of PPT running on Simian PDES engine. We validate PPT-AMMP on four standard computational physics benchmarks and present a use case of hardware parameter sensitivity analysis to identify bottleneck hardware resources on different code inputs. We further extend PPT-AMMP to predict the performance of a scientific application code, namely, the radiation transport mini-app SNAP. To this end, we analyze multi-variate regression models that accurately predict the reuse profiles and the basic block counts. We validate predicted SNAP runtimes against actual measured times. © 2021 Public Domain.",ACM Transactions on Modeling and Computer Simulation,10.1145/3450264,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105023096&doi=10.1145%2f3450264&partnerID=40&md5=affafe64a1b8aec2985e0597755db8e8,2021,2021-07-20 15:48:42,2021-07-20 15:48:42
HTKS76HQ,journalArticle,2020,"Asif, M.; Ahmed, J.",A Novel Case Base Reasoning and Frequent Pattern Based Decision Support System for Mitigating Software Risk Factors,"Software risk management is crucial for the success of software project development. The existing literature has models for risk management, but is too complex to be used in practice. The information in the existing studies is scattered over different articles which makes it difficult to find relevant knowledge to establish relationship between risk factors and mitigations. This paper presents a novel model which identifies the relationship between risk factors and mitigations automatically by using intelligent Decision Support System (DSS). The proposed model has four steps. Firstly, the input of the system has been designed where risk factors and mitigations have been inputted into it. Secondly, rule based machine learning approach has been used for mining of associations between risks and mitigations. Thirdly, Case Based Reasoning (CBR) approach has been used to determine the previous cases as rules. Finally, automated rules have been generated to develop an intelligent DSS to mitigate the software risks. The proposed technique copes with the highly cited existing limitations of risk handling like, lack of generic DSS and intelligent relationship between software risks and mitigations. Automated rules have been discovered with a novel idea of CBR and frequent pattern. The proposed model is capable of mitigating upcoming risks in future. Star schema has been implemented to support our proposed DSS. Moreover, from highly cited literature 40 studies were identified from which 26 risk factors, 57 mitigations, 14 questions and 26 automated rules have been extracted. According to the validation of IT industry experts, the average of the effectiveness of DSS is 51-55%. The novelty of the proposed research is that it uses two state of the art methods (Rule Based Machine Learning and CBR) to identify software risk mitigations. The results of the proposed model show that the chances of risks in software development have been reduced significantly. © 2020 IEEE.",IEEE Access,10.1109/ACCESS.2020.2999036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086637188&doi=10.1109%2fACCESS.2020.2999036&partnerID=40&md5=894751637900bfd13f82b2e9c0649b03,2020,2021-07-20 15:48:42,2021-07-20 15:48:42
BUF2U245,journalArticle,2021,"Asim, Y.; Malik, A.K.; Raza, B.; Shahid, A.R.; Qamar, N.","Predicting Influential Blogger's by a Novel, Hybrid and Optimized Case Based Reasoning Approach with Balanced Random Forest Using Imbalanced Data","Bloggers possess the capability of understanding and influencing mass psychology to a wide community of fans and followers by posting their online valuable content. Their dominance over audience can be used as a helping hand in the corporate world which desires to disseminate their product or services among diversified people belonging to varying localities, and is always on the lookout for suitable and quick ways to grasp public access. Due to this reason, influential bloggers are preferred in the online market to initiate marketing campaigns which is a thought-provoking task due to loads of blogger communities. The novelty of this paper lies in the proposed Framework for Influential Blogger Prediction based on Blogger and Blog Features (IBP-BBF) using Case-Based Reasoning (CBR) which is not only capable of handling labeled data but also unstructured data (blogs) and imbalanced data in an optimized way. Detailed labelled and unstructured data are collected by online survey of 129 bloggers and text mining of their 32,200 blogs respectively. The classification results are compared and validated with state-of-the-art machine learning techniques by using standard evaluation measures respectively in the context of imbalanced data. The results show that the proposed IBP-BBF framework through CBR modeling outperforms existing techniques in classifying and adapting the influential blogger prediction. The IBP-BBF framework performed better as compared to baseline imbalanced data classification techniques. It is found that the Balanced Random Forest contributes towards the performance of CBR approach than Balanced Bagging Classifier and RUSBoost classifier. By using the CBR approach, baseline techniques can be optimized for influential blogger identification in a better way. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3048610,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099096528&doi=10.1109%2fACCESS.2020.3048610&partnerID=40&md5=de827ce813769ed63229c85c86dabc5a,2021,2021-07-20 15:48:42,2021-07-20 15:48:42
4Z3PIM3Z,journalArticle,2014,"Kaliszyk, C.; Urban, J.",Learning-assisted automated reasoning with Flyspeck,"The considerable mathematical knowledge encoded by the Flyspeck project is combined with external automated theorem provers (ATPs) and machine-learning premise selection methods trained on the Flyspeck proofs, producing an AI system capable of proving a wide range of mathematical conjectures automatically. The performance of this architecture is evaluated in a bootstrapping scenario emulating the development of Flyspeck from axioms to the last theorem, each time using only the previous theorems and proofs. It is shown that 39 % of the 14185 theorems could be proved in a push-button mode (without any high-level advice and user interaction) in 30 seconds of real time on a fourteen-CPU workstation. The necessary work involves: (i) an implementation of sound translations of the HOL Light logic to ATP formalisms: untyped first-order, polymorphic typed first-order, and typed higher-order, (ii) export of the dependency information from HOL Light and ATP proofs for the machine learners, and (iii) choice of suitable representations and methods for learning from previous proofs, and their integration as advisors with HOL Light. This work is described and discussed here, and an initial analysis of the body of proofs that were found fully automatically is provided. © 2014 The Author(s).",Journal of Automated Reasoning,10.1007/s10817-014-9303-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903814844&doi=10.1007%2fs10817-014-9303-3&partnerID=40&md5=e1e94839db1d486ada9abf5778682241,2014,2021-07-20 15:48:42,2021-07-20 15:48:42
4X7XLFCR,journalArticle,2018,"Mahankali, R.; Johnson, B.R.; Anderson, A.T.",Deep learning in design workflows: The elusive design pixel,"The recent wave of developments and research in the field of deep learning and artificial intelligence is causing the border between the intuitive and deterministic domains to be redrawn, especially in computer vision and natural language processing. As designers frequently invoke vision and language in the context of design, this article takes a step back to ask if deep learning’s capabilities might be applied to design workflows, especially in architecture. In addition to addressing this general question, the article discusses one of several prototypes, BIMToVec, developed to examine the use of deep learning in design. It employs techniques like those used in natural language processing to interpret building information models. The article also proposes a homogeneous data format, provisionally called a design pixel, which can store design information as spatial-semantic maps. This would make designers’ intuitive thoughts more accessible to deep learning algorithms while also allowing designers to communicate abstractly with design software. © The Author(s) 2018.",International Journal of Architectural Computing,10.1177/1478077118800888,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058106557&doi=10.1177%2f1478077118800888&partnerID=40&md5=cd3454a1440dbf0eb96baee6e3b0311c,2018,2021-07-20 15:48:42,2021-07-20 15:48:42
YYIVF5VW,journalArticle,2021,"Huijing, J.P.; Dwight, R.P.; Schmelzer, M.",Data-driven RANS closures for three-dimensional flows around bluff bodies,"In this short note we apply the recently proposed data-driven RANS closure modelling framework of Schmelzer et al.(2020) to fully three-dimensional, high Reynolds number flows: namely wall-mounted cubes and cuboids at Re=40,000, and a cylinder at Re=140,000. For each flow, a new RANS closure is generated using sparse symbolic regression based on LES or DES reference data. This new model is implemented in a CFD solver, and subsequently applied to prediction of the other flows. We see consistent improvements compared to the baseline k−ω SST model in predictions of mean-velocity in complete flow domain. © 2021 The Authors",Computers and Fluids,10.1016/j.compfluid.2021.104997,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105574407&doi=10.1016%2fj.compfluid.2021.104997&partnerID=40&md5=80d75435a40a621d9bad55462f14c09c,2021,2021-07-20 15:48:43,2021-07-20 15:48:43
52US9EXA,journalArticle,2020,"Butora, J.; Fridrich, J.",Reverse JPEG Compatibility Attack,"A novel steganalysis method for JPEG images is introduced that is universal in the sense that it reliably detects any type of steganography as well as small payloads. It is limited to quality factors 99 and 100. The detection statistic is formed from the rounding errors in the spatial domain after decompressing the JPEG image. The attack works whenever, during compression, the discrete cosine transform is applied to integer-valued signal. Reminiscent of the well-established JPEG compatibility steganalysis, we call the new approach the 'reverse JPEG compatibility attack.' While the attack is introduced and analyzed under simplifying assumptions using reasoning based on statistical signal detection, the best detection in practice is obtained with machine learning tools. Experiments on diverse datasets of both grayscale and color images, five steganographic schemes, and with a variety of JPEG compressors demonstrate the universality and applicability of this steganalysis method in practice. © 2005-2012 IEEE.",IEEE Transactions on Information Forensics and Security,10.1109/TIFS.2019.2940904,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072175665&doi=10.1109%2fTIFS.2019.2940904&partnerID=40&md5=c5651827f859ef99b0cacc5fd12855d5,2020,2021-07-20 15:48:43,2021-07-20 15:48:43
8DCVPD9F,journalArticle,2018,"Chang, N.; Pan, S.; Srinivasan, K.; Feng, Z.; Xia, W.; Pawlak, T.; Geb, D.",Emerging ADAS Thermal Reliability Needs and Solutions,"Advanced driver assistance systems (ADASs) used for pedestrian detection, parking assist, night vision, blind-spot monitoring, collision avoidance, and other such capabilities have significantly enhanced car safety and reduced the risk of dangerous accidents. Their further evolution into a network of intelligent systems using vehicle-to-infrastructure (V2I) and vehicle-to-vehicle (V2V) communications is paving the way for autonomous driving. To support these advanced technologies, automotive electronics must be overhauled to enable machine learning capabilities, particularly deep learning, to transform the typical car into a smart system on wheels. Thermal reliability is critical because these high-power, intelligent electronics systems must last more than 10 years under often hostile thermal environments. This article presents an innovative multiphysics solution for thermal, thermal-aware electromigration, and thermal-induced stress analysis of a chip-package-system realized in 3DIC, an example of which is an AI system used in ADAS. © 2017 IEEE.",IEEE Micro,10.1109/MM.2018.112130058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041692395&doi=10.1109%2fMM.2018.112130058&partnerID=40&md5=a0f11904df96aa948ebec1938ece3cf2,2018,2021-07-20 15:48:43,2021-07-20 15:48:43
CFM48IW5,journalArticle,2012,"Kamsu-Foguem, B.; Tchuenté-Foguem, G.; Allart, L.; Zennir, Y.; Vilhelm, C.; Mehdaoui, H.; Zitouni, D.; Hubert, H.; Lemdani, M.; Ravaux, P.",User-centered visual analysis using a hybrid reasoning architecture for intensive care units,"One problem pertaining to Intensive Care Unit information systems is that, in some cases, a very dense display of data can result. To ensure the overview and readability of the increasing volumes of data, some special features are required (e.g., data prioritization, clustering, and selection mechanisms) with the application of analytical methods (e.g., temporal data abstraction, principal component analysis, and detection of events). This paper addresses the problem of improving the integration of the visual and analytical methods applied to medical monitoring systems. We present a knowledge- and machine learning-based approach to support the knowledge discovery process with appropriate analytical and visual methods. Its potential benefit to the development of user interfaces for intelligent monitors that can assist with the detection and explanation of new, potentially threatening medical events. The proposed hybrid reasoning architecture provides an interactive graphical user interface to adjust the parameters of the analytical methods based on the users' task at hand. The action sequences performed on the graphical user interface by the user are consolidated in a dynamic knowledge base with specific hybrid reasoning that integrates symbolic and connectionist approaches. These sequences of expert knowledge acquisition can be very efficient for making easier knowledge emergence during a similar experience and positively impact the monitoring of critical situations. The provided graphical user interface incorporating a user-centered visual analysis is exploited to facilitate the natural and effective representation of clinical information for patient care. © 2012 Elsevier B.V. All rights reserved.",Decision Support Systems,10.1016/j.dss.2012.06.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868644356&doi=10.1016%2fj.dss.2012.06.009&partnerID=40&md5=3a6a534d8719d1a974bfcefcbe51d376,2012,2021-07-20 15:48:43,2021-07-20 15:48:43
KUZ8IDWH,journalArticle,2019,"Abutair, H.; Belghith, A.; AlAhmadi, S.",CBR-PDS: a case-based reasoning phishing detection system,"Phishing attacks have become the preferred vehicle to gather sensitive information as well as to deliver dangerous malware. So far, there is still no phishing detection system that can perfectly detect and progressively self adapt to differentiate between phishing and legitimate websites. This paper proposes the case-based reasoning Phishing detection system (CBR-PDS) that relies on previous cases to detect phishing attacks. CBR-PDS is highly adaptive and dynamic as it can adapt to detect new phishing attacks using rather a small dataset size in contrast to other machine learning techniques. CBR-PDS aims to improve the detection accuracy and the reliability of the results by identifying a set of discriminative features and discarding irrelevant features. CBR-PDS relies on a two stage hybrid procedure using Information gain and Genetic algorithms. The reduction of the data dimensionality amounts to an improved accuracy rate, yet it necessitates a reduced processing time. The CBR-PDS is tested using different scenarios on a various balanced datasets. The obtained performances clearly show the suitability of our proposed hybrid feature selection procedure as well as the efficiency of the proposed CBR-PDS system. The obtained accuracy rates exceed 95%. We also show that the integration of an Online Phishing Threats component into the CBR-PDS system improves further the accuracy rate. Finally, CRB-PDS performances are compared to those of several known competitive classifiers. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-018-0736-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049558138&doi=10.1007%2fs12652-018-0736-0&partnerID=40&md5=348777f3028563e64fe1e53e95fe9a5c,2019,2021-07-20 15:48:43,2021-07-20 15:48:43
FZP45CHF,journalArticle,2021,"Dhar, A.; Mukherjee, H.; Dash, N.S.; Roy, K.",Text categorization: past and present,"Automatic text categorization is the operation of sorting out the text documents into pre-defined text categories using some machine learning algorithms. Normally, it defines the most important approaches to organizing and making the use of a large volume of information exists in unstructured form. Nowadays, text categorization is becoming an extensively researched field of text mining and processing of languages. Word sense, semantic relationships among terms, text documents and categories are quite essential in order of enhancing the performances of categorization. Various surveys on text categorization have already been available which involve techniques of various text representation schemes to such extent but do not include several approaches that have been explored in text categorization over the standard techniques. Here, an exhaustive analysis of different text categorization approaches over the conventional approaches has been undertaken. This survey paper explores a wide variety of algorithms used for categorizing text documents and tries to assemble the existing works into three basic fields: conventional methods, fuzzy logic-based methods, deep learning-based methods. Further, conventional methods have been categorized into three fields: text categorization using handcrafted features, text categorization using nature-inspired algorithms and text categorization using graph-based methods. Furthermore, this survey provides a clear idea about the available libraries used for different algorithms, availability of datasets, categorization technologies explored in various non-Indian and Indian languages as well. © 2020, Springer Nature B.V.",Artificial Intelligence Review,10.1007/s10462-020-09919-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091726105&doi=10.1007%2fs10462-020-09919-1&partnerID=40&md5=ffe0af23d6565950acf0068f8e09b117,2021,2021-07-20 15:48:43,2021-07-20 15:48:43
2YI822R8,journalArticle,2018,"Li, Y.; Yu, F.; Cai, Q.; Qian, M.; Liu, P.; Guo, J.; Yan, H.; Yuan, K.; Yu, J.",Design of Target Recognition System Based on Machine Learning Hardware Accelerator,"Target recognition system based on machine learning has the problems of long delay, high power-consuming and high cost, which cause it difficult to be promoted in some small embedded devices. In order to develop a target recognition system based on machine learning that can be utilized in small embedded device, this paper analyzes the commonly used design process of target recognition, the training process of machine learning algorithms, and the working method of FPGA to accelerate the algorithm. In the end, it offers a new solution of target recognition system based on machine learning hardware accelerator. In the solution, the training process of target recognition algorithm based on machine learning is completed in GPU, and then the algorithm is porting to the logic part of SOC in the form of hardware accelerator. The solution be widely used in different needs of the target recognition scenario with the advantage of effectively reduce the system delay, power consumption, size. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Wireless Personal Communications,10.1007/s11277-017-5211-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040931442&doi=10.1007%2fs11277-017-5211-2&partnerID=40&md5=9926c6beddffaf96a0ebef1ec74afa3b,2018,2021-07-20 15:48:43,2021-07-20 15:48:43
C5WP8S97,journalArticle,2020,"Villalonga, A.; Beruvides, G.; Castano, F.; Haber, R.E.",Cloud-Based Industrial Cyber-Physical System for Data-Driven Reasoning: A Review and Use Case on an Industry 4.0 Pilot Line,"Nowadays, reconfiguration and adaptation by means of optimal re-parameterization in Industrial Cyber-Physical Systems (ICPSs) is one of the bottlenecks for the digital transformation of the manufacturing industry. This article proposes a cloud-to-edge-based ICPS equipped with machine learning techniques. The proposed reasoning module includes a learning procedure based on two reinforcement learning techniques, running in parallel, for updating both the data-conditioning and processing strategy and the prediction model. The presented solution distributes computational resources and analytic engines in multiple layers and independent modules, increasing the smartness and the autonomy for monitoring and control the behavior at the shop floor level. The suitability of the proposed solution, evaluated in a pilot line, is endorsed by fast time response (i.e., 0.01 s at the edge level) and the appropriate setting of optimal operational parameters for guaranteeing the desired quality surface roughness during macro- and micro-milling operations. © 2005-2012 IEEE.",IEEE Transactions on Industrial Informatics,10.1109/TII.2020.2971057,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086072169&doi=10.1109%2fTII.2020.2971057&partnerID=40&md5=c57f1a293c9a022682cbcf0edbb625d0,2020,2021-07-20 15:48:43,2021-07-20 15:48:43
ARN3RXUK,journalArticle,2018,"Fan, F.; Cong, W.; Wang, G.",Generalized backpropagation algorithm for training second-order neural networks,"The artificial neural network is a popular framework in machine learning. To empower individual neurons, we recently suggested that the current type of neurons could be upgraded to second-order counterparts, in which the linear operation between inputs to a neuron and the associated weights is replaced with a nonlinear quadratic operation. A single second-order neurons already have a strong nonlinear modeling ability, such as implementing basic fuzzy logic operations. In this paper, we develop a general backpropagation algorithm to train the network consisting of second-order neurons. The numerical studies are performed to verify the generalized backpropagation algorithm. Copyright © 2017 John Wiley & Sons, Ltd.",International Journal for Numerical Methods in Biomedical Engineering,10.1002/cnm.2956,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041571468&doi=10.1002%2fcnm.2956&partnerID=40&md5=654bce765c4ba30b89eda93c57d80848,2018,2021-07-20 15:48:43,2021-07-20 15:48:43
JBRJXR8E,journalArticle,2015,"Katzouris, N.; Artikis, A.; Paliouras, G.",Incremental learning of event definitions with Inductive Logic Programming,"Event recognition systems rely on knowledge bases of event definitions to infer occurrences of events in time. Using a logical framework for representing and reasoning about events offers direct connections to machine learning, via Inductive Logic Programming (ILP), thus allowing to avoid the tedious and error-prone task of manual knowledge construction. However, learning temporal logical formalisms, which are typically utilized by logic-based event recognition systems is a challenging task, which most ILP systems cannot fully undertake. In addition, event-based data is usually massive and collected at different times and under various circumstances. Ideally, systems that learn from temporal data should be able to operate in an incremental mode, that is, revise prior constructed knowledge in the face of new evidence. In this work we present an incremental method for learning and revising event-based knowledge, in the form of Event Calculus programs. The proposed algorithm relies on abductive–inductive learning and comprises a scalable clause refinement methodology, based on a compressive summarization of clause coverage in a stream of examples. We present an empirical evaluation of our approach on real and synthetic data from activity recognition and city transport applications. © 2015, The Author(s).",Machine Learning,10.1007/s10994-015-5512-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939256422&doi=10.1007%2fs10994-015-5512-1&partnerID=40&md5=96d57db2958366f218a8606b5529b595,2015,2021-07-20 15:48:43,2021-07-20 15:48:43
T5YMERQC,journalArticle,2021,"Meng, Y.; Speier, W.; Ong, M.; Arnold, C.W.",HCET: Hierarchical clinical embedding with topic modeling on electronic health records for predicting future depression,"Recent developments in machine learning algorithms have enabled models to exhibit impressive performance in healthcare tasks using electronic health record (EHR) data. However, the heterogeneous nature and sparsity of EHR data remains challenging. In this work, we present a model that utilizes heterogeneous data and addresses sparsity by representing diagnoses, procedures, and medication codes with temporal Hierarchical Clinical Embeddings combined with Topic modeling (HCET) on clinical notes. HCET aggregates various categories of EHR data and learns inherent structure based on hospital visits for an individual patient. We demonstrate the potential of the approach in the task of predicting depression at various time points prior to a clinical diagnosis. We found that HCET outperformed all baseline methods with a highest improvement of 0.07 in precision-recall area under the curve (PRAUC). Furthermore, applying attention weights across EHR data modalities significantly improved the performance as well as the model's interpretability by revealing the relative weight for each data modality. Our results demonstrate the model's ability to utilize heterogeneous EHR information to predict depression, which may have future implications for screening and early detection. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2020.3004072,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104047656&doi=10.1109%2fJBHI.2020.3004072&partnerID=40&md5=07cbb156691d74c0dcdc3e31cd6677e7,2021,2021-07-20 15:48:43,2021-07-20 15:48:43
W2Q4E8E4,journalArticle,2021,"Saritha, M.; Milton, R.S.",A probabilistic logic approach to outcome prediction in team games using historical data and domain knowledge,"Relational data is structured and, in the real world, ambiguous. Logic can handle relations and probability can handle uncertainty. A probabilistic logic approach to learning can handle both relational structure and uncertainty in the data. Probabilistic logic approach works well with relational data. Incorporating domain knowledge in probabilistic logic approach further enhances learning, improving accuracy. A number of statistical techniques carry out predictive analytics based on historical data alone. Soccer, however, is a team game and the outcome of a soccer game depends on how well the team together and the players play against the opponent team. Thus, data about soccer games are better represented in relational form. In the present work, we propose to learn from soccer match data to predict their outcomes. We learn a model for the prediction of soccer game outcomes, taking into account the history of the matches played by the teams. We frame the background knowledge as rules in the logic program to enhance the prediction. Compared to the traditional machine learning approaches to soccer game outcome prediction, probabilistic logic approach is found to result in significant improvement in prediction accuracy. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-020-01989-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085142665&doi=10.1007%2fs12652-020-01989-x&partnerID=40&md5=8322df4f0b40c8f13df2205fe15c936d,2021,2021-07-20 15:48:43,2021-07-20 15:48:43
ADEGV37A,journalArticle,2021,"Nguembang Fadja, A.; Riguzzi, F.; Lamma, E.",Learning hierarchical probabilistic logic programs,"Probabilistic logic programming (PLP) combines logic programs and probabilities. Due to its expressiveness and simplicity, it has been considered as a powerful tool for learning and reasoning in relational domains characterized by uncertainty. Still, learning the parameter and the structure of general PLP is computationally expensive due to the inference cost. We have recently proposed a restriction of the general PLP language called hierarchical PLP (HPLP) in which clauses and predicates are hierarchically organized. HPLPs can be converted into arithmetic circuits or deep neural networks and inference is much cheaper than for general PLP. In this paper we present algorithms for learning both the parameters and the structure of HPLPs from data. We first present an algorithm, called parameter learning for hierarchical probabilistic logic programs (PHIL) which performs parameter estimation of HPLPs using gradient descent and expectation maximization. We also propose structure learning of hierarchical probabilistic logic programming (SLEAHP), that learns both the structure and the parameters of HPLPs from data. Experiments were performed comparing PHIL and SLEAHP with PLP and Markov Logic Networks state-of-the art systems for parameter and structure learning respectively. PHIL was compared with EMBLEM, ProbLog2 and Tuffy and SLEAHP with SLIPCOVER, PROBFOIL+, MLB-BC, MLN-BT and RDN-B. The experiments on five well known datasets show that our algorithms achieve similar and often better accuracies but in a shorter time. © 2021, The Author(s).",Machine Learning,10.1007/s10994-021-06016-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107994928&doi=10.1007%2fs10994-021-06016-4&partnerID=40&md5=d465478b9c9a9759332ccb7a4929acf2,2021,2021-07-20 15:48:44,2021-07-20 15:48:44
H5Z76FA7,journalArticle,2013,"Bellodi, E.; Riguzzi, F.",Expectation maximization over binary decision diagrams for probabilistic logic programs,"Recently much work in Machine Learning has concentrated on using expressive representation languages that combine aspects of logic and probability. A whole field has emerged, called Statistical Relational Learning, rich of successful applications in a variety of domains. In this paper we present a Machine Learning technique targeted to Probabilistic Logic Programs, a family of formalisms where uncertainty is represented using Logic Programming tools. Among various proposals for Probabilistic Logic Programming, the one based on the distribution semantics is gaining popularity and is the basis for languages such as ICL, PRISM, ProbLog and Logic Programs with Annotated Disjunctions. This paper proposes a technique for learning parameters of these languages. Since their equivalent Bayesian networks contain hidden variables, an Expectation Maximization (EM) algorithm is adopted. In order to speed the computation up, expectations are computed directly on the Binary Decision Diagrams that are built for inference. The resulting system, called EMBLEM for ""EM over Bdds for probabilistic Logic programs Efficient Mining"", has been applied to a number of datasets and showed good performances both in terms of speed and memory usage. In particular its speed allows the execution of a high number of restarts, resulting in good quality of the solutions. © 2013 - IOS Press and the authors. All rights reserved.",Intelligent Data Analysis,10.3233/IDA-130582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878938613&doi=10.3233%2fIDA-130582&partnerID=40&md5=c7dc621ea3604c31c595156a82129d64,2013,2021-07-20 15:48:44,2021-07-20 15:48:44
V5PXV8Y4,journalArticle,2019,"Remya, S.; Sasikala, R.",Classification of rubberized coir fibres using deep learning-based neural fuzzy decision tree approach,"Indian coir industry is one among the small-scale cottage industries which contributes a great amount to the economy of India. India and Sri Lanka are the top two international producers, and they account for 90% of world coir manufacturing. The modern digital technologies can monitor the physical, chemical and microbiological properties in agriculture sector and can analyse the data in a faster manner. This paper highlights the large opportunities of deep learning in agriculture as an intelligent method for classifying and predicting the quality of the raw materials and thereby correlating into suitable products in the agricultural sector. Here we gathered the dataset from Central Coir Research Institute, Alappuzha, tested the chemical properties and applied several decision tree algorithms on the dataset and compared for analysis. This paper focuses specifically on a hybrid method with the usage of the back-propagation in deep learning and fuzzy logic decision tree. Experimental results reveal that the proposed method shows an accuracy of 98.75%. The comparison results with various decision tree-based classification algorithms, such as C4.5, ID3, CART, Naïve Bayes, fuzzy, MLP, verify the effectiveness of the proposed method. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-019-03961-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064350053&doi=10.1007%2fs00500-019-03961-0&partnerID=40&md5=67fa47d4e85d4aaaba0029f06c361872,2019,2021-07-20 15:48:44,2021-07-20 15:48:44
KRNL9WN7,journalArticle,2021,"Qolomany, B.; Mohammed, I.; Al-Fuqaha, A.; Guizani, M.; Qadir, J.",Trust-Based Cloud Machine Learning Model Selection for Industrial IoT and Smart City Services,"With machine learning (ML) services now used in a number of mission-critical human-facing domains, ensuring the integrity and trustworthiness of ML models becomes all important. In this work, we consider the paradigm where cloud service providers collect big data from resource-constrained devices for building ML-based prediction models that are then sent back to be run locally on the intermittently connected resource-constrained devices. Our proposed solution comprises an intelligent polynomial-time heuristic that maximizes the level of trust of ML models by selecting and switching between a subset of the ML models from a superset of models in order to maximize the trustworthiness while respecting the given reconfiguration budget/rate and reducing the cloud communication overhead. We evaluate the performance of our proposed heuristic using two case studies. First, we consider Industrial IoT (IIoT) services, and as a proxy for this setting, we use the turbofan engine degradation simulation data set to predict the remaining useful life of an engine. Our results in this setting show that the trust level of the selected models is 0.49%-3.17% less compared to the results obtained using integer linear programming (ILP). Second, we consider smart cities services, and as a proxy of this setting, we use an experimental transportation data set to predict the number of cars. Our results show that the selected model's trust level is 0.7%-2.53% less compared to the results obtained using ILP. We also show that our proposed heuristic achieves an optimal competitive ratio in a polynomial-time approximation scheme for the problem. © 2014 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2020.3022323,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100825753&doi=10.1109%2fJIOT.2020.3022323&partnerID=40&md5=8e17e48bd39168a1cc0003acdf239141,2021,2021-07-20 15:48:44,2021-07-20 15:48:44
MB46W5JT,journalArticle,2017,"Orsini, F.; Frasconi, P.; De Raedt, L.",kProbLog: an algebraic Prolog for machine learning,"We introduce kProbLog as a declarative logical language for machine learning. kProbLog is a simple algebraic extension of Prolog with facts and rules annotated by semi-ring labels. It allows to elegantly combine algebraic expressions with logic programs. We introduce the semantics of kProbLog, its inference algorithm, its implementation and provide convergence guarantees. We provide several code examples to illustrate its potential for a wide range of machine learning techniques. In particular, we show the encodings of state-of-the-art graph kernels such as Weisfeiler-Lehman graph kernels, propagation kernels and an instance of graph invariant kernels, a recent framework for graph kernels with continuous attributes. However, kProbLog is not limited to kernel methods and it can concisely express declarative formulations of tensor-based algorithms such as matrix factorization and energy-based models, and it can exploit semirings of dual numbers to perform algorithmic differentiation. Furthermore, experiments show that kProbLog is not only of theoretical interest, but can also be applied to real-world datasets. At the technical level, kProbLog extends aProbLog (an algebraic Prolog) by allowing multiple semirings to coexist in a single program and by introducing meta-functions for manipulating algebraic values. © 2017, The Author(s).",Machine Learning,10.1007/s10994-017-5668-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031919664&doi=10.1007%2fs10994-017-5668-y&partnerID=40&md5=a9ffc5472db86f93d2ccf0a731dcfd8e,2017,2021-07-20 15:48:44,2021-07-20 15:48:44
QLRGLIJM,journalArticle,2011,"Ferro, M.; Mury, A.R.; Schulze, B.",A proposal to apply inductive logic programming to self-healing problem in grid computing: How will it work?,"As computation systems get extremely large and complex, failure diagnosis becomes even more complex. To cope with this ever increasing complexity of managing heterogeneous systems-such as grids and nowadays clouds-systems should manage their own behavior themselves. This vision of self-managing systems also referred to as autonomic computing (AC) aims to allow systems to recover themselves from various failures or malfunctions. This is known as self-healing (SH) and is one of the requirements of AC. However, dealing with these complex failure scenarios is always an open challenge. Dealing with this challenge requires prediction and control through a number of automated learning and proactive actions. In this work, we present the usage of a relational learning method known as inductive logic programming, for prediction and root casual analysis, and the development of an SH component. Copyright © 2011 John Wiley & Sons, Ltd. Copyright © 2011 John Wiley & Sons, Ltd.",Concurrency Computation Practice and Experience,10.1002/cpe.1714,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054952446&doi=10.1002%2fcpe.1714&partnerID=40&md5=1f8457987862a35ca73aa445aabfbe98,2011,2021-07-20 15:48:44,2021-07-20 15:48:44
EVGQZIY3,journalArticle,2017,"Wang, D.; Li, Z.; Dey, N.; Ashour, A.S.; Sherratt, R.S.; Shi, F.",Case-Based Reasoning for Product Style Construction and Fuzzy Analytic Hierarchy Process Evaluation Modeling Using Consumers Linguistic Variables,"Key form features are relative to the style of product, and the expression on style features depicts the product description and is a measurement of attribute knowledge. The uncertainty definition leads to an improved and effective product style retrieval when combined with fuzzy sets. First, a style knowledge and features database are constructed using fuzzy case-based reasoning technology; a similarity measurement method based on case-based reasoning and fuzzy model of the fuzzy proximity method may be defined by the fuzzy nearest-neighbor algorithm for obtaining the style knowledge extraction. Second, the linguistic variables (LV) are used to assess the product characteristics to establish the product style evaluation database for simplifying the style presentation and decreasing the computational complexity. Third, the model of product style feature set, extracted by fuzzy analytic hierarchy process (FAHP), and the final style related form features set are acquired using LV. This research involves a case study for extracting the key form features of the style of high heel shoes. The proposed algorithms are generated by calculating the weights of each component of high heel shoes using FAHP with LV. The case study and results established that the proposed method is feasible and effective for extracting the style of the product. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2017.2677950,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018967900&doi=10.1109%2fACCESS.2017.2677950&partnerID=40&md5=36297e38bbbe22a168912c5783e354d2,2017,2021-07-20 15:48:44,2021-07-20 15:48:44
R4MQZT37,journalArticle,2020,"Vakili, H.; Sakib, M.N.; Ganguly, S.; Stan, M.; Daniels, M.W.; Madhavan, A.; Stiles, M.D.; Ghosh, A.W.",Temporal Memory with Magnetic Racetracks,"Race logic is a relative timing code that represents information in a wavefront of digital edges on a set of wires in order to accelerate dynamic programming and machine learning algorithms. Skyrmions, bubbles, and domain walls are mobile magnetic configurations (solitons) with applications for Boolean data storage. We propose to use current-induced displacement of these solitons on magnetic racetracks as a native temporal memory for race logic computing. Locally synchronized racetracks can spatially store relative timings of digital edges and provide nondestructive readout. The linear kinematics of skyrmion motion, the tunability and low-voltage asynchronous operation of the proposed device, and the elimination of any need for constant skyrmion nucleation and annihilation make these magnetic racetracks a natural memory for low-power, high-Throughput race logic applications. © 2014 IEEE.",IEEE Journal on Exploratory Solid-State Computational Devices and Circuits,10.1109/JXCDC.2020.3022381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090957932&doi=10.1109%2fJXCDC.2020.3022381&partnerID=40&md5=d8978bb3e44acc4c8a1149f447c9bfb8,2020,2021-07-20 15:48:44,2021-07-20 15:48:44
T3BYK5QU,journalArticle,2014,"Rossi, F.",Collective decision making: A great opportunity for constraint reasoning,"Collective decision making is an area of increasingly growing interest, mainly due to the rise of many IT-enabled environments where people connect and share information with others. We believe that constraint reasoning can have a major impact in this field, by providing general and flexible frameworks to model agents' preferences over the alternative decisions, efficient algorithms to compute the best individual and collective decisions, and innovative approaches to deal with missing information. However, in order to do this, we claim that constraint reasoning should increase its efforts to open up to other research areas, such as voting and game theory, multi-agent systems, machine learning, and reasoning under uncertainty. © 2013 Springer Science+Business Media New York.",Constraints,10.1007/s10601-013-9153-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898774890&doi=10.1007%2fs10601-013-9153-3&partnerID=40&md5=8d339a0c3d54927ae9ca7f5f6aec796f,2014,2021-07-20 15:48:44,2021-07-20 15:48:44
NZDFEZUF,journalArticle,2019,"Sheth, A.; Gaur, M.; Kursuncu, U.; Wickramarachchi, R.",Shades of Knowledge-Infused Learning for Enhancing Deep Learning,"Deep Learning has already proven to be the primary technique to address a number of problems. It holds further promise in solving more challenging problems if we can overcome obstacles, such as the lack of quality training data and poor interpretability. The exploitation of domain knowledge and application semantics can enhance existing deep learning methods by infusing relevant conceptual information into a statistical, data-driven computational approach. This will require resolving the impedance mismatch due to different representational forms and abstractions between symbolic and statistical AI techniques. In this article, we describe a continuum that comprises of three stages for infusion of knowledge into the machine/deep learning architectures. As this continuum progresses across these three stages, it starts with shallow infusion in the form of embeddings, and attention and knowledge-based constraints improve with a semideep infusion. Toward the end reflecting deeper incorporation of knowledge, we articulate the value of incorporating knowledge at different levels of abstractions in the latent layers of neural networks. While shallow infusion is well studied and semideep infusion is in progress, we consider Deep Infusion of Knowledge as a new paradigm that will significantly advance the capabilities and promises of deep learning. © 1997-2012 IEEE.",IEEE Internet Computing,10.1109/MIC.2019.2960071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078880219&doi=10.1109%2fMIC.2019.2960071&partnerID=40&md5=b40371358e6addc357267e24838743f7,2019,2021-07-20 15:48:44,2021-07-20 15:48:44
AS6GJRJX,journalArticle,2020,"Silva, I.D.B.; Valle, M.E.; Barros, L.C.; Meyer, J.F.C.A.",A wildfire warning system applied to the state of Acre in the Brazilian Amazon,"In this paper, we present a dynamic wildfire warning map that combines both spatial and weather information. In particular, our wildfire early warning model is obtained by aggregating two indexes called wildfire risk and wildfire danger. The wildfire risk index, which is based on georeferenced features such as altitude and forest type, measures the fuel necessary for a wildfire to start at a certain location on a map. The wildfire danger uses weather conditions to yield temporal information concerning the possibility of a wildfire to spread. Machine learning techniques and fuzzy logic operations are used to determine the wildfire risk and danger indexes from available data. Although both wildfire risk and wildfire danger indexes can be used separately, using concepts from fuzzy logic, they can be combined to yield a wildfire warning system that takes into account both weather and static information. We illustrate the wildfire early warning model by considering weather and geographical data for the state of Acre. © 2020 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2020.106075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078565962&doi=10.1016%2fj.asoc.2020.106075&partnerID=40&md5=2983c5d4ecfc3c4eadbc0b75bbf687c1,2020,2021-07-20 15:48:44,2021-07-20 15:48:44
JSY3MU9J,journalArticle,2016,"Ceolin, D.; Groth, P.; Maccatrozzo, V.; Fokkink, W.; Van Hage, W.R.; Nottamkandath, A.",Combining user reputation and provenance analysis for trust assessment,"Trust is a broad concept that in many systems is often reduced to user reputation alone. However, user reputation is just one way to determine trust. The estimation of trust can be tackled from other perspectives as well, including by looking at provenance. Here, we present a complete pipeline for estimating the trustworthiness of artifacts given their provenance and a set of sample evaluations. The pipeline is composed of a series of algorithms for (1) extracting relevant provenance features, (2) generating stereotypes of user behavior from provenance features, (3) estimating the reputation of both stereotypes and users, (4) using a combination of user and stereotype reputations to estimate the trustworthiness of artifacts and (5) selecting sets of artifacts to trust. These algorithms rely on the W3C PROV recommendations for provenance and on evidential reasoning by means of subjective logic. We evaluate the pipeline over two tagging datasets: tags and evaluations from the Netherlands Institute for Sound and Vision's Waisda? video tagging platform, as well as crowdsourced annotations from the Steve. Museum project. The approach achieves up to 85% precision when predicting tag trustworthiness. Perhaps more importantly, the pipeline provides satisfactory results using relatively little evidence through the use of provenance. © 2016 ACM.",Journal of Data and Information Quality,10.1145/2818382,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957083633&doi=10.1145%2f2818382&partnerID=40&md5=53b889d5f6760f8e7a31dba0ba286ec9,2016,2021-07-20 15:48:45,2021-07-20 15:48:45
YYEVWWQY,journalArticle,2020,"Gromowski, M.; Siebers, M.; Schmid, U.",A process framework for inducing and explaining Datalog theories,"With the increasing prevalence of Machine Learning in everyday life, a growing number of people will be provided with Machine-Learned assessments on a regular basis. We believe that human users interacting with systems based on Machine-Learned classifiers will demand and profit from the systems’ decisions being explained in an approachable and comprehensive way. We developed a general process framework for logic-rule-based classifiers facilitating mutual exchange between system and user. The framework constitutes a guideline for how a system can apply Inductive Logic Programming in order to provide comprehensive explanations for classification choices and empowering users to evaluate and correct the system’s decisions. It also includes users’ corrections being integrated into the system’s core logic rules via retraining in order to increase the overall performance of the human-computer system. The framework suggests various forms of explanations—like natural language argumentations, near misses emphasizing unique characteristics, or image annotations—to be integrated into the system. © 2021, Springer-Verlag GmbH Germany, part of Springer Nature.",Advances in Data Analysis and Classification,10.1007/s11634-020-00422-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099242735&doi=10.1007%2fs11634-020-00422-7&partnerID=40&md5=902fc29d93f67b1657bc2ce44e756fcc,2020,2021-07-20 15:48:45,2021-07-20 15:48:45
CEVZ9A8R,journalArticle,2018,"Díaz-Pernil, D.; Gutiérrez-Naranjo, M.A.",Semantics of deductive databases with spiking neural P systems,"The integration of symbolic reasoning systems based on logic and connectionist systems based on the functioning of living neurons is a vivid research area in computer science. In the literature, one can find many efforts where different reasoning systems based on different logics are linked to classic artificial neural networks. In this paper, we study the relation between the semantics of reasoning systems based on propositional logic and the connectionist model in the framework of membrane computing, namely, spiking neural P systems. We prove that the fixed point semantics of deductive databases without negation can be implemented in the spiking neural P systems model and such a model can also deal with negation if it is endowed with anti-spikes and annihilation rules. © 2017 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2017.07.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023773675&doi=10.1016%2fj.neucom.2017.07.007&partnerID=40&md5=5d15452a4d460cfa464e8c8a3814bdd0,2018,2021-07-20 15:48:45,2021-07-20 15:48:45
DEC2SS8Q,journalArticle,2019,"Fu, X.; Yu, F.R.; Wang, J.; Qi, Q.; Liao, J.",Service Function Chain Embedding for NFV-Enabled IoT Based on Deep Reinforcement Learning,"It is challenging to efficiently manage different resources in the IoT. Recently, Network function virtualization has attracted attention because of its prospect to achieve efficient resource management for IoT. In NFV-enabled IoT infrastructure, a service function chain (SFC) is composed of an ordered set of virtual network functions (VNFs) that are connected based on the business logic of service providers. However, the inefficiency of the SFC embedding process is one major problem due to the dynamic nature of IoT networks and the abundance of IoT terminals. In this article, we decompose the complex VNFs into smaller VNF components (VNFCs) to make more effective decisions since VNF nodes and physical network devices are usually heterogeneous. In addition, a deep reinforcement learning (DRL)-based scheme with experience replay and target network is proposed as a solution that can efficiently handle complex and dynamic SFC embedding scenarios. Simulation results present the efficient performance of the proposed DRL-based dynamic SFC embedding scheme. © 1979-2012 IEEE.",IEEE Communications Magazine,10.1109/MCOM.001.1900097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075601321&doi=10.1109%2fMCOM.001.1900097&partnerID=40&md5=be6d32e453bfccd0715ab363594f0004,2019,2021-07-20 15:48:45,2021-07-20 15:48:45
4SSJP9ZF,journalArticle,2021,"Crafton, B.; Spetalnick, S.; Fang, Y.; Raychowdhury, A.",Merged Logic and Memory Fabrics for Accelerating Machine Learning Workloads,"Designing hardware accelerators for machine learning (ML) applications is a well-researched problem. This article presents a tutorial regarding new computing architectures, circuit techniques, and multiple promising device technologies for in-memory computing targeting ML workloads. The article states that there has been a steady increasing effort in design, fabrication, and manufacturing of novel memory technologies that are logic process and voltage compatible, while providing high density as well as target read and write performance. These new devices have new properties that have been absent in traditional charge based memory technologies. All these technologies store information through change of resistance. This enables users to perform compute in-memory (CIM) on the bitline (BL) with breakthrough improvements in throughput and energy-efficiency.",IEEE Design and Test,10.1109/MDAT.2020.3016587,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099422453&doi=10.1109%2fMDAT.2020.3016587&partnerID=40&md5=06bef6a56e412744985666338634da22,2021,2021-07-20 15:48:45,2021-07-20 15:48:45
EHDBNMTE,journalArticle,2013,"Vrtaric, D.; Ceperic, V.; Baric, A.",Area-efficient differential Gaussian circuit for dedicated hardware implementations of Gaussian function based machine learning algorithms,"A simple and area-efficient differential Gaussian circuit is presented for machine learning dedicated hardware implementations, where Gaussian functions are needed, e.g. for artificial neural networks (as transfer function), support vector machines (as kernel function) and fuzzy logic (as membership function). The proposed Gaussian circuit consists of only 4 transistors. Simulations in the 0.18-μm CMOS UMC technology show that the proposed circuit is more accurate, less susceptible to the process variations and requires less on-chip area when compared to state-of-the-art. © 2013 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2013.02.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881244397&doi=10.1016%2fj.neucom.2013.02.022&partnerID=40&md5=88fb66f623ede10688517d9590d89b2c,2013,2021-07-20 15:48:45,2021-07-20 15:48:45
MEUHSLXX,journalArticle,2019,"Venkatesan, R.; Prabu, S.",Hyperspectral Image Features Classification Using Deep Learning Recurrent Neural Networks,"The implementation of Deep learning (DL) techniques, Object detection and classification has achieved remarkable results in remote sensing application. Deep learning with Recurrent Neural Network (RNN) technique on hyper-spectral data has been presented here. The only model which can analyze the hyper-spectral pixels as the sequence of information and also to identify the additional information categories through network reasoning is RNN model. This is first time that the framework of RNN has been introduced for the classification of hyper spectral Image. An activation function is proposed by the DL-RNN and also the parameter rectified functions for analyzing the sequence of data in the hyper-spectral images. Throughout the training procedure, the higher learning rates are fairly used by the activation function which has been proposed by avoiding the risk of divergence. In the proposed system the pixels of hyper–spectral images through the sequential perspective has been processed for capturing the sequence based data. The experimental result also shows that the proposed RNN has produced the improved F- score than the traditional deep learning methods. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Medical Systems,10.1007/s10916-019-1347-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066637421&doi=10.1007%2fs10916-019-1347-9&partnerID=40&md5=17e55c1f43585be4b70ec7c6f4633de1,2019,2021-07-20 15:48:45,2021-07-20 15:48:45
7UTFGGRP,journalArticle,2020,"Forsyth, C.M.; Graesser, A.; Millis, K.",Predicting Learning in a Multi-component Serious Game,"The current study investigated predictors of shallow versus deep learning within a serious game known as Operation ARA. This game uses a myriad of pedagogical features including multiple-choice tests, adaptive natural language tutorial conversations, case-based reasoning, and an E-text to engage students. The game teaches 11 topics in research methodology across three distinct modules that target factual information, application of reasoning to specific cases, and question generation. The goal of this investigation is to discover predictors of deep and shallow learning by blending Evidence-Centered Design (ECD) with educational data mining. In line with ECD, time-honored cognitive processes or behaviors of time-on-task, discrimination, generation, and scaffolding were selected because there is a large research history supporting their importance to learning. The study included 192 college students who participated in a pretest-interaction-posttest design. These data were used to discover the best predictors of learning across the training experiences. Results revealed distinctly different patterns of predictors of deep versus shallow learning for students across the training environments of the game. Specifically, more interactivity is important for environments contributing to shallow learning whereas generation and discrimination is more important in training environments supporting deeper learning. However, in some training environments the positive impact of generation may be at the price of decreased discrimination. © 2019, Springer Nature B.V.","Technology, Knowledge and Learning",10.1007/s10758-019-09421-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071431599&doi=10.1007%2fs10758-019-09421-w&partnerID=40&md5=bf678efd9665347aa1d82218447acca1,2020,2021-07-20 15:48:45,2021-07-20 15:48:45
QYLXBZ8F,journalArticle,2020,"Wang, Q.; Hao, Y.; Cao, J.",ADRL: An attention-based deep reinforcement learning framework for knowledge graph reasoning,"Knowledge graph reasoning is one of the key technologies for knowledge graph construction, which plays an important part in application scenarios such as vertical search and intelligent question answering. It is intended to infer the desired entity from the entities and relations that already exist in the knowledge graph. Most current methods for reasoning, such as embedding-based methods, globally embed all entities and relations, and then use the similarity of vectors to infer relations between entities or whether given triples are true. However, in real application scenarios, we require a clear and interpretable target entity as the output answer. In this paper, we propose a novel attention-based deep reinforcement learning framework (ADRL) for learning multi-hop relational paths, which improves the efficiency, generalization capacity, and interpretability of conventional approaches through the structured perception of deep learning and relational reasoning of reinforcement learning. We define the entire process of reasoning as a Markov decision process. First, we employ CNN to map the knowledge graph to a low-dimensional space, and a message-passing mechanism to sense neighbor entities at each level, and then employ LSTM to memorize and generate a sequence of historical trajectories to form a policy and value functions. We design a relational module that includes a self-attention mechanism that can infer and share the weights of neighborhood entity vectors and relation vectors. Finally, we employ the actor–critic algorithm to optimize the entire framework. Experiments confirm the effectiveness and efficiency of our method on several benchmark data sets. © 2020 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2020.105910,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083306400&doi=10.1016%2fj.knosys.2020.105910&partnerID=40&md5=de8d6ab87c63ac28eca6420214134478,2020,2021-07-20 15:48:45,2021-07-20 15:48:45
3K5G87V9,journalArticle,2019,"Aksjonov, A.; Nedoma, P.; Vodovozov, V.; Petlenkov, E.; Herrmann, M.",Detection and Evaluation of Driver Distraction Using Machine Learning and Fuzzy Logic,"In addition to vehicle control, drivers often perform secondary tasks that impede driving. Reduction of driver distraction is an important challenge for the safety of intelligent transportation systems. In this paper, a methodology for the detection and evaluation of driver distraction while performing secondary tasks is described and an appropriate hardware and a software environment is offered and studied. The system includes a model of normal driving, a subsystem for measuring the errors from the secondary tasks, and a module for total distraction evaluation. A new machine learning algorithm defines driver performance in lane keeping and speed maintenance on a specific road segment. To recognize the errors, a method is proposed, which compares normal driving parameters with ones obtained while conducting a secondary task. To evaluate distraction, an effective fuzzy logic algorithm is used. To verify the proposed approach, a case study with driver-in-the-loop experiments was carried out, in which participants performed the secondary task, namely chatting on a cell phone. The results presented in this research confirm its capability to detect and to precisely measure a level of abnormal driver performance. © 2000-2011 IEEE.",IEEE Transactions on Intelligent Transportation Systems,10.1109/TITS.2018.2857222,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052713176&doi=10.1109%2fTITS.2018.2857222&partnerID=40&md5=c5630d202c132675d2f290df4bf9f68d,2019,2021-07-20 15:48:45,2021-07-20 15:48:45
XFC4A3AK,journalArticle,2019,"Saeed, S.M.; Mahendran, N.; Zulehner, A.; Wille, R.; Karri, R.",Identification of synthesis approaches for IP/IC piracy of reversible circuits,"Reversible circuits employ a computational paradigm that is beneficial for several applications, including the design of encoding and decoding devices, low-power design, and emerging applications in quantum computation. However, similarly to conventional logic, reversible circuits are expected to be subject to Intellectual Property/Integrated Circuit piracy. To counteract such attacks, an understanding of how to identify the target function from a reversible circuit is a crucial first step. In contrast to conventional logic, the target function is (implicitly or explicitly) embedded into the reversible circuit. Numerous synthesis approaches have been proposed for this embedding task. To recover the target function embedded in a reversible circuit, one needs to know what synthesis approach has been used to embed the circuit. We propose a machine-learning-based scheme to determine the used reversible synthesis approach based on the telltale signs it leaves in the synthesized reversible circuit. We study the impact of optimizing the synthesis approaches on the telltale signs that they leave. Our analysis shows that the synthesis approaches can be determined in the vast majority of cases even if optimized versions of the synthesis approaches are used. © 2019 Association for Computing Machinery.",ACM Journal on Emerging Technologies in Computing Systems,10.1145/3289392,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065768133&doi=10.1145%2f3289392&partnerID=40&md5=ba62b1e3fd4248c2d6024e60d797a78f,2019,2021-07-20 15:48:45,2021-07-20 15:48:45
PRMW99KR,journalArticle,2014,"Goyal, M.K.; Bharti, B.; Quilty, J.; Adamowski, J.; Pandey, A.","Modeling of daily pan evaporation in sub tropical climates using ANN, LS-SVR, Fuzzy Logic, and ANFIS","This paper investigates the abilities of Artificial Neural Networks (ANN), Least Squares - Support Vector Regression (LS-SVR), Fuzzy Logic, and Adaptive Neuro-Fuzzy Inference System (ANFIS) techniques to improve the accuracy of daily pan evaporation estimation in sub-tropical climates. Meteorological data from the Karso watershed in India (consisting of 3801 daily records from the year 2000 to 2010) were used to develop and test the models for daily pan evaporation estimation. The measured meteorological variables include daily observations of rainfall, minimum and maximum air temperatures, minimum and maximum humidity, and sunshine hours. Prior to model development, the Gamma Test (GT) was used to derive estimates of the noise variance for each input-output set in order to identify the most useful predictors for use in the machine learning approaches used in this study. The ANN models consisted of feed forward backpropagation (FFBP) models with Bayesian Regularization (BR), along with the Levenberg-Marquardt (LM) algorithm. A comparison was made between the estimates provided by the ANN, LS-SVR, Fuzzy Logic, and ANFIS models. The empirical Hargreaves and Samani method (HGS), as well as the Stephens-Stewart (SS) method, were also considered for comparison with the newer machine learning methods. The Root Mean Square Error (RMSE) and Correlation Coefficient (CORR) were the statistical performance indices that were used to evaluate the accuracy of the various models. Based on the comparison, it was found that the Fuzzy Logic and LS-SVR approaches can be employed successfully in modeling the daily evaporation process from the available climatic data. In addition, results showed that the machine learning models outperform the traditional HGS and SS empirical methods. © 2014 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2014.02.047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898464831&doi=10.1016%2fj.eswa.2014.02.047&partnerID=40&md5=632c5d74bb6e90e4f844a2e76f028352,2014,2021-07-20 15:48:45,2021-07-20 15:48:45
SXGNIEJ6,journalArticle,2021,"Sun, J.; Dai, Y.; Zhao, K.; Jia, Z.",Second order Takagi-Sugeno fuzzy model with domain adaptation for nonlinear regression,"In the regression analysis, Takagi-Sugeno fuzzy model gives a way of exploiting fuzzy logic to tackle nonlinear issues. However, the general Takagi-Sugeno fuzzy model encounters challenges when facing second order regression problems because of its insufficient fitting ability. In this study, the second order Takagi-Sugeno fuzzy model called TS2 fuzzy model is proposed to extend the application scope of the original model. Moreover, domain adaptation in transfer learning is applied to the proposed model by using space transformation. It aims to further reduce the model's cumulative error. The experimental results indicate that the proposed model has a better performance with not much extra processing time when dealing with second order nonlinear regression tasks. © 2021 Elsevier Inc.",Information Sciences,10.1016/j.ins.2021.04.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105696812&doi=10.1016%2fj.ins.2021.04.024&partnerID=40&md5=3b45b88ee31f94a1c9d663aa7fe95b5c,2021,2021-07-20 15:48:45,2021-07-20 15:48:45
X8HIAGSC,journalArticle,2020,"Zhang, D.; Zeng, W.; Yao, J.; Han, J.",Weakly Supervised Object Detection Using Proposal- and Semantic-Level Relationships,"Weakly supervised object detection has attracted great attention in the computer vision community. Although numerous deep learning-based approaches have been proposed in the past years, such an ill-posed problem is still challenging and the learning performance is behind the expectation. Most of the existing approaches only consider the visual appearance of each proposal region but ignore to adopt the helpful context information. To this end, this paper introduces two levels of context into the weakly supervised learning framework. The first one is the proposal-level context, i.e., the relationship of the spatially adjacent proposals. The second one is the semantic-level context, i.e., the relationship of the co-occurring object categories. Therefore, the proposed weakly supervised learning framework contains not only the cognition process on the visual appearance but also the reasoning process on the proposal- and semantic-level relationships, which leads to the novel deep multiple instance reasoning framework. Specifically, built upon a conventional CNN-based network architecture, the proposed framework is equipped with two additional graph convolutional network-based reasoning models to implement object location reasoning and multi-label reasoning within an end-to-end network training procedure. Experiments on the PASCAL VOC benchmarks have been implemented, which demonstrate the superior capacity of the proposed approach. IEEE",IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2020.3046647,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098800087&doi=10.1109%2fTPAMI.2020.3046647&partnerID=40&md5=657b6fa286cbccf0c64e6456af76e6d9,2020,2021-07-20 15:48:46,2021-07-20 15:48:46
YTWBLFZR,journalArticle,2016,"Navarro-Hellín, H.; Martínez-del-Rincon, J.; Domingo-Miguel, R.; Soto-Valles, F.; Torres-Sánchez, R.",A decision support system for managing irrigation in agriculture,"In this paper, an automatic Smart Irrigation Decision Support System, SIDSS, is proposed to manage irrigation in agriculture. Our system estimates the weekly irrigations needs of a plantation, on the basis of both soil measurements and climatic variables gathered by several autonomous nodes deployed in field. This enables a closed loop control scheme to adapt the decision support system to local perturbations and estimation errors. Two machine learning techniques, PLSR and ANFIS, are proposed as reasoning engine of our SIDSS. Our approach is validated on three commercial plantations of citrus trees located in the South-East of Spain. Performance is tested against decisions taken by a human expert. © 2016 Elsevier B.V.",Computers and Electronics in Agriculture,10.1016/j.compag.2016.04.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962652351&doi=10.1016%2fj.compag.2016.04.003&partnerID=40&md5=6a25343610a3c82dc8acecd07334931e,2016,2021-07-20 15:48:46,2021-07-20 15:48:46
LPN27JNL,journalArticle,2021,"Li, Z.; Rios, A.L.G.; Trajkovic, L.",Machine Learning for Detecting Anomalies and Intrusions in Communication Networks,"Cyber attacks are becoming more sophisticated and, hence, more difficult to detect. Using efficient and effective machine learning techniques to detect network anomalies and intrusions is an important aspect of cyber security. A variety of machine learning models have been employed to help detect malicious intentions of network users. In this paper, we evaluate performance of recurrent neural networks (Long Short-Term Memory and Gated Recurrent Unit) and Broad Learning System with its extensions to classify known network intrusions. We propose two BLS-based algorithms with and without incremental learning. The algorithms may be used to develop generalized models by using various subsets of input data and expanding the network structure. The models are trained and tested using Border Gateway Protocol routing records as well as network connection records from the NSL-KDD and Canadian Institute of Cybersecurity datasets. Performance of the models is evaluated based on selected features, accuracy, F-Score, and training time. IEEE",IEEE Journal on Selected Areas in Communications,10.1109/JSAC.2021.3078497,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105852963&doi=10.1109%2fJSAC.2021.3078497&partnerID=40&md5=57bf195dde110dd87444dfdaf9c82b4b,2021,2021-07-20 15:48:46,2021-07-20 15:48:46
EBS59373,journalArticle,2020,"Han, J.; Tao, J.; Wang, C.",FlowNet: A Deep Learning Framework for Clustering and Selection of Streamlines and Stream Surfaces,"For effective flow visualization, identifying representative flow lines or surfaces is an important problem which has been studied. However, no work can solve the problem for both lines and surfaces. In this paper, we present FlowNet, a single deep learning framework for clustering and selection of streamlines and stream surfaces. Given a collection of streamlines or stream surfaces generated from a flow field data set, our approach converts them into binary volumes and then employs an autoencoder to learn their respective latent feature descriptors. These descriptors are used to reconstruct binary volumes for error estimation and network training. Once converged, the feature descriptors can well represent flow lines or surfaces in the latent space. We perform dimensionality reduction of these feature descriptors and cluster the projection results accordingly. This leads to a visual interface for exploring the collection of flow lines or surfaces via clustering, filtering, and selection of representatives. Intuitive user interactions are provided for visual reasoning of the collection with ease. We validate and explain our deep learning framework from multiple perspectives, demonstrate the effectiveness of FlowNet using several flow field data sets of different characteristics, and compare our approach against state-of-the-art streamline and stream surface selection algorithms. © 1995-2012 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2018.2880207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056335944&doi=10.1109%2fTVCG.2018.2880207&partnerID=40&md5=67c6e8f1c40024390d689ad776415ba8,2020,2021-07-20 15:48:46,2021-07-20 15:48:46
TV3J9KRS,journalArticle,2012,"Huang, M.-L.; Hung, Y.-H.; Lee, W.-M.; Li, R.K.; Wang, T.-H.","Usage of case-based reasoning, neural network and adaptive neuro-fuzzy inference system classification techniques in breast cancer dataset classification diagnosis","Breast cancer is a common to females worldwide. Today, technological advancements in cancer treatment innovations have increased the survival rates. Many theoretical and experimental studies have shown that a multiple classifier system is an effective technique for reducing prediction errors. This study compared the particle swarm optimizer (PSO) based artificial neural network (ANN), the adaptive neuro-fuzzy inference system (ANFIS), and a case-based reasoning (CBR) classifier with a logistic regression model and decision tree model. It also applied three classification techniques to the Mammographic Mass Data Set, and measured its improvements in accuracy and classification errors. The experimental results showed that, the best CBR-based classification accuracy is 83.60%, and the classification accuracies of the PSO-based ANN classifier and ANFIS are 91.10% and 92.80%, respectively. © Springer Science+Business Media, LLC 2010.",Journal of Medical Systems,10.1007/s10916-010-9485-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863192451&doi=10.1007%2fs10916-010-9485-0&partnerID=40&md5=30a5ee56558915b69e856f8ee085811d,2012,2021-07-20 15:48:46,2021-07-20 15:48:46
Z7Z6DZLR,journalArticle,2013,"Ramanna, S.; Jain, L.C.; Howlett, R.J.",Emerging paradigms in machine learning: An introduction,"This chapter provides a broad overview of machine learning (ML) paradigms both emerging as well as well-established ones. These paradigms include: Bayesian Learning, Decision Trees, Granular Computing, Fuzzy and Rough Sets, Inductive Logic Programming, Reinforcement Learning, Neural Networks and Support Vector Machines. In addition, challenges in ML such as imbalanced data, perceptual computing, and pattern recognition of data which is episodic as well as temporal are also highlighted. © Springer-Verlag Berlin Heidelberg 2013.","Smart Innovation, Systems and Technologies",10.1007/978-3-642-28699-5_1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879319495&doi=10.1007%2f978-3-642-28699-5_1&partnerID=40&md5=4302257fceda661a66edf4cabc35ba5f,2013,2021-07-20 15:48:46,2021-07-20 15:48:46
AFDMMMUD,journalArticle,2012,"Ly, D.L.; Lipson, H.",Learning symbolic representations of hybrid dynamical systems,"A hybrid dynamical system is a mathematical model suitable for describing an extensive spectrum of multi-modal, time-series behaviors, ranging from bouncing balls to air traffic controllers. This paper describes multi-modal symbolic regression (MMSR): a learning algorithm to construct non-linear symbolic representations of discrete dynamical systems with continuous mappings from unlabeled, time-series data. MMSR consists of two subalgorithms-clustered symbolic regression, a method to simultaneously identify distinct behaviors while formulating their mathematical expressions, and transition modeling, an algorithm to infer symbolic inecualities that describe binary classification boundaries. These subalgorithms are combined to infer hybrid dynamical systems as a collection of apt, mathematical expressions. MMSR is evaluated on a collection of four synthetic data sets and outperforms other multi-modal machine learning approaches in both accuracy and interpretability, even in the presence of noise. Furthermore, the versatility of MMSR is demonstrated by identifying and inferring classical expressions of transistor modes from recorded measurements. © 2012 Daniel L. Ly and Hod Lipson.",Journal of Machine Learning Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873423185&partnerID=40&md5=0b37be76bb54e69038caa02c5c4d75b2,2012,2021-07-20 15:48:46,2021-07-20 15:48:46
JYEVPN66,journalArticle,2019,"Tsubaki, M.; Tomii, K.; Sese, J.",Compound-protein interaction prediction with end-to-end learning of neural networks for graphs and sequences,"Motivation In bioinformatics, machine learning-based methods that predict the compound-protein interactions (CPIs) play an important role in the virtual screening for drug discovery. Recently, end-to-end representation learning for discrete symbolic data (e.g. words in natural language processing) using deep neural networks has demonstrated excellent performance on various difficult problems. For the CPI problem, data are provided as discrete symbolic data, i.e. compounds are represented as graphs where the vertices are atoms, the edges are chemical bonds, and proteins are sequences in which the characters are amino acids. In this study, we investigate the use of end-to-end representation learning for compounds and proteins, integrate the representations, and develop a new CPI prediction approach by combining a graph neural network (GNN) for compounds and a convolutional neural network (CNN) for proteins. Results Our experiments using three CPI datasets demonstrated that the proposed end-to-end approach achieves competitive or higher performance as compared to various existing CPI prediction methods. In addition, the proposed approach significantly outperformed existing methods on an unbalanced dataset. This suggests that data-driven representations of compounds and proteins obtained by end-to-end GNNs and CNNs are more robust than traditional chemical and biological features obtained from databases. Although analyzing deep learning models is difficult due to their black-box nature, we address this issue using a neural attention mechanism, which allows us to consider which subsequences in a protein are more important for a drug compound when predicting its interaction. The neural attention mechanism also provides effective visualization, which makes it easier to analyze a model even when modeling is performed using real-valued representations instead of discrete features. Availability and implementation https://github.com/masashitsubaki Supplementary informationSupplementary dataare available at Bioinformatics online. © 2018 The Author(s).",Bioinformatics,10.1093/bioinformatics/bty535,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058229283&doi=10.1093%2fbioinformatics%2fbty535&partnerID=40&md5=f14fe0c51d178ef86f8e80947cf21a78,2019,2021-07-20 15:48:46,2021-07-20 15:48:46
2AVPZCMD,journalArticle,2021,"Lavanya, P.G.; Kouser, K.; Suresha, M.",Effective feature representation using symbolic approach for classification and clustering of big data,"The tremendous growth in the technology has led to the accumulation of enormous Big Data. Techniques that efficiently analyse this Big Data are in great demand. Tweets from Social media and Sensor data are some of the most common forms of Big Data. Machine learning algorithms pave way for researchers to analyze Big Data. Most Machine learning algorithms depend on efficient feature extraction and feature selection for its success. Here, we explore feature selection methods like entropy and Rough set on the sensor data. Also a symbolic approach of feature extraction is proposed which represents both sensor and twitter data efficiently for further data analysis. Some popular classifiers like Naïve Bayes, K Nearest Neighbour, Support Vector Machine and Decision Tree are used for validating the efficacy of the features selected. An ensemble classifier technique is also proposed which is compared with various state of the art ensemble classifiers. Symbolic features perform better than both entropy and Rough set features for sensor data and improves the clustering efficiency of twitter data. The proposed ensemble weighted average classifier on Symbolic features outperform all the other ensemble classifiers and independent classifiers. The results obtained from these methods have the potential to aid the public health surveillance. © 2021 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2021.114658,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101876907&doi=10.1016%2fj.eswa.2021.114658&partnerID=40&md5=943364f5d7bd95abf15c4aee8f20c833,2021,2021-07-20 15:48:46,2021-07-20 15:48:46
3SUBVMMD,journalArticle,2020,"Xu, X.; Zhao, Z.; Xu, X.; Yang, J.; Chang, L.; Yan, X.; Wang, G.",Machine learning-based wear fault diagnosis for marine diesel engine by fusing multiple data-driven models,"Wear fault is one of the dominant causes for marine diesel engine damage which significantly influences ship safety. By taking full advantage of the data generated in engine operation, machine learning-based wear fault diagnostic model can help engineers to determine fault modes correctly and take quick action to avoid severe accidents. To identify wear faults more accurately, a multi-model fusion system based on evidential reasoning (ER) rule is proposed in this paper. The outputs of three data-driven models including an artificial neural network (ANN) model, a belief rule-based inference (BRB) model, and an ER rule model are used as pieces of evidence to be fused in decision level. In this paper, the fusion system defines reliability and importance weight of every single model respectively. A novel method is presented to determine the reliability of evidence by considering the accuracy and stability of every single model. The importance weight is optimized by genetic algorithm to improve the performance of the fusion system. The proposed machine learning-based diagnostic system is validated by a set of real samples acquired from marine diesel engines in operation. The test results show that the system is more accurate and robust, and the fault tolerant ability is improved remarkably compared with every single data-driven diagnostic model. © 2019 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2019.105324,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076531207&doi=10.1016%2fj.knosys.2019.105324&partnerID=40&md5=d49791bc6050625c3f226fad9430dbe9,2020,2021-07-20 15:48:46,2021-07-20 15:48:46
KEPLHGWM,journalArticle,2011,"Kaski, S.; Peltonen, J.",Dimensionality reduction for data visualization,"Dimensionality reduction is one of the basic operations in the toolbox of data analysts and designers of machine learning and pattern recognition systems. Given a large set of measured variables but few observations, an obvious idea is to reduce the degrees of freedom in the measurements by representing them with a smaller set of more condensed variables. Another reason for reducing the dimensionality is to reduce computational load in further processing. A third reason is visualization. © 2006 IEEE.",IEEE Signal Processing Magazine,10.1109/MSP.2010.940003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032751870&doi=10.1109%2fMSP.2010.940003&partnerID=40&md5=9d465738372c7792c8ab99fb1b2513b7,2011,2021-07-20 15:48:46,2021-07-20 15:48:46
QHLZ8IM8,journalArticle,2015,"Paulheim, H.; Meusel, R.",A decomposition of the outlier detection problem into a set of supervised learning problems,"Outlier detection methods automatically identify instances that deviate from the majority of the data. In this paper, we propose a novel approach for unsupervised outlier detection, which re-formulates the outlier detection problem in numerical data as a set of supervised regression learning problems. For each attribute, we learn a predictive model which predicts the values of that attribute from the values of all other attributes, and compute the deviations between the predictions and the actual values. From those deviations, we derive both a weight for each attribute, and a final outlier score using those weights. The weights help separating the relevant attributes from the irrelevant ones, and thus make the approach well suitable for discovering outliers otherwise masked in high-dimensional data. An empirical evaluation shows that our approach outperforms existing algorithms, and is particularly robust in datasets with many irrelevant attributes. Furthermore, we show that if a symbolic machine learning method is used to solve the individual learning problems, the approach is also capable of generating concise explanations for the detected outliers. © 2015, The Author(s).",Machine Learning,10.1007/s10994-015-5507-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939270771&doi=10.1007%2fs10994-015-5507-y&partnerID=40&md5=065d0a57ed73603bd85d55156dc5492a,2015,2021-07-20 15:48:46,2021-07-20 15:48:46
KCKZY7A9,journalArticle,2020,"Hamad, R.A.; Hidalgo, A.S.; Bouguelia, M.-R.; Estevez, M.E.; Quero, J.M.",Efficient Activity Recognition in Smart Homes Using Delayed Fuzzy Temporal Windows on Binary Sensors,"Human activity recognition has become an active research field over the past few years due to its wide application in various fields such as health-care, smart home monitoring, and surveillance. Existing approaches for activity recognition in smart homes have achieved promising results. Most of these approaches evaluate real-time recognition of activities using only sensor activations that precede the evaluation time (where the decision is made). However, in several critical situations, such as diagnosing people with dementia, 'preceding sensor activations' are not always sufficient to accurately recognize the inhabitant's daily activities in each evaluated time. To improve performance, we propose a method that delays the recognition process in order to include some sensor activations that occur after the point in time where the decision needs to be made. For this, the proposed method uses multiple incremental fuzzy temporal windows to extract features from both preceding and some oncoming sensor activations. The proposed method is evaluated with two temporal deep learning models (convolutional neural network and long short-term memory), on a binary sensor dataset of real daily living activities. The experimental evaluation shows that the proposed method achieves significantly better results than the real-time approach, and that the representation with fuzzy temporal windows enhances performance within deep learning models. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2019.2918412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079094027&doi=10.1109%2fJBHI.2019.2918412&partnerID=40&md5=3b320b6bcbac1403e7c63dfde67c6a9c,2020,2021-07-20 15:48:46,2021-07-20 15:48:46
J5WFD34E,journalArticle,2021,"Sharma, A.; Kumar, H.; Mittal, K.; Kauhsal, S.; Kaushal, M.; Gupta, D.; Narula, A.",IoT and deep learning-inspired multi-model framework for monitoring Active Fire Locations in Agricultural Activities,"This paper proposes an Internet of Things (IoT) and deep learning-inspired multi-model system for detection, dissemination, and monitoring of Active Fire Locations(AFL) in agricultural activities. The IoT module of the proposed system works on the fusion of IoT sensors-based detectors and deep learning-based detectors. Fuzzy logic is used for the fusion of various sensors and providing real-time detection and location of AFL. The deep learning detector implements IP camera-based MobilenetV2 architecture for accurate and long-distance detections trained on a novel self-created dataset. The proposed framework also provides a software module for monitoring and tracking of various AFL. The software comes with several features like automatic extraction of fire locations from remote sensing sites, assigning active fire locations to multiple stakeholders, extracting farmers' names indulged in burning, automatic sending a notification to government agencies, and provisions for citizens centric participation. The results of the proposed framework are quite encouraging. © 2021 Elsevier Ltd",Computers and Electrical Engineering,10.1016/j.compeleceng.2021.107216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107976046&doi=10.1016%2fj.compeleceng.2021.107216&partnerID=40&md5=33486322da8964ad0eeae9023fed6fc1,2021,2021-07-20 15:48:46,2021-07-20 15:48:46
2C3YIFAI,journalArticle,2019,"Guo, L.; Wu, J.; Li, J.",Complexity at Mesoscales: A Common Challenge in Developing Artificial Intelligence,"Exploring the physical mechanisms of complex systems and making effective use of them are the keys to dealing with the complexity of the world. The emergence of big data and the enhancement of computing power, in conjunction with the improvement of optimization algorithms, are leading to the development of artificial intelligence (AI) driven by deep learning. However, deep learning fails to reveal the underlying logic and physical connotations of the problems being solved. Mesoscience provides a concept to understand the mechanism of the spatiotemporal multiscale structure of complex systems, and its capability for analyzing complex problems has been validated in different fields. This paper proposes a research paradigm for AI, which introduces the analytical principles of mesoscience into the design of deep learning models. This is done to address the fundamental problem of deep learning models detaching the physical prototype from the problem being solved; the purpose is to promote the sustainable development of AI. © 2019 THE AUTHORS",Engineering,10.1016/j.eng.2019.08.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072045193&doi=10.1016%2fj.eng.2019.08.005&partnerID=40&md5=708378b1411cf987624c0a08ccac9c33,2019,2021-07-20 15:48:46,2021-07-20 15:48:46
272WDNHF,journalArticle,2014,"Twala, B.",Reasoning with noisy software effort data,"Constructing an accurate effort prediction model remains a challenge in software engineering. Recently, machine learning classifiers have been used successfully for software effort evaluation decisions. However, the development and validation of these classifiers (and other modes) require good quality data. Most research on machine learning assumes that the attributes of training and tests instances are not only completely specified but are also free from noise. Real-world industrial datasets, however, suffer from corruption or noise that is not always known. However, blindly applying such machine learning techniques to noisy software effort evaluation data may fail to make very good or perfect predictions, resulting in poor decisions and ineffective project management. This article investigates the effect of noisy domains on the learning accuracy of eight machine learning and statistical pattern recognition algorithms. We further derive solutions for the problem of noisy domains in software effort prediction from a probabilistic point of view. Our experimental results show that our algorithm can improve prediction for software effort corrupted by noise with reasonable and much improved accuracy. © 2014 Taylor & Francis Group, LLC.",Applied Artificial Intelligence,10.1080/08839514.2014.923165,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904471743&doi=10.1080%2f08839514.2014.923165&partnerID=40&md5=395baeecd6c4c411afc175460c7ed96a,2014,2021-07-20 15:48:47,2021-07-20 15:48:47
LHVT8TTX,journalArticle,2018,"Cozad, A.; Sahinidis, N.V.",A global MINLP approach to symbolic regression,"Symbolic regression methods generate expression trees that simultaneously define the functional form of a regression model and the regression parameter values. As a result, the regression problem can search many nonlinear functional forms using only the specification of simple mathematical operators such as addition, subtraction, multiplication, and division, among others. Currently, state-of-the-art symbolic regression methods leverage genetic algorithms and adaptive programming techniques. Genetic algorithms lack optimality certifications and are typically stochastic in nature. In contrast, we propose an optimization formulation for the rigorous deterministic optimization of the symbolic regression problem. We present a mixed-integer nonlinear programming (MINLP) formulation to solve the symbolic regression problem as well as several alternative models to eliminate redundancies and symmetries. We demonstrate this symbolic regression technique using an array of experiments based upon literature instances. We then use a set of 24 MINLPs from symbolic regression to compare the performance of five local and five global MINLP solvers. Finally, we use larger instances to demonstrate that a portfolio of models provides an effective solution mechanism for problems of the size typically addressed in the symbolic regression literature. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature and Mathematical Optimization Society.",Mathematical Programming,10.1007/s10107-018-1289-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046715202&doi=10.1007%2fs10107-018-1289-x&partnerID=40&md5=4a1df0cdb77fcd72b38ac2f8482b209c,2018,2021-07-20 15:48:47,2021-07-20 15:48:47
TZXBCWPB,journalArticle,2019,"Liu, J.; Patwary, M.J.A.; Sun, X.Y.; Tao, K.",An experimental study on symbolic extreme learning machine,"With the advent of big data era, the volume and complexity of data have increased exponentially and the type of data has also been increased largely. Among all different types of data, symbolic data plays an important role in the study on machine learning model. It has been proved that feed-forward neural network (FNN) has a good ability to deal with numeric data but relatively clumsy with symbolic data. In this paper, a special type of FNN called Extreme Learning Machine (ELM) is discussed for handling symbolic data. Experimental results demonstrate that, unlike traditional back propagation based FNN, ELM has a better performance in comparison with C4.5 which is generally acknowledged as one of the best algorithms in handling symbolic data classification problems. In this performance comparison, some key evaluation criteria such as generalization ability, time complexity, the effect of training sample size and noise-resistance ability are taken into account. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",International Journal of Machine Learning and Cybernetics,10.1007/s13042-018-0872-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063261700&doi=10.1007%2fs13042-018-0872-z&partnerID=40&md5=2082789c78f434157582551c55a503c0,2019,2021-07-20 15:48:47,2021-07-20 15:48:47
ZJ779KRY,journalArticle,2020,"Alves, D.S.; Daniel, G.B.; Castro, H.F.D.; Machado, T.H.; Cavalca, K.L.; Gecgel, O.; Dias, J.P.; Ekwaro-Osire, S.",Uncertainty quantification in deep convolutional neural network diagnostics of journal bearings with ovalization fault,"Bearings play a crucial role in machine longevity and is, at the same time, one of the most critical sources of failure in rotor dynamics. Particularly for journal bearings, it is not completely understood how specific damages may influence the response of the rotating system. Consequently, the identification of hydrodynamic bearing faults is challenging. Most of the literature relies on large amounts of training data collections from physical experiments or from the field, which are high in cost. This paper offers a deep learning approach to identify ovalization faults aiming to develop condition monitoring model-based strategies applied to hydrodynamic journal bearings. Therefore, a numerical model was developed to simulate the ovalization fault conditions in order to build training datasets. Afterwards, a deep convolutional neural network algorithm was trained with the generated datasets and used to predict the faults conditions. Finally, the identification performance was evaluated statistically regarding the true-positive identification by both probability density function and subjective logic. The classification accuracy showed promising results for training the machine learning algorithms with simulated data. © 2020",Mechanism and Machine Theory,10.1016/j.mechmachtheory.2020.103835,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079564027&doi=10.1016%2fj.mechmachtheory.2020.103835&partnerID=40&md5=b52c13866b47b7850cff6aecadbe5611,2020,2021-07-20 15:48:47,2021-07-20 15:48:47
XBNNQS9L,journalArticle,2014,"Yeow, W.L.; Mahmud, R.; Raj, R.G.",An application of case-based reasoning with machine learning for forensic autopsy,"Case-based reasoning (CBR) is one of the matured paradigms of artificial intelligence for problem solving. CBR has been applied in many areas in the commercial sector to assist daily operations. However, CBR is relatively new in the field of forensic science. Even though forensic personnel have consciously used past experiences in solving new cases, the idea of applying machine intelligence to support decision-making in forensics is still in its infancy and poses a great challenge. This paper highlights the limitation of the methods used in forensics compared with a CBR method in the analysis of forensic evidences. The design and development of an Intelligent Forensic Autopsy Report System (I-AuReSys) basing on a CBR method along with the experimental results are presented. Our system is able to extract features by using an information extraction (IE) technique from the existing autopsy reports; then the system analyzes the case similarities by coupling the CBR technique with a Naïve Bayes learner for feature-weights learning; and finally it produces an outcome recommendation. Our experimental results reveal that the CBR method with the implementation of a learner is indeed a viable alternative method to the forensic methods with practical advantages. © 2013 Elsevier B.V. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2013.10.054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84892147804&doi=10.1016%2fj.eswa.2013.10.054&partnerID=40&md5=4a5809fda58a1c233ebfa47d4178bbbb,2014,2021-07-20 15:48:47,2021-07-20 15:48:47
IUMVGNXV,journalArticle,2020,"Berka, P.",Sentiment analysis using rule-based and case-based reasoning,"Sentiment analysis becomes increasingly popular with the rapid growth of various reviews, survey responses, tweets or posts available from social media like Facebook or Twitter. Sentiment analysis can be turned into the question of whether a piece of text is expressing positive, negative or neutral sentiment towards the discussed topic and can be thus understood as a knowledge-based classification problem. A variety of knowledge-based techniques can be used to solve this problem. The paper focuses on two complementary approaches that originate in the area of AI (artificial intelligence), rule-based reasoning and case-based reasoning. We describe basic principles of both approaches, their strengths and limitations and, based on a review of literature, show how these approaches can be used for sentiment analysis. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Intelligent Information Systems,10.1007/s10844-019-00591-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078361826&doi=10.1007%2fs10844-019-00591-8&partnerID=40&md5=90229b53754f129888641088613f2d62,2020,2021-07-20 15:48:47,2021-07-20 15:48:47
R6ZRSFTB,journalArticle,2018,"Juvina, I.; Larue, O.; Hough, A.",Modeling valuation and core affect in a cognitive architecture: The impact of valence and arousal on memory and decision-making,"A novel approach to adding primitive evaluative capabilities to a cognitive architecture is proposed. Two sub-symbolic quantities called valuation and arousal are learned for each declarative memory element based on usage statistics and the reward it generates. As a result, each memory element can be characterized as positive or negative and having a certain degree of affective intensity. In turn, these characteristics affect the latency and probability of retrieval for that memory element. Two global accumulators called core-affect-valuation and core-affect-arousal are computed as weighted sums of all retrievable valuations and arousals, respectively. The weights reflect usage history, context relevance, and reward accrual for all retrievable memory elements. These accumulators describe the general disposition or mood of the system. Core affect dynamics are used as reward signals to learn valuation and arousal values for new objects or events. The new architectural mechanism is used to develop two models that demonstrate the impact of affective valence and arousal on memory and decision-making. The models are fit to datasets from the literature and make novel predictions. The value of including valuation and core affect mechanisms in a cognitive architecture is discussed. © 2017 Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2017.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021065080&doi=10.1016%2fj.cogsys.2017.06.002&partnerID=40&md5=73e5875dab665d124e7670d9f2824bfc,2018,2021-07-20 15:48:47,2021-07-20 15:48:47
75XE98GS,journalArticle,2016,"He, H.; Li, Z.; Yao, C.; Zhang, W.",Sentiment classification technology based on Markov logic networks,"With diverse online media emerging, there is a growing concern of sentiment classification problem. At present, text sentiment classification mainly utilizes supervised machine learning methods, which feature certain domain dependency. On the basis of Markov logic networks (MLNs), this study proposed a cross-domain multi-task text sentiment classification method rooted in transfer learning. Through many-to-one knowledge transfer, labeled text sentiment classification, knowledge was successfully transferred into other domains, and the precision of the sentiment classification analysis in the text tendency domain was improved. The experimental results revealed the following: (1) the model based on a MLN demonstrated higher precision than the single individual learning plan model. (2) Multi-task transfer learning based on Markov logical networks could acquire more knowledge than self-domain learning. The cross-domain text sentiment classification model could significantly improve the precision and efficiency of text sentiment classification. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",New Review of Hypermedia and Multimedia,10.1080/13614568.2016.1152317,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962439030&doi=10.1080%2f13614568.2016.1152317&partnerID=40&md5=fb7daee053c2a45eb0c723674641c8d7,2016,2021-07-20 15:48:47,2021-07-20 15:48:47
BSVUEGBM,journalArticle,2018,"Baclawski, K.; Bennett, M.; Berg-Cross, G.; Fritzsche, D.; Schneider, T.; Sharma, R.; Sriram, R.D.; Westerinen, A.","Ontology Summit 2017 communiqué-AI, learning, reasoning and ontologies","There are many connections among artificial intelligence, learning, reasoning and ontologies. The Ontology Summit 2017 explored, identified and articulated the relationships among these areas. As part of the general advocacy of the Ontolog Forum to bring ontology science and engineering into the mainstream, we endeavored to abstract a conversational toolkit from the Ontology Summit sessions that may facilitate discussion and knowledge sharing amongst stakeholders concerned with the topic. Our findings are supported with examples from the various domains of interest. The results were captured in the form of this Communiqué, with expanded supporting material provided on the web. © 2018-IOS Press and the authors. All rights reserved.",Applied Ontology,10.3233/AO-170191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032974726&doi=10.3233%2fAO-170191&partnerID=40&md5=d23892087c267a67afc2447f4601e1dc,2018,2021-07-20 15:48:47,2021-07-20 15:48:47
9QSDA68G,journalArticle,2019,"Tang, X.-S.; Wei, H.",A segment-wise prediction based on genetic algorithm for object recognition,"Object recognition in complex backgrounds has challenged the fields of pattern recognition for years. It is even harder when the targets in images are of different poses. Current methods use descriptors of characteristic vectors and machine learning algorithms to produce classifiers for object recognition. However, the generalization ability of these methods relies on the quality of the training phase and cannot find the precise boundaries of the targets. The geometric features of objects are the most stable and consistent features, so the recognition method based on shapes can be more intuitive than those based on color and texture features. This paper proposes a novel method that uses the images represented by line segments. The recognition mission becomes to effectively filter and combine them. The contour fragments after combinations are expected to satisfy the given model, or certain parts of it. In this way, object recognition can be viewed as a combinatorial optimization problem. This paper develops a genetic algorithm-based method to solve this problem. The experimental results show that this method can solve the combinatorial optimization problem effectively and can accurately distinguish the contour of the target object from the background. This method, which is based on geometric features, may contribute to the development of explicit principals for the description of object structure and recognition method based on symbolic reasoning. © 2017, The Natural Computing Applications Forum.",Neural Computing and Applications,10.1007/s00521-017-3189-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028754741&doi=10.1007%2fs00521-017-3189-z&partnerID=40&md5=9cbe2a798366affcf99f84f37243b642,2019,2021-07-20 15:48:47,2021-07-20 15:48:47
38BIX8UA,journalArticle,2021,"Muller, S.K.; Hoffmann, J.",Modeling and analyzing evaluation cost of CUDA kernels,"General-purpose programming on GPUs (GPGPU) is becoming increasingly in vogue as applications such as machine learning and scientific computing demand high throughput in vector-parallel applications. NVIDIA's CUDA toolkit seeks to make GPGPU programming accessible by allowing programmers to write GPU functions, called kernels, in a small extension of C/C++. However, due to CUDA's complex execution model, the performance characteristics of CUDA kernels are difficult to predict, especially for novice programmers. This paper introduces a novel quantitative program logic for CUDA kernels, which allows programmers to reason about both functional correctness and resource usage of CUDA kernels, paying particular attention to a set of common but CUDA-specific performance bottlenecks. The logic is proved sound with respect to a novel operational cost semantics for CUDA kernels. The semantics, logic and soundness proofs are formalized in Coq. An inference algorithm based on LP solving automatically synthesizes symbolic resource bounds by generating derivations in the logic. This algorithm is the basis of RaCuda, an end-to-end resource-analysis tool for kernels, which has been implemented using an existing resource-analysis tool for imperative programs. An experimental evaluation on a suite of CUDA benchmarks shows that the analysis is effective in aiding the detection of performance bugs in CUDA kernels. © 2021 Owner/Author.",Proceedings of the ACM on Programming Languages,10.1145/3434306,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099034817&doi=10.1145%2f3434306&partnerID=40&md5=8bbfbc9c530975a46d2a7befd802b260,2021,2021-07-20 15:48:47,2021-07-20 15:48:47
S37GBJ72,journalArticle,2020,"Sekh, A.A.; Dogra, D.P.; Kar, S.; Roy, P.P.; Prasad, D.K.",Can we automate diagrammatic reasoning?,"Diagrammatic reasoning (DR) problems are well known. However, solving DR problems represented in 4 × 1 Raven's Progressive Matrix (RPM) form using computer vision and pattern recognition has not yet been tried. Emergence of deep learning techniques aided by advanced computing can be exploited to solve such DR problems. In this paper, we propose a new learning framework by combining LSTM and Convolutional LSTM to solve 4 × 1 DR problems. Initially, the elementary geometrical shapes in such problems are detected using a typical CNN-based detector. Next, relations of various shapes are analyzed and a high-level feature set is produced and processed in the LSTM framework. A new 4 × 1 DR dataset has been prepared and made available to the research community. We believe, it will be helpful in advancing this research further. We have compared our method with some of the existing frameworks that can be used for solving RPM-guided DR problems. We have recorded 18–20% increase in the average prediction accuracy as compared to the prior frameworks when applied to RPM-guided DR problems. We believe the CV research community will be interested to carry out similar research, particularly to investigate the feasibility of solving other types of known DR problems. © 2020 The Author(s)",Pattern Recognition,10.1016/j.patcog.2020.107412,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084642916&doi=10.1016%2fj.patcog.2020.107412&partnerID=40&md5=a8090cb77a71dd6051097330675b681b,2020,2021-07-20 15:48:47,2021-07-20 15:48:47
MG8ILZPE,journalArticle,2020,"Kang, Y.-B.; Krishnaswamy, S.; Sawangphol, W.; Gao, L.; Li, Y.-F.",Understanding and improving ontology reasoning efficiency through learning and ranking,"Ontologies are the fundamental building blocks of the Semantic Web and Linked Data. Reasoning is critical to ensure the logical consistency of ontologies, and to compute inferred knowledge from an ontology. It has been shown both theoretically and empirically that, despite decades of intensive work on optimising ontology reasoning algorithms, performing core reasoning tasks on large and expressive ontologies is time-consuming and resource-intensive. In this paper, we present the meta-reasoning framework R2O2* to tackle the important problems of understanding the source of TBox reasoning hardness and predicting and optimising TBox reasoning efficiency by exploiting machine learning techniques. R2O2* combines state-of-the-art OWL 2 DL reasoners as well as an efficient OWL 2 EL reasoner as components, and predicts the most efficient one by using an ensemble of robust learning algorithms including XGBoost and Random Forests. A comprehensive evaluation on a large and carefully curated ontology corpus shows that R2O2* outperforms all six component reasoners as well as AutoFolio, a robust and strong algorithm selection system. © 2019",Information Systems,10.1016/j.is.2019.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068877969&doi=10.1016%2fj.is.2019.07.002&partnerID=40&md5=b92663c877734287fe0f1b23c6e6c08a,2020,2021-07-20 15:48:47,2021-07-20 15:48:47
DCSICCZN,journalArticle,2021,"Krishnamurthy, P.; Khorrami, F.; Schmidt, S.; Wright, K.",Machine Learning for NetFlow Anomaly Detection with Human-Readable Annotations,"We propose a framework for anomaly detection in communication network logs along with automated extraction of human-readable annotations that explain the decision logic underlying each anomaly detection. For this purpose, we develop a machine learning methodology formulated in terms of a model comprised of an OR-combination of multiple Boolean logic based sentences. Each sentence is an empirically learned set of inequality conditions involving subsets of features. The feature set, which comprises the &#x201C;alphabet&#x201D; for human-readable annotations, is constructed using dynamic graph based spatio-temporal aggregation to extract human-understandable aggregates of network activity. These aggregates are constructed both in terms of computers (nodes in dynamic graph) and communications between computers (edges in dynamic graph). From the alphabet, the learned model identifies subsets of features that relate to each anomaly type and the combinations of conditions in terms of the feature subsets for detection of the specific anomaly type. Given a data point that the learned model detects as anomalous, the model identifies the specific features and their combinations related to the anomaly detection. These human-readable annotations provide a cyber-security analyst a transparent view into the decision logic underlying an anomaly detection. IEEE",IEEE Transactions on Network and Service Management,10.1109/TNSM.2021.3075656,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105104085&doi=10.1109%2fTNSM.2021.3075656&partnerID=40&md5=df79de107d445daf352fe15ed0f51a9b,2021,2021-07-20 15:48:48,2021-07-20 15:48:48
QGS93Z7F,journalArticle,2017,"Teso, S.; Sebastiani, R.; Passerini, A.",Structured learning modulo theories,"Modeling problems containing a mixture of Boolean and numerical variables is a long-standing interest of Artificial Intelligence. However, performing inference and learning in hybrid domains is a particularly daunting task. The ability to model these kinds of domains is crucial in “learning to design” tasks, that is, learning applications where the goal is to learn from examples how to perform automatic de novo design of novel objects. In this paper we present Structured Learning Modulo Theories, a max-margin approach for learning in hybrid domains based on Satisfiability Modulo Theories, which allows to combine Boolean reasoning and optimization over continuous linear arithmetical constraints. The main idea is to leverage a state-of-the-art generalized Satisfiability Modulo Theory solver for implementing the inference and separation oracles of Structured Output SVMs. We validate our method on artificial and real world scenarios. © 2015 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2015.04.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931088104&doi=10.1016%2fj.artint.2015.04.002&partnerID=40&md5=0270b4c5e5e5fc46bb81a574166dbe24,2017,2021-07-20 15:48:48,2021-07-20 15:48:48
8D94U4YD,journalArticle,2015,"Bouchon-Meunier, B.; Marsala, C.",Fuzzy modifiers at the core of interpretable fuzzy systems,"Fuzzy modifiers associated with linguistic hedges have been introduced by L.A. Zadeh at the early stage of approximate reasoning and they are fundamental elements in the management of interpretable systems. They can be regarded as a solution to the construction of fuzzy sets slightly different from original ones. We first present the main definitions of modifiers based on mathematical transformations of membership functions, mainly focusing on so-called post-modifiers and premodifiers, as well as definitions based on fuzzy relations.We show that measures of similarity are useful to evaluate the proximity between the original fuzzy sets and their modified form and we point out links between modifiers and similarities. We then propose an overview of application domains which can take advantage of fuzzy modifiers, for instance analogy-based reasoning, rule-based systems, gradual systems, databases, machine learning, image processing, and description logic. It can be observed that fuzzy modifiers are either constructed in a prior way by means of formal definitions or automatically learnt or tuned, for instance in hybrid systems involving genetic algorithm-based methods. © Springer International Publishing Switzerland 2015.",Studies in Fuzziness and Soft Computing,10.1007/978-3-319-19683-1_4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930206838&doi=10.1007%2f978-3-319-19683-1_4&partnerID=40&md5=e3f925b6c5e5652298c8b21bc20fad9d,2015,2021-07-20 15:48:48,2021-07-20 15:48:48
FD7A4BSF,journalArticle,2021,"Suchan, J.; Bhatt, M.; Varadarajan, S.",Commonsense visual sensemaking for autonomous driving – On generalised neurosymbolic online abduction integrating vision and semantics,"We demonstrate the need and potential of systematically integrated vision and semantics solutions for visual sensemaking in the backdrop of autonomous driving. A general neurosymbolic method for online visual sensemaking using answer set programming (ASP) is systematically formalised and fully implemented. The method integrates state of the art in visual computing, and is developed as a modular framework that is generally usable within hybrid architectures for realtime perception and control. We evaluate and demonstrate with community established benchmarks KITTIMOD, MOT-2017, and MOT-2020. As use-case, we focus on the significance of human-centred visual sensemaking —e.g., involving semantic representation and explainability, question-answering, commonsense interpolation— in safety-critical autonomous driving situations. The developed neurosymbolic framework is domain-independent, with the case of autonomous driving designed to serve as an exemplar for online visual sensemaking in diverse cognitive interaction settings in the backdrop of select human-centred AI technology design considerations. © 2021 The Author(s)",Artificial Intelligence,10.1016/j.artint.2021.103522,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106549406&doi=10.1016%2fj.artint.2021.103522&partnerID=40&md5=0f712891d2bc5ba875109dfd61ca5e11,2021,2021-07-20 15:48:48,2021-07-20 15:48:48
4S3K6AGH,journalArticle,2021,"Li, X.; Rosman, G.; Gilitschenski, I.; Vasile, C.-I.; Decastro, J.A.; Karaman, S.; Rus, D.",Vehicle Trajectory Prediction Using Generative Adversarial Network with Temporal Logic Syntax Tree Features,"In this work, we propose a novel approach for integrating rules into traffic agent trajectory prediction. Consideration of rules is important for understanding how people behave-yet, it cannot be assumed that rules are always followed. To address this challenge, we evaluate different approaches of integrating rules as inductive biases into deep learning-based prediction models. We propose a framework based on generative adversarial networks that uses tools from formal methods, namely signal temporal logic and syntax trees. This allows us to leverage information on rule obedience as features in neural networks and improves prediction accuracy without biasing towards lawful behavior. We evaluate our method on a real-world driving dataset and show improvement in performance over off-The-shelf predictors. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2021.3062807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102265507&doi=10.1109%2fLRA.2021.3062807&partnerID=40&md5=8150578347057ed946c2a7bd042fc6c8,2021,2021-07-20 15:48:48,2021-07-20 15:48:48
DAGZL4X6,journalArticle,2020,"Zhang, P.; Tang, Y.; Hu, J.-F.; Zheng, W.-S.",Fast Collective Activity Recognition under Weak Supervision,"Collective activity recognition, which tells what activity a group of people is performing, is a cutting-edge research topic in computer vision. Different from action performed by individuals, collective activity needs to consider the complex interactions among different people. However, most previous works require exhaustive annotations such as accurate label information of individual actions, pairwise interactions, and poses, which could not be easily available in practice. Moreover, most of them treat human detection as a decoupled task before collective activity recognition and leverage all detected persons. This not only ignores the mutual relation between the two tasks, which makes it hard for filtering out irrelevant people, but also probably increases the computation burden when reasoning the collective activities. In this paper, we propose a fast weakly supervised deep learning architecture for collective activity recognition. For fast inference, we propose to make the actor detection and weakly supervised collective activity reasoning collaborate in an end-to-end framework by sharing convolutional layers between them. The joint learning makes the two tasks united and reinforced each other, so that it is more effective to filter out the outliers who are not involved in the activity. For the weakly supervised learning, we propose a latent embedding scheme for mining person-group interactive relationship to get rid of the use of any pairwise relation between people and the individual action labels as well. The experimental results show that the proposed framework achieves comparable or even better performance as compared to the state-of-the-art on three datasets. Our joint modelling reasons collective activities at the speed of 22.65 fps, which is the fastest ever known and substantially makes collective activity recognition more towards real-time applications. © 1992-2012 IEEE.",IEEE Transactions on Image Processing,10.1109/TIP.2019.2918725,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072509426&doi=10.1109%2fTIP.2019.2918725&partnerID=40&md5=a0ec6ac9891dcaa1c163e4a02c76bc6c,2020,2021-07-20 15:48:48,2021-07-20 15:48:48
3YZFL64E,journalArticle,2016,"Sarkar, S.; Srivastav, A.",A composite discretization scheme for symbolic identification of complex systems,"Phase-space discretization is a necessary step for study of continuous dynamical systems using a symbolic dynamics and language-theoretic approach. It is also critical for many machine learning techniques, e.g., probabilistic graphical models (Bayesian Networks, Markov models). This paper proposes a novel composite discretization method - a univariate discretization, namely Statistical Similarity-based Discretization (SSD) followed by a multi-variate discretization called Maximally Bijective Discretization (MBD). While SSD first quantizes input variables for a complex system identifying different operating conditions, MBD finds a discretization on the output variables given the discretization on the input variables such that the correspondence between input and output variables in the continuous domain is preserved in discrete domain for the underlying dynamical system. The proposed method is applied on both simulated and experimental data and results are compared with classical uniform width, maximum entropy, clustering and self-organizing map based discretization techniques. © 2016 Elsevier B.V.",Signal Processing,10.1016/j.sigpro.2016.01.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959107619&doi=10.1016%2fj.sigpro.2016.01.018&partnerID=40&md5=c39be867bcd59ebfc94ebbb49d3dcbc4,2016,2021-07-20 15:48:48,2021-07-20 15:48:48
8PD8BXUD,journalArticle,2013,"Choi, C.; Choi, J.; Lee, E.; You, I.; Kim, P.",Probabilistic spatio-temporal inference for motion event understanding,"In Video data, moving object has temporal flow and spatial features, which can be expressed in spatio-temporal relation. The goal in this paper is the understanding of object movement of spatio-temporal relation through mapping between vocabulary and object movement. In this case, spatio-temporal relation consists of temporal relation obedient to the passage of time, directional relation obedient to changes of object movement direction, changes of object size relation, topological relation obedient to changes of object movement position, and velocity relation using concept relations between topology models. This paper in the ontology building defines the inference rules using the proposed spatio-temporal relation and the use of Markov Logic Networks for probabilistic reasoning. In the experiments, motion verbs are used to understand semantic object movement. Probability weight and learning for 10,000 times are used for value comparison. The result value from inference exists even though connection rules such as ""go through"" are not defined directly. In addition, it is indicated that the relation that includes large number of connections such as ""go to"" has the high value of probabilistic inference result and that small number of connection relations depending on the change in object size such as ""include"" leads to low value of result. © 2013 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2012.12.058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884206484&doi=10.1016%2fj.neucom.2012.12.058&partnerID=40&md5=3ebd3f38b1bb3aa4b6fb3adcb6b102c0,2013,2021-07-20 15:48:48,2021-07-20 15:48:48
BYRRI9MW,journalArticle,2020,"Wei, P.; He, F.; Li, L.; Li, J.",Research on sound classification based on SVM,"Sound is a ubiquitous natural phenomenon that contains a wealth of information that constantly enhances our understanding of the objective world. With the continuous development of computer network technology and communication technology, audio information has become a very important part. Audio is a non-semantic symbolic representation and an unstructured binary stream. Because the audio itself lacks the description of content semantics and structured organization, it brings great difficulty to the audio classification work. The research of digital audio classification will become more and more important with the increasing number of digital audio resources in the network. Digital audio classification technology is the key technology to solve this problem. It is the key to solve the problem of audio structure and extract audio structured information and content semantics. It is a research hot spot in the field of audio analysis. It has important application value in many fields, such as audio retrieval, video summary and auxiliary video analysis. This paper studies the structure of audio, the analysis and extraction of audio features, the digital audio classifier based on support vector machines (SVM) and the audio segmentation technology based on BCI. SVM is an important achievement of machine learning research in recent years. As a new machine learning method, SVM can solve practical problems such as small sample, nonlinearity and high dimension, so it has become a new research hot spot after the study of neural network. Experiments show that the SVM-based audio classification algorithm has good classification effect, and the smoothed audio segmentation results are more accurate. With the further development of the research, the research results will be well applied in practice. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-019-04182-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064655484&doi=10.1007%2fs00521-019-04182-0&partnerID=40&md5=bd780e9f1db06c2f66b6cb6807c0231b,2020,2021-07-20 15:48:48,2021-07-20 15:48:48
VYX2CHG3,journalArticle,2021,"Yao, S.; Yang, J.-B.; Xu, D.-L.",A probabilistic modeling approach for interpretable data inference and classification,"In this paper, we propose a new probabilistic modeling approach for interpretable inference and classification using the maximum likelihood evidential reasoning (MAKER) framework. This approach integrates statistical analysis, hybrid evidence combination and belief rule-based (BRB) inference, and machine learning. Statistical analysis is used to acquire evidence from data. The BRB inference is applied to analyze the relationship between system inputs and outputs. An interdependence index is used to quantify the interdependence between input variables. An adapted genetic algorithm is applied to train the models. The model established by the approach features a unique strong interpretability, which is reflected in three aspects: (1) interpretable evidence acquisition, (2) interpretable inference mechanism, and (3) interpretable parameters determination. The MAKER-based model is shown to be a competitive classifier for the Banana, Haberman's survival, and Iris data set, and generally performs better than other interpretable classifiers, e.g., complex tree, logistic regression, and naive Bayes. © 2021-IOS Press. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-201833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102410623&doi=10.3233%2fJIFS-201833&partnerID=40&md5=e9545048288ae262c38b031cf4128e19,2021,2021-07-20 15:48:48,2021-07-20 15:48:48
6XJRUFBQ,journalArticle,2021,"Yu, J.; Liu, G.",Extracting and inserting knowledge into stacked denoising auto-encoders,"Deep neural networks (DNNs) with a complex structure and multiple nonlinear processing units have achieved great successes for feature learning in image and visualization analysis. Due to interpretability of the “black box” problem in DNNs, however, there are still many obstacles to applications of DNNs in various real-world cases. This paper proposes a new DNN model, knowledge-based deep stacked denoising auto-encoders (KBSDAE), which inserts the knowledge (i.e., confidence and classification rules) into the deep network structure. This model not only can offer a good understanding of the representations learned by the deep network but also can produce an improvement in the learning performance of stacked denoising auto-encoder (SDAE). The knowledge discovery algorithm is proposed to extract confidence rules to interpret the layerwise network (i.e., denoising auto-encoder (DAE)). The symbolic language is developed to describe the deep network and shows that it is suitable for the representation of quantitative reasoning in a deep network. The confidence rule insertion to the deep network is able to produce an improvement in feature learning of DAEs. The classification rules extracted from the data offer a novel method for knowledge insertion to the classification layer of SDAE. The testing results of KBSDAE on various benchmark data indicate that the proposed method not only effectively extracts knowledge from the deep network, but also shows better feature learning performance than that of those typical DNNs (e.g., SDAE). © 2021 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2021.01.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100245528&doi=10.1016%2fj.neunet.2021.01.010&partnerID=40&md5=267e269d6f71a002911c21acb61f112f,2021,2021-07-20 15:48:49,2021-07-20 15:48:49
U78MYWZN,journalArticle,2019,"Nunes, R.; Azevedo, L.; Soares, A.",Fast geostatistical seismic inversion coupling machine learning and Fourier decomposition,"Geostatistical seismic inversion uses stochastic sequential simulation and co-simulation as techniques to generate and perturb subsurface elastic models. These steps are computational demanding and, depending on the size of the inversion grid, time consuming. This paper introduces a technique to achieve considerable reductions in the consumption of computational resources of geostatistical seismic inversion without compromising the quality of the inverse elastic models. We achieve these improvements by reducing one of the dimensions of the data domain to a periodic function approximated by a Fourier series of n terms. Then, instead of simulating over the entire original data volume, in the original data domain, we simulate independently each term of the series over the area of interest. If the number of terms in the series is considerably smaller than the size of the collapsed data domain, then the computational overhead of the inversion procedure decreases. Symbolic regression is used to obtain automatically the approximation to the periodic function that captures the behavior of the property in the reduced dimension. The advantages related to the use of symbolic regression over alternative machine learning algorithms are discussed. We use the method to simulate acoustic impedance in a geostatistical seismic inversion of a synthetic dataset representing a hydrocarbon reservoir, where the true acoustic impedance model is available. Results demonstrate a considerable speedup over traditional methods while achieving similar performance in terms of the misfit between real and synthetic seismic and a good representation of the true impedance model. The frequency content of the resulting inverted data is discussed and compared with the one inferred from traditional methods, demonstrating an expected reduction of the high-frequency component. © 2019, Springer Nature Switzerland AG.",Computational Geosciences,10.1007/s10596-019-09877-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071458222&doi=10.1007%2fs10596-019-09877-w&partnerID=40&md5=afc1f8e39fd0e3cd26af712a70bdaf24,2019,2021-07-20 15:48:49,2021-07-20 15:48:49
8EGWW25B,journalArticle,2017,"Sadi, M.; Contreras, G.K.; Chen, J.; Winemberg, L.; Tehranipoor, M.",Design of Reliable SoCs with BIST Hardware and Machine Learning,"In this paper, a novel framework is presented for designing lifetime-reliable SoCs with self-adaptation capability against aging-induced degradation. The proposed flow utilizes the existing logic built-in-self-test (LBIST) hardware, and software implemented machine learning predictor to activate appropriate countermeasures to remedy the wear out in the field. Using an innovative method, we convert ATPG-generated transition delay test patterns into LBIST patterns to activate high-usage critical/near-critical paths in-field, and the corresponding responses are utilized in developing the predictor. A gate-overlap and path-delay-aware algorithm selects the optimum set of patterns. The area and test time overhead for the framework are very low. We implemented our proposed flow on SoC benchmark designs, and the results demonstrated its efficacy. © 2017 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2017.2734685,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028464432&doi=10.1109%2fTVLSI.2017.2734685&partnerID=40&md5=b1faadad0d36a419d017fd4ba446c9e4,2017,2021-07-20 15:48:49,2021-07-20 15:48:49
XTTRFFZJ,journalArticle,2019,"Wu, C.; Zhang, L.; Li, Q.; Fu, Z.; Zhu, W.; Zhang, Y.",Enabling Flexible Resource Allocation in Mobile Deep Learning Systems,"Deep learning provides new opportunities for mobile applications to achieve higher performance than before. Rather, the deep learning implementation on mobile device today is largely demanding on expensive resource overheads, imposes a significant burden on the battery life and limited memory space. Existing methods either utilize cloud or edge infrastructure that require to upload user data, however, resulting in a risk of privacy leakage and large data transfers; or adopt compressed deep models, nevertheless, downgrading the algorithm accuracy. This paper provides DeepShark, a platform to enable mobile devices with the ability of flexible resource allocation in using commercial-off-the-shelf (COTS) deep learning systems. Compared to existing approaches, DeepShark seeks a balanced point between time and memory efficiency by user requirements, breaks down sophisticated deep model into code block stream and incrementally executes such blocks on system-on-chip (SoC). Thus, DeepShark requires significantly less memory space on mobile device and achieves the default accuracy. In addition, all referred user data of model processing is handled locally, thus to avoid unnecessary data transfer and network latency. DeepShark is now developed on two COTS deep learning systems, i.e., Caffe and TensorFlow. The experimental evaluations demonstrate its effectiveness in the aspects of memory space and energy cost. © 2018 IEEE.",IEEE Transactions on Parallel and Distributed Systems,10.1109/TPDS.2018.2865359,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051667704&doi=10.1109%2fTPDS.2018.2865359&partnerID=40&md5=da00e095a2dd85c998046a888dd76c2b,2019,2021-07-20 15:48:49,2021-07-20 15:48:49
447BZ3A4,journalArticle,2021,"Jia, X.; Xiao, J.; Wu, C.",TICS: text–image-based semantic CAPTCHA synthesis via multi-condition adversarial learning,"CAPTCHA is used to distinguish humans from automated programs and plays an important role in multimedia security mechanisms. Traditional CAPTCHA methods like image-based CAPTCHA and text-based CAPTCHA are usually based on word-level understanding, which can be easily cracked due to the recent success of deep learning techniques. To this end, this paper proposes a text–image-based CAPTCHA based on the cognition process and semantic reasoning and a novel model to generate the CAPTCHA. This method synthesizes three features: sentence, object, and location to generate a multi-conditional CAPTCHA that can resist the attack of the classification of CNN. A quantity of experiments has been conducted, and the result showed that the classification of ResNet-50 on the proposed TIC only achieves 3.38% accuracy. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.",Visual Computer,10.1007/s00371-021-02061-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100719675&doi=10.1007%2fs00371-021-02061-1&partnerID=40&md5=326d774b64b84d49f9ae44620f28a47f,2021,2021-07-20 15:48:49,2021-07-20 15:48:49
N23UM3QC,journalArticle,2021,"Pan, J.; Shan, H.; Li, R.; Wu, Y.; Wua, W.; Quek, T.Q.S.",Channel Estimation based on Deep Learning in Vehicle-to-everything Environments,"Channel estimation in vehicle-to-everything (V2X) communications is a challenging issue due to the fast time-varying and non-stationary characteristics of wireless channel. To grasp the complicated variations of channel with limited number of pilots in the IEEE 802.11p systems, data pilot-aided (DPA) channel estimation has been widely studied. However, the error propagation in the DPA procedure, caused by the noise and the channel variation within adjacent symbols, limits the performance seriously. In this letter, we propose a deep learning based channel estimation scheme, which exploits a long short-term memory network followed by a multilayer perceptron network to solve the error propagation issue. Simulation results show that the proposed scheme outperforms currently widely-used DPA schemes for the IEEE 802.11p-based V2X communications. IEEE",IEEE Communications Letters,10.1109/LCOMM.2021.3059922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100946718&doi=10.1109%2fLCOMM.2021.3059922&partnerID=40&md5=ce8cf6afc0bb3ce6d5b212becf680577,2021,2021-07-20 15:48:49,2021-07-20 15:48:49
CIIHX9CU,journalArticle,2020,"Wu, H.; Wang, C.; Nock, R.; Wang, W.; Yin, J.; Lu, K.; Zhu, L.",SMINT: Toward interpretable and robust model sharing for deep neural networks,"Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not discussed. Moreover, simply providing cases as explanations may not be sufficient for supporting instance level interpretability. Meanwhile, existing interpretation methods for DNN models generally lack the means to evaluate the reliability of the interpretation. In this article, we propose a framework named Shared Model INTerpreter (SMINT) to address the above limitations. We propose a new data structure called a boundary graph to organize training points to mimic the predictions of DNN models. We integrate local features, such as saliency maps and interpretable input masks, into the data structure to help users to infer the model decision boundaries. We show that the boundary graph is able to address the reliability issues in many local interpretation methods. We further design an algorithm named hidden-layer aware p-test to measure the reliability of the interpretations. Our experiments show that SMINT is able to achieve above 99% fidelity to corresponding DNN models on both MNIST and ImageNet by sharing only a tiny fraction of training data to make these models interpretable. The human pilot study demonstrates that SMINT provides better interpretability compared with existing methods. Moreover, we demonstrate that SMINT is able to assist model tuning for better performance on different user data. © 2020 ACM.",ACM Transactions on the Web,10.1145/3381833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092721616&doi=10.1145%2f3381833&partnerID=40&md5=90d5da9c98d4dbb2115f5cb4d352c547,2020,2021-07-20 15:48:49,2021-07-20 15:48:49
8NTXV9BV,journalArticle,2020,"Bentaiba-Lagrid, M.B.; Bouzar-Benlabiod, L.; Rubin, S.H.; Bouabana-Tebibel, T.; Hanini, M.R.",A case-based reasoning system for supervised classification problems in the medical field,"Case-Based Reasoning (CBR) system relies on reuse for solving new problems. The system uses the experiences it previously acquired and stored into its case base to address the newly faced problems. A static and non-evolutive case base hinders the system and limits the accuracy of the CBR in problem-solving. While a massive case base can affect the resolution time. Randomization represents a way to generate data without deteriorating the spatial image of the case base and by extension the search time as well. However, the cases generated by randomization are not necessarily valid and require a thorough validation process to access their validity. This paper presents a new amplification technique based on randomization for a CBR system incorporating a structured case-base that speeds up case retrieval while supporting case retention. The generated data by randomization is validated through a three-layer validation process: coherence verification, stochastic validation, and absolute validation. Furthermore, we propose a new way to segment the case base along with new similarity functions based on features’ weights to speed CBR retrieval. We carried out experiments on mammography mass and thyroid disease datasets to validate our approach, where the proposed approach is compared to several popular supervised machine-learning methods and other related works that utilize the same datasets. Experiments have shown that our approach can generate relevant data, which significantly improves the resolution accuracy and makes CBR a good competitor to classification methods. © 2020 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2020.113335,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080913192&doi=10.1016%2fj.eswa.2020.113335&partnerID=40&md5=5c6f0bb5c0c00c1de3be73e2a6287b6f,2020,2021-07-20 15:48:49,2021-07-20 15:48:49
ZZ94GCNR,journalArticle,2019,"Asim, Y.; Raza, B.; Malik, A.K.; Shahaid, A.R.; Alquhayz, H.",An adaptive model for identification of influential bloggers based on case-based reasoning using random forest,"Bloggers play a role in individual decision making of users in online social networking platforms. Their capability of addressing a wide audience gives them influence over their audience, which companies seek to exploit. Identification of influential bloggers can be seen as a machine learning (ML) task and different ML techniques can help in classifying the professional blogger. In this paper, we propose a predictive and adaptive model named as Influential Blogger based Case-Based Reasoning (IB-CBR) model for the recognition of unseen influential bloggers. It incorporates self-prediction and self-Adaptation (self-management) capabilities which are the essence of an automated system. The integration of Random Forest is found contributing to the efficiency of the IB-CBR model as compared to Nearest-Neighbor, and Artificial Neural Network. The performance of the proposed IB-CBR model is evaluated against other ML techniques by using standard performance measures on a standard blogger's dataset. It is observed that our proposed model exhibits 88-95% Accuracy and 94-97% True Positive Rate in the prediction and adaptation of professional bloggers, respectively, in three iterations of the proposed model. What's more, the IB-CBR model achieved 91-96% (increasing) F-measure, 91-98% (increasing) ROC AUC, and 36-11% (decreasing) False Positive Rate due to adaptivity. The IB-CBR model performed well when it is compared with other ML techniques using different standard datasets. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2925905,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069789149&doi=10.1109%2fACCESS.2019.2925905&partnerID=40&md5=17f39cbc4aa65823e432217525be3d23,2019,2021-07-20 15:48:49,2021-07-20 15:48:49
59C8QEM3,journalArticle,2020,"Waa, J.V.D.; Schoonderwoerd, T.; Diggelen, J.V.; Neerincx, M.",Interpretable confidence measures for decision support systems,"Decision support systems (DSS) have improved significantly but are more complex due to recent advances in Artificial Intelligence. Current XAI methods generate explanations on model behaviour to facilitate a user's understanding, which incites trust in the DSS. However, little focus has been on the development of methods that establish and convey a system's confidence in the advice that it provides. This paper presents a framework for Interpretable Confidence Measures (ICMs). We investigate what properties of a confidence measure are desirable and why, and how an ICM is interpreted by users. In several data sets and user experiments, we evaluate these ideas. The presented framework defines four properties: 1) accuracy or soundness, 2) transparency, 3) explainability and 4) predictability. These characteristics are realized by a case-based reasoning approach to confidence estimation. Example ICMs are proposed for -and evaluated on- multiple data sets. In addition, ICM was evaluated by performing two user experiments. The results show that ICM can be as accurate as other confidence measures, while behaving in a more predictable manner. Also, ICM's underlying idea of case-based reasoning enables generating explanations about the computation of the confidence value, and facilitates user's understandability of the algorithm. © 2020 The Authors",International Journal of Human Computer Studies,10.1016/j.ijhcs.2020.102493,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087889231&doi=10.1016%2fj.ijhcs.2020.102493&partnerID=40&md5=3f02bb1bdc897d82c87d9ffdc5ed5cb1,2020,2021-07-20 15:48:49,2021-07-20 15:48:49
QHKF8WS4,journalArticle,2017,"Xu, X.; Zheng, J.; Yang, J.-B.; Xu, D.-L.; Chen, Y.-W.",Data classification using evidence reasoning rule,"In Dempster–Shafer evidence theory (DST) based classifier design, Dempster's combination (DC) rule is commonly used as a multi-attribute classifier to combine evidence collected from different attributes. The main aim of this paper is to present a classification method using a novel combination rule i.e., the evidence reasoning (ER) rule. As an improvement of the DC rule, the newly proposed ER rule defines the reliability and weight of evidence. The former indicates the ability of attribute or its evidence to provide correct assessment for classification problem, and the latter reflects the relative importance of evidence in comparison with other evidence when they need to be combined. The ER rule-based classification procedure is expatiated from evidence acquisition and estimation of evidence reliability and weight to combination of evidence. It is a purely data-driven approach without making any assumptions about the relationships between attributes and class memberships, and the specific statistic distributions of attribute data. Experiential results on five popular benchmark databases taken from University of California Irvine (UCI) machine learning database show high classification accuracy that is competitive with other classical and mainstream classifiers. © 2016 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2016.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006257903&doi=10.1016%2fj.knosys.2016.11.001&partnerID=40&md5=74126b94e45d64af4eefb38c2fe23b1f,2017,2021-07-20 15:48:49,2021-07-20 15:48:49
3YFVRRKE,journalArticle,2020,"Gong, J.; Ma, H.; Teng, Z.; Teng, Q.; Zhang, H.; Du, L.; Chen, S.; Bhuiyan, M.Z.A.; Li, J.; Liu, M.",Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification,"Traditional methods of multi-label text classification, particularly deep learning, have achieved remarkable results. However, most of these methods use word2vec technology to represent sequential text information, while ignoring the logic and internal hierarchy of the text itself. Although these approaches can learn the hypothetical hierarchy and logic of the text, it is unexplained. In addition, the traditional approach treats labels as independent individuals and ignores the relationships between them, which not only does not reflect reality but also causes significant loss of semantic information. In this paper, we propose a novel Hierarchical Graph Transformer based deep learning model for large-scale multi-label text classification. We first model the text into a graph structure that can embody the different semantics of the text and the connections between them. We then use a multi-layer transformer structure with a multi-head attention mechanism at the word, sentence, and graph levels to fully capture the features of the text and observe the importance of the separate parts. Finally, we use the hierarchical relationship of the labels to generate the representation of the labels, and design a weighted loss function based on the semantic distances of the labels. Extensive experiments conducted on three benchmark datasets demonstrated that the proposed model can realistically capture the hierarchy and logic of text and improve performance compared with the state-of-the-art methods. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2972751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081058031&doi=10.1109%2fACCESS.2020.2972751&partnerID=40&md5=1376cf7721ed6ff3f046f7ca9e9b4415,2020,2021-07-20 15:48:49,2021-07-20 15:48:49
QFHEY9L3,journalArticle,2011,"Zhang, X.; Zhao, J.; Wang, W.; Cong, L.; Feng, W.",An optimal method for prediction and adjustment on byproduct gas holder in steel industry,"To maintain the balance of byproduct gas holder is an important task in optimal scheduling of byproduct energy in steel industry. However, this is often influenced by many factors and is difficult to obtain a precise mechanism model for analysis. In this paper, an optimal method for prediction and adjustment on byproduct gas holder is proposed. Considering the different operation styles of gasholders, both single and multiple gasholders level prediction models are established by machine learning methodology. And, a hybrid parameter optimization algorithm is developed to optimize the model for high prediction accuracy. Then, based on the predicted gasholder level, the optimal adjustment amount is calculated by a novel reasoning method to sustain the gasholder within safety zone. This method has been verified in the Energy Center of Baosteel, China. The results demonstrate that the proposed approach can precisely predict and adjust gasholders and provide a remarkable guidance for reasonable scheduling of byproduct gases. © 2010 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2010.09.132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650709888&doi=10.1016%2fj.eswa.2010.09.132&partnerID=40&md5=6756b1a06c5512b9e67c92a5e47ac982,2011,2021-07-20 15:48:49,2021-07-20 15:48:49
PXG38ZQ6,journalArticle,2018,"Krykunov, M.; Woo, T.K.",Bond Type Restricted Property Weighted Radial Distribution Functions for Accurate Machine Learning Prediction of Atomization Energies,"Understanding the performance of machine learning algorithms is essential for designing more accurate and efficient statistical models. It is not always possible to unravel the reasoning of neural networks. Here, we propose a method for calculating machine learning kernels in closed and analytic form by combining atomic property weighted radial distribution function (AP-RDF) descriptor with a Gaussian kernel. This allowed us to analyze and improve the performance of the Bag-of-Bonds descriptor when the bond type restriction is included in AP-RDF. The improvement is achieved for the prediction of molecular atomization energies (MAE = 1.7 kcal/mol for QM7 data set) and is due to the incorporation of a tensor product into the kernel, which captures the multidimensional representation of the AP-RDF. On the other hand, the numerical version of the AP-RDF is a constant size descriptor, making it more computationally efficient than Bag-of-Bonds. We have also discussed a connection between molecular quantum similarity and machine learning kernels with first-principles kinds of descriptors. © 2018 American Chemical Society.",Journal of Chemical Theory and Computation,10.1021/acs.jctc.8b00788,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053564109&doi=10.1021%2facs.jctc.8b00788&partnerID=40&md5=2cbd09df3dbfdc3a397b3d1f48f9ce0c,2018,2021-07-20 15:48:49,2021-07-20 15:48:49
B3U9XCNN,journalArticle,2020,"Ozaki, A.",Learning Description Logic Ontologies: Five Approaches. Where Do They Stand?,"The quest for acquiring a formal representation of the knowledge of a domain of interest has attracted researchers with various backgrounds into a diverse field called ontology learning. We highlight classical machine learning and data mining approaches that have been proposed for (semi-)automating the creation of description logic (DL) ontologies. These are based on association rule mining, formal concept analysis, inductive logic programming, computational learning theory, and neural networks. We provide an overview of each approach and how it has been adapted for dealing with DL ontologies. Finally, we discuss the benefits and limitations of each of them for learning DL ontologies. © 2020, The Author(s).",KI - Kunstliche Intelligenz,10.1007/s13218-020-00656-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089563084&doi=10.1007%2fs13218-020-00656-9&partnerID=40&md5=3c79109868dcc44112b517a8665c9823,2020,2021-07-20 15:48:49,2021-07-20 15:48:49
6M8WIGVS,journalArticle,2020,"Angelov, P.; Soares, E.",Towards explainable deep neural networks (xDNN),"In this paper, we propose an elegant solution that is directly addressing the bottlenecks of the traditional deep learning approaches and offers an explainable internal architecture that can outperform the existing methods, requires very little computational resources (no need for GPUs) and short training times (in the order of seconds). The proposed approach, xDNN is using prototypes. Prototypes are actual training data samples (images), which are local peaks of the empirical data distribution called typicality as well as of the data density. This generative model is identified in a closed form and equates to the pdf but is derived automatically and entirely from the training data with no user- or problem-specific thresholds, parameters or intervention. The proposed xDNN offers a new deep learning architecture that combines reasoning and learning in a synergy. It is non-iterative and non-parametric, which explains its efficiency in terms of time and computational resources. From the user perspective, the proposed approach is clearly understandable to human users. We tested it on challenging problems as the classification of different lighting conditions for driving scenes (iROADS), object detection (Caltech-256, and Caltech-101), and SARS-CoV-2 identification via computed tomography scan (COVID CT-scans dataset). xDNN outperforms the other methods including deep learning in terms of accuracy, time to train and offers an explainable classifier. © 2020 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2020.07.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087911322&doi=10.1016%2fj.neunet.2020.07.010&partnerID=40&md5=a42afcd99290340d0b66abc201b768db,2020,2021-07-20 15:48:49,2021-07-20 15:48:49
4DLHETA7,journalArticle,2020,"Sarabakha, A.; Kayacan, E.",Online Deep Fuzzy Learning for Control of Nonlinear Systems Using Expert Knowledge,"This article presents an online learning method for improved control of nonlinear systems by combining deep learning and fuzzy logic. Given the ability of deep learning to generalize knowledge from training samples, the proposed method requires minimum amount of information about the system to be controlled. However, in robotics, particularly in aerial robotics where the operating conditions may vary, online learning is required. In this article, fuzzy logic is preferred to provide supervising feedback to the deep model for adapting to variations in the system dynamics as well as new operational conditions. The learning method is divided into two phases: offline pretraining and online posttraining. In the former, the system is controlled by a conventional controller and a deep fuzzy neural network (DFNN) is pretrained based on the recorded input-output dataset, in order to approximate the inverse dynamical model of the system. In the latter, only the pretrained DFNN is used to control the system. In this phase, the fuzzy logic, which encodes the expert knowledge, is utilized to observe the behavior of the system and to correct the action of DFNN instantaneously. The experimental results show that the proposed online learning-based approach improves the trajectory tracking performance of the unmanned aerial vehicle. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2019.2936787,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087802100&doi=10.1109%2fTFUZZ.2019.2936787&partnerID=40&md5=f9d78ae5b5d675cc2f3e4f7bf2e41d1d,2020,2021-07-20 15:48:50,2021-07-20 15:48:50
KAYZ6FRX,journalArticle,2020,"Dean, R.T.; Forth, J.",Towards a Deep Improviser: a prototype deep learning post-tonal free music generator,"Two modest-sized symbolic corpora of post-tonal and post-metrical keyboard music have been constructed: one algorithmic and the other improvised. Deep learning models of each have been trained. The purpose was to obtain models with sufficient generalisation capacity that in response to separate fresh input seed material, they can generate outputs that are statistically distinctive, neither random nor recreative of the learned corpora or the seed material. This objective has been achieved, as judged by k-sample Anderson–Darling and Cramer tests. Music has been generated using the approach, and preliminary informal judgements place it roughly on a par with an example of composed music in a related form. Future work will aim to enhance the model such that it deserves to be fully evaluated in relation to expression, meaning and utility in real-time performance. © 2018, The Natural Computing Applications Forum.",Neural Computing and Applications,10.1007/s00521-018-3765-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055527560&doi=10.1007%2fs00521-018-3765-x&partnerID=40&md5=741bf4d8f430ada215c82c6a92762b02,2020,2021-07-20 15:48:50,2021-07-20 15:48:50
W2C4JY9K,journalArticle,2021,"Al-Helali, B.; Chen, Q.; Xue, B.; Zhang, M.",A new imputation method based on genetic programming and weighted KNN for symbolic regression with incomplete data,"Incompleteness is one of the problematic data quality challenges in real-world machine learning tasks. A large number of studies have been conducted for addressing this challenge. However, most of the existing studies focus on the classification task and only a limited number of studies for symbolic regression with missing values exist. In this work, a new imputation method for symbolic regression with incomplete data is proposed. The method aims to improve both the effectiveness and efficiency of imputing missing values for symbolic regression. This method is based on genetic programming (GP) and weighted K-nearest neighbors (KNN). It constructs GP-based models using other available features to predict the missing values of incomplete features. The instances used for constructing such models are selected using weighted KNN. The experimental results on real-world data sets show that the proposed method outperforms a number of state-of-the-art methods with respect to the imputation accuracy, the symbolic regression performance, and the imputation time. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.",Soft Computing,10.1007/s00500-021-05590-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100572237&doi=10.1007%2fs00500-021-05590-y&partnerID=40&md5=57e00c56867d68d96063393cdf9e1ed5,2021,2021-07-20 15:48:50,2021-07-20 15:48:50
Q2FRTUCJ,journalArticle,2013,"Schiller, M.R.G.",Granularity analysis for mathematical proofs,"Mathematical proofs generally allow for various levels of detail and conciseness, such that they can be adapted for a particular audience or purpose. Using automated reasoning approaches for teaching proof construction in mathematics presupposes that the step size of proofs in such a system is appropriate within the teaching context. This work proposes a framework that supports the granularity analysis of mathematical proofs, to be used in the automated assessment of students' proof attempts and for the presentation of hints and solutions at a suitable pace. Models for granularity are represented by classifiers, which can be generated by hand or inferred from a corpus of sample judgments via machine-learning techniques. This latter procedure is studied by modeling granularity judgments from four experts. The results provide support for the granularity of assertion-level proofs but also illustrate a degree of subjectivity in assessing step size. © 2013 Cognitive Science Society, Inc.",Topics in Cognitive Science,10.1111/tops.12012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876309154&doi=10.1111%2ftops.12012&partnerID=40&md5=9b9251ea80901fa86032c8453c76f71f,2013,2021-07-20 15:48:50,2021-07-20 15:48:50
6FRUK6MX,journalArticle,2019,"Nilashi, M.; Yadegaridehkordi, E.; Ibrahim, O.; Samad, S.; Ahani, A.; Sanzogni, L.",Analysis of Travellers’ Online Reviews in Social Networking Sites Using Fuzzy Logic Approach,"Social media and digital technology have had significant contributions and impacts on the hospitality and accommodation businesses. Online traveller reviews have been rich sources of information for the traveller’s decision-making process in social media websites. TripAdvisor, a popular travel review site and social media platform, is mainly developed as a free business consultation service to help the travellers to make right decisions in their trips. The aim of this research is to use the multi-criteria ratings provided by the travellers in social media networking sites for developing a new recommender system for hotel recommendations in e-tourism platforms. We extend the crisp-based multi-criteria algorithms to fuzzy-based multi-criteria algorithms for finding the similarities between the travellers based on their provided ratings. To develop the recommendation method, we use clustering and prediction machine learning techniques. We evaluate the recommendation system on TripAdvisor data. Our experiments confirm that the use of clustering and prediction machine learning with the aid of fuzzy-based recommendation algorithms can significantly improve the quality of recommendations in tourism domain. © 2019, Taiwan Fuzzy Systems Association.",International Journal of Fuzzy Systems,10.1007/s40815-019-00630-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068769615&doi=10.1007%2fs40815-019-00630-0&partnerID=40&md5=62412649e9ffa2b4f2168abd40a0cda3,2019,2021-07-20 15:48:50,2021-07-20 15:48:50
D2KJEDQ8,journalArticle,2021,"Yang, C.; Gu, W.; Ito, T.; Yang, X.",Machine learning-based consensus decision-making support for crowd-scale deliberation,"With the rapid development of Internet, the online discussion system or social democratic system has become an important and effective vehicle for group decision-making support since it can continue collecting the opinions from the public at anytime. To reach a consensus in crowd-scale deliberation, the existing online discussion systems require an experienced human facilitator to navigate and guild the discussion. When human facilitator performs the required facilitation there are several issues such as heavy burden on decision-making, the 24/7 online facilitation, bias on the social issues, etc. To address these issues it is necessary and inevitable to explore intelligent facilitation. For this purpose, we propose a novel machine learning-based method for smart facilitation, in particular the intelligent consensus decision-making support (CDMS) for crowd-scale deliberation. After presenting an overview of the crowd-scale deliberation and the COLLAGREE, the paper details the proposed approach, a machine learning-based framework for CDMS in crowd-scale deliberation. To validate the developed methods the offline evaluation experiments were conducted with the online discussion platform, COLLAGREE. The preliminary experimental results obtained from offline validation demonstrated the feasibility and usefulness of the developed machine learning-based methods for CDMS. © 2021, Crown.",Applied Intelligence,10.1007/s10489-020-02118-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098967297&doi=10.1007%2fs10489-020-02118-z&partnerID=40&md5=969390bf38dbfa861c74402d9fdea0aa,2021,2021-07-20 15:48:50,2021-07-20 15:48:50
T5PBB472,journalArticle,2021,"Karth, I.; Smith, A.M.",WaveFunctionCollapse: Content Generation via Constraint Solving and Machine Learning,"We describe WaveFunctionCollapse (WFC), a new family of algorithms for content generation. WFC was recently invented by independent game developer Maxim Gumin and has since been adopted and adapted by other game developers. Trends in academic research on content generation have only recently suggested the use of ideas from constraint solving and machine learning, so it is surprising to see these manifested in in-the-wild algorithms developed outside of an academic context. We illuminate the common components in this family of algorithms by way of a rational reconstruction. Through experiments with the reconstruction we probe the impact of design choices made in various adaptations of WFC (e.g. the role of backtracking, search heuristics, or pattern learning strategies). This work highlights a mode of incremental content generation that has been overlooked by past surveys of content generation methods. IEEE",IEEE Transactions on Games,10.1109/TG.2021.3076368,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105867414&doi=10.1109%2fTG.2021.3076368&partnerID=40&md5=988e4a426e435e6467ad9e095caf8a3f,2021,2021-07-20 15:48:50,2021-07-20 15:48:50
JJ6QJP4H,journalArticle,2020,"Katyara, S.; Staszewski, L.; Leonowicz, Z.",Signal Parameter Estimation and Classification Using Mixed Supervised and Unsupervised Machine Learning Approaches,"The increasing use of modern power electronics raises the issue of harmonics in power systems which ultimately deteriorate its optimal performance in terms of: increased power loss, breaker failure and mal-operation of equipment. It has been found that the most severe harmonics in the system are odd ones due to their unsymmetrical nature. This work presents the new framework for estimation and classification of harmonics using machine learning approaches. Initially, a shallow neural network and fuzzy logic systems are used to estimate the harmonics contents in the voltage and currents signals. Based on the sequence components and IHD level of source signals, the estimation of harmonic content is achieved. The obtained results are compared with the analytically computed data for validating the performance of designed networks. The results from neural and fuzzy systems are then used to train the explainable convolutional neural network (xCNN) for harmonics classification. The xCNN consists of pertained ALEXNET network which trains the standard binary support vector machine (SVM) for classification of harmonics. The dictionary-based approach is used to add the explanations to the SVM classifier output as a prototype. The performance of proposed framework is measured in-terms of accuracy and loss function and evaluated on the basis of its scalability and computability. The proposed approach is called a Human with Machine-In-Loop (HMIL). © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2991843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085651324&doi=10.1109%2fACCESS.2020.2991843&partnerID=40&md5=9b5ec42afe04e93692ca1d7023e593bf,2020,2021-07-20 15:48:50,2021-07-20 15:48:50
AUD25GFX,journalArticle,2017,"Sahoo, S.R.; Kumar, K.S.; Mahapatra, K.",A novel current controlled configurable RO PUF with improved security metrics,"Physical Unclonable Functions (PUFs) are promising hardware security primitives which produce unique signatures. Out of several delay based PUF circuits, Configurable Ring Oscillator (CRO) PUF has got higher uniqueness and it is resilient against modelling attacks. In this paper, we present a novel Current controlled CRO (C-CRO) PUF in which inverters of RO uses different logic styles: static CMOS and Feedthrough logic (FTL). Use of different logic styles facilitates improvement of security metrics of PUF. The analysis of security metrics of the proposed architecture is carried out in 90 nm CMOS technology shows, using FTL logic leads to better security metrics. Proposed C-CRO PUF is also both power and area efficient. Further, in order to measure the vulnerability of proposed PUF, machine learning attack is carried out and the result shows FTL RO based C-CRO PUF is highly resilient to machine learning attack because of its non-linearity property. © 2016 Elsevier B.V.","Integration, the VLSI Journal",10.1016/j.vlsi.2016.11.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008158336&doi=10.1016%2fj.vlsi.2016.11.005&partnerID=40&md5=b638e4de9cc60e74793698369e48448b,2017,2021-07-20 15:48:50,2021-07-20 15:48:50
RVGS5W42,journalArticle,2018,"Abdolrazzaghi, M.; Hashemy, S.; Abdolali, A.","Fast-forward solver for inhomogeneous media using machine learning methods: artificial neural network, support vector machine and fuzzy logic","Encountering with a nonlinear second-order differential equation including ϵr and μr spatial distributions, while computing the fields inside inhomogeneous media, persuaded us to find their known distributions that give exact solutions. Similarities between random distributions of electric properties and known functions lead us to estimate them using three mathematical tools of artificial neural networks (ANNs), support vector machines (SVMs) and Fuzzy Logic (FL). Assigning known functions after fitting with minimum error to arbitrary inputs using results of machine learning networks leads to achieve an approximate solution for the field inside materials considering boundary conditions. A comparative study between the methods according to the complexity of the structures as well as the accuracy and the calculation time for testing of unforeseen inputs, including classification, prediction and regression is presented. We examined the extracted pairs of ϵr and μr with ANN, SVM networks and FL and got satisfactory outputs with detailed results. The application of the presented method in zero reflection subjects is exemplified. © 2016, The Natural Computing Applications Forum.",Neural Computing and Applications,10.1007/s00521-016-2694-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995466550&doi=10.1007%2fs00521-016-2694-9&partnerID=40&md5=6ada5d9b33055614f29963c8e89df48e,2018,2021-07-20 15:48:50,2021-07-20 15:48:50
AVLQEA3Z,journalArticle,2020,"Hou, B.-J.; Zhou, Z.-H.",Learning with Interpretable Structure from Gated RNN,"The interpretability of deep learning models has raised extended attention these years. It will be beneficial if we can learn an interpretable structure from deep learning models. In this article, we focus on recurrent neural networks (RNNs), especially gated RNNs whose inner mechanism is still not clearly understood. We find that finite-state automaton (FSA) that processes sequential data have a more interpretable inner mechanism according to the definition of interpretability and can be learned from RNNs as the interpretable structure. We propose two methods to learn FSA from RNN based on two different clustering methods. With the learned FSA and via experiments on artificial and real data sets, we find that FSA is more trustable than the RNN from which it learned, which gives FSA a chance to substitute RNNs in applications involving humans' lives or dangerous facilities. Besides, we analyze how the number of gates affects the performance of RNN. Our result suggests that gate in RNN is important but the less the better, which could be a guidance to design other RNNs. Finally, we observe that the FSA learned from RNN gives semantic aggregated states, and its transition graph shows us a very interesting vision of how RNNs intrinsically handle text classification tasks. © 2012 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2020.2967051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079885297&doi=10.1109%2fTNNLS.2020.2967051&partnerID=40&md5=3c67ea24a7ac0eb540108ee7f5d380b4,2020,2021-07-20 15:48:50,2021-07-20 15:48:50
L434UWWX,journalArticle,2020,"Stevsic, S.; Christen, S.; Hilliges, O.",Learning to assemble: Estimating 6d poses for robotic object-object manipulation,"In this letter we propose a robotic vision task with the goal of enabling robots to execute complex assembly tasks in unstructured environments using a camera as the primary sensing device. We formulate the task as an instance of 6D pose estimation of template geometries, to which manipulation objects should be connected. In contrast to the standard 6D pose estimation task, this requires reasoning about local geometry that is surrounded by arbitrary context, such as a power outlet embedded into a wall. We propose a deep learning based approach to solve this task alongside a novel dataset that will enable future work in this direction and can serve as a benchmark. We experimentally show that state-of-The-Art 6D pose estimation methods alone are not sufficient to solve the task but that our training procedure significantly improves the performance of deep learning techniques in this context. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2020.2967325,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079128229&doi=10.1109%2fLRA.2020.2967325&partnerID=40&md5=6f3706c174e58931182e96bf34e59367,2020,2021-07-20 15:48:50,2021-07-20 15:48:50
TXRTAFYY,journalArticle,2020,"Le, D.V.-K.; Chen, Z.; Wong, Y.W.; Isa, D.",A complete online-SVM pipeline for case-based reasoning system: a study on pipe defect detection system,"Recent developments in case-based reasoning system (CBR) have led to an interest in favoring machine learning (ML) approaches as a replacement for traditional weighted distance methods. However, valuable information obtained through a training process was relinquished as transferring to other phases. This paper proposed a complete pipeline integration of CBR using kernel method designated with support vector machine (SVM) as the main engine. Since the system requires learning SVM model to be invoked in every phase, the online learning mechanism is nominated to effectively update the model when a new case adjoins. The proposed full SVM-CBR integration has been successfully built into a pipe defect detection. The achieved result indicates a substantial improvement by transferring learning information accurately. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-020-04985-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084590066&doi=10.1007%2fs00500-020-04985-7&partnerID=40&md5=b08e9b16d8c025deb517179f68c582ae,2020,2021-07-20 15:48:51,2021-07-20 15:48:51
WET9CYNX,journalArticle,2021,"Chan, A.; Ma, L.; Juefei-Xu, F.; Ong, Y.; Xie, X.; Xue, M.; Liu, Y.",Breaking Neural Reasoning Architectures With Metamorphic Relation-Based Adversarial Examples,"The ability to read, reason, and infer lies at the heart of neural reasoning architectures. After all, the ability to perform logical reasoning over language remains a coveted goal of Artificial Intelligence. To this end, models such as the Turing-complete differentiable neural computer (DNC) boast of real logical reasoning capabilities, along with the ability to reason beyond simple surface-level matching. In this brief, we propose the first probe into DNC's logical reasoning capabilities with a focus on text-based question answering (QA). More concretely, we propose a conceptually simple but effective adversarial attack based on metamorphic relations. Our proposed adversarial attack reduces DNCs' state-of-the-art accuracy from 100&#x0025; to 1.5&#x0025; in the worst case, exposing weaknesses and susceptibilities in modern neural reasoning architectures. We further empirically explore possibilities to defend against such attacks and demonstrate the utility of our adversarial framework as a simple scalable method to improve model adversarial robustness. IEEE",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2021.3072166,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104646646&doi=10.1109%2fTNNLS.2021.3072166&partnerID=40&md5=952b39e75bd116821832ef81f66fa36c,2021,2021-07-20 15:48:51,2021-07-20 15:48:51
EYV3YM7J,journalArticle,2020,"Finotti Amaral, R.P.; Menezes, I.F.M.; Ribeiro, M.V.",An extension of the type-1 and singleton fuzzy logic system trained by scaled conjugate gradient methods for multiclass classification problems,"This paper proposes an extension of the type-1 and singleton fuzzy logic system for dealing with multiclass classification problems. The proposed extension enables a fuzzy classifier to generate more than one output, thereby avoiding the use of binary decomposition strategies when multiclass classification problems are considered. Additionally, with the goal of improving classifier performance, the scaled conjugate gradient training method was applied, as well as its modified version using the differential operator R·. The effectiveness of the proposed extension was evaluated using data from the UCI Machine Learning Repository based on well-established classification metrics. The numerical results reveal a significant reduction in computational complexity when using the proposed extension compared to the traditional decomposition strategy, as well as improved convergence speed when using the scaled conjugate gradient training method. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.05.052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087275063&doi=10.1016%2fj.neucom.2020.05.052&partnerID=40&md5=a0330cf3f07a51d22ea56f195abfd81e,2020,2021-07-20 15:48:51,2021-07-20 15:48:51
6DLU66UC,journalArticle,2016,"Angelopoulos, N.; Abdallah, S.; Giamas, G.",Advances in integrative statistics for logic programming,"We present recent developments on the syntax of Real, a library for interfacing two Prolog systems to the statistical language R. We focus on the changes in Prolog syntax within SWI-Prolog that accommodate greater syntactic integration, enhanced user experience and improved features for web-services. We recount the full syntax and functionality of Real as well as presenting a full application and sister packages which include Prolog code interfacing a number of common and useful tasks that can be delegated to R. We argue that Real is a powerful extension to logic programming, providing access to a popular statistical system that has complementary strengths in areas such as machine learning, statistical inference and visualisation. Furthermore, Real has a central role to play in the uptake of semantic web, computational biology and bioinformatics as application areas for research in logic programming. © 2016 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2016.06.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978370502&doi=10.1016%2fj.ijar.2016.06.008&partnerID=40&md5=473df3256e11a532e60bd85392a35957,2016,2021-07-20 15:48:51,2021-07-20 15:48:51
GIWPH92Z,journalArticle,2018,"Muggleton, S.; Dai, W.-Z.; Sammut, C.; Tamaddoni-Nezhad, A.; Wen, J.; Zhou, Z.-H.",Meta-Interpretive Learning from noisy images,"Statistical machine learning is widely used in image classification. However, most techniques (1) require many images to achieve high accuracy and (2) do not provide support for reasoning below the level of classification, and so are unable to support secondary reasoning, such as the existence and position of light sources and other objects outside the image. This paper describes an Inductive Logic Programming approach called Logical Vision which overcomes some of these limitations. LV uses Meta-Interpretive Learning (MIL) combined with low-level extraction of high-contrast points sampled from the image to learn recursive logic programs describing the image. In published work LV was demonstrated capable of high-accuracy prediction of classes such as regular polygon from small numbers of images where Support Vector Machines and Convolutional Neural Networks gave near random predictions in some cases. LV has so far only been applied to noise-free, artificially generated images. This paper extends LV by (a) addressing classification noise using a new noise-telerant version of the MIL system Metagol, (b) addressing attribute noise using primitive-level statistical estimators to identify sub-objects in real images, (c) using a wider class of background models representing classical 2D shapes such as circles and ellipses, (d) providing richer learnable background knowledge in the form of a simple but generic recursive theory of light reflection. In our experiments we consider noisy images in both natural science settings and in a RoboCup competition setting. The natural science settings involve identification of the position of the light source in telescopic and microscopic images, while the RoboCup setting involves identification of the position of the ball. Our results indicate that with real images the new noise-robust version of LV using a single example (i.e. one-shot LV) converges to an accuracy at least comparable to a thirty-shot statistical machine learner on both prediction of hidden light sources in the scientific settings and in the RoboCup setting. Moreover, we demonstrate that a general background recursive theory of light can itself be invented using LV and used to identify ambiguities in the convexity/concavity of objects such as craters in the scientific setting and partial obscuration of the ball in the RoboCup setting. © 2018, The Author(s).",Machine Learning,10.1007/s10994-018-5710-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046536129&doi=10.1007%2fs10994-018-5710-8&partnerID=40&md5=82683f1f5e8c5a5af71cc261c550946e,2018,2021-07-20 15:48:51,2021-07-20 15:48:51
TY4MG3ST,journalArticle,2019,"Le Nguyen, T.; Gsponer, S.; Ilie, I.; O’Reilly, M.; Ifrim, G.",Interpretable time series classification using linear models and multi-resolution multi-domain symbolic representations,"The time series classification literature has expanded rapidly over the last decade, with many new classification approaches published each year. Prior research has mostly focused on improving the accuracy and efficiency of classifiers, with interpretability being somewhat neglected. This aspect of classifiers has become critical for many application domains and the introduction of the EU GDPR legislation in 2018 is likely to further emphasize the importance of interpretable learning algorithms. Currently, state-of-the-art classification accuracy is achieved with very complex models based on large ensembles (COTE) or deep neural networks (FCN). These approaches are not efficient with regard to either time or space, are difficult to interpret and cannot be applied to variable-length time series, requiring pre-processing of the original series to a set fixed-length. In this paper we propose new time series classification algorithms to address these gaps. Our approach is based on symbolic representations of time series, efficient sequence mining algorithms and linear classification models. Our linear models are as accurate as deep learning models but are more efficient regarding running time and memory, can work with variable-length time series and can be interpreted by highlighting the discriminative symbolic features on the original time series. We advance the state-of-the-art in time series classification by proposing new algorithms built using the following three key ideas: (1) Multiple resolutions of symbolic representations: we combine symbolic representations obtained using different parameters, rather than one fixed representation (e.g., multiple SAX representations); (2) Multiple domain representations: we combine symbolic representations in time (e.g., SAX) and frequency (e.g., SFA) domains, to be more robust across problem types; (3) Efficient navigation in a huge symbolic-words space: we extend a symbolic sequence classifier (SEQL) to work with multiple symbolic representations and use its greedy feature selection strategy to effectively filter the best features for each representation. We show that our multi-resolution multi-domain linear classifier (mtSS-SEQL+LR) achieves a similar accuracy to the state-of-the-art COTE ensemble, and to recent deep learning methods (FCN, ResNet), but uses a fraction of the time and memory required by either COTE or deep models. To further analyse the interpretability of our classifier, we present a case study on a human motion dataset collected by the authors. We discuss the accuracy, efficiency and interpretability of our proposed algorithms and release all the results, source code and data to encourage reproducibility. © 2019, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",Data Mining and Knowledge Discovery,10.1007/s10618-019-00633-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066127893&doi=10.1007%2fs10618-019-00633-3&partnerID=40&md5=8677e277d719a0e436360656378d7ff5,2019,2021-07-20 15:48:51,2021-07-20 15:48:51
LX6SZVZA,journalArticle,2016,"Fox, R.",Digital libraries: the systems analysis perspective machine erudition,"Purpose: The purpose of this paper is to explore the concept of machine learning. Current trends in the field are explored, along with the potential impact on information science. Machine learning is both an old and new field. It has been theoretically explored since the 1940s, but advances in technology, statistics and mathematics have recently created conditions, wherein it can be put into practice. Design/methodology/approach: This is a conceptual column exploring the notion of machine learning and the applications for information science. Findings: Some of the objections to machine intelligence are common philosophical problems dealing with the nature of thinking, self-awareness, understanding and other human traits that allow us to relate to people, develop intuitions and have situational awareness. Originality/value: While machine learning is being taken advantage of in the commercial sector, it has not been effectively exploited in the academic sphere. Libraries have traditionally focused on structured analysis and strictly controlled vocabularies to enable information discovery. Machine learning opens up possibilities for unstructured data to be analyzed intelligently. Over 80 per cent of regularly consumed information on the Internet is unstructured, so this field has huge implications for discovery from a library perspective. © 2016, © Emerald Group Publishing Limited.",Digital Library Perspectives,10.1108/DLP-02-2016-0006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015330497&doi=10.1108%2fDLP-02-2016-0006&partnerID=40&md5=947f221163cb297e11c77d430da8298a,2016,2021-07-20 15:48:51,2021-07-20 15:48:51
R25V6S6G,journalArticle,2021,"Wu, D.; Yin, R.; Miguel, J.S.",In-Stream Correlation-Based Division and Bit-Inserting Square Root in Stochastic Computing,"Stochastic Computing (SC) has shown great promise in achieving low hardware area and power consumption for neuromorphic architectures compared to traditional binary-encoded computation, due to its bit-serial data representation and extremely straightforward logic. With emerging deep learning models requiring more sophisticated nonlinear operations, we propose new designs for SC division and square root. Our designs are novel in their ability to leverage SC correlation via low-cost in-stream mechanisms that eliminate expensive bit stream regeneration. Our experiments show that, compared to state-of-the-art designs, our proposed division and square root units simultaneously achieve higher accuracy and consume less area. IEEE",IEEE Design and Test,10.1109/MDAT.2021.3050716,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099571354&doi=10.1109%2fMDAT.2021.3050716&partnerID=40&md5=94944b0fcad907d08a392d94199344e4,2021,2021-07-20 15:48:51,2021-07-20 15:48:51
V3J6IF6I,journalArticle,2018,"Troiano, L.; Villa, E.M.; Loia, V.",Replicating a Trading Strategy by Means of LSTM for Financial Industry Applications,"This paper investigates the possibility of learning a trading rule looking at the relationship between market indicators and decisions undertaken regarding entering or quitting a position. As means to achieve this objective, we employ a long short-term memory machine, due its capability to relate past and recent events. Our solution is a first step in the direction of building a model-free robot, based on deep learning, able to identify the logic that links the market mood given by technical indicators to the undertaken investment decisions. Although preliminary, experimental results show that the proposed solution is viable and promising. © 2005-2012 IEEE.",IEEE Transactions on Industrial Informatics,10.1109/TII.2018.2811377,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042883444&doi=10.1109%2fTII.2018.2811377&partnerID=40&md5=fb0f6a260e8cba87f6e016c58dd7b546,2018,2021-07-20 15:48:51,2021-07-20 15:48:51
SGY5XAFL,journalArticle,2020,"Xu, X.; Zhang, D.; Bai, Y.; Chang, L.; Li, J.",Evidence reasoning rule-based classifier with uncertainty quantification,"In Dempster–Shafer evidence theory (DST)-based classifier design, the newly proposed evidence reasoning (ER) rule can be used as a multi-attribute classifier to combine multiple pieces of evidence generated from quantitative samples or qualitative knowledge of many attributes. Different from the classical Dempster's combination (DC) rule and its improved forms, ER rule definitely distinguishes the reliability and importance weight of evidence. The former reflects the ability of a single attribute or the corresponding evidence to give correct classification results whereas the latter clarifies the relative importance of evidence when it is combined with other pieces of evidence. Here how to determine the reliability factor is a key problem because it is the connection between the preceding evidence acquisition and the following evidence combination with the importance weights. Therefore, the main aim of this paper is to present a universal method for obtaining the reliability factor by quantifying the uncertainties of samples and the generated evidence. Experiential results on five popular benchmark databases taken from University of California Irvine (UCI) machine learning database show the improved classifier can give higher classification accuracy than the original ER-based classifier without considering uncertainty quantification and other classical or mainstream classifiers. © 2019 Elsevier Inc.",Information Sciences,10.1016/j.ins.2019.12.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077034169&doi=10.1016%2fj.ins.2019.12.037&partnerID=40&md5=93d3ed5fc20af65b61ba0206908a4ce1,2020,2021-07-20 15:48:51,2021-07-20 15:48:51
9VTEJVYM,journalArticle,2019,"Recio-García, J.A.; Díaz-Agudo, B.; Kazemi, A.; Jorro, J.L.",A data-driven predictive system using Case-Based Reasoning for the configuration of device-assisted back pain therapy,"Lower back Pain (LBP) is pathological and occurs in about 80% of the population at least once in their life. Physiotherapists personalise manual treatments to heal or relieve pain according to the patient characteristics. The contribution of this methodological paper is the description and evaluation of the configuration software associated to a therapy machine that executes back segment mobilisations. The configuration software uses Case-Based Reasoning (CBR), a successful Machine Learning technique, based on mimicking the human decision making process by reusing previously applied configuration episodes on similar individuals. This paper demonstrates its feasibility and cost-effectiveness for the configuration of treatments as it reuses expert knowledge and maximises effectiveness by taking into account the patient’s personal medical record and similar patterns among different patients. Having a baseline of 31% success rate using a standard solution based on interpolation, the CBR engine can achieve, on average, up to 70% success rate when proposing a machine configuration to the physiotherapist. Regarding clinical results, we run a longitudinal observational study that achieves an average improvement of 31.63% using the pain Visual Analogue Scale (VAS), a 7% according to the Oswestry Disability Index (ODI), and 13% in the 36-Item Short Form Health Survey (SF-36). © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",Journal of Experimental and Theoretical Artificial Intelligence,10.1080/0952813X.2019.1704441,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077040345&doi=10.1080%2f0952813X.2019.1704441&partnerID=40&md5=b4d7b96d0eacba0fd36398202f5aa181,2019,2021-07-20 15:48:51,2021-07-20 15:48:51
LZLNB7QU,journalArticle,2021,"Amroune, M.",Machine Learning Techniques Applied to On-Line Voltage Stability Assessment: A Review,"Electric power systems have become larger, more complex and found to be operating close to their stability limits with small security margin. In such situation, fast and accurate assessment of voltage stability is necessary in order to prevent large-scale blackouts. Due to its ability to learn off-line and produce accurate results on-line, machine learning (ML) techniques i.e., artificial neural networks, decision trees, support vector machines, fuzzy logic and adaptive neuro-fuzzy inference system are widely applied for on-line voltage stability assessment. This paper focuses on providing a clear review of the latest ML techniques employed in on-line voltage stability assessment. For each technique, a brief description is first presented and then a detailed review of the finding published research papers discussed the application of this technique in on-line voltage stability assessment is presented. Based on the conducted review, some discussions and limitations of ML techniques are finally presented. © 2019, CIMNE, Barcelona, Spain.",Archives of Computational Methods in Engineering,10.1007/s11831-019-09368-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074324383&doi=10.1007%2fs11831-019-09368-2&partnerID=40&md5=604c14d36689a45b1e9112327fea40dd,2021,2021-07-20 15:48:51,2021-07-20 15:48:51
ZJCKGVJF,journalArticle,2013,"Ahmed, M.A.; Al-Jamimi, H.A.",Machine learning approaches for predicting software maintainability: A fuzzy-based transparent model,"Software quality is one of the most important factors for assessing the global competitive position of any software company. Thus, the quantification of the quality parameters and integrating them into the quality models is very essential. Many attempts have been made to precisely quantify the software quality parameters using various models such as Boehm's Model, McCall's Model and ISO/IEC 9126 Quality Model. A major challenge, although, is that effective quality models should consider two types of knowledge: imprecise linguistic knowledge from the experts and precise numerical knowledge from historical data.Incorporating the experts' knowledge poses a constraint on the quality model; the model has to be transparent.In this study, the authorspropose a process for developing fuzzy logic-based transparent quality prediction models. They applied the process to a case study where Mamdani fuzzy inference engine is used to predict software maintainability. Theycompared the Mamdani-based model with other machine learning approaches.The resultsshow that the Mamdani-based model is superior to all. © 2013 The Institution of Engineering and Technology.",IET Software,10.1049/iet-sen.2013.0046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883782952&doi=10.1049%2fiet-sen.2013.0046&partnerID=40&md5=a42f2272d907c4f08871e8578d2d8ce4,2013,2021-07-20 15:48:51,2021-07-20 15:48:51
BCNCJZJT,journalArticle,2021,"Alrahis, L.; Patnaik, S.; Knechtel, J.; Saleh, H.; Mohammad, B.; Al-Qutayri, M.; Sinanoglu, O.",UNSAIL: Thwarting Oracle-Less Machine Learning Attacks on Logic Locking,"Logic locking aims to protect the intellectual property (IP) of integrated circuit (IC) designs throughout the globalized supply chain. The SAIL attack, based on tailored machine learning (ML) models, circumvents combinational logic locking with high accuracy and is amongst the most potent attacks as it does not require a functional IC acting as an oracle. In this work, we propose UNSAIL, a logic locking technique that inserts key-gate structures with the specific aim to confuse ML models like those used in SAIL. More specifically, UNSAIL serves to prevent attacks seeking to resolve the structural transformations of synthesis-induced obfuscation, which is an essential step for logic locking. Our approach is generic; it can protect any local structure of key-gates against such ML-based attacks in an oracle-less setting. We develop a reference implementation for the SAIL attack and launch it on both traditionally locked and UNSAIL-locked designs. For SAIL, two ML models have been proposed (which we implement accordingly), namely a change-prediction model and a reconstruction model; the change-prediction model is used to determine which key-gate structures to restore using the reconstruction model. Our study on benchmarks ranging from the ISCAS-85 and ITC-99 suites to the OpenRISC Reference Platform System-on-Chip (ORPSoC) confirms that UNSAIL degrades the accuracy of the change-prediction model and the reconstruction model by an average of 20.13 and 17 percentage points (pp), respectively. When the aforementioned models are combined, which is the most powerful scenario for SAIL, UNSAIL reduces the attack accuracy of SAIL by an average of 11pp. We further demonstrate that UNSAIL thwarts other oracle-less attacks, i.e., SWEEP and the redundancy attack, indicating the generic nature and strength of our approach. Detailed layout-level evaluations illustrate that UNSAIL incurs minimal area and power overheads of 0.26% and 0.61%, respectively, on the million-gate ORPSoC design. © 2005-2012 IEEE.",IEEE Transactions on Information Forensics and Security,10.1109/TIFS.2021.3057576,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100850513&doi=10.1109%2fTIFS.2021.3057576&partnerID=40&md5=2ccebdf45dfc4e3901bfeef3c81e6e6d,2021,2021-07-20 15:48:51,2021-07-20 15:48:51
ZJJECNSU,journalArticle,2020,"Ortin, F.; Rodriguez-Prieto, O.; Pascual, N.; Garcia, M.",Heterogeneous tree structure classification to label Java programmers according to their expertise level,"Open-source code repositories are a valuable asset to creating different kinds of tools and services, utilizing machine learning and probabilistic reasoning. Syntactic models process Abstract Syntax Trees (AST) of source code to build systems capable of predicting different software properties. The main difficulty of building such models comes from the heterogeneous and compound structures of ASTs, and that traditional machine learning algorithms require instances to be represented as n-dimensional vectors rather than trees. In this article, we propose a new approach to classify ASTs using traditional supervised-learning algorithms, where a feature learning process selects the most representative syntax patterns for the child subtrees of different syntax constructs. Those syntax patterns are used to enrich the context information of each AST, allowing the classification of compound heterogeneous tree structures. The proposed approach is applied to the problem of labeling the expertise level of Java programmers. The system is able to label expert and novice programs with an average accuracy of 99.6%. Moreover, other code fragments such as types, fields, methods, statements and expressions could also be classified, with average accuracies of 99.5%, 91.4%, 95.2%, 88.3% and 78.1%, respectively. © 2019 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2019.12.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076669521&doi=10.1016%2fj.future.2019.12.016&partnerID=40&md5=adeb15642c41323a5e7a2f3515508f83,2020,2021-07-20 15:48:51,2021-07-20 15:48:51
RKWLKKSY,journalArticle,2020,"Al-Hmouz, R.; Pedrycz, W.; Balamash, A.; Morfeq, A.",Logic -oriented autoencoders and granular logic autoencoders: Developing interpretable data representation,"The plethora of ways of data representation and their applications to system modeling is inherently associated with dimensionality reduction. In a nutshell, the result of dimensionality reduction should support efficient ways of constructing ensuing models (classifiers, predictors) as well as an interpretation of the data themselves. Furthermore, there should be a suitable measure quantifying the quality of data positioned in the reduced space. We advocate that what makes the reduced data interpretable, goes hand in hand with revealing a logic fabric of the data, suppressing redundancy, and finally arriving at a logic description of data. The anticipation is that the reduced data can be described in the form of logic expressions formed over the original highly dimensional data. Evidently, having these above stated points in mind, the aim of this study is two-fold: (i) to develop a logic-oriented data representation, and (ii) to quantify the quality of results of dimensionality reduction by incorporating a facet of information granularity. In other words, we argue that the result of dimensionality reduction gives rise to information granules whose level of granularity associates with the quality of processing completed by the autoencoder. In light of the recent surge of architectures of deep learning, the study is focused on the construction and analysis of logic-oriented autoencoders. We propose a two-level architecture composed of the logic-oriented processing units (and processing carried out at the first layer of the autoencoder) followed by the or processing completed at the second layer. As data representation provided by the autoencoder is not ideal, we augment the original architecture by granular parameters which give rise to granular logic-oriented autoencoders. A suite of experiments is also reported. IEEE",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2020.3043659,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097924201&doi=10.1109%2fTFUZZ.2020.3043659&partnerID=40&md5=28d358ebb5c92eb30cfccf9bf4a9c2d4,2020,2021-07-20 15:48:51,2021-07-20 15:48:51
S8ZUEN7B,journalArticle,2019,"Nikolić, M.; Marinković, V.; Kovács, Z.; Janičić, P.",Portfolio theorem proving and prover runtime prediction for geometry,"In recent years, portfolio problem solving found many applications in automated reasoning, primarily in SAT solving and in automated and interactive theorem proving. Portfolio problem solving is an approach in which for an individual instance of a specific problem, one particular, hopefully most appropriate, solving technique is automatically selected among several available ones and used. The selection usually employs machine learning methods. To our knowledge, this approach has not been used in automated theorem proving in geometry so far and it poses a number of new challenges. In this paper we propose a set of features which characterize a specific geometric theorem, so that machine learning techniques can be used in geometry. Relying on these features and using different machine learning techniques, we constructed several portfolios for theorem proving in geometry and also runtime prediction models for provers involved. The evaluation was performed on two corpora of geometric theorems: one coming from geometric construction problems and one from a benchmark set of the GeoGebra tool. The obtained results show that machine learning techniques can be useful in automated theorem proving in geometry, while there is still room for further progress. © 2018, Springer Nature Switzerland AG.",Annals of Mathematics and Artificial Intelligence,10.1007/s10472-018-9598-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053454787&doi=10.1007%2fs10472-018-9598-6&partnerID=40&md5=600f6bfdee25cc9da0ab2760eb451f6a,2019,2021-07-20 15:48:52,2021-07-20 15:48:52
JDSMTE86,journalArticle,2021,"del Campo, M.; Carlson, A.; Manninger, S.",Towards Hallucinating Machines - Designing with Computational Vision,"There are particular similarities in how machines learn about the nature of their environment, and how humans learn to process visual stimuli. Machine Learning (ML), more specifically Deep Neural network algorithms rely on expansive image databases and various training methods (supervised, unsupervised) to “make sense” out of the content of an image. Take for example how students of architecture learn to differentiate various architectural styles. Whether this be to differentiate between Gothic, Baroque or Modern Architecture, students are exposed to hundreds, or even thousands of images of the respective styles, while being trained by faculty to be able to differentiate between those styles. A reversal of the process, striving to produce imagery, instead of reading it and understanding its content, allows machine vision techniques to be utilized as a design methodology that profoundly interrogates aspects of agency and authorship in the presence of Artificial Intelligence in architecture design. This notion forms part of a larger conversation on the nature of human ingenuity operating within a posthuman design ecology. The inherent ability of Neural Networks to process large databases opens up the opportunity to sift through the enormous repositories of imagery generated by the architecture discipline through the ages in order to find novel and bespoke solutions to architectural problems. This article strives to demystify the romantic idea of individual artistic design choices in architecture by providing a glimpse under the hood of the inner workings of Neural Network processes, and thus the extent of their ability to inform architectural design. The approach takes cues from the language and methods employed by experts in Deep Learning such as Hallucinations, Dreaming, Style Transfer and Vision. The presented approach is the base for an in-depth exploration of its meaning as a cultural technique within the discipline. Culture in the extent of this article pertains to ideas such as the differentiation between symbolic and material cultures, in which symbols are defined as the common denominator of a specific group of people.1 The understanding and exchange of symbolic values is inherently connected to language and code, which ultimately form the ingrained texture of any form of coded environment, including the coded structure of Neural Networks. A first proof of concept project was devised by the authors in the form of the Robot Garden. What makes the Robot Garden a distinctively novel project is the motion from a purely two dimensional approach to designing with the aid of Neural Networks, to the exploration of 2D to 3D Neural Style Transfer methods in the design process. © The Author(s) 2020.",International Journal of Architectural Computing,10.1177/1478077120963366,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092700279&doi=10.1177%2f1478077120963366&partnerID=40&md5=5b3420fee696c780da100ce380197ee5,2021,2021-07-20 15:48:52,2021-07-20 15:48:52
GW9R9ULR,journalArticle,2014,"Augello, A.; Gentile, M.; Pilato, G.; Vassallo, G.",A geometric algebra based distributional model to encode sentences semantics,"Word space models are used to encode the semantics of natural language elements by means of high dimensional vectors [23]. Latent Semantic Analysis (LSA) methodology [15] is well known and widely used for its generalization properties. Despite of its good performance in several applications, the model induced by LSA ignores dynamic changes in sentences meaning that depend on the order of the words, because it is based on a bag of words analysis. In this chapter we present a technique that exploits LSA-based semantic spaces and geometric algebra in order to obtain a sub-symbolic encoding of sentences taking into account the words sequence in the sentence. © 2014 Springer-Verlag Berlin Heidelberg.",Studies in Computational Intelligence,10.1007/978-3-642-40621-8_6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958552873&doi=10.1007%2f978-3-642-40621-8_6&partnerID=40&md5=7d9827fbd99a335fb2f3cc34a59a36e4,2014,2021-07-20 15:48:52,2021-07-20 15:48:52
MS5DZ5LX,journalArticle,2021,"Márquez-Grajales, A.; Acosta-Mesa, H.G.; Mezura-Montes, E.; Hernández-Jiménez, R.; Pérez-Castro, N.; Aguilar-Justo, A.E.; Salas-Martínez, F.",Classification of colposcopic images using a multi-breakpoints discretization approach on temporal patterns,"Cervical cancer represents the fourth cause of death in women worldwide. One of the efforts to decrease this mortality has focused on implementing automatic tools for supporting the experts in diagnosing this illness. In this work, eMODiTS was implemented to explore its performance in this particular domain. A comparison among the most used symbolic discretization (EP, SAX, αSAX, ESAX, ESAXKMeans, 1D-SAX, rSAX, and TD4C) methods and the well-known machine learning algorithms (kNN with k = 1,5,7, SVM, J48, and MLP) was done to analyze the eMODiTS performance. Results suggest that eMODiST outperforms most of the symbolic discretization methods concerning the statistical test results; only the results of the 1D-SAX algorithm do not present a significant statistical difference compared with eMODiTS. Although our proposal results are comparable with 1D-SAX, eMODiTS achieves these results with a more compressed data version. Concerning the comparison of machine learning-based methods, MLP outperforms eMODiTS. However, the classification performance and the ability to detect truly healthy and non-diseased cases are not sacrificed by eMODiTS. Moreover, the colposcopist validated the classification results confirming that the algorithm matched the real valorization. Finally, eMODiTS provides a visual tool for making decisions by experts about cervical cancer detection in the early stages. © 2021 Elsevier Ltd",Biomedical Signal Processing and Control,10.1016/j.bspc.2021.102918,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109459383&doi=10.1016%2fj.bspc.2021.102918&partnerID=40&md5=8cc966a34774156fe0152c7d6f6f75b7,2021,2021-07-20 15:48:52,2021-07-20 15:48:52
8DPUD7GQ,journalArticle,2015,"Mojtahedzadeh, R.; Bouguerra, A.; Schaffernicht, E.; Lilienthal, A.J.",Support relation analysis and decision making for safe robotic manipulation tasks,"In this article, we describe an approach to address the issue of automatically building and using high-level symbolic representations that capture physical interactions between objects in static configurations. Our work targets robotic manipulation systems where objects need to be safely removed from piles that come in random configurations. We assume that a 3D visual perception module exists so that objects in the piles can be completely or partially detected. Depending on the outcome of the perception, we divide the issue into two sub-issues: (1) all objects in the configuration are detected; (2) only a subset of objects are correctly detected. For the first case, we use notions from geometry and static equilibrium in classical mechanics to automatically analyze and extract act and support relations between pairs of objects. For the second case, we use machine learning techniques to estimate the probability of objects supporting each other. Having the support relations extracted, a decision making process is used to identify which object to remove from the configuration so that an expected minimum cost is optimized. The proposed methods have been extensively tested and validated on data sets generated in simulation and from real world configurations for the scenario of unloading goods from shipping containers. © 2014 Elsevier B.V.",Robotics and Autonomous Systems,10.1016/j.robot.2014.12.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920902075&doi=10.1016%2fj.robot.2014.12.014&partnerID=40&md5=8f4870c06d2900e468ba3cdd6fd8df37,2015,2021-07-20 15:48:52,2021-07-20 15:48:52
G7FART2Q,journalArticle,2018,"Raza, B.; Kumar, Y.J.; Malik, A.K.; Anjum, A.; Faheem, M.",Performance prediction and adaptation for database management system workload using Case-Based Reasoning approach,"Workload management in a Database Management System (DBMS) has become difficult and challenging because of workload complexity and heterogeneity. During and after execution of the workload, it is hard to control and handle the workload. Before executing the workload, predicting its performance can help us in workload management. By knowing the type of workload in advance, we can predict its performance in an adaptive way that will enable us to monitor and control the workload, which ultimately leads to performance tuning of the DBMS. This study proposes a predictive and adaptive framework named as the Autonomic Workload Performance Prediction (AWPP) framework. The proposed AWPP framework predicts and adapts the DBMS workload performance on the basis of information available in advance before executing the workload. The Case-Based Reasoning (CBR) approach is used to solve the workload management problem. The proposed CBR approach is compared with other machine learning techniques. To validate the AWPP framework, a number of benchmark workloads of the Decision Support System (DSS) and the Online Transaction Processing (OLTP) are executed on the MySQL DBMS. For preparation of training and testing data, we executed more than 1000 TPC-H and TPC-C like workloads on a standard data set. The results show that our proposed AWPP framework through CBR modeling performs better in predicting and adapting the DBMS workload. DBMSs algorithms can be optimized for this prediction and workload can be controlled and managed in a better way. In the end, the results are validated by performing post-hoc tests. © 2018 Elsevier Ltd",Information Systems,10.1016/j.is.2018.04.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046167873&doi=10.1016%2fj.is.2018.04.005&partnerID=40&md5=efa39c699de3925b6eda56e0cc65283a,2018,2021-07-20 15:48:52,2021-07-20 15:48:52
ZZD5CR9J,journalArticle,2016,"Gómez-Vallejo, H.J.; Uriel-Latorre, B.; Sande-Meijide, M.; Villamarín-Bello, B.; Pavón, R.; Fdez-Riverola, F.; Glez-Peña, D.",A case-based reasoning system for aiding detection and classification of nosocomial infections,"Nowadays, it is recognized worldwide that healthcare-associated infections are responsible for an increase in patient morbidity, mortality, and higher costs related to prolonged hospital stays. As electronic health data are increasingly available today, there is a unique opportunity to implement real-time decision support systems for automating the surveillance of healthcare-associated infections. As a consequence, different electronic surveillance systems have been implemented to date with varying degrees of success. However, there have been few instances in which clinical data and physician narratives with the potential to significantly improve electronic surveillance alternatives have been adopted. In this context, the present work introduces a case-based reasoning system for the automatic surveillance and diagnosis of healthcare-associated infections. The developed system makes use of different machine learning techniques in order to (i) automatically extract evidence from different types of data including clinical unstructured documents, (ii) incorporate static a priori knowledge handled by infection preventionists, and (iii) dynamically generate new knowledge as well as understandable explanations about the system's decisions. Results obtained from a real deployment in a public hospital belonging to the Spanish National Health System trained with 2569 samples belonging to 1800 patients during more than 10 consecutive months recognize the usefulness of the system. © 2016 Elsevier B.V. All rights reserved.",Decision Support Systems,10.1016/j.dss.2016.02.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959377705&doi=10.1016%2fj.dss.2016.02.005&partnerID=40&md5=c72750c96947d898d82c5029666e093c,2016,2021-07-20 15:48:52,2021-07-20 15:48:52
KF7JL3EM,journalArticle,2013,"Campos, J.; Lopez-Sanchez, M.; Salamó, M.; Avila, P.; Rodríguez-Aguilar, J.A.",Robust regulation adaptation in multi-agent systems,"Adaptive organisation-centred multi-agent systems can dynamically modify their organisational components to better accomplish their goals. Our research line proposes an abstract distributed architecture (2- LAMA) to endow an organisation with adaptation capabilities. This article focuses on regulation-adaptation based on a machine learning approach, in which adaptation is learned by applying a tailored case-based reasoning method. We evaluate the robustness of the system when it is populated by non compliant agents. The evaluation is performed in a peer-to-peer sharing network scenario. Results show that our proposal significantly improves system performance and can cope with regulation violators without incorporating any specific regulation-compliance enforcement mechanisms. © 2013 ACM.",ACM Transactions on Autonomous and Adaptive Systems,10.1145/2517328,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885593556&doi=10.1145%2f2517328&partnerID=40&md5=ff7606af03fee32dd5be00fae8476b92,2013,2021-07-20 15:48:52,2021-07-20 15:48:52
XULCVY56,journalArticle,2021,"Wu, D.; Li, J.; Yin, R.; Hsiao, H.; Kim, Y.; Miguel, J.S.",UGEMM: Unary Computing for GEMM Applications,"General matrix multiplication (GEMM) is pervasive in various domains, such as signal processing, computer vision, and machine learning. Conventional binary architectures for GEMM exhibit poor scalability in area and energy efficiency, due to the spatial nature of number representation and computing. On the contrary, unary computing processes data in temporal domain with extremely simple logic. However, to date, there rarely exist efficient architectures for unary GEMM. In this work, we first present uGEMM, a hardware-efficient unary GEMM architecture enabled by universally compatible arithmetic units, which simultaneously achieves input-insensitivity and high output accuracy. Next, we demonstrate that the proposed uGEMM can reliably early terminate the computation and offers dynamic energy-accuracy scaling for real-world applications via an accuracy-aware metric. Finally, to propel the future research for unary computing, we open source our unary computing simulator, UnarySim. © 1981-2012 IEEE.",IEEE Micro,10.1109/MM.2021.3065369,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102679697&doi=10.1109%2fMM.2021.3065369&partnerID=40&md5=7a19067874bd74e393be9042f3d2fd92,2021,2021-07-20 15:48:52,2021-07-20 15:48:52
6JPNBNK7,journalArticle,2021,"Pilarski, S.; Staniszewski, M.; Bryan, M.; Villeneuve, F.; Varró, D.",Predictions-on-chip: model-based training and automated deployment of machine learning models at runtime: For multi-disciplinary design and operation of gas turbines,"The design of gas turbines is a challenging area of cyber-physical systems where complex model-based simulations across multiple disciplines (e.g., performance, aerothermal) drive the design process. As a result, a continuously increasing amount of data is derived during system design. Finding new insights in such data by exploiting various machine learning (ML) techniques is a promising industrial trend since better predictions based on real data result in substantial product quality improvements and cost reduction. This paper presents a method that generates data from multi-paradigm simulation tools, develops and trains ML models for prediction, and deploys such prediction models into an active control system operating at runtime with limited computational power. We explore the replacement of existing traditional prediction modules with ML counterparts with different architectures. We validate the effectiveness of various ML models in the context of three (real) gas turbine bearings using over 150,000 data points for training, validation, and testing. We introduce code generation techniques for automated deployment of neural network models to industrial off-the-shelf programmable logic controllers. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.",Software and Systems Modeling,10.1007/s10270-020-00856-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102052095&doi=10.1007%2fs10270-020-00856-9&partnerID=40&md5=f5ff23dff0d8d64e28b9b65273add1b2,2021,2021-07-20 15:48:52,2021-07-20 15:48:52
N5YP5BGA,journalArticle,2016,"Banchi, L.; Pancotti, N.; Bose, S.",Quantum gate learning in qubit networks: Toffoli gate without time-dependent control,"We put forward a strategy to encode a quantum operation into the unmodulated dynamics of a quantum network without the need for external control pulses, measurements or active feedback. Our optimisation scheme, inspired by supervised machine learning, consists in engineering the pairwise couplings between the network qubits so that the target quantum operation is encoded in the natural reduced dynamics of a network section. The efficacy of the proposed scheme is demonstrated by the finding of uncontrolled four-qubit networks that implement either the Toffoli gate, the Fredkin gate or remote logic operations. The proposed Toffoli gate is stable against imperfections, has a high fidelity for fault-tolerant quantum computation and is fast, being based on the non-equilibrium dynamics. © The Author(s) 2016.",npj Quantum Information,10.1038/npjqi.2016.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999239422&doi=10.1038%2fnpjqi.2016.19&partnerID=40&md5=c96c20e97e80065beeb14f0783a9a756,2016,2021-07-20 15:48:52,2021-07-20 15:48:52
UUMM6HWA,journalArticle,2020,"Wei, X.; Du, J.; Xue, Z.; Liang, M.; Geng, Y.; Xu, X.; Lee, J.",A very deep two-stream network for crowd type recognition,"Crowd type identification is a crucial task in the emergency alert. In this paper, to solve accurate identification of crowd type, the crowd type description triad C-BMO < Behavior, Mood, Organized > and a novel crowd type recognition network (CTRN): very deep two-stream network architecture are proposed, respectively. The very deep two-stream network architecture is based on the static map and motion map in the video. To early warn the emergency, the reasoning rules of the emergency alert are proposed based on joining the crowd type and the crowd characteristics. To verify the proposed method, the crowd type dataset is collected, and we experiment with the proposed plan on the crowd type dataset. The experimental results demonstrate that the proposed model is competitive compared with the state-of-the-art techniques. © 2019",Neurocomputing,10.1016/j.neucom.2018.10.106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085867931&doi=10.1016%2fj.neucom.2018.10.106&partnerID=40&md5=8dbc5ed9f579a11128d4512cd211e922,2020,2021-07-20 15:48:52,2021-07-20 15:48:52
EM6JZ6VL,journalArticle,2020,"Shankar, G.S.; Manikandan, K.",Remote diagnosis of diabetics patient through speech engine and fuzzy based machine learning algorithm,"As recent development of technology, it enables patients to get treatment remotely from doctors through audio conversation. The fourth highest number of death every year is caused by diabetics. Almost 50% to 80% of patients can avoid diabetics if the cause is found at the early stage. In this paper, we propose a new methodology to detect Diabetes at an early stage and recommend few attributes in which the patient needs to be careful in order to avoid diabetics. The proposed methodology makes use of fuzzy logic and kNN classifier to find out the caution attributes and recommends them as soon as possible. The proposed algorithm detects the audio signals from patients or clinical labs to process the data. We implemented our proposed methodology on Pima Indian dataset and compared with existing algorithms and the result shows that our algorithm outperforms existing algorithms. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",International Journal of Speech Technology,10.1007/s10772-020-09742-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090216125&doi=10.1007%2fs10772-020-09742-7&partnerID=40&md5=4189fe3a8a2a30afcf3a44bf8c52d48f,2020,2021-07-20 15:48:52,2021-07-20 15:48:52
Y377YRJ7,journalArticle,2019,"Sakama, C.; Inoue, K.; Ribeiro, T.",Learning Inference Rules from Data,"This paper considers the possibility of designing AI that can learn logical or non-logical inference rules from data. We first provide an abstract framework for learning logics. In this framework, an agent A provides training examples that consist of formulas S and their logical consequences T. Then a machine M builds an axiomatic system that makes T a consequence of S. Alternatively, in the absence of an agent A, a machine M seeks an unknown logic underlying given data. We next consider the problem of learning logical inference rules by induction. Given a set S of propositional formulas and their logical consequences T, the goal is to find deductive inference rules that produce T from S. We show that an induction algorithm LF1T, which learns logic programs from interpretation transitions, successfully produces deductive inference rules from input data. Finally, we consider the problem of learning non-logical inference rules. We address three case studies for learning abductive inference, frame axioms, and conversational implicature. Each case study uses machine learning techniques together with metalogic programming. © 2019, Gesellschaft für Informatik e.V. and Springer-Verlag GmbH Germany, part of Springer Nature.",KI - Kunstliche Intelligenz,10.1007/s13218-019-00597-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085562807&doi=10.1007%2fs13218-019-00597-y&partnerID=40&md5=40e914f537e636b89b56a586b2aaf948,2019,2021-07-20 15:48:52,2021-07-20 15:48:52
9CU6G7II,journalArticle,2018,"Green, N.L.",Towards mining scientific discourse using argumentation schemes,"The dominant approach to argument mining has been to treat it as a machine learning problem based upon superficial text features, and to treat the relationships between arguments as either support or attack. However, accurately summarizing argumentation in scientific research articles requires a deeper understanding of the text and a richer model of relationships between arguments. First, this paper presents an argumentation scheme-based approach to mining a class of biomedical research articles. Argumentation schemes implemented as logic programs are formulated in terms of semantic predicates that could be obtained from a text by use of biomedical/biological natural language processing tools. The logic programs can be used to extract the underlying scheme name, premises, and implicit or explicit conclusion of an argument. Then this paper explores how arguments in a research article occur within a narrative of scientific discovery, how they are related to each other, and some implications. © 2018-IOS Press and the authors.",Argument and Computation,10.3233/AAC-180038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050853497&doi=10.3233%2fAAC-180038&partnerID=40&md5=d4c468c9dff9ce956cf3fe69aa0f609a,2018,2021-07-20 15:48:53,2021-07-20 15:48:53
WPJT4YPF,journalArticle,2018,"Jha, S.; Raman, V.; Sadigh, D.; Seshia, S.A.",Safe Autonomy Under Perception Uncertainty Using Chance-Constrained Temporal Logic,"Autonomous vehicles have found wide-ranging adoption in aerospace, terrestrial as well as marine use. These systems often operate in uncertain environments and in the presence of noisy sensors, and use machine learning and statistical sensor fusion algorithms to form an internal model of the world that is inherently probabilistic. Autonomous vehicles need to operate using this uncertain world-model, and hence, their correctness cannot be deterministically specified. Even once probabilistic correctness is specified, proving that an autonomous vehicle will operate correctly is a challenging problem. In this paper, we address these challenges by proposing a correct-by-synthesis approach to autonomous vehicle control. We propose a probabilistic extension of temporal logic, named Chance Constrained Temporal Logic (C2TL), that can be used to specify correctness requirements in presence of uncertainty. C2TL extends temporal logic by including chance constraints as predicates in the formula which allows modeling of perception uncertainty while retaining its ease of reasoning. We present a novel automated synthesis technique that compiles C2TL specification into mixed integer constraints, and uses second-order (quadratic) cone programming to synthesize optimal control of autonomous vehicles subject to the C2TL specification. We also present a risk distribution approach that enables synthesis of plans with lower cost without increasing the overall risk. We demonstrate the effectiveness of the proposed approach on a diverse set of illustrative examples. © 2017, Springer Science+Business Media Dordrecht.",Journal of Automated Reasoning,10.1007/s10817-017-9413-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019761417&doi=10.1007%2fs10817-017-9413-9&partnerID=40&md5=c4cfd2a3c8158dc2e11d6fa5dee0f2fc,2018,2021-07-20 15:48:53,2021-07-20 15:48:53
PEWIPSFT,journalArticle,2017,"Forbus, K.D.; Liang, C.; Rabkina, I.",Representation and Computation in Cognitive Models,"One of the central issues in cognitive science is the nature of human representations. We argue that symbolic representations are essential for capturing human cognitive capabilities. We start by examining some common misconceptions found in discussions of representations and models. Next we examine evidence that symbolic representations are essential for capturing human cognitive capabilities, drawing on the analogy literature. Then we examine fundamental limitations of feature vectors and other distributed representations that, despite their recent successes on various practical problems, suggest that they are insufficient to capture many aspects of human cognition. After that, we describe the implications for cognitive architecture of our view that analogy is central, and we speculate on roles for hybrid approaches. We close with an analogy that might help bridge the gap. Copyright © 2017 Cognitive Science Society, Inc.",Topics in Cognitive Science,10.1111/tops.12277,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021324802&doi=10.1111%2ftops.12277&partnerID=40&md5=c3e0b2d59cc2ae1e283bc2530ca7ad23,2017,2021-07-20 15:48:53,2021-07-20 15:48:53
N3GREVPQ,journalArticle,2017,"Sun, Z.; Zhao, Y.; Wei, Z.; Zhang, W.; Wang, J.",Scalable learning and inference in Markov logic networks,"Markov logic networks (MLNs) have emerged as a powerful representation that incorporates first-order logic and probabilistic graphical models. They have shown very good results in many problem domains. However, current implementations of MLNs do not scale well due to the large search space and the intractable clause groundings, which is preventing their widespread adoption. In this paper, we propose a general framework named Ground Network Sampling (GNS) for scaling up MLN learning and inference. GNS offers a new instantiation perspective by encoding ground substitutions as simple paths in the Herbrand universe, which uses the interactions existing among the objects to constrain the search space. To further make this search tractable for large scale problems, GNS integrates random walks and subgraph pattern mining, gradually building up a representative subset of simple paths. When inference is concerned, a template network is introduced to quickly locate promising paths that can ground given logical statements. The resulting sampled paths are then transformed into ground clauses, which can be used for clause creation and probabilistic inference. The experiments on several real-world datasets demonstrate that our approach offers better scalability while maintaining comparable or better predictive performance compared to state-of-the-art MLN techniques. © 2016 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2016.12.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006307364&doi=10.1016%2fj.ijar.2016.12.003&partnerID=40&md5=1a8931fc159a5d9ec559823ea9b9b210,2017,2021-07-20 15:48:53,2021-07-20 15:48:53
BEGC7W2Z,journalArticle,2020,"Dai, X.; Yin, H.; Jha, N.K.",Incremental Learning Using a Grow-and-Prune Paradigm with Efficient Neural Networks,"Deep neural networks (DNNs) have become a widely deployed model for numerous machine learning applications. However, their fixed architecture, substantial training cost, and significant model redundancy make it difficult to efficiently update them to accommodate previously unseen data. To solve these problems, we propose an incremental learning framework based on a grow-and-prune neural network synthesis paradigm. When new data arrive, the neural network first grows new connections based on the gradients to increase the network capacity to accommodate new data. Then, the framework iteratively prunes away connections based on the magnitude of weights to enhance network compactness, and hence recover efficiency. Finally, the model rests at a lightweight DNN that is both ready for inference and suitable for future grow-and-prune updates. The proposed framework improves accuracy, shrinks network size, and significantly reduces the additional training cost for incoming data compared to conventional approaches, such as training from scratch and network fine-tuning. For the LeNet-300-100 (LeNet-5) neural network architectures derived for the MNIST dataset, the framework reduces training cost by up to 64% (67%), 63% (63%), and 69% (73%) compared to training from scratch, network fine-tuning, and grow-and-prune from scratch, respectively. For the ResNet-18 architecture derived for the ImageNet dataset (DeepSpeech2 for the AN4 dataset), the corresponding training cost reductions against training from scratch, network fine-tunning, and grow-and-prune from scratch are 64% (67%), 60% (62%), and 72% (71%), respectively. Our derived models contain fewer network parameters but achieve higher accuracy relative to conventional baselines. IEEE",IEEE Transactions on Emerging Topics in Computing,10.1109/TETC.2020.3037052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096843639&doi=10.1109%2fTETC.2020.3037052&partnerID=40&md5=ac6b40dc81625787f6611422681a8dce,2020,2021-07-20 15:48:53,2021-07-20 15:48:53
UBT6J2HF,journalArticle,2020,"Qureshi, B.; Kamiran, F.; Karim, A.; Ruggieri, S.; Pedreschi, D.",Causal inference for social discrimination reasoning,"The discovery of discriminatory bias in human or automated decision making is a task of increasing importance and difficulty, exacerbated by the pervasive use of machine learning and data mining. Currently, discrimination discovery largely relies upon correlation analysis of decisions records, disregarding the impact of confounding biases. We present a method for causal discrimination discovery based on propensity score analysis, a statistical tool for filtering out the effect of confounding variables. We introduce causal measures of discrimination which quantify the effect of group membership on the decisions, and highlight causal discrimination/favoritism patterns by learning regression trees over the novel measures. We validate our approach on two real world datasets. Our proposed framework for causal discrimination has the potential to enhance the transparency of machine learning with tools for detecting discriminatory bias both in the training data and in the learning algorithms. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Intelligent Information Systems,10.1007/s10844-019-00580-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074944255&doi=10.1007%2fs10844-019-00580-x&partnerID=40&md5=c3756100b717538715ce4027486fa279,2020,2021-07-20 15:48:53,2021-07-20 15:48:53
UXZI5FA2,journalArticle,2014,"Vinogradov, D.V.",VKF-method of hypotheses generation,"We present an intelligent system for VKF-method based on a Markov chain approach to generation of hypotheses about causes of presence/absence of effect under study. The system uses coupling Markov chains that terminate with probability 1. Since each hypothesis is generated by an independent run of the Markov chain, the system makes the induction step in parallel by several threads. After that the abduction step refines the hypotheses by the CloseByOne operation with training examples (in several threads too). Then the system predicts presence/ absence of the effect by the analogical reasoning.We test the system on SPECT dataset from UCI machine learning repository. The accuracy is 85.56 percent (that exceeds 84.0 percent accuracy of the CLIP3 algorithm developed by the authors of the dataset). © Springer International Publishing Switzerland 2014.",Communications in Computer and Information Science,10.1007/978-3-319-12580-0_25,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916204573&doi=10.1007%2f978-3-319-12580-0_25&partnerID=40&md5=e78bf1929b3e7707d241834b27ae4752,2014,2021-07-20 15:48:53,2021-07-20 15:48:53
Q9QVE8BI,journalArticle,2018,"Xiao, Y.; Keung, J.; Bennin, K.E.; Mi, Q.",Machine translation-based bug localization technique for bridging lexical gap,"Context: The challenge of locating bugs in mostly large-scale software systems has led to the development of bug localization techniques. However, the lexical mismatch between bug reports and source codes degrades the performances of existing information retrieval or machine learning-based approaches. Objective: To bridge the lexical gap and improve the effectiveness of localizing buggy files by leveraging the extracted semantic information from bug reports and source code. Method: We present BugTranslator, a novel deep learning-based machine translation technique composed of an attention-based recurrent neural network (RNN) Encoder-Decoder with long short-term memory cells. One RNN encodes bug reports into several context vectors that are decoded by another RNN into code tokens of buggy files. The technique studies and adopts the relevance between the extracted semantic information from bug reports and source files. Results: The experimental results show that BugTranslator outperforms a current state-of-the-art word embedding technique on three open-source projects with higher MAP and MRR. The results show that BugTranslator can rank actual buggy files at the second or third places on average. Conclusion: BugTranslator distinguishes bug reports and source code into different symbolic classes and then extracts deep semantic similarity and relevance between bug reports and the corresponding buggy files to bridge the lexical gap at its source, thereby further improving the performance of bug localization. © 2018 Elsevier B.V.",Information and Software Technology,10.1016/j.infsof.2018.03.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043476087&doi=10.1016%2fj.infsof.2018.03.003&partnerID=40&md5=ce240b89938a1d0e637eee7a61a557d4,2018,2021-07-20 15:48:53,2021-07-20 15:48:53
IEMP8Y5M,journalArticle,2019,"Jha, S.; Tiwari, A.; Seshia, S.A.; Sahai, T.; Shankar, N.",TeLEx: learning signal temporal logic from positive examples using tightness metric,"We propose a novel passive learning approach, TeLex, to infer signal temporal logic (STL) formulas that characterize the behavior of a dynamical system using only observed signal traces of the system. First, we present a template-driven learning approach that requires two inputs: a set of observed traces and a template STL formula. The unknown parameters in the template can include time-bounds of the temporal operators, as well as the thresholds in the inequality predicates. TeLEx finds the value of the unknown parameters such that the synthesized STL property is satisfied by all the provided traces and it is tight. This requirement of tightness is essential to generating interesting properties when only positive examples are provided and there is no option to actively query the dynamical system to discover the boundaries of legal behavior. We propose a novel quantitative semantics for satisfaction of STL properties which enables TeLEx to learn tight STL properties without multidimensional optimization. The proposed new metric is also smooth. This is critical to enable the use of gradient-based numerical optimization engines and it produces a 30x to 100x speed-up with respect to the state-of-art gradient-free optimization. Second, we present a novel technique for automatically learning the structure of the STL formula by incrementally constructing more complex formula guided by the robustness metric of subformula. We demonstrate the effectiveness of the overall approach for learning STL formulas from only positive examples on a set of synthetic and real-world benchmarks. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Formal Methods in System Design,10.1007/s10703-019-00332-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060644152&doi=10.1007%2fs10703-019-00332-1&partnerID=40&md5=4d47f8218b4dce0861b339e48f335444,2019,2021-07-20 15:48:53,2021-07-20 15:48:53
GUE6J6SK,journalArticle,2018,"Alves, T.; Das, R.; Morris, T.",Embedding Encryption and Machine Learning Intrusion Prevention Systems on Programmable Logic Controllers,"During its nascent stages, programmable logic controllers (PLCs) were made robust to sustain tough industrial environments, but little care was taken to raise defenses against potential cyberthreats. The recent interconnectivity of legacy PLCs and supervisory control and data acquisition (SCADA) systems with corporate networks and the Internet has significantly increased the threats to critical infrastructure. To counter these threats, researchers have put their efforts in finding defense mechanisms that can protect the SCADA network and the PLCs. Encryption and intrusion prevention systems (IPSs) have been used by many organizations to protect data and the network against cyber-attacks. However, since PLC vendors do not make available information about their hardware or software, it becomes challenging for researchers to embed security mechanisms into their devices. This letter describes an alternative design using an open source PLC that was modified to encrypt all data it sends over the network, independently of the protocol used. Additionally, a machine learning-based IPS was added to the PLC network stack providing a secure mechanism against network flood attacks like denial of service (DoS). Experimental results indicated that the encryption layer and the IPS increased the security of the link between the PLC and the supervisory software, preventing interception, injection, and DoS attacks. © 2018 IEEE.",IEEE Embedded Systems Letters,10.1109/LES.2018.2823906,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045210099&doi=10.1109%2fLES.2018.2823906&partnerID=40&md5=915289e20540e0cc1b913c13f8942c7a,2018,2021-07-20 15:48:53,2021-07-20 15:48:53
NCK7SHXX,journalArticle,2019,"Pradhan, M.; Bhattacharya, B.B.; Chakrabarty, K.; Bhattacharya, B.B.",Predicting X-Sensitivity of Circuit-Inputs on Test-Coverage: A Machine-Learning Approach,"Digital circuits are often prone to suffer from uncertain timing, inadequate sensor feedback, limited controllability of past states or inability of initializing memory-banks, and erroneous behavior of analog-to-digital converters, which may produce an unknown (X) logic value at various circuit nodes. Additionally, many design bugs that are identified during the post-silicon validation phase manifest themselves as X-values. The presence of such X-sources on certain primary or secondary inputs of a logic circuit may cause loss of fault-coverage of a test set, which, in turn, may impact its reliability and robustness. In this paper, we provide a mechanism for predicting the sensitivity of X-sources in terms of loss of fault-coverage, on the basis of learning only a few structural features of the circuit that are easy to extract from the netlist. We show that the X-sources can be graded satisfactorily according to their sensitivity using support vector regression, thereby obviating the need for costly explicit simulation. Experimental results on several benchmark circuits demonstrate the efficacy, speed, and accuracy of prediction. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2878169,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055685053&doi=10.1109%2fTCAD.2018.2878169&partnerID=40&md5=86d843941b3067bc6d29581114f813ba,2019,2021-07-20 15:48:53,2021-07-20 15:48:53
8BD55I5J,journalArticle,2018,"Liang, J.; Liu, X.; Liao, K.",Soil Moisture Retrieval Using UWB Echoes via Fuzzy Logic and Machine Learning,"Soil moisture (SM) retrieval using wireless signals has become a research focus with the development of sensor devices in Internet of Things. Studies applying ground-penetrating radar have improved the accuracy of SM retrieval; however, the field-scaled data are hardly satisfactory mainly due to the frequency response of the antenna, and it is not cost-effective for farmers to monitor the soil conditions. In this paper, we compare two fuzzy logic systems (FLSs): 1) type-1 FLS and 2) adaptive network-based fuzzy inference system (ANFIS) to extract fuzzy parameters of soil. Moreover, two machine learning algorithms: 1) random forest (RF) and 2) artificial neural network with principal component analysis are applied in the SM classifications. Nine types of UWB soil echoes of different texture and volume water content (VWC) are collected and investigated using our approaches. Final analysis shows that ANFIS with RF provides the best VWC correct recognition rate compared to other algorithms. © 2014 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2017.2760338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031780207&doi=10.1109%2fJIOT.2017.2760338&partnerID=40&md5=f99362b30797e9d6c372b46376f0c44d,2018,2021-07-20 15:48:53,2021-07-20 15:48:53
R4VX93N5,journalArticle,2018,"Ren, P.; Chen, W.; Dai, H.; Zhang, H.",Distributed cooperative learning over networks via fuzzy logic systems: Performance analysis and comparison,"This paper studies a distributed machine learning problem by applying a distributed optimization algorithm over an undirected and connected communication network. Each node has its own fuzzy logic system (FLS) based machine whose weights are trained by the proposed FLS-based distributed cooperative learning (DCL) algorithm to reach the optimum of the global cost function. The training process utilizes the data that are distributed among different nodes and cannot be gathered at any node in the network. The main advantages of the FLS-based DCL algorithm are as follows: It has an exponential convergence; it requires a small amount of computation and communication at each iteration step; and the private and confidential information is protected without exchanging raw data between neighboring nodes. These advantages are verified by performing simulation experiments to compare the FLS-based DCL algorithm with the distributed average consensus based learning algorithm, the alternating direction method of multipliers based learning algorithm and the diffusion least-mean square algorithms. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2017.2762285,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032282537&doi=10.1109%2fTFUZZ.2017.2762285&partnerID=40&md5=d34f5c3d68ca0827446ef78dd3e41666,2018,2021-07-20 15:48:53,2021-07-20 15:48:53
FYPY67Y2,journalArticle,2015,"Wolf, B.J.; Slate, E.H.; Hill, E.G.",Ordinal Logic Regression: A classifier for discovering combinations of binary markers for ordinal outcomes,"In medicine, it is often useful to stratify patients according to disease risk, severity, or response to therapy. Since many diseases arise from complex gene-gene and gene-environment interactions, patient strata may be defined by combinations of genetic and environmental factors. Traditional statistical methods require specifying interactions a priori making it difficult to identify high order interactions. Alternatively, machine learning methods can model complex interactions, however these models are often difficult to interpret in a clinical setting. Logic regression (LR) enables modeling a binary outcome using logical combinations of binary predictors yielding easily interpretable models. However LR, as currently available, cannot model ordinal responses. This paper extends LR to model an ordinal response and the resulting method is called Ordinal Logic Regression (OLR). Several simulations comparing OLR and Classification and Regression Trees (CART) demonstrate that OLR is superior to CART for identifying variable interactions associated with an ordinal response. OLR is applied to data from a study to determine associations between genetic and health factors with severity of adult periodontitis. Ordinal Logic Regression is publicly available on CRAN in the OrdLogReg package, http://cran.r-project.org/. ©2014 Elsevier Ltd. All rights reserved.",Computational Statistics and Data Analysis,10.1016/j.csda.2014.08.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907891196&doi=10.1016%2fj.csda.2014.08.013&partnerID=40&md5=c1ded0f4dc25e25039cd4c5aefdcc854,2015,2021-07-20 15:48:53,2021-07-20 15:48:53
JEAI7TP6,journalArticle,2017,"Sadi, M.; Kannan, S.; Winemberg, L.; Tehranipoor, M.",SoC Speed Binning Using Machine Learning and On-Chip Slack Sensors,"Speed binning of system-on-chips (SoCs) using conventional Fmax test requires application of complex functional test patterns. Functional workload-based speed binning techniques incur high test-cost in terms of long test-time and complexity in functional test generation, and require high-end automatic test equipment. In this paper, we propose a novel speed binning flow that uses path timing slacks, extracted with robust digital embedded sensor IPs, of selected critical/near-critical paths. We apply machine learning techniques to model a predictor considering the extracted slacks and the Fmax values from a set of randomly tested die during wafer sort. The trained predictor is used to obtain the Fmax for the remaining chips. The proposed flow has been demonstrated in an SoC benchmark circuit at 28 nm technology. For sufficient number of training samples, Fmax is correctly predicted for 99% of the prediction samples. © 2017 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2016.2602806,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018791086&doi=10.1109%2fTCAD.2016.2602806&partnerID=40&md5=01cf42c0614a53730ee19bef4fe38603,2017,2021-07-20 15:48:53,2021-07-20 15:48:53
EX7QELKF,journalArticle,2017,"Bailis, P.","Research for practice: Cryptocurrencies, blockchains, and smart contracts; Hardware for deep learning","Arvind Narayanan and Andrew Miller, coauthors of the increasingly popular open access Bitcoin textbook, provide an overview of ongoing research in cryptocurrencies. They have selected three prominent areas of inquiry from this young field. Our selections of research papers within each area focus on relevance to practitioners and avoid such areas as scalability that are of interest primarily to cryptocurrency designers. Overall, the research not only exposes important limitations and pitfalls of the technology, but also suggests ways to overcome them. S. Meiklejohn, and colleagues believe that ananomity in cryptocurrencies is a matter of not just personal privacy, but also confidentiality for enterprises. Given advanced transaction graph analysis techniques, without precautions, the blockchain could easily reveal cash flow and other financial details. Sasson and colleagues feel that cryptocurrency ecosystem has been plagued by thefts and losses resulting from lost devices, corrupted hard drives, malware, and targeted intrusions. Eskandari, S., Barrera, D., Stobert, E., Clark, J believe that one of the hottest areas within cryptocurrencies, so-called smart contracts, are agreements between two or more parties that can be automatically enforced without the need for an intermediary. Song Han researches on deep neural networks (DNNs), where he says that deep learning algorithms, however, are both computationally and memory intensive, making them power-hungry to deploy on embedded systems. Running deeplearning algorithms in real time at subwatt power consumption would be ideal in embedded devices, but general-purpose hardware is not providing satisfying energy efficiency to deploy such a DNN. Deep-learning algorithms are memory intensive, and accessing memory consumes energy more than two orders of magnitude more than ALU (arithmetic logic unit) operations. Thus, it's critical to develop dataflow that can reduce memory reference.",Communications of the ACM,10.1145/3024928,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018707018&doi=10.1145%2f3024928&partnerID=40&md5=706a0b6d233fadeae872030ceba575e1,2017,2021-07-20 15:48:54,2021-07-20 15:48:54
8XYACC46,journalArticle,2020,"Notivol Calleja, L.D.; Spadaro, S.; Perelló, J.; Junyent, G.",Applying cognitive dynamic learning strategies for margins reduction in operational optical networks,"Today's optical transport networks are complex already and the support of the new arising services will further increase such complexity. Traditional deterministic network procedures will need to be revisited, especially their operations. Network Operators will require more dynamic approaches to get the best out of their infrastructure. In this context, cognition and machine learning techniques can provide innovative management solutions for traditional telecom operators. In this paper, we explore a dynamic cognitive approach to improve the adaption of Network Operators' operational processes to the new digital age. We propose a dynamic strategy considering the Case-Base Reasoning (CBR) technique for helping to reduce overall costs by optimizing operation margins. In this way, highly competitive exploitation methods to support new services can be deployed. The proposed dynamic algorithms can achieve higher transmitted power efficiency, up to 20% versus previously proposed static solutions, prolonging the transceivers' lifetime and thus addressing telco operator costs reduction. © 2020 Elsevier B.V.",Optical Switching and Networking,10.1016/j.osn.2020.100585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089242828&doi=10.1016%2fj.osn.2020.100585&partnerID=40&md5=886c5f4c85b1ad32fb6692a040498e26,2020,2021-07-20 15:48:54,2021-07-20 15:48:54
FEWP3V6C,journalArticle,2020,"Kumar, A.; Hosseinnia, A.; Gagarinova, A.; Phanse, S.; Kim, S.; Aly, K.A.; Zilles, S.; Babu, M.",A Gaussian process-based definition reveals new and bona fide genetic interactions compared to a multiplicative model in the Gram-negative Escherichia coli,"Motivation: A digenic genetic interaction (GI) is observed when mutations in two genes within the same organism yield a phenotype that is different from the expected, given each mutation's individual effects. While multiplicative scoring is widely applied to define GIs, revealing underlying gene functions, it remains unclear if it is the most suitable choice for scoring GIs in Escherichia coli. Here, we assess many different definitions, including the multiplicative model, for mapping functional links between genes and pathways in E.coli. Results: Using our published E.coli GI datasets, we show computationally that a machine learning Gaussian process (GP)-based definition better identifies functional associations among genes than a multiplicative model, which we have experimentally confirmed on a set of gene pairs. Overall, the GP definition improves the detection of GIs, biological reasoning of epistatic connectivity, as well as the quality of GI maps in E.coli, and, potentially, other microbes. © 2019 The Author(s) 2019. Published by Oxford University Press. All rights reserved. For permissions, please e-mail: journals.permissions@oup.com.",Bioinformatics,10.1093/bioinformatics/btz673,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079076984&doi=10.1093%2fbioinformatics%2fbtz673&partnerID=40&md5=ea0cdc4a6c9934cc24b44e5f3203372b,2020,2021-07-20 15:48:54,2021-07-20 15:48:54
XXGAWQ9I,journalArticle,2014,"Stocker, M.; Ronkko, M.; Kolehmainen, M.",Situational knowledge representation for traffic observed by a pavement vibration sensor network,"Information systems that build on sensor networks often process data produced by measuring physical properties. These data can serve in the acquisition of knowledge for real-world situations that are of interest to information services and, ultimately, to people. Such systems face a common challenge, namely the considerable gap between the data produced by measurement and the abstract terminology used to describe real-world situations. We present and discuss the architecture of a software system that utilizes sensor data, digital signal processing, machine learning, and knowledge representation and reasoning to acquire, represent, and infer knowledge about real-world situations observable by a sensor network. We demonstrate the application of the system to vehicle detection and classification by measurement of road pavement vibration. Thus, real-world situations involve vehicles and information for their type, speed, and driving direction. © 2014 IEEE.",IEEE Transactions on Intelligent Transportation Systems,10.1109/TITS.2013.2296697,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905902892&doi=10.1109%2fTITS.2013.2296697&partnerID=40&md5=eea2534712254d1eb46f955df8eed152,2014,2021-07-20 15:48:54,2021-07-20 15:48:54
N5RLUPDA,journalArticle,2020,"Yeganejou, M.; Dick, S.; Miller, J.",Interpretable Deep Convolutional Fuzzy Classifier,"While deep learning has proven to be a powerful new tool for modeling and predicting a wide variety of complex phenomena, those models remain incomprehensible black boxes. This is a critical impediment to the widespread deployment of deep learning technology, as decades of research have found that users simply will not trust (i.e., make decisions based on) a model whose solutions cannot be explained. Fuzzy systems, on the other hand, are by design much more easily understood. In this article, we propose to create more comprehensible deep networks by hybridizing them with fuzzy logic. Our proposed architecture first employs a convolutional neural network as an automated feature extractor and then performs a fuzzy clustering in the derived feature space. After hardening the clusters, we employ Rocchio's algorithm to classify the data points. Experiments on three datasets show that the automated feature extraction substantially improves the accuracy of the fuzzy classifier, and while the substitution of a fuzzy classifier slightly decreases the network's performance, we are able to introduce an effective interpretation mechanism. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2019.2946520,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087900786&doi=10.1109%2fTFUZZ.2019.2946520&partnerID=40&md5=9a0582ffbeac8d5e8a50ca29447b04f3,2020,2021-07-20 15:48:54,2021-07-20 15:48:54
3RZXW428,journalArticle,2020,"Messina, N.; Amato, G.; Carrara, F.; Falchi, F.; Gennaro, C.",Learning visual features for relational CBIR,"Recent works in deep-learning research highlighted remarkable relational reasoning capabilities of some carefully designed architectures. In this work, we employ a relationship-aware deep learning model to extract compact visual features used relational image descriptors. In particular, we are interested in relational content-based image retrieval (R-CBIR), a task consisting in finding images containing similar inter-object relationships. Inspired by the relation networks (RN) employed in relational visual question answering (R-VQA), we present novel architectures to explicitly capture relational information from images in the form of network activations that can be subsequently extracted and used as visual features. We describe a two-stage relation network module (2S-RN), trained on the R-VQA task, able to collect non-aggregated visual features. Then, we propose the aggregated visual features relation network (AVF-RN) module that is able to produce better relationship-aware features by learning the aggregation directly inside the network. We employ an R-CBIR ground-truth built by exploiting scene-graphs similarities available in the CLEVR dataset in order to rank images in a relational fashion. Experiments show that features extracted from our 2S-RN model provide an improved retrieval performance with respect to standard non-relational methods. Moreover, we demonstrate that the features extracted from the novel AVF-RN can further improve the performance measured on the R-CBIR task, reaching the state-of-the-art on the proposed dataset. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",International Journal of Multimedia Information Retrieval,10.1007/s13735-019-00178-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073946622&doi=10.1007%2fs13735-019-00178-7&partnerID=40&md5=9098579980a7c5dd4b6a1cdfadb64934,2020,2021-07-20 15:48:54,2021-07-20 15:48:54
ZMFJPX6L,journalArticle,2020,"Shi, K.; Lu, Y.; Chang, J.; Wei, Z.",PathPair2Vec: An AST path pair-based code representation method for defect prediction,"Software project defect prediction (SDP) can predict the bug probability of software by their features and allocate their testing efforts. The existing software defect prediction methods can be divided into two categories: methods based on traditional handcrafted features and methods based on automatically made abstract features, especially those made by deep learning. The current research indicates that deep learning-based automatic features can achieve better performance than handcrafted features. Code2vec (Alon et al. 2019) is one of the best source code representation models, which leverages deep learning to learn automatic representations from code. In this paper, inspired by code2vec, we propose a new AST path pair-based source code representation method (PathPair2Vec) and apply it to software project defect prediction. We first propose the concept of the short path to describe each terminal node and its control logic. Then, we design a new sequence encoding method to code the different parts of the terminal node and its control logic. Finally, by pairs of short paths, we describe the semantic information of code and fuse them by an attention mechanism. Experiments on the PROMISE dataset show that our method improves the F1 score by 17.88% over the state-of-the-art SDP method, and the AST path pair-based source code representation can better identify the defect features of the source code. © 2020 Elsevier Ltd",Journal of Computer Languages,10.1016/j.cola.2020.100979,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085928963&doi=10.1016%2fj.cola.2020.100979&partnerID=40&md5=2f38eee29116449336d8e934062d0794,2020,2021-07-20 15:48:54,2021-07-20 15:48:54
HHYKU45N,journalArticle,2021,"Sridharan, M.",Generalized Regression Neural Network Model Based Estimation of Global Solar Energy Using Meteorological Parameters,"The global solar irradiance data plays a vital role in evaluating the performance of all the solar energy conversion devices. In general there are two methods to predict the performance of such irradiance, namely physical models and the machine learning models. This paper presents a generalized regression neural network model (a machine learning technique) for estimating the global solar irradiance using seasonal and meteorological factors as input parameters. Results obtained from this proposed generalized regression neural network approach are compared with the results estimated by extensively used machine learning based methodologies such as fuzzy and artificial neural network models. Such a comparative results clearly indicate that prediction accuracy of proposed generalized regression neural network model is in good agreement with experimentally measured values. The mean percentage error for using GRNN, fuzzy logic and artificial neural network are 3.55%, 4.64%, and 5.49%. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.",Annals of Data Science,10.1007/s40745-020-00319-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098551941&doi=10.1007%2fs40745-020-00319-4&partnerID=40&md5=09b30bcbdedd97aa0f4ed8e0f772d7bc,2021,2021-07-20 15:48:54,2021-07-20 15:48:54
2CRTXYBF,journalArticle,2018,"Fan, F.; Cong, W.; Wang, G.",A new type of neurons for machine learning,"In machine learning, an artificial neural network is the mainstream approach. Such a network consists of many neurons. These neurons are of the same type characterized by the 2 features: (1) an inner product of an input vector and a matching weighting vector of trainable parameters and (2) a nonlinear excitation function. Here, we investigate the possibility of replacing the inner product with a quadratic function of the input vector, thereby upgrading the first-order neuron to the second-order neuron, empowering individual neurons and facilitating the optimization of neural networks. Also, numerical examples are provided to illustrate the feasibility and merits of the second-order neurons. Finally, further topics are discussed. Copyright © 2017 John Wiley & Sons, Ltd.",International Journal for Numerical Methods in Biomedical Engineering,10.1002/cnm.2920,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029414767&doi=10.1002%2fcnm.2920&partnerID=40&md5=d0efcb8973bf966f267bda8a86b87487,2018,2021-07-20 15:48:54,2021-07-20 15:48:54
PKF4A9PX,journalArticle,2021,"Jiang, H.; Shen, F.; Gao, F.; Han, W.","Learning efficient, explainable and discriminative representations for pulmonary nodules classification","Automatic pulmonary nodules classification is significant for early diagnosis of lung cancers. Recently, deep learning techniques have enabled remarkable progress in this field. However, these deep models are typically of high computational complexity and work in a black-box manner. To combat these challenges, in this work, we aim to build an efficient and (partially) explainable classification model. Specially, we use neural architecture search (NAS) to automatically search 3D network architectures with excellent accuracy/speed trade-off. Besides, we use the convolutional block attention module (CBAM) in the networks, which helps us understand the reasoning process. During training, we use A-Softmax loss to learn angularly discriminative representations. In the inference stage, we employ an ensemble of diverse neural networks to improve the prediction accuracy and robustness. We conduct extensive experiments on the LIDC-IDRI database. Compared with previous state-of-the-art, our model shows highly comparable performance by using less than 1/40 parameters. Besides, empirical study shows that the reasoning process of learned networks is in conformity with physicians’ diagnosis. Related code and results have been released at: https://github.com/fei-hdu/NAS-Lung. © 2021",Pattern Recognition,10.1016/j.patcog.2021.107825,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100008806&doi=10.1016%2fj.patcog.2021.107825&partnerID=40&md5=7f67a74080c536cb7b7ed98194f26571,2021,2021-07-20 15:48:54,2021-07-20 15:48:54
QKN9U9FK,journalArticle,2020,"Muthu, B.A.; Sivaparthipan, C.B.; Manogaran, G.; Sundarasekar, R.; Kadry, S.; Shanthini, A.; Dasel, A.",IOT based wearable sensor for diseases prediction and symptom analysis in healthcare sector,"Humans with good health condition is some more difficult in today’s life, because of changing food habit and environment. So we need awareness about the health condition to the survival. The health-support systems faces significant challenges like lack of adequate medical information, preventable errors, data threat, misdiagnosis, and delayed transmission. To overcome this problem, here we proposed wearable sensor which is connected to Internet of things (IoT) based big data i.e. data mining analysis in healthcare. Moreover, here we design Generalize approximate Reasoning base Intelligence Control (GARIC) with regression rules to gather the information about the patient from the IoT. Finally, Train the data to the Artificial intelligence (AI) with the use of deep learning mechanism Boltzmann belief network. Subsequently Regularization _ Genome wide association study (GWAS) is used to predict the diseases. Thus, if the people has affected by some diseases they will get warning by SMS, emails. Etc., after that they got some treatments and advisory from the doctors. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Peer-to-Peer Networking and Applications,10.1007/s12083-019-00823-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078718530&doi=10.1007%2fs12083-019-00823-2&partnerID=40&md5=587738051ded2cfbb188fef96e6c6a2d,2020,2021-07-20 15:48:54,2021-07-20 15:48:54
CBI5DETP,journalArticle,2018,"Vijayan, A.; Kiamehr, S.; Ebrahimi, M.; Chakrabarty, K.; Tahoori, M.B.",Online soft-error vulnerability estimation for memory arrays and logic cores,"Radiation-induced soft errors are a major reliability concern in circuits fabricated at advanced technology nodes. Online soft-error vulnerability estimation offers the flexibility of exploiting dynamic fault-tolerant mechanisms for cost-effective reliability enhancement. We propose a generic run-time method with low area and power overhead to predict the soft-error vulnerability of on-chip memory arrays as well as logic cores. The vulnerability prediction is based on signal probabilities (SPs) of a small set of flip-flops, chosen at design time, by studying the correlation between the soft-error vulnerability and the flip-flop SPs for representative workloads. We exploit machine learning to develop a predictive model that can be deployed in the system in software form. Simulation results on two processor designs show that the proposed technique can accurately estimate the soft-error vulnerability of on-chip logic core, such as sequential pipeline logic and functional units as well as memory arrays that constitute the instruction cache, the data cache, and the register file. © 2017 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2017.2706558,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040967773&doi=10.1109%2fTCAD.2017.2706558&partnerID=40&md5=076b546cc41f63cbd2324074730a6346,2018,2021-07-20 15:48:54,2021-07-20 15:48:54
BZ39Y225,journalArticle,2015,"Long, N.C.; Meesad, P.; Unger, H.",A highly accurate firefly based algorithm for heart disease prediction,"Abstract This paper proposes a heart disease diagnosis system using rough sets based attribute reduction and interval type-2 fuzzy logic system (IT2FLS). The integration between rough sets based attribute reduction and IT2FLS aims to handle with high-dimensional dataset challenge and uncertainties. IT2FLS utilizes a hybrid learning process comprising fuzzy c-mean clustering algorithm and parameters tuning by chaos firefly and genetic hybrid algorithms. This learning process is computationally expensive, especially when employed with high-dimensional dataset. The rough sets based attribute reduction using chaos firefly algorithm is investigated to find optimal reduction which therefore reduces computational burden and enhances performance of IT2FLS. Experiment results demonstrate a significant dominance of the proposed system compared to other machine learning methods namely Naive Bayers, support vector machines, and artificial neural network. The proposed model is thus useful as a decision support system for heart disease diagnosis. © 2015 Elsevier Ltd.",Expert Systems with Applications,10.1016/j.eswa.2015.06.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937611319&doi=10.1016%2fj.eswa.2015.06.024&partnerID=40&md5=476fd32efe34189505e32a3aa6203254,2015,2021-07-20 15:48:54,2021-07-20 15:48:54
4YLXMMZT,journalArticle,2018,"Zhang, J.; Li, Z.; Pu, Z.; Xu, C.",Comparing prediction performance for crash injury severity among various machine learning and statistical methods,"Crash injury severity prediction is a promising research target in traffic safety. Traditionally, various statistical methods were used for modeling crash injury severities. In recent years, machine learning-based methods are becoming popular due to their good predictive performance. However, the machine learning-based models are usually criticized as they perform like a black-box. In this paper, we aim at comparing the predictive performance, including prediction accuracy and estimation of variable importance, among various machine learning and statistical methods with distinct modeling logic for crash severity analysis. The crash severity, road geometry, and traffic flow data were collected at freeway diverge areas in Florida. We estimated two most commonly used statistical methods which were ordered probit (OP) model and multinomial logit model, and four popular machine learning methods, including K-Nearest Neighbor, Decision Tree, Random Forest (RF), and Support Vector Machine. The correct prediction rate for each crash severity level and the overall correct prediction rate were calculated. The results showed that the machine learning methods had higher predicting accuracy than the statistical methods, though they suffered from over-fitting issue. The RF method had the best prediction in overall and severe crashes while OP was the weakest one. We compared variable importance on crash severity via perturbation-based sensitivity analyses. The results showed that the inferences of variable importance from different methods were not always consistent and should be paid careful attention. © 2018 IEEE.",IEEE Access,10.1109/ACCESS.2018.2874979,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054608847&doi=10.1109%2fACCESS.2018.2874979&partnerID=40&md5=46f0e05273e75bb919fb5f7f6b03b884,2018,2021-07-20 15:48:54,2021-07-20 15:48:54
L4TLETZG,journalArticle,2011,"McSherry, D.",Conversational case-based reasoning in medical decision making,"Objectives: Balancing the trade-offs between solution quality, problem-solving efficiency, and transparency is an important challenge in medical applications of conversational case-based reasoning (CCBR). For example, test selection in CCBR is often based on strategies in which the absence of a specific hypothesis (e.g., diagnosis) to be confirmed makes it difficult to explain the relevance of test results that users are asked to provide. In this paper, we present an approach to CCBR in medical classification and diagnosis that aims to increase transparency while also providing high levels of accuracy and efficiency. Methods: We present an algorithm for CCBR called iNN(k) in which feature selection is driven by the goal of confirming a target class and informed by a measure of a feature's discriminating power in favor of the target class. As we demonstrate in a CCBR system called CBR-Confirm, this enables a CCBR system to explain the relevance of any question it asks the user. We evaluate the algorithm's accuracy and efficiency on a selection of datasets related to medicine and health care. Results: The performance of iNN(k) on a given dataset is shown to depend on the value of k and on whether local or global feature selection is used in the algorithm. The combination of these parameters for which iNN(k) is most effective in addressing the trade-off between accuracy and efficiency is identified for each of the selected datasets. For example, only 42% and 51% on average of features in a complete problem description were needed by iNN(k) to provide accuracy levels of 86.5% and 84.3% respectively on the lymphography and SPECT heart datasets from the UCI machine learning repository. Conclusion: Our results demonstrate the ability of iNN(k) to provide high levels of accuracy on most of the selected datasets, while often requiring the user to provide only a small subset of the features in a complete problem description, and enabling a CCBR system to explain the relevance of any question it asks the user. © 2011 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2011.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959694665&doi=10.1016%2fj.artmed.2011.04.007&partnerID=40&md5=5d6c05c8e2a076b13a789323fb2269b2,2011,2021-07-20 15:48:54,2021-07-20 15:48:54
6TA6UVA3,journalArticle,2018,"Jo, K.; Lee, M.; Sunwoo, M.",Track Fusion and Behavioral Reasoning for Moving Vehicles Based on Curvilinear Coordinates of Roadway Geometries,"This paper presents track fusion and behavioral reasoning for moving vehicles in close proximity based on the curvilinear coordinates of roadway geometries. The inferred track and behavior of other vehicles can be used to perform safe actions in intelligent vehicle applications and autonomous driving. Vehicle detections from multiple perception sensors are integrated using track-to-track (T2T) fusion based on a cross-covariance method, and this T2T fusion is performed with curvilinear coordinates created using prebuilt roadway geometry on a digital map. The coordinate conversion to curvilinear space has many benefits for behavioral reasoning and tracking, such as constraining problem spaces and dimensions. A machine learning classifier based on a support vector machine is then applied to deduce the behavior of nearby vehicles. The algorithms presented here for track fusion and behavioral reasoning based on curvilinear coordinates have been verified through experiments in various real traffic scenarios. © 2000-2011 IEEE.",IEEE Transactions on Intelligent Transportation Systems,10.1109/TITS.2017.2759904,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035149166&doi=10.1109%2fTITS.2017.2759904&partnerID=40&md5=b8f748ca61125f7d1f8ccfea7a8d38a3,2018,2021-07-20 15:48:55,2021-07-20 15:48:55
BNTKU5X8,journalArticle,2015,"Raad, E.; Evermann, J.",The role of analogy in ontology alignment: A study on LISA,"Ontologies are explicit specifications of concepts and their relationships. In the context of a semantic web of independently developed ontologies, overcoming interoperability and heterogeneity issues is of considerable importance. Many semantic web applications, such as matching of instances in social networks, reasoning over combined knowledge bases, and knowledge sharing among services, rely on ontology alignment. While existing research in this area has developed a wide range of different heuristics, in this paper we propose to look towards cognitive science, specifically analogical reasoning, to support ontology alignment. We investigate the question whether ontology alignment is rooted in the same cognitive process as analogical reasoning. We apply the LISA system, a cognitively-based model of human analogical reasoning, to ontology alignment and present a comprehensive experimental study to determine its performance on ontology alignment problems. © 2014 Published by Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2014.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920156549&doi=10.1016%2fj.cogsys.2014.09.001&partnerID=40&md5=2f3a518461f70b45ac4105e237bfd63e,2015,2021-07-20 15:48:55,2021-07-20 15:48:55
PTDNCH4X,journalArticle,2018,"Koitz-Hristov, R.; Wotawa, F.",Applying algorithm selection to abductive diagnostic reasoning,"The complexity of technical systems requires increasingly advanced fault diagnosis methods to ensure safety and reliability during operation. Particularly in domains where maintenance constitutes an extensive portion of the entire operation cost, efficient and effective failure identification holds the potential to provide large economic value. Abduction offers an intuitive concept for diagnostic reasoning relying on the notion of logical entailment. Nevertheless, abductive reasoning is an intractable problem and computing solutions for instances of reasonable size and complexity persists to pose a challenge. In this paper, we investigate algorithm selection as a mechanism to predict the “best” performing technique for a specific abduction scenario within the framework of model-based diagnosis. Based on a set of structural attributes extracted from the system models, our meta-approach trains a machine learning classifier that forecasts the most runtime efficient abduction technique given a new diagnosis problem. To assess the predictor’s selection capabilities and the suitability of the meta-approach in general, we conducted an empirical analysis featuring seven abductive reasoning approaches. The results obtained indicate that applying algorithm selection is competitive in comparison to always choosing a single abductive reasoning method. © 2018, The Author(s).",Applied Intelligence,10.1007/s10489-018-1171-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046762510&doi=10.1007%2fs10489-018-1171-9&partnerID=40&md5=30f2cfc1a7861723967d99782d514dd8,2018,2021-07-20 15:48:55,2021-07-20 15:48:55
CSWYYL8Q,journalArticle,2013,"Fong, S.; Mohammed, S.; Fiaidhi, J.; Kwoh, C.K.",Using causality modeling and fuzzy lattice reasoning algorithm for predicting blood glucose,"Blood glucose measurement is an important feedback in the course of diabetes treatment and prognosis. However, predicting the blood glucose level is not an easy task in the course of insulin treatment. There are many factors influencing the results (internal, environmental and behavioral factors). Previous attempts for predicting high levels of blood glucose utilize data related to insulin production, insulin action, or both by using time series forecasting and using of non-linear classification model. In this paper, we propose a more generic approach for predicting blood glucose levels using Fuzzy Lattice Reasoning (FLR). FLR allows us to deal with reasoning using specialist's knowledge acquisition and generation of rules base to increase the accuracy of predicting blood glucose level. In addition to the improved accuracy by FLR, the resultant rules contain some min-max ranges of variables making them flexible for diagnosis at the precise timing of the intervention and alarm. The new model is tested in comparison to other classical machine learning methods by using real-life diabetes dataset from AAAI Spring Symposium on Interpreting Clinical Data; superior accuracy is found and the efficacy of the model is verified through computer experiments. As far as we know, this is the pioneer work modeling temporal diabetes datasets into descriptive rules using FLR. © 2013 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2013.07.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881303036&doi=10.1016%2fj.eswa.2013.07.035&partnerID=40&md5=34502cb44f952e71256431f1b2a2f1a7,2013,2021-07-20 15:48:55,2021-07-20 15:48:55
9FM8EJ5G,journalArticle,2021,"Chi, C.; Jiang, J.R.",Logic Synthesis of Binarized Neural Networks for Efficient Circuit Implementation,"Neural networks (NNs) are key to deep learning systems. Their efficient hardware implementation is crucial to applications at the edge. Binarized NNs (BNNs), where the weights and output of a neuron are of binary values &#x007B;-1,+1&#x2775; (or encoded in &#x007B;0,1&#x2775;), have been proposed. As no multiplier required, binarized neural networks are particularly attractive and suitable for hardware realization. Most prior NN synthesis methods target on hardware architectures with neural processing elements (NPEs), where the weights of a neuron are loaded and the output of the neuron is computed. The load-and-compute method, though area efficient, requires expensive memory access, which deteriorates energy and performance efficiency. In this work we aim at synthesizing BNN layers into dedicated logic circuits. We formulate the corresponding model pruning problem and matrix covering problem to reduce the area and routing cost of BNNs. For model pruning, we propose and compare three strategies at the BNN training stage. For matrix covering, we propose a scalable logic-sharing algorithm. By combining these two methods, experimental results justify the effectiveness of the method in terms of area and net savings on FPGA implementation. Our method provides an alternative implementation of BNNs, and can be applied in combination with NPE-based implementation for area, speed, and power tradeoffs. IEEE",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2021.3078606,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105850641&doi=10.1109%2fTCAD.2021.3078606&partnerID=40&md5=173402d241fe42ce19b3dfa5e4579a4d,2021,2021-07-20 15:48:55,2021-07-20 15:48:55
L2MYRV8E,journalArticle,2020,"Ginés, J.; Rodríguez-Lera, F.J.; Martín, F.; Guerrero, Á.M.; Matellán, V.",Depicting probabilistic context awareness knowledge in deliberative architectures,"Facing long-term autonomy with a cognitive architecture raises several difficulties for processing symbolic and sub-symbolic information under different levels of uncertainty, and deals with complex decision-making scenarios. For reducing environment uncertainty and simplify the decision-making process, this paper establishes a method for translating robot knowledge to a conceptual graph to later extract probabilistic context information that allows to bound of the actions present at the deliberative layer. This research develops two ROS components, one for translating robot knowledge to the conceptual graphs and one for extracting context knowledge from this graph using Bayesian networks. We evaluate these components in a real-world scenario, performing a task where a robot notifies to a user a message of an event at home. Our results show an improvement in task completion when using our approach, decreasing the planning requests by 65% and doing the task in a third of the time. © 2020, Springer Nature B.V.",Natural Computing,10.1007/s11047-020-09798-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089073948&doi=10.1007%2fs11047-020-09798-z&partnerID=40&md5=74efbbae0fcf6fa128e7dab98ba110a5,2020,2021-07-20 15:48:55,2021-07-20 15:48:55
KIZSC2UC,journalArticle,2021,"Song, D.; Baek, A.M.; Kim, N.",Forecasting Stock Market Indices Using Padding-based Fourier Transform Denoising and Time Series Deep Learning Models,"Approaches for predicting financial markets, including conventional statistical methods and recent deep learning methods, have been investigated in many studies. However, financial time series data (e.g., daily stock market index) contain noises that prevent stable predictive model learning. Using these noised data in predictions results in performance deterioration and time lag. This study proposes padding-based Fourier transform denoising (P-FTD) that eliminates the noise waveform in the frequency domain of financial time series data and solves the problem of data divergence at both ends when restoring to the original time series. Experiments were conducted to predict the closing prices of S&#x0026;amp;P500, SSE, and KOSPI by applying data, from which noise was removed by P-FTD, to different deep learning models based on time series. Results show that the combination of the deep learning models and the proposed denoising technique not only outperforms the basic models in terms of predictive performance but also mitigates the time lag problem. CCBYNCND",IEEE Access,10.1109/ACCESS.2021.3086537,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107351093&doi=10.1109%2fACCESS.2021.3086537&partnerID=40&md5=3748d5aad824df3224192ac503e987a7,2021,2021-07-20 15:48:55,2021-07-20 15:48:55
WHI62TTX,journalArticle,2021,"Meng, Y.; Speier, W.F.; Ong, M.K.; Arnold, C.",Bidirectional Representation Learning from Transformers using Multimodal Electronic Health Record Data to Predict Depression,"Advancements in machine learning algorithms have had a beneficial impact on representation learning, classification, and prediction models built using electronic health record (EHR) data. Effort has been put both on increasing models overall performance as well as improving their interpretability, particularly regarding the decision-making process. In this study, we present a temporal deep learning model to perform bidirectional representation learning on EHR sequences with a transformer architecture to predict future diagnosis of depression. This model is able to aggregate five heterogenous and high-dimensional data sources from the EHR and process them in a temporal manner for chronic disease prediction at various prediction windows. We applied the current trend of pretraining and fine-tuning on EHR data to outperform the current state-of-the-art in chronic disease prediction, and to demonstrate the underlying relation between EHR codes in the sequence. The model generated the highest increases of precision-recall area under the curve (PRAUC) from 0.70 to 0.76 in depression prediction compared to the best baseline model. Furthermore, the self-attention weights in each sequence quantitatively demonstrated the inner relationship between various codes, which improved the model&#x0027;s interpretability. These results demonstrate the model&#x0027;s ability to utilize heterogeneous EHR data to predict depression while achieving high accuracy and interpretability, which may facilitate constructing clinical decision support systems in the future for chronic disease screening and early detection. IEEE",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2021.3063721,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102286368&doi=10.1109%2fJBHI.2021.3063721&partnerID=40&md5=e3a83a9f6a2a37b5ed8a30d0f95444b6,2021,2021-07-20 15:48:55,2021-07-20 15:48:55
98KSPQSV,journalArticle,2017,"Herrero-Reder, I.; Urdiales, C.; Peula, J.M.; Sandoval, F.",CBR based reactive behavior learning for the memory-prediction framework,"Some approaches to intelligence state that the brain works as a memory system which stores experiences to reflect the structure of the world in a hierarchical, organized way. Case Based Reasoning (CBR) is well suited to test this view. In this work we propose a CBR based learning methodology to build a set of nested behaviors in a bottom up architecture. To cope with complexity-related CBR scalability problems, we propose a new 2-stage retrieval process. We have tested our framework by training a set of cooperative/competitive reactive behaviors for Aibo robots in a RoboCup environment. © 2017 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2016.10.075,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012237178&doi=10.1016%2fj.neucom.2016.10.075&partnerID=40&md5=67a3ed6e76b0d789a5d288b9ff38a4c6,2017,2021-07-20 15:48:55,2021-07-20 15:48:55
GYPI2AN5,journalArticle,2016,"Sharma, S.; Lather, J.S.; Dave, M.",Semantic approach for Web service classification using machine learning and measures of semantic relatedness,"Web service classification, a task of assigning a category to the service from a predefined set, is a challenging task nowadays as manually organizing and searching services are simply not feasible, given the time constraints or the exponentially growing number of services. In this paper, a hybrid approach independent of service description models is suggested for automatic classification of Web services to improve classification accuracy. The proposed classification approach assists the repository administrator and the users during registration and service retrieval, respectively. It utilizes the semantic as well as syntactic information present within the service description by combining the techniques from machine learning, data mining, logical reasoning, statistical methods and measures of semantic relatedness. The proposed approach applies Omiotis measure of semantic relatedness to transform the service vectors into semantically enriched service vectors which are used by the classification algorithms. Supervised machine learning-based support vector machines and k-Nearest Neighbor classifiers are used to categorize service profiles under different categories. Empirical evaluation and comparison of the proposed approach implemented on OWL-X dataset is presented for enabling the discovery and reusability of the existing services. © 2015, Springer-Verlag London.",Service Oriented Computing and Applications,10.1007/s11761-015-0182-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941702786&doi=10.1007%2fs11761-015-0182-1&partnerID=40&md5=8043f3df62e8b7ef1d9e75189248ad49,2016,2021-07-20 15:48:55,2021-07-20 15:48:55
EBRNSDE8,journalArticle,2020,"Sengan, S.; Priya, V.; Syed Musthafa, A.; Ravi, L.; Palani, S.; Subramaniyaswamy, V.",A fuzzy based high-resolution multi-view deep CNN for breast cancer diagnosis through SVM classifier on visual analysis,"Breast cancer should be diagnosed as early as possible. A new approach of the diagnosis using deep learning for breast cancer and the particular process using segmentation strategies presented in this article. Medical imagery is an essential tool used for both diagnosis and treatment in many fields of medical applications. But, it takes specially trained medical specialists to read medical images and make diagnoses or treatment decisions. New practices of interpreting medical images are labour exhaustive, time-wasting, expensive, and prone to error. Using a computer-aided program which can render diagnosis and treatment decisions automatically would be more beneficial. A new computer-based detection method for the classification between compassionate and malignant mass tumours in mammography images of the breast proposed. (a) We planned to determine how to use the challenging definition, which produces severe examples that boost the segmentation of mammograms. (b) Employing well designing multi-instance learning through deep learning, we validated employing inadequately labelled data of breast cancer diagnosis using a mammogram. (c) The study is going through the Deep Lung method incorporating deep multi-dimensional automated identification and classification of the lung nodule. (d) By combining a probabilistic graphic model in deep learning, it authorizes how weakly labelled data can be used to improve the existing breast cancer identification method. This automated system involves manually defining the Region Of Interest (ROI), with the region and threshold values based on the next region. The High-Resolution Multi-View Deep Convolutional Neural Network (HRMP-DCNN) mainly developed for the extraction of function. The findings collected through the subsequent in available public databases like mammography screening information database and DDSM Curated Breast Imaging Subset. Ultimately, we'll show the VGG that's thousands of times quicker, and it is more reliable than earlier programmed anatomy segmentation. © 2020 - IOS Press and the authors. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-189174,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097623030&doi=10.3233%2fJIFS-189174&partnerID=40&md5=d15360ab40fecc0e38922c772a27245e,2020,2021-07-20 15:48:55,2021-07-20 15:48:55
6E2QYTTA,journalArticle,2018,"Morais, R.M.; Pedro, J.",Machine learning models for estimating quality of transmission in DWDM networks,"It is estimated that 5G and the Internet of Things (IoT) will impact traffic, both in volume and dynamicity, at unprecedented rates. Thus, to cost-efficiently accommodate these challenging requirements, optical networks must become more responsive to changes impacting the traffic and network state as well as operate more closely to optimality. In this context, knowledge-defined networking (KDN) promises to play a paramount role in improving network flexibility and automation. KDN is a solution that introduces reasoning processes and machine learning techniques into the control plane of the network, enabling it to operate autonomously and faster. One of the key aspects in this environment is the accurate validation of lightpaths. Accurate lightpath validation demands running computationally intensive performance models, which can be time-consuming and impact time-critical applications (e.g., optical channel restoration). This work evaluates the effectiveness of various machine learning models when used to predict the quality of transmission (QoT) of an unestablished lightpath, speeding up the process of lightpath provisioning. Three network scenarios to efficiently generate the knowledge database used to train the models are proposed as well as an overview of the mostused machine learning models. The considered models are: K-nearest neighbors, logistic regression, support vector machines, and artificial neural networks. Results showthat, in general, allmachine learningmodels are able to correctly predict the QoTofmore than 90% of the lightpaths. However, the artificial neural networks (ANN) model is the model presenting better generalization, being able to correctly predict the QoT of almost 99.9% of the lightpaths. Moreover, ANN is able to estimate the residual margin of a lightpath with an average error of only 0.4 dB. © 2009-2012 OSA.",Journal of Optical Communications and Networking,10.1364/JOCN.10.000D84,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055978609&doi=10.1364%2fJOCN.10.000D84&partnerID=40&md5=1cd618aa2bbc63837762cc5daf4e9507,2018,2021-07-20 15:48:55,2021-07-20 15:48:55
WK6M489N,journalArticle,2020,"Alqaralleh, B.A.Y.; Mohanty, S.N.; Gupta, D.; Khanna, A.; Shankar, K.; Vaiyapuri, T.",Reliable Multi-Object Tracking Model Using Deep Learning and Energy Efficient Wireless Multimedia Sensor Networks,"Presently, sensor-cloud based environment becomes highly beneficial due to its applicability in several domains. Wireless multimedia sensor network (WMSN) is one among them, which involves a set of multimedia sensors to collect data about the deployed region. Compared to traditional object tracking models, animal tracking in WMSN is a tedious process owing to the harsh, dynamic, and energy limited sensors. This article introduces a new Reliable Multi-Object Tracking Model using Deep Learning (DL) and Energy Efficient WMSN. Initially, the fuzzy logic technique is employed to determine the cluster heads (CHs) to attain energy efficiency. Next, in the second stage, a novel tracking algorithm by the use of Recurrent Neural Network (RNN) with a tumbling effect called RNN-T is developed. The proposed RNN-T model gets executed by every sensor node and the CHs execute the tracking algorithm to track the animals. Finally, the tracking results are transmitted to the cloud server for investigation purposes. In order to assess the performance of the presented model, an extensive experimental analysis is carried out by the use of a real-time wildlife video. The obtained results ensured that the RNN-T model has achieved better performance over the compared methods in different aspects. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3039695,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096824471&doi=10.1109%2fACCESS.2020.3039695&partnerID=40&md5=d44dda269c3c3c96fa2e021e52976f42,2020,2021-07-20 15:48:55,2021-07-20 15:48:55
5CKBVK32,journalArticle,2021,"Weber, R.O.; Duarte, K.B.",Data-driven artificial intelligence to automate researcher assessment,"This article describes how to utilize data-driven artificial intelligence (AI) to automate researcher assessment using data from profiling systems. We consider that a researcher assessment is done for a purpose and not divorced from a specific target placement. We formulate researcher assessment as a binary classification task, that is, a candidate researcher is classified as either fit or unfit for a given placement. For classifying researchers, we adopt case-based reasoning, a transparent artificial intelligence methodology that implements analogical reasoning, allows adaptation, machine learning, and explainability. This work addresses a human limitation through AI. Given a small number of candidates for a job or award and a clear job description, even if capable of selecting the best fit candidate, human decisions may be neither transparent nor reproducible. The approach in this article describes how to use AI methods to, from a job description, select the best fit candidate while considering career trajectories, providing explanations, and being reproducible. We describe the implementation of the methodology for a hypothetical placement in a real research institute from real but anonymized curriculum vitae from the Brazilian Lattes Database. We describe an experiment demonstrating that the purpose-oriented approach is more accurate than purpose-independent classifiers. The proposed methodology meets various principles from the Leiden Manifesto. © 2021, Akadémiai Kiadó, Budapest, Hungary.",Scientometrics,10.1007/s11192-020-03859-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100578379&doi=10.1007%2fs11192-020-03859-x&partnerID=40&md5=b23bbbf7bfd924759dbc394595214165,2021,2021-07-20 15:48:55,2021-07-20 15:48:55
LF34PFXG,journalArticle,2021,"Maldonado, S.; López, J.; Vairetti, C.",Time-weighted Fuzzy Support Vector Machines for classification in changing environments,"The predictive performance of classification methods relies heavily on the nature of the environment, as in the joint distribution of inputs and outputs may evolve over time. This issue is known as dataset shift. Given that most statistical and machine learning techniques assume that the training sample is drawn from the same distribution as the test data used for evaluation, an appreciable amount of researchers and practitioners tend to ignore this issue at the model construction stage. In this paper, we propose a novel Fuzzy Support Vector Machine strategy, in which the traditional hinge loss function is redefined to account for dataset shift. Additionally, we propose a general version of this loss function applying aggregation operators in order to improve performance by dealing with dataset shift via fuzzy logic. Originally developed as linear approaches, our proposals are extended to kernel-based classification for non-linear machine learning. Our methods are able to perform best compared to traditional classifiers in terms of out-of-time prediction using simulated and real-world dataset for credit scoring, confirming the theoretical virtues of our approach. © 2021 Elsevier Inc.",Information Sciences,10.1016/j.ins.2021.01.070,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100733859&doi=10.1016%2fj.ins.2021.01.070&partnerID=40&md5=7a3f3f3f7f119b0e944d7d5934354b4e,2021,2021-07-20 15:48:55,2021-07-20 15:48:55
5LUKX4N3,journalArticle,2019,"Efendioglu, M.; Sen, A.; Koroglu, Y.",Bug prediction of systemC models using machine learning,"In system-on-chip design, resources for verification is limited by time-to-market and cost. In order to allocate verification resources effectively, managers need to rely on their experience backed by design related metrics. However, often there are also other aspects of development process, such as bug history and developer information that can improve the effectiveness of verification. Software bug prediction is a machine learning (ML)-based technique which predicts whether a given software module is bug-prone by using product and process metrics of the module. Therefore, it can help direct verification effort, reduce costs, and improve the quality of software. Although there is a plethora of work in software bug prediction, no such work exists for SystemC. We propose an ML-based software bug prediction solution for verification of SystemC models used in virtual prototypes that takes into account system level design metrics and demonstrate its effectiveness on several open source system level designs. We find that 96% of modules could be correctly predicted as buggy or clean. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2878193,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055701490&doi=10.1109%2fTCAD.2018.2878193&partnerID=40&md5=84407bd1c6c33ef7b57a01092a79b75d,2019,2021-07-20 15:48:56,2021-07-20 15:48:56
48NS46BH,journalArticle,2017,"Zhang, H.; Li, F.; Wang, J.; Wang, Z.; Shi, L.; Zhao, J.; Sanín, C.; Szczerbicki, E.",Adding Intelligence to Cars Using the Neural Knowledge DNA,"In this paper, we propose a Neural Knowledge DNA (NK-DNA)-based framework that is capable of learning from the car’s daily operations and reusing such learned knowledge in future tasks. The NK-DNA is a novel knowledge representation and reasoning approach designed to support discovering, storing, reusing, improving, and sharing knowledge among machines and computing devices. We examine our framework for drivers’ classification based on their driving behaviors. The experimental data are collected via smartphone sensors. The initial results are presented, and the direction for our future research is defined. © 2017 Taylor & Francis.",Cybernetics and Systems,10.1080/01969722.2016.1276780,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014598248&doi=10.1080%2f01969722.2016.1276780&partnerID=40&md5=de8bd5b5cb966971956f1cb7405b85fe,2017,2021-07-20 15:48:56,2021-07-20 15:48:56
W4HQGRLB,journalArticle,2017,"Anandakumar, H.; Umamaheswari, K.",Supervised machine learning techniques in cognitive radio networks during cooperative spectrum handovers,"Cognitive communication model perform the investigation and surveillance of spectrum in cognitive radio networks abetment in advertent primary users (PUs) and in turn help in allocation of transmission space for secondary users (SUs). In effective performance of regulation of wireless channel handover strategy in cognitive computing systems, new computing models are desired in operating set of tasks to process business model, and interact naturally with humans or machine rather being programmed. Cognitive wireless network are trained via artificial intelligence (AI) and machine learning (ML) algorithms for dynamic processing of spectrum handovers. They assist human experts in making enhanced decisions by penetrating into the complexity of the handovers. This paper focuses on learning and reasoning features of cognitive radio (CR) by analyzing primary user (PU) and secondary user (SU) data communication using home location register (HLR) and visitor location register (VLR) database respectively. The SpecPSO is proposed for optimizing handovers using supervised machine learning technique for performing dynamic handover by adapting to the environment and make smart decisions compared to the traditional cooperative spectrum sensing (CSS) techniques. © 2017, Springer Science+Business Media New York.",Cluster Computing,10.1007/s10586-017-0798-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014145391&doi=10.1007%2fs10586-017-0798-3&partnerID=40&md5=5108e850d4c4129fdbc811df46d01ad6,2017,2021-07-20 15:48:56,2021-07-20 15:48:56
F8KBNCC4,journalArticle,2016,"Jin, S.; Ye, F.; Zhang, Z.; Chakrabarty, K.; Gu, X.",Efficient Board-Level Functional Fault Diagnosis with Missing Syndromes,"Functional fault diagnosis is widely used in board manufacturing to ensure product quality and improve product yield. Advanced machine-learning techniques have recently been advocated for reasoning-based diagnosis; these techniques are based on the historical record of successfully repaired boards. However, traditional diagnosis systems fail to provide appropriate repair suggestions when the diagnostic logs are fragmented and some error outcomes, or syndromes, are not available during diagnosis. We describe the design of a diagnosis system that can handle missing syndromes and can be applied to four widely used machine-learning techniques. Several imputation methods are discussed and compared in terms of their effectiveness for addressing missing syndromes. Moreover, a syndrome-selection technique based on the minimum-redundancy-maximum-relevance criteria is also incorporated to further improve the efficiency of the proposed methods. Two large-scale synthetic data sets generated from the log information of complex industrial boards in volume production are used to validate the proposed diagnosis system in terms of diagnosis accuracy and training time. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2015.2481859,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971232343&doi=10.1109%2fTCAD.2015.2481859&partnerID=40&md5=109573b259cec5999a477264c3c0f337,2016,2021-07-20 15:48:56,2021-07-20 15:48:56
9TWTSM5T,journalArticle,2021,"Manne, S.; Lydia, E.L.; Pustokhina, I.V.; Pustokhin, D.A.; Parvathy, V.S.; Shankar, K.",An intelligent energy management and traffic predictive model for autonomous vehicle systems,"In recent times, the utilization of autonomous vehicles (AVs) has been significantly increased over the globe. It is because of the tremendous rise in familiarity and the usage of artificial intelligence approaches in distinct application areas. Though AVs offer several benefits like congestion control, accident prevention, and so on, energy management and traffic flow prediction (TFP) remain a challenging issue. This paper concentrates on the design of intelligent energy management and TFP (IEMTFP) technique for AVs using multi-objective reinforced whale optimization algorithm (RWOA) and deep learning (DL). The proposed model involves an energy management module using fuzzy logic system to reach the specified engine torque with respect to different measures. For optimal tuning of the variables involved in the fuzzy logic membership functions (MFs), RWOA is employed to further reduce the energy utilization. Besides, the proposed model uses a DL-based bidirectional long short-term memory (Bi-LSTM) technique to perform TFP. For validating the efficacy of the IEMTFP technique, an extensive experimental validation is carried out. The resultant values ensured the goodness of the IEMTFP model in terms of energy management and TFP. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.",Soft Computing,10.1007/s00500-021-05614-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100351887&doi=10.1007%2fs00500-021-05614-7&partnerID=40&md5=0a2b6eb77ba7bf9bb9df10bb55dba02c,2021,2021-07-20 15:48:56,2021-07-20 15:48:56
XNQ43FUM,journalArticle,2020,"Pena-Aguirre, J.C.; Barranco-Gutierrez, A.-I.; Padilla-Medina, J.A.; Espinosa-Calderon, A.; Perez-Pinal, F.J.",Fuzzy Logic Power Management Strategy for a Residential DC-Microgrid,"Power management strategies (PMS) are applied to keep a balance, between different energy sources (i.e. solar, wind, geothermal, hydro), storage units (i.e. fuel cell, batteries, fly wheel) and loads. Up to date, there has been reported several advance techniques to solve this task, for instance: Fuzzy Logic, Deep Learning, Droop Control, Bayesian Networks, among others. Nevertheless, some of those PMS are over simplified and others are too complex to be programmed in devices with limited resources. To solve these issues, this paper proposes a PMS based on Fuzzy Logic, which keeps a balance between those two goals. Characteristics of the proposed PMS are a small number of rules; fulfillment of the demanded power at every time; reducing use of the storage unit; and keeping a balance between the different sources, storage unit and loads. The proposed PMS is numerically evaluated by using SIMULINK-MATLAB®, in a 10kW residential DC Microgrid (MG), and validated by using a Hardware in the Loop platform (NI myRio-1900 and Typhoon HIL402). A comparison with three popular advance techniques demonstrates the feasibility of the proposed PMS. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3004611,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087823802&doi=10.1109%2fACCESS.2020.3004611&partnerID=40&md5=d8265b74a70f299051d5b6e43926aa28,2020,2021-07-20 15:48:56,2021-07-20 15:48:56
D4FAGPT6,journalArticle,2020,"Mekami, H.; Bounoua, A.; Benabderrahmane, S.",Leveraging deep learning with symbolic sequences for robust head poses estimation,"Head pose estimation is a challenging topic in computer vision with a large area of applications. There are a lot of methods which have been presented in the literature to undertake pose estimation so far. Even though the efficiency of these methods is acceptable, the sensitivity to external conditions is still being a big challenge. In this paper, we come up with a new model to overcome the problem of head poses estimation. First, the face images are converted into one-dimensional vectors as a time series using the Peano–Hilbert space-filling curve. Then, we convert these numerical series into symbolic sequences with adequate dimensionality reduction approaches. These sequences are then used as input of an encode–decoder neural network to learn and generate labels of the faces orientations. We have evaluated our model on several databases, and the experimental results have shown that the proposed method is very competitive compared to other well-known approaches. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Pattern Analysis and Applications,10.1007/s10044-019-00857-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074829684&doi=10.1007%2fs10044-019-00857-5&partnerID=40&md5=d1e7e1870932424671367dee461c0a8e,2020,2021-07-20 15:48:56,2021-07-20 15:48:56
8QQX7JWJ,journalArticle,2020,"Sioutis, M.",Just-In-Time Constraint-Based Inference for Qualitative Spatial and Temporal Reasoning,"We discuss a research roadmap for going beyond the state of the art in qualitative spatial and temporal reasoning (QSTR). Simply put, QSTR is a major field of study in Artificial Intelligence that abstracts from numerical quantities of space and time by using qualitative descriptions instead (e.g., precedes, contains, is left of); thus, it provides a concise framework that allows for rather inexpensive reasoning about entities located in space or time. Applications of QSTR can be found in a plethora of areas and domains such as smart environments, intelligent vehicles, and unmanned aircraft systems. Our discussion involves researching novel local consistencies in the aforementioned discipline, defining dynamic algorithms pertaining to these consistencies that can allow for efficient reasoning over changing spatio-temporal information, and leveraging the structures of the locally consistent related problems with regard to novel decomposability and theoretical tractability properties. Ultimately, we argue for pushing the envelope in QSTR via defining tools for tackling dynamic variants of the fundamental reasoning problems in this discipline, i.e., problems stated in terms of changing input data. Indeed, time is a continuous flow and spatial objects can change (e.g., in shape, size, or structure) as time passes; therefore, it is pertinent to be able to efficiently reason about dynamic spatio-temporal data. Finally, these tools are to be integrated into the larger context of highly active areas such as neuro-symbolic learning and reasoning, planning, data mining, and robotic applications. Our final goal is to inspire further discussion in the community about constraint-based QSTR in general, and the possible lines of future research that we outline here in particular. © 2020, Gesellschaft für Informatik e.V. and Springer-Verlag GmbH Germany, part of Springer Nature.",KI - Kunstliche Intelligenz,10.1007/s13218-020-00652-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088024634&doi=10.1007%2fs13218-020-00652-z&partnerID=40&md5=831573d40bfbc6ffc8bdee655d477f45,2020,2021-07-20 15:48:56,2021-07-20 15:48:56
BE3NJ42N,journalArticle,2021,"Messina, N.; Amato, G.; Carrara, F.; Gennaro, C.; Falchi, F.",Solving the same-different task with convolutional neural networks,"Deep learning demonstrated major abilities in solving many kinds of different real-world problems in computer vision literature. However, they are still strained by simple reasoning tasks that humans consider easy to solve. In this work, we probe current state-of-the-art convolutional neural networks on a difficult set of tasks known as the same-different problems. All the problems require the same prerequisite to be solved correctly: understanding if two random shapes inside the same image are the same or not. With the experiments carried out in this work, we demonstrate that residual connections, and more generally the skip connections, seem to have only a marginal impact on the learning of the proposed problems. In particular, we experiment with DenseNets, and we examine the contribution of residual and recurrent connections in already tested architectures, ResNet-18, and CorNet-S respectively. Our experiments show that older feed-forward networks, AlexNet and VGG, are almost unable to learn the proposed problems, except in some specific scenarios. We show that recently introduced architectures can converge even in the cases where the important parts of their architecture are removed. We finally carry out some zero-shot generalization tests, and we discover that in these scenarios residual and recurrent connections can have a stronger impact on the overall test accuracy. On four difficult problems from the SVRT dataset, we can reach state-of-the-art results with respect to the previous approaches, obtaining super-human performances on three of the four problems. © 2021 Elsevier B.V.",Pattern Recognition Letters,10.1016/j.patrec.2020.12.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100220392&doi=10.1016%2fj.patrec.2020.12.019&partnerID=40&md5=eeb6d64dc0e72dfbbd9355b509b14c6d,2021,2021-07-20 15:48:56,2021-07-20 15:48:56
SCVW4W8F,journalArticle,2019,"Fazeli, N.; Oller, M.; Wu, J.; Wu, Z.; Tenenbaum, J.B.; Rodriguez, A.","See, feel, act: Hierarchical learning for complex manipulation skills with multisensory fusion","Humans are able to seamlessly integrate tactile and visual stimuli with their intuitions to explore and execute complex manipulation skills. They not only see but also feel their actions. Most current robotic learning methodologies exploit recent progress in computer vision and deep learning to acquire data-hungry pixel-to-action policies. These methodologies do not exploit intuitive latent structure in physics or tactile signatures. Tactile reasoning is omnipresent in the animal kingdom, yet it is underdeveloped in robotic manipulation. Tactile stimuli are only acquired through invasive interaction, and interpretation of the data stream together with visual stimuli is challenging. Here, we propose a methodology to emulate hierarchical reasoning and multisensory fusion in a robot that learns to play Jenga, a complex game that requires physical interaction to be played effectively. The game mechanics were formulated as a generative process using a temporal hierarchical Bayesian model, with representations for both behavioral archetypes and noisy block states. This model captured descriptive latent structures, and the robot learned probabilistic models of these relationships in force and visual domains through a short exploration phase. Once learned, the robot used this representation to infer block behavior patterns and states as it played the game. Using its inferred beliefs, the robot adjusted its behavior with respect to both its current actions and its game strategy, similar to the way humans play the game. We evaluated the performance of the approach against three standard baselines and show its fidelity on a real-world implementation of the game. © 2019 The Authors.",Science Robotics,10.1126/SCIROBOTICS.AAV3123,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074307371&doi=10.1126%2fSCIROBOTICS.AAV3123&partnerID=40&md5=7f57d96c137b9d5a6e937c76e6a36eef,2019,2021-07-20 15:48:56,2021-07-20 15:48:56
56HPK69P,journalArticle,2021,"Zhai, M.; Xiang, X.",Geometry understanding from autonomous driving scenarios based on feature refinement,"Nowadays, many deep learning applications benefit from multi-task learning with several related objectives. In autonomous driving scenarios, being able to infer motion and spatial information accurately is essential for scene understanding. In this paper, we propose a unified framework for unsupervised joint learning of optical flow, depth and camera pose. Specifically, we use a feature refinement module to adaptively discriminate and recalibrate feature, which can integrate local features with their global dependencies to capture rich contextual relationships. Given a monocular video, our network firstly calculates rigid optical flow by estimating depth and camera pose. Then, we design an auxiliary flow network for inferring non-rigid flow field. In addition, a forward–backward consistency check is adopted for occlusion reasoning. Extensive analyses on KITTI dataset are conducted to verify the effectiveness of our proposed approach. The experimental results show that our proposed network can produce sharper, clearer and detailed depth and flow maps. In addition, our network achieves potential performance compared to the recent state-of-the-art approaches. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-020-05192-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088384588&doi=10.1007%2fs00521-020-05192-z&partnerID=40&md5=f38b2949d08d6553653b7d076ccc5019,2021,2021-07-20 15:48:56,2021-07-20 15:48:56
C95BG4IA,journalArticle,2019,"Zhu, W.; Wu, H.; Deng, M.",LTL model checking based on binary classification of machine learning,"Linear Temporal Logic (LTL) Model Checking (MC) has been applied to many fields. However, the state explosion problem and the exponentially computational complexity restrict the further applications of LTL model checking. A lot of approaches have been presented to address these problems. And they work well. However, the essential issue has not been resolved due to the limitation of inherent complexity of the problem. As a result, the running time of LTL model checking algorithms will be inacceptable if a LTL formula is too long. To this end, this study tries to seek an acceptable approximate solution for LTL model checking by introducing the Machine Learning (ML) technique. And a method for predicting LTL model checking results is proposed, using the several ML algorithms including Boosted Tree (BT), Random Forest (RF), Decision tree (DT) or Logistic Regression (LR), respectively. First, for a number of Kripke structures and LTL formulas, a data set A containing model checking results is obtained, using one of the existing LTL model checking algorithm. Second, the LTL model checking problem can be induced to a binary classification problem of machine learning. In other words, some records in A form a training set for the given machine learning algorithm, where formulas and kripke structures are the two features, and model checking results are the one label. On the basis of it, a ML model M is obtained to predict the results of LTL model checking. As a result, an approximate LTL model checking technique occurs. The experiments show that the new method has the similar max accuracy with the state of the art algorithm in the classical LTL model checking technique, while the average efficiency of the former method is at most 6.3 million times higher than that of the latter algorithms, if the length of each of LTL formulas equals to 500. These results indicate that the new method can quickly and accurately determine LTL model checking result for a given Kripke structure and a given long LTL formula, since the new method avoids the famous state explosion problem. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2942762,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078271653&doi=10.1109%2fACCESS.2019.2942762&partnerID=40&md5=9d3bdda496ff3961f2f02bebbebef7d3,2019,2021-07-20 15:48:56,2021-07-20 15:48:56
GNLF38VF,journalArticle,2020,"Zhao, P.; Wu, G.; Yao, S.; Liu, H.",A Transductive Transfer Learning Approach Based on Manifold Learning,"Traditional machine learning approaches usually assume that the training data and the test data follow the same distribution. When the training data and the test data are not drawn from the same distribution, most traditional machine learning approaches cannot work well. Transfer learning approaches allow the distributions of the training data and the test data to be different. Transfer learning approaches mainly focus on how to reduce the difference between the training data and the test data, but mostly ignore the potential semantic information. This paper proposes a transductive transfer learning approach based on manifold learning (TTL-ML). TTL-ML designs a new feature representation space, where the semantic information hidden in the dataset structure is preserved, and the distribution difference between the source domain and the target domain is reduced. Experimental results on the common transfer learning datasets demonstrate that the proposed approach outperforms the several state-of-the-art approaches on image classification. © 2019 IEEE.",Computing in Science and Engineering,10.1109/MCSE.2018.2882699,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057154099&doi=10.1109%2fMCSE.2018.2882699&partnerID=40&md5=ad0907cbe32b9f4b4a3ab7c03611332f,2020,2021-07-20 15:48:56,2021-07-20 15:48:56
D82YIGAJ,journalArticle,2019,"Sandoval, R.M.; Garcia-Sanchez, A.-J.; Garcia-Haro, J.",Optimizing and updating lora communication parameters: A machine learning approach,"LoRa is an extremely flexible low-power wide-area technology that enables each IoT node to individually adjust its transmission parameters. Consequently, the average per-node throughput of LoRa-based networks has been mathematically formulated and the optimal network-level configuration derived. For end nodes to update their transmission parameters, this centrally computed global configuration must then be disseminated by LoRa gateways. Unfortunately, the regional limitations imposed on the usage of ISM bands—especially those related to the maximum utilization of the band—pose a potential handicap to this parameter dissemination. To solve this problem, a set of tools from the machine learning field have been used. Precisely, the updating process has been formulated as a reinforcement learning (RL) problem whose solution prescribes optimal disseminating policies. The use of these policies together with the optimal network configuration has been extensively analyzed and compared to other well-established alternatives. Results show an increase of up to 147% in the accumulated per-node throughput when our RL-based approach is employed. © 2019 IEEE.",IEEE Transactions on Network and Service Management,10.1109/TNSM.2019.2927759,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069488514&doi=10.1109%2fTNSM.2019.2927759&partnerID=40&md5=00cace362432c06a171e0918a36f0d8a,2019,2021-07-20 15:48:56,2021-07-20 15:48:56
LXSCW8ZB,journalArticle,2011,"Porto-Díaz, I.; Bolón-Canedo, V.; Alonso-Betanzos, A.; Fontenla-Romero, O.",A study of performance on microarray data sets for a classifier based on information theoretic learning,"Gene-expression microarray is a novel technology that allows the examination of tens of thousands of genes at a time. For this reason, manual observation is not feasible and machine learning methods are progressing to face these new data. Specifically, since the number of genes is very high, feature selection methods have proven valuable to deal with these unbalanced-high dimensionality and low cardinality-data sets. In this work, the FVQIT (Frontier Vector Quantization using Information Theory) classifier is employed to classify twelve DNA gene-expression microarray data sets of different kinds of cancer. A comparative study with other well-known classifiers is performed. The proposed approach shows competitive results outperforming all other classifiers. © 2011 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2011.05.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051795952&doi=10.1016%2fj.neunet.2011.05.010&partnerID=40&md5=965a85e2fcc603c673c61681606d75dc,2011,2021-07-20 15:48:56,2021-07-20 15:48:56
XFHZHD3R,journalArticle,2018,"Wang, H.-P.; Lin, C.-C.; Wu, C.-C.; Chen, Y.-C.; Wang, C.-Y.",On Synthesizing Memristor-Based Logic Circuits with Minimal Operational Pulses,"Memristor, which is a two-terminal nanodevice, widely used in various fields, e.g., machine learning and neuromorphic systems, has attracted much attention these years. Memristor can also be used to realize an implication logic gate and thus logic circuits. However, the fanouts in a memristor-based logic circuit have some constraints and need to be processed with special care. On the other hand, in addition to the number of memristors, the number of operational pulses is another metric to measure the quality of a memristor-based logic circuit. Hence, in this paper, we propose a synthesis algorithm to deal with the fanout problems in memristor-based logic circuits using implication logic gates for having a minimal number of operational pulses. We conducted experiments on a set of MCNC benchmarks. The experimental results show that the proposed algorithm can reduce 29% operational pulses and 36% memristor count on average compared with the state-of-the-art. © 1993-2012 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2018.2816023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044721350&doi=10.1109%2fTVLSI.2018.2816023&partnerID=40&md5=90b498227f1dee80e05886b5be7a5c53,2018,2021-07-20 15:48:57,2021-07-20 15:48:57
RHDLUMKA,journalArticle,2021,"Pasquadibisceglie, V.; Appice, A.; Castellano, G.; Malerba, D.",A multi-view deep learning approach for predictive business process monitoring,"The predictive business process monitoring is a family of online approaches to predict the unfolding of running traces basedon the knowledge learned from historical event logs. In this paper, we address the task of predicting the next trace activity from the completed events in a running trace. This is an important business capability as counting on accurate predictions of the future activities may allow companies to guarantee the higher utilization by acting proactively in anticipation. We propose a novel predictive process approach that couples multi-view learning and deep learning, in order to gain predictive accuracy by accounting for the variety ofinformation possibly recorded in event logs. Experiments with various benchmark event logs prove the effectiveness of the proposed approach compared to several recent state-of-the-art methods. IEEE",IEEE Transactions on Services Computing,10.1109/TSC.2021.3051771,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099726294&doi=10.1109%2fTSC.2021.3051771&partnerID=40&md5=ee83fb5cf487ef3584cc8ea3f1d08903,2021,2021-07-20 15:48:57,2021-07-20 15:48:57
7JU9GBNP,journalArticle,2021,"Cheng, X.; Wang, H.; Hua, J.; Xu, G.; Sui, Y.",DeepWukong: Statically Detecting Software Vulnerabilities Using Deep Graph Neural Network,"Static bug detection has shown its effectiveness in detecting well-defined memory errors, e.g., memory leaks, buffer overflows, and null dereference. However, modern software systems have a wide variety of vulnerabilities. These vulnerabilities are extremely complicated with sophisticated programming logic, and these bugs are often caused by different bad programming practices, challenging existing bug detection solutions. It is hard and labor-intensive to develop precise and efficient static analysis solutions for different types of vulnerabilities, particularly for those that may not have a clear specification as the traditional well-defined vulnerabilities. This article presents DeepWukong, a new deep-learning-based embedding approach to static detection of software vulnerabilities for C/C++ programs. Our approach makes a new attempt by leveraging advanced recent graph neural networks to embed code fragments in a compact and low-dimensional representation, producing a new code representation that preserves high-level programming logic (in the form of control-and data-flows) together with the natural language information of a program. Our evaluation studies the top 10 most common C/C++ vulnerabilities during the past 3 years. We have conducted our experiments using 105,428 real-world programs by comparing our approach with four well-known traditional static vulnerability detectors and three state-of-the-art deep-learning-based approaches. The experimental results demonstrate the effectiveness of our research and have shed light on the promising direction of combining program analysis with deep learning techniques to address the general static code analysis challenges. © 2021 ACM.",ACM Transactions on Software Engineering and Methodology,10.1145/3436877,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105713814&doi=10.1145%2f3436877&partnerID=40&md5=f2f88288533288e48de75486035787b6,2021,2021-07-20 15:48:57,2021-07-20 15:48:57
TEUJNY74,journalArticle,2013,"Nguyen, H.S.; Skowron, A.",Rough Sets: From Rudiments to Challenges,"In the development of rough set theory and applications, one can distinguish three main stages. At the beginning, the researchers concentrated on descriptive properties such as reducts of information systems preserving indiscernibility relations or description of concepts or classifications. Next, they moved to applications of rough sets in machine learning, pattern recognition and data mining. After gaining some experiences, they developed foundations for inductive reasoning leading to, for example, inducing classifiers. While the first period was based on the assumption that objects are perceived by means of partial information represented by attributes, the second period was based on the assumption that information about the approximated concepts is partial too. Approximation spaces and searching strategies for relevant approximation spaces were recognized as the basic tools for rough sets. Important achievements both in theory and applications were obtained using Boolean reasoning and approximate Boolean reasoning applied, for example, in searching for relevant features, discretization, symbolic value grouping, or, in more general sense, in searching for relevant approximation spaces. Nowadays, we observe that a new period is emerging in which two new important topics are investigated: (i) strategies for discovering relevant (complex) contexts of analysed objects or granules, what is strongly related to information granulation process and granular computing, and (ii) interactive computations on granules. Both directions are aiming at developing tools for approximation of complex vague concepts, such as behavioural patterns or adaptive strategies, making it possible to achieve the satisfactory qualities of realized interactive computations. This chapter presents this development from rudiments of rough sets to challenges, for example, related to ontology approximation, process mining, context inducing or Perception-Based Computing (PBC). The approach is based on Interactive Rough-Granular Computing (IRGC). © Springer-Verlag Berlin Heidelberg 2013.",Intelligent Systems Reference Library,10.1007/978-3-642-30344-9_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885452480&doi=10.1007%2f978-3-642-30344-9_3&partnerID=40&md5=e59e8523ddcff663a699b3f25a5c90ef,2013,2021-07-20 15:48:57,2021-07-20 15:48:57
YEX7YM2Z,journalArticle,2019,"Sherazi, H.H.R.; Iqbal, R.; Ahmad, F.; Khan, Z.A.; Chaudary, M.H.",DDoS attack detection: A key enabler for sustainable communication in internet of vehicles,"Privacy and trustworthiness are the key apprehensions for the users of Internet of Vehicle (IoV) services. Having multiple components involved in the communication (i.e., sensors, vehicles, humans, and infrastructures), the IoV platforms are exposed to a range of attacks. This manuscript will focus on Distributed Denial of Service (DDOS) attacks by adding the design of an Intrusion Detection Systems (IDS) tailored to IoV systems. Moreover, Artificial Intelligence (AI) and Machine Learning (ML) techniques will be investigated that can help in making refined defense architecture for countering DDOS attacks in IoV networks. Furthermore, a fuzzy logic and Q-learning based proposed solution is tested through simulations which argues about the usefulness of the proposed approach in comparison with conventional techniques. © 2019 Elsevier Inc.",Sustainable Computing: Informatics and Systems,10.1016/j.suscom.2019.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066247745&doi=10.1016%2fj.suscom.2019.05.002&partnerID=40&md5=5e74157ce6aa2286b64ecb0b8ea94884,2019,2021-07-20 15:48:57,2021-07-20 15:48:57
93IZB3KL,journalArticle,2015,"Ontañón, S.; Meseguer, P.",Speeding up operations on feature terms using constraint programming and variable symmetry,"Feature terms are a generalization of first-order terms which have recently received increased attention for their usefulness in structured machine learning, natural language processing and other artificial intelligence applications. One of the main obstacles for their wide usage is that, when set-valued features are allowed, their basic operations (subsumption, unification, and antiunification) have a very high computational cost. We present a Constraint Programming formulation of these operations, which in some cases provides orders of magnitude speed-ups with respect to the standard approaches. In addition, exploiting several symmetries - that often appear in feature terms databases - causes substantial additional savings. We provide experimental results of the benefits of this approach. © 2014 Elsevier B.V. All rights reserved.",Artificial Intelligence,10.1016/j.artint.2014.11.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920721203&doi=10.1016%2fj.artint.2014.11.010&partnerID=40&md5=c9f780913feed4eb3679397f94a0bae7,2015,2021-07-20 15:48:57,2021-07-20 15:48:57
UHF4KSQN,journalArticle,2020,"Montagna, S.; Mariani, S.; Gamberini, E.; Ricci, A.; Zambonelli, F.",Complementing Agents with Cognitive Services: A Case Study in Healthcare,"Personal Agents (PAs) have longly been explored as assistants to support users in their daily activities. Surprisingly, few works refer to the adoption of PAs in the healthcare domain, where they can assist physicians’ activities reducing medical errors. Although literature proposes different approaches for modelling and engineering PAs, none of them discusses how they can be integrated with cognitive services in order to empower their reasoning capabilities. In this paper we present an integration model, specifically devised for healthcare applications, that enhances Belief-Desire-Intention agents reasoning with advanced cognitive capabilities. As a case study, we adopt this integrated model in the critical care path of trauma resuscitation, stepping forward to the vision of Smart Hospitals. © 2020, The Author(s).",Journal of Medical Systems,10.1007/s10916-020-01621-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091010061&doi=10.1007%2fs10916-020-01621-7&partnerID=40&md5=67e4d2db82ee902b02baae7f2da2e1a4,2020,2021-07-20 15:48:57,2021-07-20 15:48:57
Y8G9T9WU,journalArticle,2019,"Gacanin, H.; Perenda, E.; Atawia, R.",Self-Deployment of Non-Stationary Wireless Systems by Knowledge Management with Artificial Intelligence,"In this paper, we propose a self-deployment strategy for non-stationary wireless extenders, where both back-haul and front-haul links are optimized. We present an artificial intelligence (AI) case based reasoning (CBR) framework that enables self-deployment with learning the environment by means of sensing and perception. New actions, i.e., extender positions, are created by problem-specific optimization and semi-supervised learning that balance exploration and exploitation of the search space. An IEEE 802.11 standard compliant simulations are performed to evaluate the framework on a large scale and compare its performance against existing conventional coverage maximization approaches. Experimental evaluation is also performed in an enterprise environment to demonstrate the competence of the proposed AI-framework. © 2015 IEEE.",IEEE Transactions on Cognitive Communications and Networking,10.1109/TCCN.2019.2932418,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070371007&doi=10.1109%2fTCCN.2019.2932418&partnerID=40&md5=bcbac4ce0f97cafc601dd305e4deacbf,2019,2021-07-20 15:48:57,2021-07-20 15:48:57
FL75M3I3,journalArticle,2015,"Anthony, M.; Ratsaby, J.",A probabilistic approach to case-based inference,"The central problem in case based reasoning (CBR) is to infer a solution for a new problem-instance by using a collection of existing problem-solution cases. The basic heuristic guiding CBR is the hypothesis that similar problems have similar solutions. Recently, some attempts at formalizing CBR in a theoretical framework have been made, including work by Hüllermeier who established a link between CBR and the probably approximately correct (PAC) theoretical model of learning in his 'case-based inference' (CBI) formulation. In this paper we develop further such probabilistic modelling, framing CBI it as a multi-category classification problem. We use a recently-developed notion of geometric margin of classification to obtain generalization error bounds. © 2015 Elsevier B.V.",Theoretical Computer Science,10.1016/j.tcs.2015.04.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944891482&doi=10.1016%2fj.tcs.2015.04.016&partnerID=40&md5=d85dd898ade2ae849e6989ccf10e9c27,2015,2021-07-20 15:48:57,2021-07-20 15:48:57
YKFJ9U5X,journalArticle,2020,"Ke, F.; Choi, S.; Kang, Y.H.; Cheon, K.-A.; Lee, S.W.",Exploring the Structural and Strategic Bases of Autism Spectrum Disorders with Deep Learning,"Deep learning models are applied in clinical research in order to diagnose disease. However, diagnosing autism spectrum disorders (ASD) remains challenging due to its complex psychiatric symptoms as well as a generally insufficient amount of neurobiological evidence. We investigated the structural and strategic bases of ASD using 14 different types of models, including convolutional and recurrent neural networks. Using an open source autism dataset consisting of more than 1000 MRI scan images and a high-resolution structural MRI dataset, we demonstrated how deep neural networks could be used as tools for diagnosing and analyzing psychiatric disorders. We trained 3D convolutional neural networks to visualize combinations of brain regions, thus representing the most referred-to regions used by the model whilst classifying the images. We also implemented recurrent neural networks to classify the sequence of brain regions efficiently. We found emphatic structural and strategic evidence on which the model heavily relies during the classification process. For instance, we observed that the structural and strategic evidence tends to be associated with subcortical structures, including the basal ganglia (BG). Our work identifies the distinct brain structures that characterize a complex psychiatric disorder while streamlining the deductive reasoning that clinicians can use to ensure an economical and time-efficient diagnosis process. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3016734,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090554681&doi=10.1109%2fACCESS.2020.3016734&partnerID=40&md5=8a10f0f897eebe3c62cdb77f102d99f3,2020,2021-07-20 15:48:57,2021-07-20 15:48:57
7YED2XWT,journalArticle,2021,"Samiee, A.; Borulkar, P.; DeMara, R.F.; Zhao, P.; Bai, Y.",Low-energy acceleration of binarized convolutional neural networks using a spin hall effect based logic-in-memory architecture,"Deep Learning (DL) offers the advantages of high accuracy performance at tasks such as image recognition, learning of complex intelligent behaviors, and large-scale information retrieval problems such as intelligent web search. To attain the benefits of DL, the high computational and energy-consumption demands imposed by the underlying processing, interconnect, and memory devices on which software-based DL executes can benefit substantially from innovative hardware implementations. Logic-in-Memory (LIM) architectures offer potential approaches to attaining such throughput goals within area and energy constraints starting with the lowest layers of the hardware stack. In this paper, we develop a Spintronic Logic-in-Memory (S-LIM) XNOR neural network (S-LIM XNN) which can perform binary convolution with reconfigurable in-memory logic without supplementing distinct logic circuits for computation within the memory module itself. Results indicate that the proposed S-LIM XNN designs achieve 1.2-fold energy reduction, 1.26-fold throughput increase, and 1.4-fold accuracy improvement compared to the state-of-the-art binarized convolutional neural network hardware. Design considerations, architectural approaches, and the impact of process variation on the proposed hybrid spin-CMOS design are identified and assessed, including comparisons and recommendations for future directions with respect to LIM approaches for neuromorphic computing. © 2013 IEEE.",IEEE Transactions on Emerging Topics in Computing,10.1109/TETC.2019.2915589,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065656009&doi=10.1109%2fTETC.2019.2915589&partnerID=40&md5=9fb1391a221c8a78d705cc7b820c8126,2021,2021-07-20 15:48:57,2021-07-20 15:48:57
4BP6W2ZS,journalArticle,2020,"Zhao, X.; Lin, S.; Chen, X.; Ou, C.; Liao, C.",Application of face image detection based on deep learning in privacy security of intelligent cloud platform,"The application of deep learning-based face detection in the privacy and security of intelligent cloud platforms is studied, in order to resolve the risk of private photo leakage on such platforms. Five key feature points of human faces (two eyes, two noses and two corners of mouth) are sought using the multitask cascaded convolution network (MTCNN). The algorithm utilizes the intrinsic links among Proposal Network (P-Net), Refine Network (R-Net) and Output Network (O-Net) to improve their face detection performances substantially. Since many existing data encryption methods, such as DES, RSA and AES, are applicable only to test data instead of digital images, the encryption of eigenvalues is achieved with a combination of chaotic logic diagrams and RC4 stream ciphers. Meanwhile, the MTCNN-generated face coordinates and user passwords are hash-converted and double-encrypted using hash table. The results show that the face detection accuracy of MTCNN reaches 95.04%, and that the image encryption method is suitable for network transmission. The chaotic logic diagrams increase the security of S-box initialization in the RC4 algorithm. The hash structure accelerates the file reading, whereas the hash conversion improves the security of critical data. In conclusion, the proposed encryption scheme is computationally fast and highly secure. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-019-08014-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071306512&doi=10.1007%2fs11042-019-08014-0&partnerID=40&md5=566e640c7451e53b1e53cde5c40e52fc,2020,2021-07-20 15:48:57,2021-07-20 15:48:57
ZCU7C7XV,journalArticle,2020,"Dufek, A.S.; Augusto, D.A.; Dias, P.L.S.; Barbosa, H.J.C.",Data-driven symbolic ensemble models for wind speed forecasting through evolutionary algorithms,"Non-linear data-driven symbolic models have been gaining traction in many fields due to their distinctive combination of modeling expressiveness and interpretability. Despite that, they are still rather unexplored for ensemble wind speed forecasting, leaving behind new promising avenues for advancing the development of more accurate models which impact the efficiency of energy production. In this work, we develop a methodology based on the evolutionary algorithm known as grammatical evolution, and apply it to build forecasting models of near-surface wind speed over five locations in northeastern Brazil. Taking advantage of the symbolic nature of the models built, we conducted an extensive series of post-analyses. Overall, our models reduced the forecasting errors by 7%–56% when compared with other techniques, including a real-world operational ensemble model used in Brazil. © 2019 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2019.105976,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076018868&doi=10.1016%2fj.asoc.2019.105976&partnerID=40&md5=0850c4da7d401cba9176e5f3bf0c97f1,2020,2021-07-20 15:48:57,2021-07-20 15:48:57
LW74W6EU,journalArticle,2017,"Yazdanbakhsh, O.; Dick, S.",Forecasting of Multivariate Time Series via Complex Fuzzy Logic,"Multivariate time series consist of sequential vector-valued observations of some phenomenon over time. Time series forecasting (for both univariate and multivariate case) is a well-known, high-value machine learning problem, in which the goal is to predict future observations of the time series based on prior ones. Several learning algorithms based on complex fuzzy logic have recently been shown to be very accurate and compact forecasting models. However, these models have only been tested on univariate and bivariate datasets. There has as yet been no investigation of more general multivariate datasets. We report on the extension of the adaptive neuro-complex-fuzzy inferential system learning architecture to the multivariate case. We investigate single-input-single-output, multiple-input-single-output, and multiple-input-multiple-output variations of the architecture, exploring their performance on four multivariate time series. We also explore modifications to the forward- and backward-pass computations in the architecture. We find that our best designs are superior to the published results on these datasets, and at least as accurate as kernel-based prediction algorithms. © 2013 IEEE.","IEEE Transactions on Systems, Man, and Cybernetics: Systems",10.1109/TSMC.2016.2630668,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029219320&doi=10.1109%2fTSMC.2016.2630668&partnerID=40&md5=94be4ebe8ad09ffde92b7a5e85f766e7,2017,2021-07-20 15:48:57,2021-07-20 15:48:57
PNCT6UZJ,journalArticle,2021,"Wen, G.; Qin, J.; Fu, X.; Yu, W.",DLSTM: Distributed Long Short-Term Memory Neural Networks for the Internet of Things,"Although the development of Internet of Things (IoT) provides a significant boost for the applications of deep learning algorithms, it is generally hard to fully implement the deep learning algorithms by IoT devices due to their limited calculation capacity. The problem could be alleviated by deploying the deep learning system with edge computing. Herein, we propose a kind of distributed long short-term memory (DLSTM) neural networks and deploy them on the IoT environment to handle the large-scale spatiotemporal correlation regression tasks. Specifically, the presented DLSTM neural networks adopt the collaborative computing architecture with the terminals, edges and cloud, in order to realize the lightweight deep learning on the IoT devices and improve the learning efficiency. The generalization ability of LSTM neural networks is promoted through introducing the distributed memory cells to implement the information sharing between different edge servers and employing the attention mechanism in LSTM neural networks. Meanwhile, the deep fully connected networks are deployed among the cloud to extract the spatiotemporal correlations in the variety of data from different time and space regions, which enhances the transferability of LSTM neural networks. IEEE",IEEE Transactions on Network Science and Engineering,10.1109/TNSE.2021.3054244,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100458453&doi=10.1109%2fTNSE.2021.3054244&partnerID=40&md5=d199c639cef0a81941a11f2d59350779,2021,2021-07-20 15:48:57,2021-07-20 15:48:57
PBXZ5BYF,journalArticle,2020,"Akhtar, J.",An interactive multi-agent reasoning model for sentiment analysis: a case for computational semiotics,"Semiotics is a domain that studies signs. For Peircean semiotics, a sign is not a dyadic entity, composed of a word and its meaning. Instead, meaning-making is a process of signification borne out of a strictly triadic relationship; in which a representative of a sign (word(s)) stands for its object (or meaning,) but never in a vacuum, and always for an interpretant. For Peirce, it is this third, this interpretant, through which the sign displays its meaning. What is even more important is that this rational process of signification is never being carried out by a single Mind, it requires a community of reasoners. In semiotic terms this article translates the sentiment analysis problem as follows: A sentence/comment is a representamen which has a sentiment value (its object) for an evolving community of reasoning agents (interpretant.) This article presents an interactive multi-agent system in which the agents implicitly model other agents, with a semiotic based approach towards sentiment analysis. It then tests the system on an original student evaluation of teachers dataset, compares the results with deep learning and other baseline techniques, and aims to propose semiotics as a reparative alternative to the dominant dichotomies—rule-based and data-based camps within artificial intelligence. © 2019, Springer Nature B.V.",Artificial Intelligence Review,10.1007/s10462-019-09785-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075132674&doi=10.1007%2fs10462-019-09785-6&partnerID=40&md5=39f88e793a741da0d0facc6a5653d05e,2020,2021-07-20 15:48:57,2021-07-20 15:48:57
Z9QQYW3B,journalArticle,2015,"Murari, A.; Vega, J.; Mazon, D.; Courregelongue, T.; Contributors, JET-EFDA",Preliminary numerical investigations of conformal predictors based on fuzzy logic classifiers,"A new family of techniques, called conformal predictors, have very recently been developed to hedge the estimates of machine learning methods, by providing two parameters, credibility and confidence, which can assess the level of trust that can be attributed to their outputs. In this paper, the main steps required to extend this approach to fuzzy logic classifiers are reported. The more delicate aspect is the definition of an appropriate nonconformity score, which has to be based on the fuzzy membership function to preserve the specificities of Fuzzy Logic. Various examples of increasing complexity are introduced, to describe the main properties of fuzzy logic based conformal predictors and to compare their performance with alternative approaches. The obtained results are quite promising, since conformal predictors based on fuzzy classifiers outperform solutions based on the nearest neighbour in terms of ambiguity, robustness and interpretability. © 2014, Springer Science+Business Media Dordrecht.",Annals of Mathematics and Artificial Intelligence,10.1007/s10472-014-9399-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930415209&doi=10.1007%2fs10472-014-9399-5&partnerID=40&md5=34117eb43dd74c0bbad0851984891b1c,2015,2021-07-20 15:48:57,2021-07-20 15:48:57
BXD8N3PT,journalArticle,2021,"Rivadeneira, L.; Yang, J.-B.; López-Ibáñez, M.",Predicting tweet impact using a novel evidential reasoning prediction method,"This study presents a novel evidential reasoning (ER) prediction model called MAKER-RIMER to examine how different features embedded in Twitter posts (tweets) can predict the number of retweets achieved during an electoral campaign. The tweets posted by the two most voted candidates during the official campaign for the 2017 Ecuadorian Presidential election were used for this research. For each tweet, five features including type of tweet, emotion, URL, hashtag, and date are identified and coded to predict if tweets are of either high or low impact. The main contributions of the new proposed model include its suitability to analyse tweet datasets based on likelihood analysis of data. The model is interpretable, and the prediction process relies only on the use of available data. The experimental results show that MAKER-RIMER performed better, in terms of misclassification error, when compared against other predictive machine learning approaches. In addition, the model allows observing which features of the candidates’ tweets are linked to high and low impact. Tweets containing allusions to the contender candidate, either with positive or negative connotations, without hashtags, and written towards the end of the campaign, were persistently those with the highest impact. URLs, on the other hand, is the only variable that performs differently for the two candidates in terms of achieving high impact. MAKER-RIMER can provide campaigners of political parties or candidates with a tool to measure how features of tweets are predictors of their impact, which can be useful to tailor Twitter content during electoral campaigns. © 2020 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2020.114400,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098720369&doi=10.1016%2fj.eswa.2020.114400&partnerID=40&md5=93e4f42547528a91990e6b5c9f546b60,2021,2021-07-20 15:48:58,2021-07-20 15:48:58
Y4UGJMZB,journalArticle,2021,"Franciscatto, M.H.; Del Fabro, M.D.; Damasceno Lima, J.C.; Trois, C.; Moro, A.; Maran, V.; Keske-Soares, M.",Towards a speech therapy support system based on phonological processes early detection,"Phonological disorders are characterized by substitutions, insertion and/or deletions of sounds during the process of language acquisition, which are known as Phonological Processes (PPs). In the speech therapy domain, an early identification of PPs allows the diagnosis and treatment of various pathologies and may improve clinical tasks, however, there are few proposals that focus on the identification of PPs for supporting Speech-Language Pathologists (SLPs). Recent research applied Case-Based Reasoning (CBR) in medical domain to identify specific cases related to patients. Situation-Awareness (SA) is a technique that allows computing systems to adapt itself and respond to users or other systems according to environment information. Moreover, there is no indicative in related literature of CBR and SA being used for detecting PPs that may occur in pronunciation. In this paper, we introduce the union of SA and CBR, tied to machine learning algorithms for proposing a system to predict PPs, supporting specialists in their clinical decisions. To evaluate the system, we implemented it in a software architecture prototype and evaluated the prototypes using a knowledge base containing near one hundred thousand audio files, collected from more than 1,000 pronunciation assessments. The evaluation of the prototypes showed an accuracy over 93% in the prediction of PPs, resulting in a efficient tool for clinical decision support and therapeutic planning. We also presented a direct qualitative comparison between our approach and related work. © 2020 Elsevier Ltd",Computer Speech and Language,10.1016/j.csl.2020.101130,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087105855&doi=10.1016%2fj.csl.2020.101130&partnerID=40&md5=7f462ce0592e9abbbf94cb5a74586dec,2021,2021-07-20 15:48:58,2021-07-20 15:48:58
CN82MUPQ,journalArticle,2021,"Beninger, J.; Hamilton-Wright, A.; Walker, H.E.K.; Trick, L.M.",Machine learning techniques to identify mind-wandering and predict hazard response time in fully immersive driving simulation,"This work presents machine learning-based techniques for detecting mind-wandering and predicting hazard response time in driving using only easily measurable driving performance data (speed, horizontal and frontal acceleration, lane gap, and brake pressure). Such classifiers are relevant as research tools in the driving simulation community. We present a simple method, and a feature extraction-based method, of representing time-series driving performance data that both support machine learning-based predictions. We use the two types of representations to compare the effectiveness of support vector machines, random forest, and multi-layer perceptrons on data from 117 drives performed by 39 participants during a previous study in the high-fidelity driving simulator at the University of Guelph. Classification of mind-wandering and prediction of hazard response time was successful when compared to baseline measures. Specifically, random forest methods were most effective in both types of prediction and feature extraction supported the strongest random forest prediction of hazard response time. A discussion of the reasoning for this is included. While this has previously been studied using a single screen experimental setup, to our knowledge this is the first driving pattern-based classification of mind-wandering in a fully immersive driving simulator. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-020-05217-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089018940&doi=10.1007%2fs00500-020-05217-8&partnerID=40&md5=70dbd56bd9661edd9b2d0546ab52bcfa,2021,2021-07-20 15:48:58,2021-07-20 15:48:58
69YT4H38,journalArticle,2018,"Neves, J.; Vicente, H.; Esteves, M.; Ferraz, F.; Abelha, A.; Machado, J.; Machado, J.; Neves, J.; Ribeiro, J.; Sampaio, L.",A Deep-Big Data Approach to Health Care in the AI Age,"The intersection of these two trends is what we call The Issue and it is helping businesses in every industry to become more efficient and productive. One’s aim is to have an insight into the development and maintenance of comprehensive and integrated health information systems that enable sound policy and effective health system management in order to improve health and health care. Undeniably, different sorts of technologies have been developed, each with their own advantages and disadvantages, which will be sorted out by attending at the impact that Artificial Intelligence and Decision Support Systems have to everyone in the healthcare sector engaged to quality-of-care, i.e., making sure that doctors, nurses, and staff have the training and tools they need to do their jobs. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Mobile Networks and Applications,10.1007/s11036-018-1071-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049118338&doi=10.1007%2fs11036-018-1071-6&partnerID=40&md5=29e3778023e191c94a2f9373999bab44,2018,2021-07-20 15:48:58,2021-07-20 15:48:58
HY5JG733,journalArticle,2012,"Chang, P.-C.; Lin, J.-J.; Liu, C.-H.",An attribute weight assignment and particle swarm optimization algorithm for medical database classifications,"In this research, a hybrid model is developed by integrating a case-based reasoning approach and a particle swarm optimization model for medical data classification. Two data sets from UCI Machine Learning Repository, i.e., Liver Disorders Data Set and Breast Cancer Wisconsin (Diagnosis), are employed for benchmark test. Initially a case-based reasoning method is applied to preprocess the data set thus a weight vector for each feature is derived. A particle swarm optimization model is then applied to construct a decision-making system for diseases identified. The PSO algorithm starts by partitioning the data set into a relatively large number of clusters to reduce the effects of initial conditions and then reducing the number of clusters into two. The average forecasting accuracy for breast cancer of CBRPSO model is 97.4% and for liver disorders is 76.8%. The proposed case-based particle swarm optimization model is able to produce more accurate and comprehensible results for medical experts in medical diagnosis. © 2010 Elsevier Ireland Ltd.",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2010.12.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863879044&doi=10.1016%2fj.cmpb.2010.12.004&partnerID=40&md5=116b18be3c04ca6250b548a09ace17c6,2012,2021-07-20 15:48:58,2021-07-20 15:48:58
DEL9KI7L,journalArticle,2017,"Nayebi, M.; Kabeer, S.J.; Ruhe, G.; Carlson, C.; Chew, F.",Hybrid Labels Are the New Measure!,"Developing minimum viable products (MVPs) is critical for start-up companies to hit the market fast with an accepted level of performance. The US Food and Drug Administration mandates additional nonfunctional requirements in healthcare systems, meaning that the MVP should provide the best availability, privacy, and security. This critical demand is motivating companies to further rely on analytics to optimize the development process. In a collaborative project with Brightsquid, the authors provided a decision-support system based on analogical reasoning to assist in effort estimation, scoping, and assignment of change requests. This experience report proposes a new metric, change request labels, for better prediction. Using different methods for textual-similarity analysis, the authors found that the combination of machine-learning techniques with experts' manually added labels has the highest prediction accuracy. Better prediction of change impacts allows a company to optimize its resources and provide proper timing of releases to target MVPs. This article is part of a special issue on Actionable Analytics for Software Engineering. © 1984-2012 IEEE.",IEEE Software,10.1109/MS.2017.4541048,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040312234&doi=10.1109%2fMS.2017.4541048&partnerID=40&md5=df248e20ecf9dd6584e9b2393af3fc90,2017,2021-07-20 15:48:58,2021-07-20 15:48:58
5RXJR4YB,journalArticle,2017,"Ko, T.; Hyuk Lee, J.; Cho, H.; Cho, S.; Lee, W.; Lee, M.","Machine learning-based anomaly detection via integration of manufacturing, inspection and after-sales service data","Purpose - Quality management of products is an important part of manufacturing process. One way to manage and assure product quality is to use machine learning algorithms based on relationship among various process steps. The purpose of this paper is to integrate manufacturing, inspection and after-sales service data to make full use of machine learning algorithms for estimating the products' quality in a supervised fashion. Proposed frameworks and methods are applied to actual data associated with heavy machinery engines. Design/methodology/approach - By following Lenzerini's formula, manufacturing, inspection and aftersales service data from various sources are integrated. The after-sales service data are used to label each engine as normal or abnormal. In this study, one-class classification algorithms are used due to class imbalance problem. To address multi-dimensionality of time series data, the symbolic aggregate approximation algorithm is used for data segmentation. Then, binary genetic algorithm-based wrapper approach is applied to segmented data to find the optimal feature subset. Findings - By employing machine learning-based anomaly detection models, an anomaly score for each engine is calculated. Experimental results show that the proposed method can detect defective engines with a high probability before they are shipped. Originality/value - Through data integration, the actual customer-perceived quality from after-sales service is linked to data from manufacturing and inspection process. In terms of business application, data integration and machine learning-based anomaly detection can help manufacturers establish quality management policies that reflect the actual customer-perceived quality by predicting defective engines.",Industrial Management and Data Systems,10.1108/IMDS-06-2016-0195,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019623490&doi=10.1108%2fIMDS-06-2016-0195&partnerID=40&md5=5e25031304522921996158e7471b7045,2017,2021-07-20 15:48:58,2021-07-20 15:48:58
URNQLME2,journalArticle,2021,"Wei, X.; Yang, Z.; Zhang, X.; Liao, G.; Sheng, A.; Kevin Zhou, S.; Wu, Y.; Du, L.",Deep Collocative Learning for Immunofixation Electrophoresis Image Analysis,"Immunofixation Electrophoresis (IFE) analysis is of great importance to the diagnosis of Multiple Myeloma, which is among the top-9 cancer killers in the United States, but has rarely been studied in the context of deep learning. Two possible reasons are: 1) the recognition of IFE patterns is dependent on the co-location of bands that forms a binary relation, different from the unary relation (visual features to label) that deep learning is good at modeling; 2) deep classification models may perform with high accuracy for IFE recognition but is not able to provide firm evidence (where the co-location patterns are) for its predictions, rendering difficulty for technicians to validate the results. We propose to address these issues with collocative learning, in which a collocative tensor has been constructed to transform the binary relations into unary relations that are compatible with conventional deep networks, and a location-label-free method that utilizes the Grad-CAM saliency map for evidence backtracking has been proposed for accurate localization. In addition, we have proposed Coached Attention Gates that can regulate the inference of the learning to be more consistent with human logic and thus support the evidence backtracking. The experimental results show that the proposed method has obtained a performance gain over its base model ResNet18 by 741.30% in IoU and also outperformed popular deep networks of DenseNet, CBAM, and Inception-v3. IEEE",IEEE Transactions on Medical Imaging,10.1109/TMI.2021.3068404,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103258882&doi=10.1109%2fTMI.2021.3068404&partnerID=40&md5=3e830863e1e5140a390179d89dab3609,2021,2021-07-20 15:48:58,2021-07-20 15:48:58
A6DWFY9T,journalArticle,2019,"Han, Z.; Liang, J.",The Analysis of Node Planning and Control Logic Optimization of 5G Wireless Networks under Deep Mapping Learning Algorithms,"In order to meet the needs of blowout growth of data flow and 10-100 times increase of user experience rate, the next generation mobile communication(5G) heterogeneous network deployment will use ultra-dense network. Accurate fault location is the basis of slice management. In virtualized network model, the function of network elements is software, the underlying layer is highly abstract, the structure changes dynamically, and common cause faults occur frequently, which brings difficulties to fault location. Deep Learning is a machine Learning method that USES Deep Neural Network (DNN) with multiple hidden layers to complete Learning tasks. The essence is that by building a neural network model with multiple hidden layers and using a large amount of training data to learn more useful features, the accuracy of model prediction or classification can be improved. Based on the theory of deep mapping learning algorithm, a scheduling algorithm based on event-driven access point, improved time-driven access point and idle network is designed for centralized wireless network control system. Aiming at the centralized wireless network control system, this paper designs a scheduling algorithm based on event-driven access point, improved time-driven access point and network idleness. The designed sensor network node platform uses the programmable logic controller as the main control chip, contains the data interface module that realizes the communication function between the node and the upper computer, and the acquisition and conditioning module responsible for the collection and conditioning and conversion of the analog signal. At the same time, the TCP and UDP are compared and studied. The accuracy of the model is verified, and the interference distribution of the model and the influence of the D2D inter-pair distance on the system performance are simulated. The distance between the D2D pairs under the given simulation parameters has a certain influence on the system performance. As the distance increases, the system performance will deteriorate, but the impact is not a big conclusion. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2949631,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074701182&doi=10.1109%2fACCESS.2019.2949631&partnerID=40&md5=f09aaa26d9bdb50652ea6556ede9ac6a,2019,2021-07-20 15:48:58,2021-07-20 15:48:58
PCB97HVL,journalArticle,2013,"Bresso, E.; Grisoni, R.; Marchetti, G.; Karaboga, A.S.; Souchet, M.; Devignes, M.-D.; Smaïl-Tabbone, M.",Integrative relational machine-learning for understanding drug side-effect profiles,"Background: Drug side effects represent a common reason for stopping drug development during clinical trials. Improving our ability to understand drug side effects is necessary to reduce attrition rates during drug development as well as the risk of discovering novel side effects in available drugs. Today, most investigations deal with isolated side effects and overlook possible redundancy and their frequent co-occurrence.Results: In this work, drug annotations are collected from SIDER and DrugBank databases. Terms describing individual side effects reported in SIDER are clustered with a semantic similarity measure into term clusters (TCs). Maximal frequent itemsets are extracted from the resulting drug x TC binary table, leading to the identification of what we call side-effect profiles (SEPs). A SEP is defined as the longest combination of TCs which are shared by a significant number of drugs. Frequent SEPs are explored on the basis of integrated drug and target descriptors using two machine learning methods: decision-trees and inductive-logic programming. Although both methods yield explicit models, inductive-logic programming method performs relational learning and is able to exploit not only drug properties but also background knowledge. Learning efficiency is evaluated by cross-validation and direct testing with new molecules. Comparison of the two machine-learning methods shows that the inductive-logic-programming method displays a greater sensitivity than decision trees and successfully exploit background knowledge such as functional annotations and pathways of drug targets, thereby producing rich and expressive rules. All models and theories are available on a dedicated web site.Conclusions: Side effect profiles covering significant number of drugs have been extracted from a drug ×side-effect association table. Integration of background knowledge concerning both chemical and biological spaces has been combined with a relational learning method for discovering rules which explicitly characterize drug-SEP associations. These rules are successfully used for predicting SEPs associated with new drugs. © 2013 Bresso et al.; licensee BioMed Central Ltd.",BMC Bioinformatics,10.1186/1471-2105-14-207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879426876&doi=10.1186%2f1471-2105-14-207&partnerID=40&md5=ad337205828c933377b5ab0d54db32bf,2013,2021-07-20 15:48:58,2021-07-20 15:48:58
UR7QDPSE,journalArticle,2021,"Radhika, S.; Rangarajan, P.",Fuzzy Based Sleep Scheduling Algorithm with Machine Learning Techniques to Enhance Energy Efficiency in Wireless Sensor Networks,"Wireless sensor networks, generally, are grouped into clusters to collect information effectively. Such grouping of nodes helps immensely to elongate the life of Wireless Sensor Networks. Message exchanges between nodes for consecutive and periodic clustering overload the sensor nodes and cause a shortfall of energy. Additional overhead during cluster formation, instability in energy use and the difficulty of information sharing during clustering, uncertain network structure, etc. are the current clustering problems. There is also a need to enhance intra-cluster transmission and to find effective methods to extend the network's lifespan. This paper aims to reduce the energy loss of nodes by reducing the message transmission overhead and simplifying the creation and upgrading of clusters to improve the lifespan of the network. A clustering strategy where the cluster is regularly restructured to decrease the overhead on cluster head nodes is also proposed in the paper. The suggested approach reduces data transmission using machine learning by the cluster member nodes and reduces the energy consumption of individual sensor nodes by implementing a suitable active/sleep schedule. To calculate the cluster update cycle and sleep cycle, it also makes use of the advantages of fuzzy logic by selecting appropriate fuzzy descriptors such as average data rate, distance from the head node to the sink and the remaining energy. The proposed approach optimizes the energy utilization of cluster heads and node members thereby enhancing the lifespan of the sensor network. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Wireless Personal Communications,10.1007/s11277-021-08167-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100811749&doi=10.1007%2fs11277-021-08167-y&partnerID=40&md5=64694bdd845e362ecad2a17d7cf3d8cd,2021,2021-07-20 15:48:58,2021-07-20 15:48:58
9EWSNTFD,journalArticle,2020,"Yalur, T.",Interperforming in AI: question of ‘natural’ in machine learning and recurrent neural networks,"This article offers a critical inquiry of contemporary neural network models as an instance of machine learning, from an interdisciplinary perspective of AI studies and performativity. It shows the limits on the architecture of these network systems due to the misemployment of ‘natural’ performance, and it offers ‘context’ as a variable from a performative approach, instead of a constant. The article begins with a brief review of machine learning-based natural language processing systems and continues with a concentration on the relevant model of recurrent neural networks, which is applied in most commercial research such as Facebook AI Research. It demonstrates that the logic of performativity is not brought into account in all recurrent nets, which is an integral part of human performance and languaging, and it argues that recurrent network models, in particular, fail to grasp human performativity. This logic works similarly to the theory of performativity articulated by Jacques Derrida in his critique of John L. Austin’s concept of the performative. Applying Jacques Derrida’s work on performativity, and linguistic traces as spatially organized entities that allow for this notion of performance, the article argues that recurrent nets fall into the trap of taking ‘context’ as a constant, of treating human performance as a ‘natural’ fix to be encoded, instead of performative. Lastly, the article applies its proposal more concretely to the case of Facebook AI Research’s Alice and Bob. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",AI and Society,10.1007/s00146-019-00910-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073917651&doi=10.1007%2fs00146-019-00910-1&partnerID=40&md5=5a1a2ebc4727f675b4a9ac453b86aa50,2020,2021-07-20 15:48:58,2021-07-20 15:48:58
98BKP8UY,journalArticle,2012,"Hsieh, S.-L.; Hsieh, S.-H.; Cheng, P.-H.; Chen, C.-H.; Hsu, K.-P.; Lee, I.-S.; Wang, Z.; Lai, F.",Design ensemble machine learning model for breast cancer diagnosis,"In this paper, we classify the breast cancer of medical diagnostic data. Information gain has been adapted for feature selections. Neural fuzzy (NF), k-nearest neighbor (KNN), quadratic classifier (QC), each single model scheme as well as their associated, ensemble ones have been developed for classifications. In addition, a combined ensemble model with these three schemes has been constructed for further validations. The experimental results indicate that the ensemble learning performs better than individual single ones. Moreover, the combined ensemble model illustrates the highest accuracy of classifications for the breast cancer among all models. © 2011 Springer Science+Business Media, LLC.",Journal of Medical Systems,10.1007/s10916-011-9762-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867293214&doi=10.1007%2fs10916-011-9762-6&partnerID=40&md5=5dfee241fa7dee52f0c30f2b01157197,2012,2021-07-20 15:48:58,2021-07-20 15:48:58
8XDDKV39,journalArticle,2021,"Sun, H.; Arndt, D.; De Roo, J.; Mannens, E.",Predicting future state for adaptive clinical pathway management,"Clinical decision support systems are assisting physicians in providing care to patients. However, in the context of clinical pathway management such systems are rather limited as they only take the current state of the patient into account and ignore the possible evolvement of that state in the future. In the past decade, the availability of big data in the healthcare domain did open a new era for clinical decision support. Machine learning technologies are now widely used in the clinical domain, nevertheless, mostly as a tool for disease prediction. A tool that not only predicts future states, but also enables adaptive clinical pathway management based on these predictions is still in need. This paper introduces weighted state transition logic, a logic to model state changes based on actions planned in clinical pathways. Weighted state transition logic extends linear logic by taking weights – numerical values indicating the quality of an action or an entire clinical pathway – into account. It allows us to predict the future states of a patient and it enables adaptive clinical pathway management based on these predictions. We provide an implementation of weighted state transition logic using semantic web technologies, which makes it easy to integrate semantic data and rules as background knowledge. Executed by a semantic reasoner, it is possible to generate a clinical pathway towards a target state, as well as to detect potential conflicts in the future when multiple pathways are coexisting. The transitions from the current state to the predicted future state are traceable, which builds trust from human users on the generated pathway. © 2021 Elsevier Inc.",Journal of Biomedical Informatics,10.1016/j.jbi.2021.103750,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102664726&doi=10.1016%2fj.jbi.2021.103750&partnerID=40&md5=28914ef1af36169d108dd395e132391d,2021,2021-07-20 15:48:59,2021-07-20 15:48:59
SW3IVU8E,journalArticle,2018,"Yang, M.; Tu, W.; Qu, Q.; Zhao, Z.; Chen, X.; Zhu, J.",Personalized response generation by Dual-learning based domain adaptation,"Open-domain conversation is one of the most challenging artificial intelligence problems, which involves language understanding, reasoning, and the utilization of common sense knowledge. The goal of this paper is to further improve the response generation, using personalization criteria. We propose a novel method called PRGDDA (Personalized Response Generation by Dual-learning based Domain Adaptation) which is a personalized response generation model based on theories of domain adaptation and dual learning. During the training procedure, PRGDDA first learns the human responding style from large general data (without user-specific information), and then fine-tunes the model on a small size of personalized data to generate personalized conversations with a dual learning mechanism. We conduct experiments to verify the effectiveness of the proposed model on two real-world datasets in both English and Chinese. Experimental results show that our model can generate better personalized responses for different users. © 2018 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2018.03.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045705590&doi=10.1016%2fj.neunet.2018.03.009&partnerID=40&md5=62c0e7468a872e17eeb031613314833a,2018,2021-07-20 15:48:59,2021-07-20 15:48:59
57N6TFUE,journalArticle,2021,"de Franca, F.O.; de Lima, M.Z.",Interaction-transformation symbolic regression with extreme learning machine,"Symbolic Regression searches for a mathematical expression that fits the input data set by minimizing the approximation error. The search space explored by this technique is composed of any mathematical function representable as an expression tree. This provides more flexibility for fitting the data but it also makes the task more challenging. The search space induced by this representation becomes filled with redundancy and ruggedness, sometimes requiring a higher computational budget in order to achieve good results. Recently, a new representation for Symbolic Regression was proposed, called Interaction-Transformation, which can represent function forms as a composition of interactions between predictors and the application of a single transformation function. In this work, we show how this representation can be modeled as a multi-layer neural network with the weights adjusted following the Extreme Learning Machine procedure. The results show that this approach is capable of finding equally good or better results than the current state-of-the-art with a smaller computational cost. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.10.062,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096398409&doi=10.1016%2fj.neucom.2020.10.062&partnerID=40&md5=c1840456235d2e67ef9fd3da83c81969,2021,2021-07-20 15:48:59,2021-07-20 15:48:59
FDFXQTPI,journalArticle,2013,"Chuang, C.-C.; Jeng, J.-T.; Chang, S.-C.",Hausdorff distance measure based interval fuzzy possibilistic c-means clustering algorithm,"Clustering algorithms have been widely used artificial intelligence, data mining and machine learning, etc. It is unsupervised classification and is divided into groups according to data sets. That is, the data sets of similarity partition belong to the same group; otherwise data sets divide other groups in the clustering algorithms. In general, to analysis interval data needs Type II fuzzy logical. Therefore, the interval fuzzy c-means clustering method was proposed to deal with symbolic interval data. However, it still has noisy and outliers problems on these symbolic interval data. Hence, we propose Hausdorff distance measure based interval fuzzy possibilistic c-means clustering algorithm to overcome the interval fuzzy c-means clustering algorithm for the symbolic interval data clustering in noisy and outlier environments. From the simulation results, the proposed Hausdorff distance measure based interval fuzzy possibilistic c-means clustering algorithm has better performance than interval fuzzy c-means clustering algorithm. © 2013 TFSA.",International Journal of Fuzzy Systems,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893648628&partnerID=40&md5=2f94c1a2edbec16ca598013b6859671c,2013,2021-07-20 15:48:59,2021-07-20 15:48:59
VRF6UTTU,journalArticle,2016,"Gennari, R.; Vittorini, P.",Qualitative Temporal Reasoning Can Improve on Temporal Annotation Quality: How and Why,"The analysis of time helps in extracting knowledge from web contents. In order to analyze massive amounts of data, current natural language processing systems rely mainly on supervised approaches: machine learning algorithms that learn how to classify data based on corpora, annotated with the TimeML mark-up language or one of its derivatives. The quality of annotation data, thus, affects the performances of such systems. Quality can be improved by reasoning on temporal annotations. This article takes such a view. After a review of the strictly necessary background, it focuses on and discusses open issues in the area of quality of temporal annotations: inconsistency and incompleteness of annotations. Then it proposes a semantic reasoning approach as solution for improving on their quality, viz., the SOA-based Qualitative Temporal Reasoner for reasoning about temporal annotations, which leverages on existing theories and tools for qualitative reasoning. The article presents the design of the reasoner and its two main reasoning services: consistency checking for tackling inconsistency, and deduction for addressing incompleteness on demand. Afterward, the paper presents an experimental evaluation of the reasoner, sustaining why the chosen semantic reasoning approach can help improve on quality of annotations. The experiment assesses the reasoner’s performances on two different corpora and from several perspectives, e.g., the effectiveness of consistency checking in terms of the number of inconsistent documents found, and of deduction in terms of the number of annotations added. It concludes with discussion of the results of the evaluation and possible routes for future work. © 2016 Taylor & Francis.",Applied Artificial Intelligence,10.1080/08839514.2016.1214360,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994029426&doi=10.1080%2f08839514.2016.1214360&partnerID=40&md5=d9fabb57bad43a4a046804cb6abfba37,2016,2021-07-20 15:48:59,2021-07-20 15:48:59
KD52S268,journalArticle,2021,"Tiger, M.; Bergstrom, D.; Norrstig, A.; Heintz, F.",Enhancing Lattice-Based Motion Planning with Introspective Learning and Reasoning,"Lattice-based motion planning is a hybrid planning method where a plan is made up of discrete actions, while simultaneously also being a physically feasible trajectory. The planning takes both discrete and continuous aspects into account, for example action pre-conditions and collision-free action-duration in the configuration space. Safe motion planning rely on well-calibrated safety-margins for collision checking. The trajectory tracking controller must further be able to reliably execute the motions within this safety margin for the execution to be safe. In this work we are concerned with introspective learning and reasoning about controller performance over time. Normal controller execution of the different actions is learned using machine learning techniques with explicit uncertainty quantification, for safe usage in safety-critical applications. By increasing the model accuracy the safety margins can be reduced while maintaining the same safety as before. Reasoning takes place to both verify that the learned models stays safe and to improve collision checking effectiveness in the motion planner using more accurate execution predictions with a smaller safety margin. The presented approach allows for explicit awareness of controller performance under normal circumstances, and detection of incorrect performance in abnormal circumstances. Evaluation is made on the nonlinear dynamics of a quadcopter in 3D using simulation. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2021.3068550,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103258124&doi=10.1109%2fLRA.2021.3068550&partnerID=40&md5=3ca899cf207d4fd29b0e7f784991b42d,2021,2021-07-20 15:48:59,2021-07-20 15:48:59
WKY6S4B3,journalArticle,2019,"Ramírez-Duque, A.A.; Frizera-Neto, A.; Bastos, T.F.",Robot-Assisted Autism Spectrum Disorder Diagnostic Based on Artificial Reasoning,"Autism spectrum disorder (ASD) is a neurodevelopmental disorder that affects people from birth, whose symptoms are found in the early developmental period. The ASD diagnosis is usually performed through several sessions of behavioral observation, exhaustive screening, and manual coding behavior. The early detection of ASD signs in naturalistic behavioral observation may be improved through Child-Robot Interaction (CRI) and technological-based tools for automated behavior assessment. Robot-assisted tools using CRI theories have been of interest in intervention for children with Autism Spectrum Disorder (CwASD), elucidating faster and more significant gains from the diagnosis and therapeutic intervention when compared to classical methods. Additionally, using computer vision to analyze child’s behaviors and automated video coding to summarize the responses would help clinicians to reduce the delay of ASD diagnosis. In this article, a CRI to enhance the traditional tools for ASD diagnosis is proposed. The system relies on computer vision and an unstructured and scalable network of RGBD sensors built upon Robot Operating System (ROS) and machine learning algorithms for automated face analysis. Also, a proof of concept is presented, with participation of three typically developing (TD) children and three children in risk of suffering from ASD. © 2019, Springer Nature B.V.",Journal of Intelligent and Robotic Systems: Theory and Applications,10.1007/s10846-018-00975-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064197630&doi=10.1007%2fs10846-018-00975-y&partnerID=40&md5=725381edefec9ab255f1528b77b422c1,2019,2021-07-20 15:48:59,2021-07-20 15:48:59
FVSE7PZC,journalArticle,2020,"Yang, H.",Venture capital decision based on FPGA and machine learning,"Venture capital decision based Machine Learning is the Intelligence is a way to build, economic and execute stock market that allow you to make your own decisions. Global market intelligence technology attracts a wide range of people by making it an essential adaptation of various industries internal activities. It accelerates all internal (external or internal) tasks by increasing the focus on data-based decision-making methods. The traditional sub-sectors of the financial services venture capital industry have a lot to do with human interaction. Venture capital investment is considered a high-risk, high-return asset class. In the previous method based on IOT (Internet of Thing) and data mining for Venture capital decision. The existing method gives the data security, personal security, low data monitoring. The proposed method is based on FPGA (Field Programmable Gate Arrays) and Machine learning for Venture capital decision. Venture investment decisions can use machine learning to apply pre-optimized transactions, company data, founder data, etc. Understand how venture capitalists use artificial intelligence tools and methods to improve venture capital decisions. By focusing on the following eight value chain areas involved in commercial procurement, contract selection, valuation, transaction formation, post-investment appreciation, termination, and the company's internal and external enterprise-decision-making venture capital to improve human decision-making in the venture capital industry FPGA (Field Programmable Gate Arrays) tools and machine learning techniques can be used to Determine how to use artificial intelligence in such critical scenarios and venture capital decisions and the advantages and disadvantages of using artificial intelligence systems. © 2020 Elsevier B.V.",Microprocessors and Microsystems,10.1016/j.micpro.2020.103457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096616137&doi=10.1016%2fj.micpro.2020.103457&partnerID=40&md5=dfd8f2ec93b4280622c714e1fb3860c2,2020,2021-07-20 15:48:59,2021-07-20 15:48:59
2KQPE2X9,journalArticle,2019,"Seiffertt, J.",Adaptive Resonance Theory in the time scales calculus,"Engineering applications of algorithms based on Adaptive Resonance Theory have proven to be fast, reliable, and scalable solutions to modern industrial machine learning problems. A key emerging area of research is in the combination of different kinds of inputs within a single learning architecture along with ensuring the systems have the capacity for lifelong learning. We establish a dynamic equation model of ART in the time scales calculus capable of handling inputs in such mixed domains. We prove theorems establishing that the orienting subsystem can affect learning in the long-term memory storage unit as well as that those remembered exemplars result in stable categories. Further, we contribute to the mathematics of time scales literature itself with novel takes on logic functions in the calculus as well as new representations for the action of weight matrices in generalized domains. Our work extends the core ART theory and algorithms to these important mixed input domains and provides the theoretical foundation for further extensions of ART-based learning strategies for applied engineering work. © 2019 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2019.08.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071719328&doi=10.1016%2fj.neunet.2019.08.010&partnerID=40&md5=4c832d19a98dd3f875ca14861e744c41,2019,2021-07-20 15:48:59,2021-07-20 15:48:59
JPE9YDXC,journalArticle,2019,"Li, W.; Xie, L.; Wang, Z.",Two-Loop Covert Attacks Against Constant Value Control of Industrial Control Systems,"In the field of covert data integrity attacks, considerable attention has focused on two important issues. One is the issue of how to change the state of a plant, and the other is how to avoid being detected by anomaly detectors. A two-loop covert attack is presented to provide an integrated solution for these two issues. As an exploratory attempt to establish the feasibility of machine learning-based covert attacks, it applies the least squares support vector machine to constructing covert attacks. The proposed attack consists of an attack loop and a covert loop, which are based on an attack agent and a covert agent, respectively. The attack agent can move the steady state of a target plant to a desired state, and the covert agent can closely imitate the normal steady state of the plant to cover up the attack agent. In particular, the attack is directed to proportional-integral-derivative algorithms. Experiments are carried out to demonstrate the feasibility of the proposed attack and show the applicability of machine learning methods in constructing covert attacks. © 2005-2012 IEEE.",IEEE Transactions on Industrial Informatics,10.1109/TII.2018.2819677,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044363896&doi=10.1109%2fTII.2018.2819677&partnerID=40&md5=2f53d8a1734ce673f0f58d4e51834fa6,2019,2021-07-20 15:48:59,2021-07-20 15:48:59
FXXW7U9X,journalArticle,2020,"Abdel-Karim, B.M.; Pfeuffer, N.; Rohde, G.; Hinz, O.",How and What Can Humans Learn from Being in the Loop?: Invoking Contradiction Learning as a Measure to Make Humans Smarter,"This article discusses the counterpart of interactive machine learning, i.e., human learning while being in the loop in a human-machine collaboration. For such cases we propose the use of a Contradiction Matrix to assess the overlap and the contradictions of human and machine predictions. We show in a small-scaled user study with experts in the area of pneumology (1) that machine-learning based systems can classify X-rays with respect to diseases with a meaningful accuracy, (2) humans partly use contradictions to reconsider their initial diagnosis, and (3) that this leads to a higher overlap between human and machine diagnoses at the end of the collaboration situation. We argue that disclosure of information on diagnosis uncertainty can be beneficial to make the human expert reconsider her or his initial assessment which may ultimately result in a deliberate agreement. In the light of the observations from our project, it becomes apparent that collaborative learning in such a human-in-the-loop scenario could lead to mutual benefits for both human learning and interactive machine learning. Bearing the differences in reasoning and learning processes of humans and intelligent systems in mind, we argue that interdisciplinary research teams have the best chances at tackling this undertaking and generating valuable insights. © 2020, The Author(s).",KI - Kunstliche Intelligenz,10.1007/s13218-020-00638-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088029967&doi=10.1007%2fs13218-020-00638-x&partnerID=40&md5=b0004dcd73de2ff54771a89a5baf5579,2020,2021-07-20 15:48:59,2021-07-20 15:48:59
GBXKTI2T,journalArticle,2018,"Kleyko, D.; Osipov, E.; Papakonstantinou, N.; Vyatkin, V.",Hyperdimensional computing in industrial systems: The use-case of distributed fault isolation in a power plant,"This paper presents an approach for distributed fault isolation in a generic system of systems. The proposed approach is based on the principles of hyperdimensional computing. In particular, the recently proposed method called Holographic Graph Neuron is used. We present a distributed version of Holographic Graph Neuron and evaluate its performance on the problem of fault isolation in a complex power plant model. Compared to conventional machine learning methods applied in the context of the same scenario the proposed approach shows comparable performance while being distributed and requiring simple binary operations, which allow for a fast and efficient implementation in hardware. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2840128,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047613488&doi=10.1109%2fACCESS.2018.2840128&partnerID=40&md5=48b98c01d0d758e0a961d846d8bc0e13,2018,2021-07-20 15:48:59,2021-07-20 15:48:59
HMJCZNG7,journalArticle,2021,"Woodbright, M.; Verma, B.; Haidar, A.",Autonomous deep feature extraction based method for epileptic EEG brain seizure classification,"Epilepsy is a highly prevalent disorder that can affect a person's quality of life. People with epilepsy are commonly affected by reoccurring seizures that potentially cause injury or death. Neurologists frequently use Electroencephalography (EEG) recordings to diagnose people with epilepsy. However, this can be both laborious and error-prone, as this can rely on competency and insight from undertrained neurologists. Machine learning-based methods have been recently proposed for seizure detection so that neurologists can make a quick and correct diagnosis. However, these methods often require features of the EEG signal to be extracted from data before the data can be used. Furthermore, the choice of features often requires domain knowledge about the data, and depending on the expert knowledge of the user, the selection of which features to extract can have a dramatic impact on the classifier's performance. In this paper, we propose a novel method that can autonomously extract features from deep within a convolutional neural network (CNN) and generate easy to understand rules/explanations used for the classification of seizures from EEG signals. The aim of creating rules/explanations is to explain the internal logic of our method to give the neurologist insight into the decision-making process. Thus, distilling trust. We evaluate our method against other classifiers in terms of accuracy, sensitivity, and specificity, and achieve an accuracy of 98.65%, sensitivity of 96.29%, specificity of 99.25%. © 2021 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2021.02.052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104935316&doi=10.1016%2fj.neucom.2021.02.052&partnerID=40&md5=d52f76ca29077be9f7b692cce16a6d18,2021,2021-07-20 15:48:59,2021-07-20 15:48:59
2WUX3A5X,journalArticle,2020,"Lin, Y.; Huang, T.; Chung, W.; Ueng, Y.",Forecasting Fluctuations in the Financial Index Using a Recurrent Neural Network Based on Price Features,"Profits can be made from a trading strategy where long or short positions are placed in advance, based on the ability to forecast a future stock price or index, such as the closing or opening price. In addition to predicting stock index values, a prediction of the sign for the difference between closing and opening prices is important in order to earn a profit. This article presents an approach based on a Recurrent Neural Network (RNN) to forecast the opening price, the closing price, and the difference between them. Compared to previously reported approaches that were based on machine learning, the method proposed here emphasizes the pre-processing of the data, including the normalized first order difference method, as well as the focusing on the characteristics of the stock data, such as the zero-crossing rate (ZCR), which denotes the ratio of changes in the sign within a specific time interval. We propose a decision-making approach that is based on an estimate of both the ZCR and the cross-validation data so as to enhance the ability to forecast the difference between the opening and closing prices. We apply our technique to the S&amp;P500 (Standard &amp; Poor&#x2019;s 500) and the Dow Jones stock indices. As indicated by the results, our method can achieve a better performance compared to previous work. IEEE",IEEE Transactions on Emerging Topics in Computational Intelligence,10.1109/TETCI.2020.2971218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089764169&doi=10.1109%2fTETCI.2020.2971218&partnerID=40&md5=ad41447468d624e622592eb9fb40ec1d,2020,2021-07-20 15:48:59,2021-07-20 15:48:59
YT8G54QC,journalArticle,2018,"Mu, T.-Y.; Al-Fuqaha, A.; Shuaib, K.; Sallabi, F.M.; Qadir, J.",SDN flow entry management using reinforcement learning,"Modern information technology services largely depend on cloud infrastructures to provide their services. These cloud infrastructures are built on top of Datacenter Networks (DCNs) constructed with high-speed links, fast switching gear, and redundancy to offer better flexibility and resiliency. In this environment, network traffic includes long-lived (elephant) and short-lived (mice) flows with partitioned/aggregated traffic patterns. Although SDN-based approaches can efficiently allocate networking resources for such flows, the overhead due to network reconfiguration can be significant. With limited capacity of Ternary Content- Addressable Memory (TCAM) deployed in an OpenFlow enabled switch, it is crucial to determine which forwarding rules should remain in the flow table and which rules should be processed by the SDN controller in case of a table-miss on the SDN switch. This is needed in order to obtain the flow entries that satisfy the goal of reducing the long-term control plane overhead introduced between the controller and the switches. To achieve this goal, we propose a machine learning technique that utilizes two variations of Reinforcement Learning (RL) algorithms-the first of which is a traditional RL-based algorithm, while the other is deep reinforcement learning-based. Emulation results using the RL algorithm show around 60% improvement in reducing the long-term control plane overhead and around 14% improvement in the table-hit ratio compared to the Multiple Bloom Filters (MBF) method, given a fixed size flow table of 4KB. © 2018 Association for Computing Machinery.",ACM Transactions on Autonomous and Adaptive Systems,10.1145/3281032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061207256&doi=10.1145%2f3281032&partnerID=40&md5=efe4cf9520d38e785e96bb139289f185,2018,2021-07-20 15:48:59,2021-07-20 15:48:59
ILW2DACB,journalArticle,2021,"Liu, Y.; Zhou, C.; Zhang, F.; Zhang, Q.; Wang, S.; Zhou, J.; Sheng, F.; Wang, X.; Liu, W.; Wang, Y.; Yu, Y.; Lu, G.",Compare and contrast: Detecting mammographic soft-tissue lesions with C2-Net,"Detecting breast soft-tissue lesions including masses, structural distortions and asymmetries is of great importance due to the high risk leading to breast cancer. Most existing deep learning based approaches detect lesions with only unilateral images. However, multi-view mammogram images provide highly related and complementary information which helps to make the clinical analysis more comprehensive and reliable. In this paper, we propose a multi-view network for breast soft-tissue lesion detection called C2-Net (Compare and Contrast, C2) that fuses information across different views. The proposed model contains the following three modules. The spatial context enhancing (SCE) module compares ipsilateral views and extracts complementary features to model lesion inherent 3D structure. The multi-scale kernel pooling (MKP) module contrasts contralateral views with added misalignment tolerance. Finally, the logic guided fusion (LGF) module fuses multi-view features by enhancing logic modeling capacity. Experimental results on both the public DDSM dataset and the in-house multi-center dataset demonstrate that the proposed method has achieved state-of-the-art performance. © 2021",Medical Image Analysis,10.1016/j.media.2021.101999,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103304042&doi=10.1016%2fj.media.2021.101999&partnerID=40&md5=fdb0dcec5d2e0fe944bcd176bd28b567,2021,2021-07-20 15:49:00,2021-07-20 15:49:00
D95V9J3I,journalArticle,2015,"Osman, J.; Inglada, J.; Dejoux, J.-F.",Assessment of a Markov logic model of crop rotations for early crop mapping,"Detailed and timely information on crop area, production and yield is important for the assessment of environmental impacts of agriculture, for the monitoring of the land use and management practices, and for food security early warning systems. A machine learning approach is proposed to model crop rotations which can predict with good accuracy, at the beginning of the agricultural season, the crops most likely to be present in a given field using the crop sequence of the previous 3-5. years. The approach is able to learn from data and to integrate expert knowledge represented as first-order logic rules. Its accuracy is assessed using the French Land Parcel Information System implemented in the frame of the EU's Common Agricultural Policy. This assessment is done using different settings in terms of temporal depth and spatial generalization coverage. The obtained results show that the proposed approach is able to predict the crop type of each field, before the beginning of the crop season, with an accuracy as high as 60%, which is better than the results obtained with current approaches based on remote sensing imagery. © 2015 .",Computers and Electronics in Agriculture,10.1016/j.compag.2015.02.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924962439&doi=10.1016%2fj.compag.2015.02.015&partnerID=40&md5=9ce13859b43f59dba94560b2decc1867,2015,2021-07-20 15:49:00,2021-07-20 15:49:00
BW6N9BEU,journalArticle,2018,"Kumar, A.; Sharma, P.; Gupta, M.K.; Kumar, R.",Machine Learning Based Resource Utilization and Pre-estimation for Network on Chip (NoC) Communication,"Network on chip (NoC) is the solution to solve the problem of larger system on chip and bus based communication system. NoC provides scalable, highly reliable and modular approach for on chip communication and related problems. The wireless communication technologies such as IEEE 802.15.4 Zigbee technology follow mesh, star and cluster tree topology. The paper focuses on the development of machine learning model for design and FPGA synthesis of mesh, ring and fat tree NoC for different cluster size (N = 2, 4, 8, 16, 32, 64, 128 and 256). The fat-tree based topologies incorporate more links near the root of the tree, in order to fulfill the requirement for higher communication demand closer to the root of the tree, as compared to its leafs. It is an indirect topology in which not all routers are identical in terms of number of ports connecting to other routers or elements in the network. The research article presents the use of machine learning techniques to predict the FPGA resource utilization for NoC in advance. The present study helps in NoC chip planning before designing the chip itself by taking into account known hardware design parameters, memory utilization and timing parameters such as minimum and maximum period, frequency support etc. The machine learning is carried out based on multiple linear regression, decision tree regression and random forest regression which estimate the accuracy of the design and good performance. The interprocess communication among nodes is verified using Virtex-5 FPGA, in which data flows in packets and can vary up to ‘n’ bit. The designs are developed in Xilinx ISE 14.2 and simulated in Modelsim 10.1b with the help of VHDL programming language. The developed model has been validated and has performed well on independent test data. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Wireless Personal Communications,10.1007/s11277-018-5376-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045154166&doi=10.1007%2fs11277-018-5376-3&partnerID=40&md5=16e0b8184f7e6dd63c65cd98c966647e,2018,2021-07-20 15:49:00,2021-07-20 15:49:00
FKH98MKK,journalArticle,2018,"Peres, L.M.; Barbon Jr, S.; Fuzyi, E.M.; Barbon, A.P.A.C.; Barbin, D.F.; Maeda Saito, P.T.; Andreo, N.; Bridi, A.M.",Fuzzy approach for classification of pork into quality grades: coping with unclassifiable samples,"Meat classification methods are commonly based on quality parameters standardized by numeric ranges. However, some animal samples from different production chains do not match the current grades proposed. These unclassifiable samples are not capable to fit into a standard created by crisp range of values due to being infeasible toward its definition. An alternative to handle this kind of sample classification is the fuzzy logic, which could deal with uncertainty and ambiguity degree like human reasoning. In this work, we compare the traditional classification method and fuzzy approaches with the objective to handle the infeasible samples. This was compared to traditional pork standards using eleven real-life datasets with a total of 1798 samples described by pH, water holding capacity and/or L∗ value. The results demonstrated that traditional classification could not predict the unclassifiable samples. On the other hand, the fuzzy approaches improve significantly the number of classified samples. Performance of the fuzzy approaches were compared with several machine learning algorithms, but no significant statistical difference was observed. Finally, a real-life study case was explored, highlighting some advantages and further achievements of the fuzzy modeling. © 2018 Elsevier B.V.",Computers and Electronics in Agriculture,10.1016/j.compag.2018.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047604748&doi=10.1016%2fj.compag.2018.05.009&partnerID=40&md5=8cae37421b5bb9efe04c5387d24cd069,2018,2021-07-20 15:49:00,2021-07-20 15:49:00
IT2WMG4F,journalArticle,2021,"Varley, M.; Belle, V.",Fairness in machine learning with tractable models,"Machine Learning techniques have become pervasive across a range of different applications, and are now widely used in areas as disparate as recidivism prediction, consumer credit–risk analysis and insurance pricing. The prevalence of machine learning techniques has raised concerns about the potential for learned algorithms to become biased against certain groups. Many definitions have been proposed in the literature, but the fundamental task of reasoning about probabilistic events is a challenging one, owing to the intractability of inference. The focus of this paper is taking steps towards the application of tractable probabilistic models to fairness in machine learning. Tractable probabilistic models have recently emerged that guarantee that conditional marginal can be computed in time linear in the size of the model. In particular, we show that sum product networks (SPNs) enable an effective technique for determining the statistical relationships between protected attributes and other training variables. We will also motivate the concept of “fairness through percentile equivalence”, a new definition predicated on the notion that individuals at the same percentile of their respective distributions should be treated equivalently, and this prevents unfair penalisation of those individuals who lie at the extremities of their respective distributions. We compare the efficacy of this pre-processing technique with an alternative approach that assumes an additive contribution. It was found that when these two approaches were compared on a data set containing the results of law school applicants, the percentile equivalence method reduced the average underestimation in the exam score of ethnic minority applicants black applicants at the bottom end of their conditional distribution by about a fifth. We conclude by outlining potential improvements to our existing methodology and suggest opportunities for further work in this field. © 2021 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2020.106715,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100222887&doi=10.1016%2fj.knosys.2020.106715&partnerID=40&md5=8145ba329ddbe0d2bf74884c17de88dc,2021,2021-07-20 15:49:00,2021-07-20 15:49:00
WDR7PKIG,journalArticle,2021,"Liu, Y.; Yixuan, Y.; Liu, M.",Ground-Aware Monocular 3D Object Detection for Autonomous Driving,"Estimating the 3D position and orientation of objects in the environment with a single RGB camera is a critical and challenging task for low-cost urban autonomous driving and mobile robots. Most of the existing algorithms are based on the geometric constraints in 2D-3D correspondence, which stems from generic 6D object pose estimation. We first identify how the ground plane provides additional clues in depth reasoning in 3D detection in driving scenes. Based on this observation, we then improve the processing of 3D anchors and introduce a novel neural network module to fully utilize such application-specific priors in the framework of deep learning. Finally, we introduce an efficient neural network embedded with the proposed module for 3D object detection. We further verify the power of the proposed module with a neural network designed for monocular depth prediction. The two proposed networks achieve state-of-the-art performances on the KITTI 3D object detection and depth prediction benchmarks, respectively. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2021.3052442,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099723090&doi=10.1109%2fLRA.2021.3052442&partnerID=40&md5=ad9e4afea9251538caae54cb119f0496,2021,2021-07-20 15:49:00,2021-07-20 15:49:00
47NUL4SY,journalArticle,2021,"Dos Santos Gomes, D.C.; De Oliveira Serra, G.L.",Machine Learning Model for Computational Tracking and Forecasting the COVID-19 Dynamic Propagation,"A computational model with intelligent machine learning for analysis of epidemiological data, is proposed. The innovations of adopted methodology consist of an interval type-2 fuzzy clustering algorithm based on adaptive similarity distance mechanism for defining specific operation regions associated to the behavior and uncertainty inherited to epidemiological data, and an interval type-2 fuzzy version of Observer/Kalman Filter Identification (OKID) algorithm for adaptive tracking and real time forecasting according to unobservable components computed by recursive spectral decomposition of experimental epidemiological data. Experimental results and comparative analysis illustrate the efficiency and applicability of proposed methodology for adaptive tracking and real time forecasting the dynamic propagation behavior of novel coronavirus 2019 (COVID-19) outbreak in Brazil. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2021.3052134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099723379&doi=10.1109%2fJBHI.2021.3052134&partnerID=40&md5=1530b93ec006aeeb54063f8b8d13737d,2021,2021-07-20 15:49:00,2021-07-20 15:49:00
XIC5NJMS,journalArticle,2020,"Dai, M.; Hu, J.-M.",Field-free spin–orbit torque perpendicular magnetization switching in ultrathin nanostructures,"Magnetic-field-free current-controlled switching of perpendicular magnetization via spin–orbit torque (SOT) is necessary for developing a fast, long data retention, and high-density SOT magnetoresistive random access memory (MRAM). Here, we use both micromagnetic simulations and atomistic spin dynamics (ASD) simulations to demonstrate an approach to field-free SOT perpendicular magnetization switching without requiring any changes in the architecture of a standard SOT-MRAM cell. We show that this field-free switching is enabled by a synergistic effect of lateral geometrical confinement, interfacial Dyzaloshinskii–Moriya interaction (DMI), and current-induced SOT. Both micromagnetic and atomistic understanding of the nucleation and growth kinetics of the reversed domain are established. Notably, atomically resolved spin dynamics at the early stage of nucleation is revealed using ASD simulations. A machine learning model is trained based on 1000 groups of benchmarked micromagnetic simulation data. This machine learning model can be used to rapidly and accurately identify the nanomagnet size, interfacial DMI strength, and the magnitude of current density required for the field-free switching. © 2020, The Author(s).",npj Computational Materials,10.1038/s41524-020-0347-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086363762&doi=10.1038%2fs41524-020-0347-0&partnerID=40&md5=20b1ad6080035e5e2718e37e4306bf60,2020,2021-07-20 15:49:00,2021-07-20 15:49:00
MJ673VF6,journalArticle,2014,"Bridge, J.P.; Holden, S.B.; Paulson, L.C.",Machine learning for first-order theorem proving: Learning to select a good Heuristic,"We applied two state-of-the-art machine learning techniques to the problem of selecting a good heuristic in a first-order theorem prover. Our aim was to demonstrate that sufficient information is available from simple feature measurements of a conjecture and axioms to determine a good choice of heuristic, and that the choice process can be automatically learned. Selecting from a set of 5 heuristics, the learned results are better than any single heuristic. The same results are also comparable to the prover's own heuristic selection method, which has access to 82 heuristics including the 5 used by our method, and which required additional human expertise to guide its design. One version of our system is able to decline proof attempts. This achieves a significant reduction in total time required, while at the same time causing only a moderate reduction in the number of theorems proved. To our knowledge no earlier system has had this capability. © 2014 Springer Science+Business Media Dordrecht.",Journal of Automated Reasoning,10.1007/s10817-014-9301-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84903814860&doi=10.1007%2fs10817-014-9301-5&partnerID=40&md5=e248cc2a83d68db34395ca708c1b2742,2014,2021-07-20 15:49:01,2021-07-20 15:49:01
Z9YHAJZI,journalArticle,2019,"Kim, Y.; Pak, D.; Lee, J.",ScanAT: Identification of Bytecode-Only Smart Contracts with Multiple Attribute Tags,"Smart contracts on blockchain systems implement business logic and directly handle important assets. Although smart contracts play these critical roles, it is hard for users interacting with the system to understand the real behavior of the deployed bytecodes of smart contracts. The quirks of smart contracts, such as code reuse and limited unique datasets, make it challenging to recognize the functional details of smart contracts. In this paper, we propose a new method for characterizing bytecode-only smart contracts by automatically assigning multiple attribute tags. Using a deep learning approach, our system, the ScanAT, extracts attribute tags from the source code and metadata of known smart contracts and trains their bytecode with the attribute tags. The ScanAT then infers attribute tags from the bytecode of smart contracts alone. Our experiments show that ScanAT can achieve 81% accuracy in predicting attribute tags, using convolutional neural networks and a customized autoencoder. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2927003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079246413&doi=10.1109%2fACCESS.2019.2927003&partnerID=40&md5=d2bb56d803abc9e22938bc3268899c89,2019,2021-07-20 15:49:01,2021-07-20 15:49:01
Z6T53HM9,journalArticle,2019,"Eicher, T.; Patt, A.; Kautto, E.; Machiraju, R.; Mathé, E.; Zhang, Y.",Challenges in proteogenomics: A comparison of analysis methods with the case study of the DREAM proteogenomics sub-challenge,"Background: Proteomic measurements, which closely reflect phenotypes, provide insights into gene expression regulations and mechanisms underlying altered phenotypes. Further, integration of data on proteome and transcriptome levels can validate gene signatures associated with a phenotype. However, proteomic data is not as abundant as genomic data, and it is thus beneficial to use genomic features to predict protein abundances when matching proteomic samples or measurements within samples are lacking. Results: We evaluate and compare four data-driven models for prediction of proteomic data from mRNA measured in breast and ovarian cancers using the 2017 DREAM Proteogenomics Challenge data. Our results show that Bayesian network, random forests, LASSO, and fuzzy logic approaches can predict protein abundance levels with median ground truth-predicted correlation values between 0.2 and 0.5. However, the most accurately predicted proteins differ considerably between approaches. Conclusions: In addition to benchmarking aforementioned machine learning approaches for predicting protein levels from transcript levels, we discuss challenges and potential solutions in state-of-the-art proteogenomic analyses. © 2019 The Author(s).",BMC Bioinformatics,10.1186/s12859-019-3253-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076994533&doi=10.1186%2fs12859-019-3253-z&partnerID=40&md5=f911e0b3399431288d9e5dc762123fb1,2019,2021-07-20 15:49:01,2021-07-20 15:49:01
HR64R47Q,journalArticle,2018,"Deklel, A.K.; Hamdy, A.M.; Saad, E.M.",Multi-objective symbolic regression using long-term artificial neural network memory (LTANN-MEM) and neural symbolization algorithm (NSA),"Symbolic regression is commonly performed using evolutionary algorithms like genetic programming (GP). The goal of this research work is to construct symbolic models from examples where a new symbolic regression approach based on artificial neural networks is proposed. This approach is composed of a long-term artificial neural network memory (LTANN-MEM), a working memory (WM) in addition to a proposed neural symbolization algorithm (NSA) which uses LTANN-MEM and WM for synthesizing symbolic models equivalent to learning examples. The proposed LTANN-MEM is composed of two separate multilayer perceptron (MLP) feed-forward neural networks as well as the working memory which is composed of a single MLP. The core idea of the proposed approach is based on memorizing the learning experience of individual perceptrons in long-term memory (LTM), so they become available to be reused in generating and developing hypotheses about the learning examples. Although this idea is generic and could be used for the purpose of symbolization in general, it is applied here in symbolic regression for Boolean domain only. The obtained results show the ability of the proposed approach to search the solutions space using learning experience stored previously in LTM to guide the search process. A comparison is done with GP and found that the proposed NSA algorithm outperforms GP in its performance when increasing the number of inputs and outputs in the same problem by comparing the number of emerged candidate solutions in both approaches. © 2016, The Natural Computing Applications Forum.",Neural Computing and Applications,10.1007/s00521-016-2500-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980022403&doi=10.1007%2fs00521-016-2500-8&partnerID=40&md5=fe5fce6c69f22a6519c000f31ff3db98,2018,2021-07-20 15:49:01,2021-07-20 15:49:01
E79J9D2F,journalArticle,2019,"Alshammari, M.; Nasraoui, O.; Sanders, S.",Mining Semantic Knowledge Graphs to Add Explainability to Black Box Recommender Systems,"Recommender systems are being increasingly used to predict the preferences of users on online platforms and recommend relevant options that help them cope with information overload. In particular, modern model-based collaborative filtering algorithms, such as latent factor models, are considered state-of-the-art in recommendation systems. Unfortunately, these black box systems lack transparency, as they provide little information about the reasoning behind their predictions. White box systems, in contrast, can, by nature, easily generate explanations. However, their predictions are less accurate than sophisticated black box models. Recent research has demonstrated that explanations are an essential component in bringing the powerful predictions of big data and machine learning methods to a mass audience without compromising trust. Explanations can take a variety of formats, depending on the recommendation domain and the machine learning model used to make predictions. The objective of this work is to build a recommender system that can generate both accurate predictions and semantically rich explanations that justify the predictions. We propose a novel approach to build an explanation generation mechanism into a latent factor-based black box recommendation model. The designed model is trained to learn to make predictions that are accompanied by explanations that are automatically mined from the semantic web. Our evaluation experiments, which carefully study the trade-offs between the quality of predictions and explanations, show that our proposed approach succeeds in producing explainable predictions without a significant sacrifice in prediction accuracy. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2934633,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079639757&doi=10.1109%2fACCESS.2019.2934633&partnerID=40&md5=11d0c29f3d6a70fe8a95c79b12006977,2019,2021-07-20 15:49:01,2021-07-20 15:49:01
VHF2H43S,journalArticle,2017,"Kingston, J.",Using artificial intelligence to support compliance with the general data protection regulation,"The General Data Protection Regulation (GDPR) is a European Union regulation that will replace the existing Data Protection Directive on 25 May 2018. The most significant change is a huge increase in the maximum fine that can be levied for breaches of the regulation. Yet fewer than half of UK companies are fully aware of GDPR—and a number of those who were preparing for it stopped doing so when the Brexit vote was announced. A last-minute rush to become compliant is therefore expected, and numerous companies are starting to offer advice, checklists and consultancy on how to comply with GDPR. In such an environment, artificial intelligence technologies ought to be able to assist by providing best advice; asking all and only the relevant questions; monitoring activities; and carrying out assessments. The paper considers four areas of GDPR compliance where rule based technologies and/or machine learning techniques may be relevant: Following compliance checklists and codes of conduct; Supporting risk assessments; Complying with the new regulations regarding technologies that perform automatic profiling; Complying with the new regulations concerning recognising and reporting breaches of security. It concludes that AI technology can support each of these four areas. The requirements that GDPR (or organisations that need to comply with GDPR) state for explanation and justification of reasoning imply that rule-based approaches are likely to be more helpful than machine learning approaches. However, there may be good business reasons to take a different approach in some circumstances. © 2017, Springer Science+Business Media B.V.",Artificial Intelligence and Law,10.1007/s10506-017-9206-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028774071&doi=10.1007%2fs10506-017-9206-9&partnerID=40&md5=4dcf090c8eab4a1af7cab83a4f0b8b71,2017,2021-07-20 15:49:01,2021-07-20 15:49:01
DL37QJE8,journalArticle,2021,"Shanta, A.S.; Majumder, M.B.; Hasan, M.S.; Rose, G.S.",Physically Unclonable and Reconfigurable Computing System (PURCS) for Hardware Security Applications,"A physically unclonable and reconfigurable computing system is introduced which provides both logic locking and authentication of devices. A chaotic oscillator is required to generate the chaotic signals and can produce different Boolean functions using different tuning parameters, including a control bit, iteration number, threshold voltage, and bifurcation parameter. The aim of this article is to build a hybrid computing system with the mixed implementation of standard logic gates and reconfigurable chaos-based logic gates. The tuning parameters of the oscillator make up the secret key for logic locking. Process variation due to fabrication can be leveraged to generate unique keys for each chip. The whole computing system exhibits physical unclonable function (PUF) characteristics and can be used to generate challenge-response pairs (CRPs) for authenticating devices. We have used ISCAS'85 combinational benchmark circuits to demonstrate the results. The Hamming distance between correct and wrong outputs is calculated to ensure that 50% of the output bits are flipped when the wrong key is applied. A Boolean SAT attack has been carried out on the system and it displays exponential complexity with an increase in the total number of chaos gates and key size of each chaos gate. The hybrid system demonstrates near-ideal PUF metrics, including uniqueness, uniformity, and bit aliasing. Common machine learning attacks have been executed on the CRPs generated from the whole system and results show that the proposed chaos-based PUF is robust against modeling attacks. The hybrid system has significantly less overhead compared to traditional systems containing both logic locking and PUF circuitry. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.2999907,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087062510&doi=10.1109%2fTCAD.2020.2999907&partnerID=40&md5=432232653534f9eaf169b96714bdbd44,2021,2021-07-20 15:49:01,2021-07-20 15:49:01
WQJU4WUD,journalArticle,2020,"Koponen, V.","Conditional probability logic, lifted Bayesian networks, and almost sure quantifier elimination","We introduce a formal logical language, called conditional probability logic (CPL), which extends first-order logic and which can express probabilities, conditional probabilities and which can compare conditional probabilities. Intuitively speaking, although formal details are different, CPL can express the same kind of statements as some languages which have been considered in the artificial intelligence community. We also consider a way of making precise the notion of lifted Bayesian network, where this notion is a type of (lifted) probabilistic graphical model used in machine learning, data mining and artificial intelligence. A lifted Bayesian network (in the sense defined here) determines, in a natural way, a probability distribution on the set of all structures (in the sense of first-order logic) with a common finite domain D. Our main result (Theorem 3.14) is that for every “noncritical” CPL-formula φ(x¯) there is a quantifier-free formula φ⁎(x¯) which is “almost surely” equivalent to φ(x¯) as the cardinality of D tends towards infinity. This is relevant for the problem of making probabilistic inferences on large domains D, because (a) the problem of evaluating, by “brute force”, the probability of φ(x¯) being true for some sequence d¯ of elements from D has, in general, (highly) exponential time complexity in the cardinality of D, and (b) the corresponding probability for the quantifier-free φ⁎(x¯) depends only on the lifted Bayesian network and not on D. Some conclusions regarding the computational complexity of finding φ⁎ are given in Remark 3.17. The main result has two corollaries, one of which is a convergence law (and zero-one law) for noncritial CPL-formulas. © 2020 The Author",Theoretical Computer Science,10.1016/j.tcs.2020.08.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089656547&doi=10.1016%2fj.tcs.2020.08.006&partnerID=40&md5=63dd3b64dd91bd56c97a20bc9fd47f67,2020,2021-07-20 15:49:01,2021-07-20 15:49:01
343UXVIM,journalArticle,2017,"Lan, A.S.; Waters, A.E.; Studer, C.; Baraniuk, R.G.",BLAh: Boolean Logic Analysis for Graded Student Response Data,"Machine learning (ML) models and algorithms can enable a personalized learning experience for students in an inexpensive and scalable manner. At the heart of ML-driven personalized learning is the automated analysis of student responses to assessment items. Existing statistical models for this task enable the estimation of student knowledge and question difficulty solely from graded response data with only minimal effort from instructors. However, most existing student-response models are generalized linear models, meaning that they characterize the probability that a student answers a question correctly through a linear combination of their knowledge and the question's difficulty with respect to each concept that is being assessed. Such models cannot characterize complicated, nonlinear student-response associations and, hence, lack human interpretability in practice. In this paper, we propose a nonlinear student-response model called Boolean logic analysis (BLAh) that models a student's binary-valued graded response to a question as the output of a Boolean logic function. We develop a Markov chain Monte Carlo inference algorithm that learns the Boolean logic functions for each question solely from graded response data. A refined BLAh model improves the identifiability, tractability, and interpretability by considering a restricted set of ordered Boolean logic functions. Experimental results on a variety of real-world educational datasets demonstrate that BLAh not only achieves best-in-class prediction performance on unobserved student responses on some datasets but also provides easily interpretable parameters when questions are tagged with metadata by domain experts, which can provide useful feedback to instructors and content designers to improve the quality of assessment items. © 2007-2012 IEEE.",IEEE Journal on Selected Topics in Signal Processing,10.1109/JSTSP.2017.2722419,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023194336&doi=10.1109%2fJSTSP.2017.2722419&partnerID=40&md5=e8c8997edcf476dfccae0ffdd5e8e727,2017,2021-07-20 15:49:01,2021-07-20 15:49:01
CJSN76WR,journalArticle,2020,"Freund, M.",Ordered models for concept representation,"Basic notions linked with concept theory can be accounted for by partial order relations. These orders translate the fact that, for an agent, an object may be seen as a better or a more typical exemplar of a concept than anyother. They adequately model notions linked with categorial membership, typicality and resemblance, without any of the drawbacks that are classically encountered in conjunction theory. An interesting consequence of such a concept representation is the possibility of using the tools of non-monotonic logic to address some well-known problems of cognitive psychology. Thus, conceptual entailment and concept induction can be reexamined in the framework of preferential inference relations. This leads to a rigorous definition of the basic notions used in the study of category-based induction. © The Author(s) 2020. Published by Oxford University Press. All rights reserved.",Journal of Logic and Computation,10.1093/logcom/exaa034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096951109&doi=10.1093%2flogcom%2fexaa034&partnerID=40&md5=30450e2212cad8c742db39901f9f3542,2020,2021-07-20 15:49:01,2021-07-20 15:49:01
NYRSDNUL,journalArticle,2014,"Herman, G.L.; Zilles, C.; Loui, M.C.",A psychometric evaluation of the digital logic concept inventory,"Concept inventories hold tremendous promise for promoting the rigorous evaluation of teaching methods that might remedy common student misconceptions and promote deep learning. The measurements from concept inventories can be trusted only if the concept inventories are evaluated both by expert feedback and statistical scrutiny (psychometric evaluation). Classical Test Theory and Item Response Theory provide two psychometric frameworks for evaluating the quality of assessment tools. We discuss how these theories can be applied to assessment tools generally and then apply them to the Digital Logic Concept Inventory (DLCI). We demonstrate that the DLCI is sufficiently reliable for research purposes when used in its entirety and as a post-course assessment of students’ conceptual understanding of digital logic. The DLCI can also discriminate between students across a wide range of ability levels, providing the most information about weaker students’ ability levels. © 2014, Taylor & Francis.",Computer Science Education,10.1080/08993408.2014.970781,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908144731&doi=10.1080%2f08993408.2014.970781&partnerID=40&md5=7c62316b77c79ee6458624f19676fbf8,2014,2021-07-20 15:49:01,2021-07-20 15:49:01
Y7Z4S598,journalArticle,2021,"Tiwari, P.; Zhu, H.; Pandey, H.M.",DAPath: Distance-aware knowledge graph reasoning based on deep reinforcement learning,"Knowledge graph reasoning aims to find reasoning paths for relations over incomplete knowledge graphs (KG). Prior works may not take into account that the rewards for each position (vertex in the graph) may be different. We propose the distance-aware reward in the reinforcement learning framework to assign different rewards for different positions. We observe that KG embeddings are learned from independent triples and therefore cannot fully cover the information described in the local neighborhood. To this effect, we integrate a graph self-attention (GSA) mechanism to capture more comprehensive entity information from the neighboring entities and relations. To let the model remember the path, we incorporate the GSA mechanism with GRU to consider the memory of relations in the path. Our approach can train the agent in one-pass, thus eliminating the pre-training or fine-tuning process, which significantly reduces the problem complexity. Experimental results demonstrate the effectiveness of our method. We found that our model can mine more balanced paths for each relation. © 2020 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2020.11.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097578405&doi=10.1016%2fj.neunet.2020.11.012&partnerID=40&md5=a2caa01cae010212d970d69a2c391266,2021,2021-07-20 15:49:01,2021-07-20 15:49:01
NPTZVV3I,journalArticle,2021,"Yan, R.; Yu, Y.; Qiu, D.",Emotion-enhanced classification based on fuzzy reasoning,"Texts and emoticons expressing sentiment can be used to analyse emotion. In an Internet environment, emoticons are frequently used, which have explicated information for emotion analysis. Considering the characteristics of short texts including sparseness, non-standardization and ambiguities in a subject, two models based on word embedding, emotion-dictionary and fuzzy reasoning are proposed: the low-dimensional hybrid feature model and the emotion-enhanced inference model. The low-dimensional hybrid feature model includes the number of emoticons, the emotion-word number and the negative-word number in a text. The emotion-enhanced reference model includes some fuzzy reasoning rules and a variety of the combinations of emotion-words, negative-words, and question marks and exclamation points. The validity of the model has been verified based on Douyin reviews and the data of the 2nd CCF Conference on Natural Language Processing and Chinese Computing (NLPCC 2013), where the average accuracy rate on Douyin reviews achieved is 89.16 %. Through the comparative experiment, the results show that the models are more effective in ultra-short emotion text classification than the comparison models. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",International Journal of Machine Learning and Cybernetics,10.1007/s13042-021-01356-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108818937&doi=10.1007%2fs13042-021-01356-y&partnerID=40&md5=93fc1af8542cadcb177bf5887c64c4ff,2021,2021-07-20 15:49:01,2021-07-20 15:49:01
QIWZN4UI,journalArticle,2021,"Yan, H.; Song, C.",Multi-scale deep relational reasoning for facial kinship verification,"In this paper, we propose a deep relational network which exploits multi-scale information of facial images for kinship verification. Unlike most existing deep learning based facial kinship verification methods which employ convolutional neural networks to extract holistic features, we present a deep model to exploit facial kinship relationship from local regions. For each given pair of face images, our method uses two convolutional neural networks which share parameters to extract different scales of features, which are expected to provide global contextual information of face images. We split a set of features at the same scale into multiple groups, where different groups capture information of different local regions. For each pair of local feature groups which are extracted from the same scale and position, we propose a relation network to reason their relationship, and use a verification network to infer the kin relation based on the results of local relations from different facial regions. We conduct experiments on two widely used facial kinship datasets: KinFaceW-I and KinFaceW-II, and our experimental results are presented to demonstrate the effectiveness of our approach. © 2020 Elsevier Ltd",Pattern Recognition,10.1016/j.patcog.2020.107541,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087750073&doi=10.1016%2fj.patcog.2020.107541&partnerID=40&md5=b100bbb8ec286e8ff477c963ec05d600,2021,2021-07-20 15:49:02,2021-07-20 15:49:02
DWXF6YVD,journalArticle,2019,"Coulibaly, S.; Kamsu-Foguem, B.; Kamissoko, D.; Traore, D.",Deep neural networks with transfer learning in millet crop images,"Plant or crop diseases are important items in the reduction of quality and quantity in agriculture. Therefore, the detection and diagnosis of these diseases are very necessary. The appropriate classification with small datasets in Deep Learning is a major scientific challenge. Furthermore, it is difficult and expensive to generate labeled data manually according to certain selection criteria. The approaches using transfer learning aims to resolve this problem by recognizing and applying knowledge and abilities learned in previous tasks to novel tasks (in new domains). In this paper, we propose an approach using transfer learning with feature extraction to build an identification system of mildew disease in pearl millet. The deep learning facilitates a practically fast and interesting data analysis in precision agriculture. The expected advantage of the proposal is to provide support to stakeholders (researchers and farmers) through the information and knowledge generated by the reasoning process. The experimental result gives an encouraging performance that is the accuracy of 95.00%, the precision of 90.50%, the recall of 94.50% and the f1-score of 91.75%. © 2019 Elsevier B.V.",Computers in Industry,10.1016/j.compind.2019.02.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062554065&doi=10.1016%2fj.compind.2019.02.003&partnerID=40&md5=3e60c59b212cabc1261815a4f6860ecc,2019,2021-07-20 15:49:02,2021-07-20 15:49:02
4AYRBJ85,journalArticle,2020,"Yirui, Z.; Jian, S.",Analysis of vibration high-frequency dynamic characteristics of wheelless rail vehicle system based on fuzzy logic control,"Track irregularity is the main source of vibration excitation of vehicle-track weighing system, which has an important impact on the safety, stability and comfort of train operation, and it is also the main factor limiting the speed of train operation. Higher requirements are put forward for track smoothness with the rapid development of China's railways. Therefore, it has a great theoretical and practical significance to study the relationship between track irregularity and random vibration of vehicle-track cooker-in system and the evaluation method of track smoothness. In this paper, a model construction method is proposed based on machine learning fuzzy logic control and neural network algorithm to analyze the high frequency dynamic characteristics of ballastless track wheel-rail vehicle system. Firstly, a numerical analysis model of high frequency dynamic characteristics of ballastless track wheel-rail vehicle system is established by using linear discrete elastic-yellow damper element connection model; Secondly, the Rough Set Block Neural Network is introduced to optimize the dynamic characteristic analysis model, and the intelligent model of model optimization analysis is established. Finally, the validity of the proposed algorithm is verified by simulation experiments of practical examples. © 2020 - IOS Press and the authors. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-179920,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091075865&doi=10.3233%2fJIFS-179920&partnerID=40&md5=689c3c3b2818b751eebc9bed96725a53,2020,2021-07-20 15:49:02,2021-07-20 15:49:02
5A7944LD,journalArticle,2014,"Muggleton, S.",Alan turing and the development of artificial intelligence,"During the centennial year of his birth Alan Turing (1912-1954) has been widely celebrated as having laid the foundations for Computer Science, Automated Decryption, Systems Biology and the Turing Test. In this paper we investigate Turing's motivations and expectations for the development of Machine Intelligence, as expressed in his 1950 article in Mind. We show that many of the trends and developments within AI over the last 50 years were foreseen in this foundational paper. In particular, Turing not only describes the use of Computational Logic but also the necessity for the development of Machine Learning in order to achieve human-level AI within a 50 year time-frame. His description of the Child Machine (a machine which learns like an infant) dominates the closing section of the paper, in which he provides suggestions for how AI might be achieved. Turing discusses three alternative suggestions which can be characterised as: (1) AI by programming, (2) AI by ab initio machine learning and (3) AI using logic, probabilities, learning and background knowledge. He argues that there are inevitable limitations in the first two approaches and recommends the third as the most promising. We compare Turing's three alternatives to developments within AI, and conclude with a discussion of some of the unresolved challenges he posed within the paper. © 2014 - IOS Press and the authors.",AI Communications,10.3233/AIC-130579,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84889667265&doi=10.3233%2fAIC-130579&partnerID=40&md5=8f35b384c0c2190c011f3749a71f29c8,2014,2021-07-20 15:49:02,2021-07-20 15:49:02
U93UFH7K,journalArticle,2021,"Grover, H.; Alladi, T.; Chamola, V.; Singh, D.; Choo, K.R.",Edge Computing and Deep Learning Enabled Secure Multi-Tier Network for Internet of Vehicles,"Internet of Vehicles (IoV) are fast becoming the norm in our society, but such a trend also comes with its own set of challenges (e.g., new security and privacy risks due to the expanded attack vectors). In this work, we propose an edge computing-based secure, efficient, and intelligent multi-tier heterogeneous IoV network. We first discuss the functionality and objectives of such an architecture. Then, we demonstrate how unsupervised deep learning techniques can facilitate the identification of suspicious vehicle behavior and ensure the security of such an architecture. The findings from our evaluations demonstrate the learning spatio-temporal information and parameter efficiency of the proposed stacked LSTM model over single LSTMs. IEEE",IEEE Internet of Things Journal,10.1109/JIOT.2021.3071362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103877770&doi=10.1109%2fJIOT.2021.3071362&partnerID=40&md5=dfac592cd170901e0c32abd448de93d4,2021,2021-07-20 15:49:02,2021-07-20 15:49:02
DNQ648DW,journalArticle,2014,"Bermejo-Das-Neves, C.; Nguyen, H.-N.; Poch, O.; Thompson, J.D.",A comprehensive study of small non-frameshift insertions/deletions in proteins and prediction of their phenotypic effects by a machine learning method (KD4i),"Background: Small insertion and deletion polymorphisms (Indels) are the second most common mutations in the human genome, after Single Nucleotide Polymorphisms (SNPs). Recent studies have shown that they have significant influence on genetic variation by altering human traits and can cause multiple human diseases. In particular, many Indels that occur in protein coding regions are known to impact the structure or function of the protein. A major challenge is to predict the effects of these Indels and to distinguish between deleterious and neutral variants. When an Indel occurs within a coding region, it can be either frameshifting (FS) or non-frameshifting (NFS). FS-Indels either modify the complete C-terminal region of the protein or result in premature termination of translation. NFS-Indels insert/delete multiples of three nucleotides leading to the insertion/deletion of one or more amino acids.Results: In order to study the relationships between NFS-Indels and Mendelian diseases, we characterized NFS-Indels according to numerous structural, functional and evolutionary parameters. We then used these parameters to identify specific characteristics of disease-causing and neutral NFS-Indels. Finally, we developed a new machine learning approach, KD4i, that can be used to predict the phenotypic effects of NFS-Indels.Conclusions: We demonstrate in a large-scale evaluation that the accuracy of KD4i is comparable to existing state-of-the-art methods. However, a major advantage of our approach is that we also provide the reasons for the predictions, in the form of a set of rules. The rules are interpretable by non-expert humans and they thus represent new knowledge about the relationships between the genotype and phenotypes of NFS-Indels and the causative molecular perturbations that result in the disease. © 2014 Bermejo-Das-Neves et al.; licensee BioMed Central Ltd.",BMC Bioinformatics,10.1186/1471-2105-15-111,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899967710&doi=10.1186%2f1471-2105-15-111&partnerID=40&md5=ec9b1cbaba981365f0327dad1be20345,2014,2021-07-20 15:49:02,2021-07-20 15:49:02
8KE4F2HP,journalArticle,2021,"Rios, V.D.M.; Inácio, P.R.M.; Magoni, D.; Freire, M.M.",Detection of reduction-of-quality DDoS attacks using Fuzzy Logic and machine learning algorithms,"Distributed Denial of Service (DDoS) attacks are still among the most dangerous attacks on the Internet. With the advance of methods for detecting and mitigating these attacks, crackers have improved their skills in creating new DDoS attack types with the aim of mimicking normal traffic behavior therefore becoming silently powerful. Among these advanced DDoS attack types, the so-called low-rate DoS attacks aim at keeping a low level of network traffic. In this paper, we study one of these techniques, called Reduction of Quality (RoQ) attack. To investigate the detection of this type of attack, we evaluate and compare the use of four machine learning algorithms: Multi-Layer Perceptron (MLP) neural network with backpropagation, K-Nearest Neighbors (K-NN), Support Vector Machine (SVM) and Multinomial Naive Bayes (MNB). We also propose an approach for detecting this kind of attack based on three methods: Fuzzy Logic (FL), MLP and Euclidean Distance (ED). We evaluate and compare the approach based on FL, MLP and ED to the above machine learning algorithms using both emulated and real traffic traces. We show that among the four Machine Learning algorithms, the best classification results are obtained with MLP, which, for emulated traffic, leads to a F1-score of 98.04% for attack traffic and 99.30% for legitimate traffic, while, for real traffic, it leads to a F1-score of 99.87% for attack traffic and 99.95% for legitimate traffic. Regarding the approach using FL, MLP and EC, for classification of emulated traffic, we obtained a F1-score of 98.80% for attack traffic and 99.60% for legitimate traffic, while, for real traffic, we obtained a F1-score of 100% for attack traffic and 100% for legitimate traffic. However, the better performance of the approach based on FL, MLP and ED is obtained at the cost of larger execution time, since MLP required 0.74 ms and 0.87 ms for classification of the emulated and real traffic datasets, respectively, where as the approach using FL, MLP and ED required 11'46” and 46'48” to classify the emulated and real traffic datasets, respectively. © 2021 Elsevier B.V.",Computer Networks,10.1016/j.comnet.2020.107792,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098946918&doi=10.1016%2fj.comnet.2020.107792&partnerID=40&md5=8b0918804b65dd4a1f14e90e19a283b9,2021,2021-07-20 15:49:02,2021-07-20 15:49:02
EJAZZH8D,journalArticle,2020,"Yu, J.; Liu, G.",Knowledge-based deep belief network for machining roughness prediction and knowledge discovery,"The surface roughness prediction and knowledge discovery in the machining process are very important to optimize those process variables (e.g., spin speed, feed rate) online and then obtain high machining quality of products. Deep neural networks (DNNs) consist of a complex structure and multiple nonlinear processing units to perform deep feature learning. It has achieved great success in computer vision, natural language processing, and speech recognition. It is very appropriate to apply DNNs for modeling complex non-linear relationship between the process variables (e.g., spin speed, vibration) and the surface roughness. Due to the “black box” and the huge data demand problem, there are still huge obstacles to the applications of DNNs in real-world cases. This paper proposes a new DNN model, knowledge-based deep belief network (KBDBN), which integrates the symbol rules and classification rules with the deep network. This not only enables the model to have good feature learning performance, but also can discover the knowledge (i.e., rules) from the deep network. A KBDBN-based prediction model for workpiece surface roughness prediction is further proposed, which not only effectively discovery the knowledge (i.e., symbolic rules and classification rules) for process control, but also shows better recognition performance than that of the typical machine learning models (e.g., support vector machine, artificial neural network, logistic regression) and DNNs (e.g., deep belief network, stacked auto-encoder). Moreover, the interpretable DNN model makes it be applied easily in real-world cases. © 2020 Elsevier B.V.",Computers in Industry,10.1016/j.compind.2020.103262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086466390&doi=10.1016%2fj.compind.2020.103262&partnerID=40&md5=0979e7a89e18d6d2615a9188f4f8a8e1,2020,2021-07-20 15:49:02,2021-07-20 15:49:02
JGIJ5XT2,journalArticle,2017,"Hadj Mabrouk, H.",Contribution of learning CHARADE system of rules for the prevention of rail accidents,"The modes of reasoning employed by the domain experts to analyze and assess the safety of railway transport and the very nature of knowledge about safety mean that a conventional computing solution is unsuitable and the utilization of artificial intelligence techniques would seem to be more appropriate. In artificial intelligence, we perceive two major independent research activities: the acquisition of knowledge which to better understand the transfer of expertise and the machine learning proposing the implementation of inductive, deductive, abductive techniques or by analogy to equip the system of learning abilities. This paper describes our contribution to improving the usual safety analysis methods used in the certification of railway transport systems in France. The methodology is based on the complementary and simultaneous use of knowledge acquisition and machine learning. The purpose is contributed to the generation of new accident scenarios that could help experts to conclude on the safe character of a new rail transport system. ©2017 - IOS Press and the authors. All rights reserved.",Intelligent Decision Technologies,10.3233/IDT-170304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040056966&doi=10.3233%2fIDT-170304&partnerID=40&md5=db21645018d7d8f44ce852aaed34aef8,2017,2021-07-20 15:49:02,2021-07-20 15:49:02
WDWYXV4W,journalArticle,2020,"Chakraborty, M.; Biswas, S.K.; Purkayastha, B.",Rule extraction from neural network trained using deep belief network and back propagation,"Representing the knowledge learned by neural networks in the form of interpretable rules is a prudent technique to justify the decisions made by neural networks. Heretofore many algorithms exist to extract symbolic rules from neural networks, but among them, a few extract rules from deep neural networks trained using deep learning techniques. So, this paper proposes an algorithm to extract rules from a multi-hidden layer neural network, pre-trained using deep belief network and fine-tuned using back propagation. The algorithm analyzes each node of a layer and extracts knowledge from each layer separately. The process of knowledge extraction from the first hidden layer is different from the other layers. Consecutively, the algorithm combines all the knowledge extracted and refines them to construct a final ruleset consisting of symbolic rules. The algorithm further subdivides the subspace of a rule in the ruleset if it satisfies certain conditions. Results show that the algorithm extracted rules with higher accuracy compared to some existing rule extraction algorithms. Other than accuracy, the efficacy of the extracted rules is also validated with fidelity and various other performance measures. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",Knowledge and Information Systems,10.1007/s10115-020-01473-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085080273&doi=10.1007%2fs10115-020-01473-0&partnerID=40&md5=5fe2c035d5d7c117d36315294ca03069,2020,2021-07-20 15:49:02,2021-07-20 15:49:02
F82SSRUW,journalArticle,2016,"Panda, M.; Abraham, A.; Tripathy, B.K.",Soft granular computing based classification using hybrid fuzzy-KNN-SVM,"This paper aims at providing the concept of information granulation in Granular computing based pattern classification that is used to deal with incomplete, unreliable, uncertain knowledge from the view of a dataset. Data Discretization provides us the granules which further can be used to classify the instances. We use Equal width and Equal frequency Discretization as unsupervised ones; Fayyad-Irani's Minimum description length and Kononenko's supervised discretization approaches along with Fuzzy logic, neural network, Support vector machine and their hybrids to develop an efficient granular information processing paradigm. The experimental results show the effectiveness of our approach. We use benchmark datasets in UCI Machine Learning Repository in order to verify the performance of granular computing based approach in comparison with other existing approaches. Finally, we perform statistical significance test for confirming validity of the results obtained. © 2016 IOS Press and the authors. All rights reserved.",Intelligent Decision Technologies,10.3233/IDT-150243,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960844134&doi=10.3233%2fIDT-150243&partnerID=40&md5=144a1af2b7b1fa9981aecf8a29c273b8,2016,2021-07-20 15:49:02,2021-07-20 15:49:02
GX5IR2BJ,journalArticle,2021,"Dasgupta, P.; Hughes, J.A.; Daley, M.; Sejdić, E.",Is Human Walking a Network Medicine Problem? An Analysis Using Symbolic Regression Models with Genetic Programming,"Background and Objective: Human walking is typically assessed using a sensor placed on the lower back or the hip. Such analyses often ignore that the arms, legs, and body trunk movements all have significant roles during walking; in other words, these body nodes with accelerometers form a body sensor network (BSN). BSN refers to a network of wearable sensors or devices on the human body that collects physiological signals. Our study proposes that human locomotion could be considered as a network of well-connected nodes. Methods: While hypothesizing that accelerometer data can model this BSN, we collected accelerometer signals from six body areas from ten healthy participants performing a cognitive task. Machine learning based on genetic programming was used to produce a collection of non-linear symbolic models of human locomotion. Results: With implications in precision medicine, our primary finding was that our BSN models fit the data from the lower back's accelerometer and describe subject-specific data the best compared to all other models. Across subjects, models were less effective due to the diversity of human sizes. Conclusions: A BSN relationship between all six body nodes has been shown to describe the subject-specific data, which indicates that the network-medicine relationship between these nodes is essential in adequately describing human walking. Our gait analyses can be used for several clinical applications such as medical diagnostics as well as creating a baseline for healthy walking with and without a cognitive load. © 2021",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2021.106104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105019551&doi=10.1016%2fj.cmpb.2021.106104&partnerID=40&md5=22700d644bfb1d222c6cf8296dbc9e2d,2021,2021-07-20 15:49:02,2021-07-20 15:49:02
WDGGXRFT,journalArticle,2021,"Žegklitz, J.; Pošík, P.",Benchmarking state-of-the-art symbolic regression algorithms,"Symbolic regression (SR) is a powerful method for building predictive models from data without assuming any model structure. Traditionally, genetic programming (GP) was used as the SR engine. However, for these purely evolutionary methods it was quite hard to even accommodate the function to the range of the data and the training was consequently inefficient and slow. Recently, several SR algorithms emerged which employ multiple linear regression. This allows the algorithms to create models with relatively small error right from the beginning of the search. Such algorithms are claimed to be by orders of magnitude faster than SR algorithms based on classic GP. However, a systematic comparison of these algorithms on a common set of problems is still missing and there is no basis on which to decide which algorithm to use. In this paper we conceptually and experimentally compare several representatives of such algorithms: GPTIPS, FFX, and EFS. We also include GSGP-Red, which is an enhanced version of geometric semantic genetic programming, an important algorithm in the field of SR. They are applied as off-the-shelf, ready-to-use techniques, mostly using their default settings. The methods are compared on several synthetic SR benchmark problems as well as real-world ones ranging from civil engineering to aerodynamics and acoustics. Their performance is also related to the performance of three conventional machine learning algorithms: multiple regression, random forests and support vector regression. The results suggest that across all the problems, the algorithms have comparable performance. We provide basic recommendations to the user regarding the choice of the algorithm. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Genetic Programming and Evolvable Machines,10.1007/s10710-020-09387-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083357133&doi=10.1007%2fs10710-020-09387-0&partnerID=40&md5=b5bb64cd927189fe1c4ba922c5bca694,2021,2021-07-20 15:49:02,2021-07-20 15:49:02
XWFGXQVI,journalArticle,2016,"Chang, J.W.; Lee, M.C.; Wang, T.I.",Integrating a semantic-based retrieval agent into case-based reasoning systems: A case study of an online bookstore,"Natural language search engines should be developed to provide a friendly environment for business-to-consumer e-commerce that reduce the fatigue customers experience and help them decide what to buy. To support product information retrieval and reuse, this paper presents a novel framework for a case-based reasoning system that includes a collaborative filtering mechanism and a semantic-based case retrieval agent. Furthermore, the case retrieval agent integrates short-text semantic similarity (STSS) and recognizing textual entailment (RTE). The proposed approach was evaluated using competitive methods in the performance of STSS and RTE, and according to the results, the proposed approach outperforms most previously described approaches. Finally, the effectiveness of the proposed approach was investigated using a case study of an online bookstore, and according to the results of case study, the proposed approach outperforms a compared system using string similarity and an existing e-commerce system, Amazon. © 2015 Elsevier B.V. All rights reserved.",Computers in Industry,10.1016/j.compind.2015.10.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949948395&doi=10.1016%2fj.compind.2015.10.007&partnerID=40&md5=5d50ebbcb2986b1ddeb5d29317d902b8,2016,2021-07-20 15:49:02,2021-07-20 15:49:02
YNXWV2VC,journalArticle,2015,"Yan, A.; Shao, H.; Wang, P.",A soft-sensing method of dissolved oxygen concentration by group genetic case-based reasoning with integrating group decision making,"To estimate the dissolved oxygen (DO) concentration in the wastewater treatment process more accurately, a group genetic case-based reasoning (GGCBR) soft-sensing method of DO concentration is proposed by the integrated use of genetic algorithms (GA) and group decision making (GDM) in this paper. First, an attribute weight iterative optimization using GA is discussed in the case retrieval phase. Then, a case reuse method is carried out by using the GDM theory. Finally, the historical data of DO concentration in a wastewater treatment process are used to carry out a comparison experiment by 10-fold cross validation. The results show that this proposed GGCBR is superior to other methods and significantly reduces the fitting error of DO concentration. © 2015 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2014.07.081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938205604&doi=10.1016%2fj.neucom.2014.07.081&partnerID=40&md5=cdbfc554b932abe8b3702b5b16401a8e,2015,2021-07-20 15:49:03,2021-07-20 15:49:03
6D9FGR2F,journalArticle,2018,"Zhao, J.; Hei, X.; Shi, Z.; Dong, L.; Liu, Y.; Yan, R.; Li, X.",Regression learning based on incomplete relationships between attributes,"In recent years, machine learning researchers have focused on methods to construct flexible and interpretable regression models. However, the method of obtaining complete knowledge from incomplete and fuzzy prior knowledge and the trade-off between the generalization performance and the interpretability of the model are very important factors to consider. In this paper, we propose a new regression learning method. Complete relationships are obtained from the incomplete fuzzy relationships between attributes by using Markov logic networks [29]. The complete relationships are then applied to constrain the shape of the regression model in the optimization procedure to solve the trade-off problem. Finally, the benefits of our approach are illustrated on benchmark data sets and in real-world experiments. © 2017 Elsevier Inc.",Information Sciences,10.1016/j.ins.2017.09.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029445554&doi=10.1016%2fj.ins.2017.09.023&partnerID=40&md5=68bc8ba9680a10683af2b1ca642f64e4,2018,2021-07-20 15:49:03,2021-07-20 15:49:03
JE7GR44Q,journalArticle,2020,"Shaikhha, A.; Elseidy, M.; Mihaila, S.; Espino, D.; Koch, C.",Synthesis of incremental linear algebra programs,"This article targets the Incremental View Maintenance (IVM) of sophisticated analytics (such as statistical models, machine learning programs, and graph algorithms) expressed as linear algebra programs. We present LAGO, a unified framework for linear algebra that automatically synthesizes efficient incremental trigger programs, thereby freeing the user from error-prone manual derivations, performance tuning, and low-level implementation details. The key technique underlying our framework is abstract interpretation, which is used to infer various properties of analytical programs. These properties give the reasoning power required for the automatic synthesis of efficient incremental triggers. We evaluate the effectiveness of our framework on a wide range of applications from regression models to graph computations. © 2020 ACM.",ACM Transactions on Database Systems,10.1145/3385398,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092364799&doi=10.1145%2f3385398&partnerID=40&md5=98a0a9be95e43fb97f4f9f1a602dee86,2020,2021-07-20 15:49:03,2021-07-20 15:49:03
CUIWJLIZ,journalArticle,2017,"Benavoli, A.; Corani, G.; Demšar, J.; Zaffalon, M.",Time for a change: A tutorial for comparing multiple classifiers through Bayesian analysis,"The machine learning community adopted the use of null hypothesis significance testing (NHST) in order to ensure the statistical validity of results. Many scientific fields however realized the shortcomings of frequentist reasoning and in the most radical cases even banned its use in publications. We should do the same: just as we have embraced the Bayesian paradigm in the development of new machine learning methods, so we should also use it in the analysis of our own results. We argue for abandonment of NHST by exposing its fallacies and, more importantly, offer better—more sound and useful—alternatives for it. © 2017 Alessio Benavoli, Giorgio Corani, Janez Demsar, Marco Zaffalon.",Journal of Machine Learning Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030157696&partnerID=40&md5=d7bc95ef19ee5e8731d281ea25f37e35,2017,2021-07-20 15:49:03,2021-07-20 15:49:03
F3LKUNLQ,journalArticle,2016,"Hadjimichael, A.; Comas, J.; Corominas, L.",Do machine learning methods used in data mining enhance the potential of decision support systems? A review for the urban water sector,"With sustainable development as their overarching goal, Urban Water System (UWS) managers need to take into account all social, economic, technical and environmental facets related to their decisions. Decision support systems (DSS) have been used widely for handling such complexity in water treatment, having a high level of popularity as academic exercises, although little validation and few full-scale implementations reported in practice. The objective of this paper is to review the application of artificial intelligence methods (mainly machine learning) to UWS and to investigate the integration of these methods into DSS. The results of the review suggest that artificial neural networks is the most popular method in the water and wastewater sectors followed by clustering. Bayesian networks and swarm intelligence/optimization have shown a spectacular increase in the water sector in the last 10 years, being the latest techniques to be incorporated but overtaking case-based reasoning. Whereas artificial intelligence applications to the water sector focus on modelling, optimization or data mining for knowledge generation, their encapsulation into functional DSS is not fully explored. Few academic applications have made it into decision making practice. We believe that the reason behind this misuse is not related to the methods themselves but rather to the disassociation between the fields of water and computer engineering, the limited practical experience of academics, and the great complexity inherently present. © 2016 - IOS Press and the authors. All rights reserved.",AI Communications,10.3233/AIC-160714,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85002156811&doi=10.3233%2fAIC-160714&partnerID=40&md5=4d3c15840307bd7f74b2320a6d14fe09,2016,2021-07-20 15:49:03,2021-07-20 15:49:03
X9FXTBPE,journalArticle,2020,"Zhang, K.; Zheng, L.; Liu, Z.; Jia, N.",A deep learning based multitask model for network-wide traffic speed prediction,"This paper proposes a deep learning based multitask learning (MTL) model to predict network-wide traffic speed, and introduces two methods to improve the prediction performance. The nonlinear Granger causality analysis is used to detect the spatiotemporal causal relationship among various links so as to select the most informative features for the MTL model. Bayesian optimization is employed to tune the hyperparameters of the MTL model with limited computational costs. Numerical experiments are carried out with taxis’ GPS data in an urban road network of Changsha, China, and some conclusions are drawn as follows. The deep learning based MTL model outperforms four deep learning based single task learning (STL) models (i.e., Gated Recurrent Units network, Long Short-term Memory network, Convolutional Gated Recurrent Units network and Temporal Convolutional Network) and three other classic models (i.e., Support Vector Machine, k-Nearest Neighbors and Evolving Fuzzy Neural Network). The nonlinear Granger causality test provides a reliable guide to select the informative features from network-wide links for the MTL model. Compared with two other optimization approaches (i.e., grid search and random search), Bayesian optimization yields a better tuning performance for the MTL model in the prediction accuracy under the budgeted computation cost. In summary, the deep learning based MTL model with nonlinear Granger causality analysis and Bayesian optimization promises the accurate and efficient traffic speed prediction for a large-scale network. © 2019 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2018.10.097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064951294&doi=10.1016%2fj.neucom.2018.10.097&partnerID=40&md5=96c1784d829089ede6e3d60397f08dbd,2020,2021-07-20 15:49:03,2021-07-20 15:49:03
7ERNIFYV,journalArticle,2014,"Ly, D.L.; Lipson, H.",Optimal experiment design for coevolutionary active learning,"This paper presents a policy for selecting the most informative individuals in a teacher-learner type coevolution. We propose the use of the surprisal of the mean, based on Shannon information theory, which best disambiguates a collection of arbitrary and competing models based solely on their predictions. This policy is demonstrated within an iterative coevolutionary framework consisting of symbolic regression for model inference and a genetic algorithm for optimal experiment design. Complex symbolic expressions are reliably inferred using fewer than 32 observations. The policy requires 21% fewer experiments for model inference compared to the baselines and is particularly effective in the presence of noise corruption, local information content as well as high dimensional systems. Furthermore, the policy was applied in a real-world setting to model concrete compression strength, where it was able to achieve 96.1% of the passive machine learning baseline performance with only 16.6% of the data. © 2013 IEEE.",IEEE Transactions on Evolutionary Computation,10.1109/TEVC.2013.2281529,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901842931&doi=10.1109%2fTEVC.2013.2281529&partnerID=40&md5=c680f430a6278cf957cb8f27c8537418,2014,2021-07-20 15:49:03,2021-07-20 15:49:03
4PMTIAUP,journalArticle,2020,"Campagner, A.; Cabitza, F.; Ciucci, D.",The three-way-in and three-way-out framework to treat and exploit ambiguity in data,"In this paper, we address ambiguity, intended as a characteristic of any data expression for which a unique meaning cannot be associated by the computational agent for either lack of information or multiple interpretations of the same configuration. In particular, we will propose and discuss ways in which a decision-support classifier can accept ambiguous data and make some (informative) value out of them for the decision maker. Towards this goal we propose a set of learning algorithms within what we call the three-way-in and three-way-out approach, that is, respectively, learning from partially labeled data and learning classifiers that can abstain. This approach is based on orthopartitions, as a common representation framework, and on three-way decisions and evidence theory, as tools to enable uncertain and approximate reasoning and inference. For both the above learning settings, we provide experimental results and comparisons with standard Machine Learning techniques, and show the advantages and promising performances of the proposed approaches on a collection of benchmarks, including a real-world medical dataset. © 2020 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2020.01.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078281465&doi=10.1016%2fj.ijar.2020.01.010&partnerID=40&md5=9a3734950193ea5fadf1f53c6b108da9,2020,2021-07-20 15:49:03,2021-07-20 15:49:03
CAANARKB,journalArticle,2011,"Quteishat, A.; Lim, C.P.; Saleh, J.M.; Tweedale, J.; Jain, L.C.",A neural network-based multi-agent classifier system with a Bayesian formalism for trust measurement,"In this paper, a neural network (NN)-based multi-agent classifier system (MACS) utilising the trust-negotiation-communication (TNC) reasoning model is proposed. A novel trust measurement method, based on the combination of Bayesian belief functions, is incorporated into the TNC model. The Fuzzy Min-Max (FMM) NN is used as learning agents in the MACS, and useful modifications of FMM are proposed so that it can be adopted for trust measurement. Besides, an auctioning procedure, based on the sealed bid method, is applied for the negotiation phase of the TNC model. Two benchmark data sets are used to evaluate the effectiveness of the proposed MACS. The results obtained compare favourably with those from a number of machine learning methods. The applicability of the proposed MACS to two industrial sensor data fusion and classification tasks is also demonstrated, with the implications analysed and discussed. © 2010 Springer-Verlag.",Soft Computing,10.1007/s00500-010-0592-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952039171&doi=10.1007%2fs00500-010-0592-0&partnerID=40&md5=5735ecb0dc06c771faf3b3a4a456ee61,2011,2021-07-20 15:49:03,2021-07-20 15:49:03
YLDVGTM3,journalArticle,2013,"Luqman, M.M.; Ramel, J.-Y.; Lladós, J.; Brouard, T.",Fuzzy multilevel graph embedding,"Structural pattern recognition approaches offer the most expressive, convenient, powerful but computational expensive representations of underlying relational information. To benefit from mature, less expensive and efficient state-of-the-art machine learning models of statistical pattern recognition they must be mapped to a low-dimensional vector space. Our method of explicit graph embedding bridges the gap between structural and statistical pattern recognition. We extract the topological, structural and attribute information from a graph and encode numeric details by fuzzy histograms and symbolic details by crisp histograms. The histograms are concatenated to achieve a simple and straightforward embedding of graph into a low-dimensional numeric feature vector. Experimentation on standard public graph datasets shows that our method outperforms the state-of-the-art methods of graph embedding for richly attributed graphs. © 2012 Elsevier Ltd.",Pattern Recognition,10.1016/j.patcog.2012.07.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867396397&doi=10.1016%2fj.patcog.2012.07.029&partnerID=40&md5=f25df5d18599e7e3253f9e5b67393f35,2013,2021-07-20 15:49:03,2021-07-20 15:49:03
T8Q34594,journalArticle,2021,"Calvanese Strinati, E.; Barbarossa, S.",6G networks: Beyond Shannon towards semantic and goal-oriented communications,"The goal of this paper is to promote the idea that including semantic and goal-oriented aspects in future 6G networks can produce a significant leap forward in terms of system effectiveness and sustainability. Semantic communication goes beyond the common Shannon paradigm of guaranteeing the correct reception of each single transmitted bit, irrespective of the meaning conveyed by the transmitted bits. The idea is that, whenever communication occurs to convey meaning or to accomplish a goal, what really matters is the impact that the received bits have on the interpretation of the meaning intended by the transmitter or on the accomplishment of a common goal. Focusing on semantic and goal-oriented aspects, and possibly combining them, helps to identify the relevant information, i.e. the information strictly necessary to recover the meaning intended by the transmitter or to accomplish a goal. Combining knowledge representation and reasoning tools with machine learning algorithms paves the way to build semantic learning strategies enabling current machine learning algorithms to achieve better interpretation capabilities and contrast adversarial attacks. 6G semantic networks can bring semantic learning mechanisms at the edge of the network and, at the same time, semantic learning can help 6G networks to improve their efficiency and sustainability. © 2021",Computer Networks,10.1016/j.comnet.2021.107930,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102257856&doi=10.1016%2fj.comnet.2021.107930&partnerID=40&md5=a80c1e457a4f17775b5ba193eb45b64c,2021,2021-07-20 15:49:04,2021-07-20 15:49:04
HTQ2HY8F,journalArticle,2019,"Junior, W.; Oliveira, E.; Santos, A.; Dias, K.",A context-sensitive offloading system using machine-learning classification algorithms for mobile cloud environment,"Computational offloading in Mobile Cloud Computing (MCC) has attracted attention due to benefits in energy saving and improved mobile application performance. Nevertheless, this technique underperforms if the offloading decision ignores contextual information. While recent studies have highlighted the use of contextual information to improve the computational offloading decision, there still remain challenges regarding the dynamic nature of the MCC environment. Most solutions design a single reasoner for the offloading decision and do not know how accurate and precise this technique is, so that when applied in real-world environments it can contribute to inaccurate decisions and consequently the low performance of the overall system. Thus, this paper proposes a Context-Sensitive Offloading System (CSOS) that takes advantage of the main machine-learning reasoning techniques and robust profiling system to provide offloading decisions with high levels of accuracy. We first evaluate the main classification algorithms under our database and the results show that JRIP and J48 classifiers achieves 95% accuracy. Secondly, we develop and evaluate our system under controlled and real scenarios, where context information changes from one experiment to another. Under these conditions, CSOS makes correct decisions as well as ensuring performance gains and energy efficiency. © 2018 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2018.08.026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052442762&doi=10.1016%2fj.future.2018.08.026&partnerID=40&md5=86eb3f631ca80a1cb06af0478765f8b9,2019,2021-07-20 15:49:04,2021-07-20 15:49:04
SYJ9W3NF,journalArticle,2019,"El-Sappagh, S.; Elmogy, M.; Ali, F.; Kwak, K.-S.",A case-base fuzzification process: diabetes diagnosis case study,"Medical case-based reasoning (CBR) systems require the handling of vague or imprecise data. The fuzzy set theory is particularly suitable for this purpose. This paper proposes a case-base preparation framework for CBR systems, which converts the electronic health record medical data into fuzzy CBR knowledge. It generates fuzzy case-base knowledge by suggesting a standard crisp entity–relationship data model for CBR case-base. The resulting data model is fuzzified using a proposed relational data model fuzzification methodology. The performances of this methodology and its resulting fuzzy case-base structure are evaluated. Diabetes diagnosis is used as a case study. A set of 60 real diabetic cases is used in the study. A fuzzy CBR system is implemented to check the diagnoses accuracy. It combines the resulting fuzzy case-base with a proposed fuzzy similarity measure. Experimental results indicate that the proposed fuzzy CBR method is superior to traditional CBR and other machine-learning methods. Our fuzzy CBR achieves an accuracy of 95%, a precision of 96%, a recall 97.96%, an f-measure of 96.97%, a specificity of 81.82%, and good robustness for dealing with vagueness. The resulting fuzzy case-base relational database enhances the representation of case-base knowledge, the performance of retrieval algorithms, and the querying capabilities of CBR systems. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-018-3245-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047794137&doi=10.1007%2fs00500-018-3245-3&partnerID=40&md5=fbe4066897fd9bcb08c35379be76cf25,2019,2021-07-20 15:49:04,2021-07-20 15:49:04
L4HLA26N,journalArticle,2017,"Jalali, V.; Leake, D.; Forouzandehmehr, N.",Learning and applying adaptation rules for categorical features: An ensemble approach,"Acquiring knowledge for case adaptation is a classic challenge for case-based reasoning (CBR). To provide CBR systems with adaptation knowledge, machine learning methods have been developed for automatically generating adaptation rules. An influential approach uses the case difference heuristic (CDH) to generate rules by comparing pairs of cases in the case base. The CDH method has been studied for case-based prediction of numeric values (regression) from inputs with primarily numeric features, and has proven effective in that context. However, previous work has not attempted to apply the CDH method to classification tasks, to generate rules for adapting categorical solutions. This article introduces an approach to applying the CDH to cases with categorical features and target values, based on the generalized case value difference heuristic (GCVDH). It also proposes a classification method using ensembles of GCVDH-generated rules, ensemble of adaptations for classification (EAC), an extension to our previous work on ensembles of adaptations for regression (EAR). It reports on an evaluation comparing the accuracy of EAC to three baseline methods on six standard domains, as well as comparing EAC to an ablation relying on single adaptation rules, and assesses the effect of training/test size on accuracy. Results are encouraging for the effectiveness of the GCVDH approach and for the value of applying ensembles of learned adaptation rules for classification. © 2017 - IOS Press and the authors. All rights reserved.",AI Communications,10.3233/AIC-170731,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024121073&doi=10.3233%2fAIC-170731&partnerID=40&md5=e726c3d3a0a3fabb3b47909fad688218,2017,2021-07-20 15:49:04,2021-07-20 15:49:04
WNG5EMUI,journalArticle,2020,"Yap, K.Y.; Sarimuthu, C.R.; Lim, J.M.-Y.",Grid Integration of Solar Photovoltaic System Using Machine Learning-Based Virtual Inertia Synthetization in Synchronverter,"In recent years, the domination of power electronics-interfaced renewable energy source (RES) such as solar photovoltaic (PV) system causes grid frequency instability issue. This paper proposes a new machine learning (ML)-based virtual inertia (VI) synthetization in synchronverter topology to integrate the solar PV system and the power grid with high-frequency stability. The proposed ML-based VI is synthetized by amalgamating the action and critic network to decouple active and reactive power control. Therefore, the proposed synchronverter exhibits decoupled control and flexible moment of inertia ( $J$ ) changes that lead to high stability and fast transient response as compared to the conventional proportional-integral (PI) and fuzzy logic (FL)-based synchronverters. Various case studies in MATLAB/ Simulink simulation have been carried out, and the results proved the feasibility and effectiveness of the proposed ML-based synchronverter. Through the proposed control strategy, the maximum frequency deviation from the nominal value, settling time to reach quasi-steady-state frequency and steady-state error has been reduced by 0.1Hz, 35% and 27% respectively. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2980187,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082397263&doi=10.1109%2fACCESS.2020.2980187&partnerID=40&md5=717c55eb68e3e9d18d91a3dff21ab6cb,2020,2021-07-20 15:49:04,2021-07-20 15:49:04
HQYD7DXN,journalArticle,2011,"Ferilli, S.; Basile, T.M.A.; Di Mauro, N.; Esposito, F.",Automatic document layout analysis through relational machine learning,"The current spread of digital documents raised the need of effective content-based retrieval techniques. Since manual indexing is infeasible and subjective, automatic techniques are the obvious solution. In particular, the ability of properly identifying and understanding a document's structure is crucial, in order to focus on the most significant components only. At a geometrical level, this task is known as Layout Analysis, and thoroughly studied in the literature. On suitable descriptions of the document layout, Machine Learning techniques can be applied to automatically infer models of classes of documents and of their components. Indeed, organizing the documents on the grounds of the knowledge they contain is fundamental for being able to correctly access them according to the user's needs. Thus, the quality of the layout analysis outcome biases the next understanding steps. Unfortunately, due to the variety of document styles and formats, the automatically found structure often needs to be manually adjusted. We propose the application of supervised Machine Learning techniques to infer correction rules to be applied to forthcoming documents. A first-order logic representation is suggested, because corrections often depend on the relationships of the wrong components with the surrounding ones. Moreover, as a consequence of the continuous flow of documents, the learned models often need to be updated and refined, which calls for incremental abilities. The proposed technique, embedded in a prototypical version of the document processing system DOMINUS, using the incremental first-order logic learner INTHELEX, revealed good performance in real-world experiments. © 2011 Springer-Verlag Berlin Heidelberg.",Studies in Computational Intelligence,10.1007/978-3-642-22913-8_4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455132053&doi=10.1007%2f978-3-642-22913-8_4&partnerID=40&md5=2f1f8e6218b3a35efaebee67a4d480a0,2011,2021-07-20 15:49:04,2021-07-20 15:49:04
88EIMIB9,journalArticle,2015,"Ibrahim, S.; Chowriappa, P.; Dua, S.; Acharya, U.R.; Noronha, K.; Bhandary, S.; Mugasa, H.",Classification of diabetes maculopathy images using data-adaptive neuro-fuzzy inference classifier,"Prolonged diabetes retinopathy leads to diabetes maculopathy, which causes gradual and irreversible loss of vision. It is important for physicians to have a decision system that detects the early symptoms of the disease. This can be achieved by building a classification model using machine learning algorithms. Fuzzy logic classifiers group data elements with a degree of membership in multiple classes by defining membership functions for each attribute. Various methods have been proposed to determine the partitioning of membership functions in a fuzzy logic inference system. A clustering method partitions the membership functions by grouping data that have high similarity into clusters, while an equalized universe method partitions data into predefined equal clusters. The distribution of each attribute determines its partitioning as fine or coarse. A simple grid partitioning partitions each attribute equally and is therefore not effective in handling varying distribution amongst the attributes. A data-adaptive method uses a data frequency-driven approach to partition each attribute based on the distribution of data in that attribute. A data-adaptive neuro-fuzzy inference system creates corresponding rules for both finely distributed and coarsely distributed attributes. This method produced more useful rules and a more effective classification system. We obtained an overall accuracy of 98.55 %. © 2015, International Federation for Medical and Biological Engineering.",Medical and Biological Engineering and Computing,10.1007/s11517-015-1329-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948719894&doi=10.1007%2fs11517-015-1329-0&partnerID=40&md5=c835590e0e29bc119cdff16e49f9106e,2015,2021-07-20 15:49:04,2021-07-20 15:49:04
K7BY8PWM,journalArticle,2015,"Nguyen, T.; Khosravi, A.; Creighton, D.; Nahavandi, S.",Medical data classification using interval type-2 fuzzy logic system and wavelets,"This paper introduces an automated medical data classification method using wavelet transformation (WT) and interval type-2 fuzzy logic system (IT2FLS). Wavelet coefficients, which serve as inputs to the IT2FLS, are a compact form of original data but they exhibits highly discriminative features. The integration between WT and IT2FLS aims to cope with both high-dimensional data challenge and uncertainty. IT2FLS utilizes a hybrid learning process comprising unsupervised structure learning by the fuzzy c-means (FCM) clustering and supervised parameter tuning by genetic algorithm. This learning process is computationally expensive, especially when employed with high-dimensional data. The application of WT therefore reduces computational burden and enhances performance of IT2FLS. Experiments are implemented with two frequently used medical datasets from the UCI Repository for machine learning: the Wisconsin breast cancer and Cleveland heart disease. A number of important metrics are computed to measure the performance of the classification. They consist of accuracy, sensitivity, specificity and area under the receiver operating characteristic curve. Results demonstrate a significant dominance of the wavelet-IT2FLS approach compared to other machine learning methods including probabilistic neural network, support vector machine, fuzzy ARTMAP, and adaptive neuro-fuzzy inference system. The proposed approach is thus useful as a decision support system for clinicians and practitioners in the medical practice. copy; 2015 Elsevier B.V. All rights reserved.",Applied Soft Computing Journal,10.1016/j.asoc.2015.02.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924388742&doi=10.1016%2fj.asoc.2015.02.016&partnerID=40&md5=7fa71aac8f6ca7cbaa3089b8593c761a,2015,2021-07-20 15:49:04,2021-07-20 15:49:04
2KKWBRNS,journalArticle,2020,"Mežnar, S.; Lavrač, N.; Škrlj, B.",SNoRe: Scalable Unsupervised Learning of Symbolic Node Representations,"Learning from complex real-life networks is a lively research area, with recent advances in learning information-rich, low-dimensional network node representations. However, state-of-the-art methods are not necessarily interpretable and are therefore not fully applicable to sensitive settings in biomedical or user profiling tasks, where explicit bias detection is highly relevant. The proposed SNoRe (Symbolic Node Representations) algorithm is capable of learning symbolic, human-understandable representations of individual network nodes, based on the similarity of neighborhood hashes which serve as features. SNoRe's interpretable features are suitable for direct explanation of individual predictions, which we demonstrate by coupling it with the widely used instance explanation tool SHAP to obtain nomograms representing the relevance of individual features for a given classification. To our knowledge, this is one of the first such attempts in a structural node embedding setting. In the experimental evaluation on eleven real-life datasets, SNoRe proved to be competitive to strong baselines, such as variational graph autoencoders, node2vec and LINE. The vectorized implementation of SNoRe scales to large networks, making it suitable for contemporary network learning and analysis tasks. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3039541,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096825835&doi=10.1109%2fACCESS.2020.3039541&partnerID=40&md5=e8441cda5e249a2b1df7ec5142d27062,2020,2021-07-20 15:49:04,2021-07-20 15:49:04
LR3FI9PH,journalArticle,2021,"Gutierrez, C.; Sequeda, J.F.",Knowledge graphs,"THE NOTION OF Knowledge Graph stems from scientifc advancements in diverse research areas such as Semantic Web, databases, knowledge representation and reasoning, NLP, and machine learning, among others. The integration of ideas and techniques from such disparate disciplines presents a challenge to practitioners and researchers to know how current advances develop from, and are rooted in, early techniques. Understanding the historical context and background of one's research area is of utmost importance in order to understand the possible avenues of the future. Today, this is more important than ever due to the almost infnite sea of information one faces everyday. When it comes to the Knowledge Graph area, we have noticed that students and junior researchers are not completely aware of the source of the ideas, concepts, and techniques they command. © 2021 ACM.",Communications of the ACM,10.1145/3418294,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101524472&doi=10.1145%2f3418294&partnerID=40&md5=575456a7d4dbfdb42716bc7d4170278a,2021,2021-07-20 15:49:04,2021-07-20 15:49:04
IN5NTL48,journalArticle,2014,"Riguzzi, F.",Speeding up inference for probabilistic logic programs,"Probabilistic Logic Programming (PLP) allows one to represent domains containing many entities connected by uncertain relations and has many applications in particular in Machine Learning. PITA is a PLP algorithm for computing the probability of queries, which exploits tabling, answer subsumption and Binary Decision Diagrams (BDDs). PITA does not impose any restriction on the programs. Other algorithms, such as PRISM, reduce computation time by imposing restrictions on the program, namely that subgoals are independent and that clause bodies are mutually exclusive. Another assumption that simplifies inference is that clause bodies are independent. In this paper, we present the algorithms PITA(IND,IND) and PITA(OPT). PITA(IND,IND) assumes that subgoals and clause bodies are independent. PITA(OPT) instead first checks whether these assumptions hold for subprograms and subgoals: if they do, PITA(OPT) uses a simplified calculation, otherwise it resorts to BDDs. Experiments on a number of benchmark datasets show that PITA(IND,IND) is the fastest on datasets respecting the assumptions, while PITA(OPT) is a good option when nothing is known about a dataset. © The British Computer Society 2013.",Computer Journal,10.1093/comjnl/bxt096,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897708709&doi=10.1093%2fcomjnl%2fbxt096&partnerID=40&md5=8274044b9208d95c212b556dd8a845e8,2014,2021-07-20 15:49:04,2021-07-20 15:49:04
EPE47P4B,journalArticle,2020,"Pati, C.; Panda, A.K.; Tripathy, A.K.; Pradhan, S.K.; Patnaik, S.",A novel hybrid machine learning approach for change detection in remote sensing images,"Change detection can play an essential role in satellite surveillance. With the availability of satellite images of a certain geographical area captured in different time instances, change detection is considered a tough task in the field of satellite applications. This research proposes a novel hybrid machine learning change detection technique from satellite images. The proposed hybrid learning approach is designed based on supervised and unsupervised learning techniques that considers the local association of adjacent pixels of the satellite images. Hybridization of clustering, soft labeling using fuzzy logic, Support Vector Machine (SVM) and Genetic Algorithm (GA) are used in change detection. Radial Basis Function (RBF) is used as the kernel function in SVM, and the RBF kernel parameters such as C and σ are optimized using GA for additional improvement of the performance. To demonstrate the efficiency of the approach, tests are performed on two satellite images captured in two different time instances on a particular geographical area. Change detection accuracy is used to validate the performance. Outcomes are compared with existing approaches and found to be superior. © 2020 Karabuk University","Engineering Science and Technology, an International Journal",10.1016/j.jestch.2020.01.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078749502&doi=10.1016%2fj.jestch.2020.01.002&partnerID=40&md5=e9158a6966a45b10cb3869c8eaedae2f,2020,2021-07-20 15:49:04,2021-07-20 15:49:04
5ICD7QYP,journalArticle,2020,"González, J.E.; Cire, A.A.; Lodi, A.; Rousseau, L.-M.",Integrated integer programming and decision diagram search tree with an application to the maximum independent set problem,"We propose an optimization framework which integrates decision diagrams (DDs) and integer linear programming (ILP) to solve combinatorial optimization problems. The hybrid DD-ILP approach explores the solution space based on a recursive compilation of relaxed DDs and incorporates ILP calls to solve subproblems associated with DD nodes. The selection of DD nodes to be explored by ILP technology is a significant component of the approach. We show how supervised machine learning can be useful to detect, on-the-fly, a subproblem structure for ILP technology. We use the maximum independent set problem as a case study. Computational experiments show that, in presence of suitable problem structure, the integrated DD-ILP approach can exploit complementary strengths and improve upon the performance of both a stand-alone DD solver and an ILP solver in terms of solution time and number of solved instances. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Constraints,10.1007/s10601-019-09306-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078633837&doi=10.1007%2fs10601-019-09306-w&partnerID=40&md5=37d03ae642a37f95d1165c102d132234,2020,2021-07-20 15:49:05,2021-07-20 15:49:05
JWXLTYZ6,journalArticle,2013,"Jaganathan, P.; Kuppuchamy, R.",A threshold fuzzy entropy based feature selection for medical database classification,"Feature selection is one of the most common and critical tasks in database classification. It reduces the computational cost by removing insignificant features. Consequently, this makes the diagnosis process accurate and comprehensible. This paper presents the measurement of feature relevance based on fuzzy entropy, tested with a Radial Basis Function Network classifier for a medical database classification. Three feature selection strategies are devised to obtain the valuable subset of relevant features. Five benchmarked datasets, which are available in the UCI Machine Learning Repository, have been used in this work. The classification accuracy shows that the proposed method is capable of producing good results with fewer features than the original datasets. © 2013 Elsevier Ltd.",Computers in Biology and Medicine,10.1016/j.compbiomed.2013.10.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887325021&doi=10.1016%2fj.compbiomed.2013.10.016&partnerID=40&md5=4a62d7a1eadcfb4027bb548fc3087748,2013,2021-07-20 15:49:05,2021-07-20 15:49:05
TZB3DZY9,journalArticle,2019,"Westphal, P.; Bühmann, L.; Bin, S.; Jabeen, H.; Lehmann, J.",SML-Bench - A benchmarking framework for structured machine learning,"The availability of structured data has increased significantly over the past decade and several approaches to learn from structured data have been proposed. These logic-based, inductive learning methods are often conceptually similar, which would allow a comparison among them even if they stem from different research communities. However, so far no efforts were made to define an environment for running learning tasks on a variety of tools, covering multiple knowledge representation languages. With SML-Bench, we propose a benchmarking framework to run inductive learning tools from the ILP and semantic web communities on a selection of learning problems. In this paper, we present the foundations of SML-Bench, discuss the systematic selection of benchmarking datasets and learning problems, and showcase an actual benchmark run on the currently supported tools. © 2019 - IOS Press and the authors. All rights reserved.",Semantic Web,10.3233/SW-180308,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060949735&doi=10.3233%2fSW-180308&partnerID=40&md5=24363c13d83ab892a4fbe831c04e5afa,2019,2021-07-20 15:49:05,2021-07-20 15:49:05
K75C7DPI,journalArticle,2014,"Won, S.Y.; Lee, H.; Kim, T.",Ontology mediation approach using formal concept analysis,"Ontology mediation enables the interoperability of heterogeneous semantic data sources. Ontology mediation includes operations such as, mapping, alignment, matching, merging and integration. Formal Concept Analysis (FCA) is a machine learning technique which represents some association between the merged concepts. This paper proposes ontology mediation approach using FCA techniques. FCA has the capability of deriving a concept hierarchy or formal ontology from a collection of objects and properties. Thus, FCA can discover and position new concepts in the merged ontology. Ontology mediation includes ontology mapping, ontology alignment and ontology merging. The proposed model is implemented using an example ontology case. © 2014 ICIC International.","ICIC Express Letters, Part B: Applications",,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893194882&partnerID=40&md5=acce44322158fce6db4126076bd4625b,2014,2021-07-20 15:49:05,2021-07-20 15:49:05
MXE9GU4L,journalArticle,2021,"Chaki, J.",Shadow detection from images using fuzzy logic and PCPerturNet,"Shadow detection is a challenging and essential task for interpreting the scene. Regardless of encouraging findings from current Deep Learning (DL) approaches used for shadow detection, the methods are also dealing with inconsistent situations where the visual representation of non-shadow and shadow regions is equivalent. In this article, a DL based approach is introduced for image pixel-level shadow detection. The proposed CNN-based approach, pattern conserver convolutional neural network (PCPerturNet) profits from a new design where shadow features are defined utilizing an effective skip-connection mapping arrangement. To make PCPerturNet robust from the change in brightness and contrast, several perturbed instances are generated by using a fuzzy-logic based method to train the system. Also, five types of augmentations are applied to images during training to make the system robust from the change in scale, orientation and flip. PCPerturNet derives and conserves shadow patterns in manifold layers and uses those layers progressively in several units to produce the shadow mask. The output of the proposed method is tested on two freely accessible databases and one self-created database where the accuracy rate obtained is 96.4%, 96.8%, and 89.4% which indicates that the proposed method outperforms the other shadow detection approaches used in the literature. © 2021 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology",IET Image Processing,10.1049/ipr2.12221,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104711442&doi=10.1049%2fipr2.12221&partnerID=40&md5=9ca9bd437e8054675cfb192ad389b00a,2021,2021-07-20 15:49:05,2021-07-20 15:49:05
ZTURK5EW,journalArticle,2011,"Polkowski, L.",Reductive Reasoning Rough and Fuzzy Sets as Frameworks for Reductive Reasoning,"Reductive reasoning, in particular inductive reasoning, Bocheński [9], Łukasiewicz [30], is concerned with finding a proper p satisfying a premise p⇒q for a given conclusion q. With some imprecision of language, one can say that its concern lies in finding a right cause for a given consequence. As such, inductive reasoning does encompass many areas of research like Machine Learning, see Mitchell [37], Pattern Recognition and Classification, see Duda et al. [16], Data Mining and Knowledge Discovery, see Kloesgen and Zytkow [26], all of which are concerned with a right interpretation of data and a generalization of findings from them. The matter of induction opens up an abyss of speculative theories, concerned with hypotheses making, verification and confirmation of them, means for establishing optimality criteria, consequence relations, non-monotonic reasoning etc. etc., see, e.g., Carnap [12], Popper [55], Hempel [22], Bochman [10]. Our purpose in this chapter is humble; we wish to give an insight into two paradigms intended for inductive reasoning and producing decision rules from data: rough set theory and fuzzy set theory. We pay attention to structure and basic tools of these paradigms; rough sets are interesting for us, as forthcoming exposition of rough mereology borders on rough sets and uses knowledge representation in the form of information and decision systems as studied in rough set theory. Fuzzy set theory, as already observed in Introduction, is to rough mereology as set theory is to mereology, a guiding motive; in addition, main tools of fuzzy set theory: t-norms and residual implications are also of fundamental importance to rough mereology, as demonstrated in following chapters.",Intelligent Systems Reference Library,10.1007/978-3-642-22279-5_4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885621819&doi=10.1007%2f978-3-642-22279-5_4&partnerID=40&md5=5f9642d7890f69ba2ef628e916e04c7e,2011,2021-07-20 15:49:05,2021-07-20 15:49:05
NM743NT3,journalArticle,2015,"El-Sappagh, S.; Elmogy, M.; Riad, A.M.",A fuzzy-ontology-oriented case-based reasoning framework for semantic diabetes diagnosis,"Objective Case-based reasoning (CBR) is a problem-solving paradigm that uses past knowledge to interpret or solve new problems. It is suitable for experience-based and theory-less problems. Building a semantically intelligent CBR that mimic the expert thinking can solve many problems especially medical ones. Methods Knowledge-intensive CBR using formal ontologies is an evolvement of this paradigm. Ontologies can be used for case representation and storage, and it can be used as a background knowledge. Using standard medical ontologies, such as SNOMED CT, enhances the interoperability and integration with the health care systems. Moreover, utilizing vague or imprecise knowledge further improves the CBR semantic effectiveness. This paper proposes a fuzzy ontology-based CBR framework. It proposes a fuzzy case-base OWL2 ontology, and a fuzzy semantic retrieval algorithm that handles many feature types. Material This framework is implemented and tested on the diabetes diagnosis problem. The fuzzy ontology is populated with 60 real diabetic cases. The effectiveness of the proposed approach is illustrated with a set of experiments and case studies. Results The resulting system can answer complex medical queries related to semantic understanding of medical concepts and handling of vague terms. The resulting fuzzy case-base ontology has 63 concepts, 54 (fuzzy) object properties, 138 (fuzzy) datatype properties, 105 fuzzy datatypes, and 2640 instances. The system achieves an accuracy of 97.67%. We compare our framework with existing CBR systems and a set of five machine-learning classifiers; our system outperforms all of these systems. Conclusion Building an integrated CBR system can improve its performance. Representing CBR knowledge using the fuzzy ontology and building a case retrieval algorithm that treats different features differently improves the accuracy of the resulting systems. © 2015 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2015.08.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983487369&doi=10.1016%2fj.artmed.2015.08.003&partnerID=40&md5=c92bb25101e1e255442b8265972ca0d4,2015,2021-07-20 15:49:05,2021-07-20 15:49:05
ICX94CMM,journalArticle,2021,"Zhao, T.-H.; Wang, M.-H.; Zhou, B.",Optimal quantum state transformations based on machine learning,"It is well known that quantum algorithms may solve problems efficiently that are intractable using conventional algorithms. Quantum algorithms can be designed with a set of universal quantum gates that transform input states into desired output states. However, designing quantum algorithms that transform states in desired ways is challenging due to its complexity. In this paper, we propose a machine learning framework for the transformation of unknown states into their corresponding target states. Specifically, a parameterized quantum circuit learns a given task by tuning its parameters. After the learning is done, the circuit is competent for the quantum task. This allows us to circumvent cumbersome circuit design based on universal quantum gates. If perfect transformation is forbidden by quantum theory, an optimal transformation can be obtained in terms of fidelity. This provides a research method to study various quantum no-go theorems that characterize the intrinsic gap between quantum and classical information. As examples, quantum state rotation and quantum state cloning are studied using numerical simulations. We also show the good robustness of our machine learning framework to corrupted training data, which is a very nice property for physical implementation on near-term noisy intermediate-scale quantum devices. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Quantum Information Processing,10.1007/s11128-021-03148-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108362105&doi=10.1007%2fs11128-021-03148-3&partnerID=40&md5=7f2c9a3dd28c1f65d3259df20a8a5f57,2021,2021-07-20 15:49:05,2021-07-20 15:49:05
LXSKDGQ2,journalArticle,2021,"Negi, S.; Rauthan, M.M.S.; Vaisla, K.S.; Panwar, N.",CMODLB: an efficient load balancing approach in cloud computing environment,"A hybrid of supervised (artificial neural network), unsupervised (clustering) machine learning, and soft computing (interval type 2 fuzzy logic system)-based load balancing algorithm, i.e., clustering-based multiple objective dynamic load balancing technique (CMODLB), is introduced to balance the cloud load in the present work. Initially, our previously introduced artificial neural network-based dynamic load balancing (ANN-LB) technique is implemented to cluster the virtual machines (VMs) into underloaded and overloaded VMs using Bayesian optimization-based enhanced K-means (BOEK-means) algorithm. In the second stage, the user tasks are scheduled for underloading VMs to improve load balance and resource utilization. Scheduling of tasks is supported by multi-objective-based technique of order preference by similarity to ideal solution with particle swarm optimization (TOPSIS-PSO) algorithm using different cloud criteria. To realize load balancing among PMs, the VM manager makes decisions for VM migration. VM migration decision is done based on the suitable conditions, if a PM is overloaded, and if another PM is minimum loaded. The former condition balances load, while the latter condition minimizes energy consumption in PMs. VM migration is achieved through interval type 2 fuzzy logic system (IT2FS) whose decisions are based on multiple significant parameters. Experimental results show that the CMODLB method takes 31.067% and 71.6% less completion time than TaPRA and BSO, respectively. It has maintained 65.54% and 68.26% less MakeSpan than MaxMin and R.R algorithms, respectively. The proposed method has achieved around 75% of resource utilization, which is highest compared to DHCI and CESCC. The use of novel and innovative hybridization of machine learning, multi-objective, and soft computing methods in the proposed algorithm offers optimum scheduling and migration processes to balance PMs and VMs. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Journal of Supercomputing,10.1007/s11227-020-03601-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099968388&doi=10.1007%2fs11227-020-03601-7&partnerID=40&md5=ebb28cbadcd424d42ab38440a4253ca7,2021,2021-07-20 15:49:05,2021-07-20 15:49:05
LFL6YQGV,journalArticle,2021,"Chen, T.; Shang, C.; Su, P.; Keravnou-Papailiou, E.; Zhao, Y.; Antoniou, G.; Shen, Q.",A Decision Tree-Initialised Neuro-fuzzy Approach for Clinical Decision Support,"Apart from the need for superior accuracy, healthcare applications of intelligent systems also demand the deployment of interpretable machine learning models which allow clinicians to interrogate and validate extracted medical knowledge. Fuzzy rule-based models are generally considered interpretable that are able to reflect the associations between medical conditions and associated symptoms, through the use of linguistic if-then statements. Systems built on top of fuzzy sets are of particular appealing to medical applications since they enable the tolerance of vague and imprecise concepts that are often embedded in medical entities such as symptom description and test results. They facilitate an approximate reasoning framework which mimics human reasoning and supports the linguistic delivery of medical expertise often expressed in statements such as ‘weight low’ or ‘glucose level high’ while describing symptoms. This paper proposes an approach by performing data-driven learning of accurate and interpretable fuzzy rule bases for clinical decision support. The approach starts with the generation of a crisp rule base through a decision tree learning mechanism, capable of capturing simple rule structures. The crisp rule base is then transformed into a fuzzy rule base, which forms the input to the framework of adaptive network-based fuzzy inference system (ANFIS), thereby further optimising the parameters of both rule antecedents and consequents. Experimental studies on popular medical data benchmarks demonstrate that the proposed work is able to learn compact rule bases involving simple rule antecedents, with statistically better or comparable performance to those achieved by state-of-the-art fuzzy classifiers. © 2020 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2020.101986,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097572903&doi=10.1016%2fj.artmed.2020.101986&partnerID=40&md5=24441e2e89b50871fb078ee747a1e230,2021,2021-07-20 15:49:05,2021-07-20 15:49:05
H3QCZ6XR,journalArticle,2018,"Cocarascu, O.; Toni, F.",Combining deep learning and argumentative reasoning for the analysis of social media textual content using small data sets,"The use of social media has become a regular habit for many and has changed the way people interact with each other. In this article, we focus on analyzing whether news headlines support tweets and whether reviews are deceptive by analyzing the interaction or the influence that these texts have on the others, thus exploiting contextual information. Concretely, we define a deep learning method for relation–based argument mining to extract argumentative relations of attack and support. We then use this method for determining whether news articles support tweets, a useful task in fact-checking settings, where determining agreement toward a statement is a useful step toward determining its truthfulness. Furthermore, we use our method for extracting bipolar argumentation frameworks from reviews to help detect whether they are deceptive. We show experimentally that our method performs well in both settings. In particular, in the case of deception detection, our method contributes a novel argumentative feature that, when used in combination with other features in standard supervised classifiers, outperforms the latter even on small data sets. © 2018, 2018 Association for Computational Linguistics Published under a Creative Commons Attribution-NonCommercial-NoDerivatives 4.0 International (CC BY-NC-ND 4.0) license.",Computational Linguistics,10.1162/coli_a_00338,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059282814&doi=10.1162%2fcoli_a_00338&partnerID=40&md5=84a4ba12b55e9580be7c28297868b1ff,2018,2021-07-20 15:49:05,2021-07-20 15:49:05
HK9P9MMN,journalArticle,2013,"Cabanac, G.",Experimenting with the partnership ability φ-index on a million computer scientists,"Schubert introduced the partnership ability φ-index relying on a researcher's number of co-authors and collaboration rate. As a Hirsch-type index, φ was expected to be consistent with Schubert-Glänzel's model of h-index. Schubert demonstrated this relationship with the 34 awardees of the Hevesy medal in the field of nuclear and radiochemistry (r2=0.8484). In this paper, we upscale this study by testing the φ-index on a million researchers in computer science. We found that the Schubert-Glänzel's model correlates with the million empirical φ values (r2=0.8695). In addition, machine learning through symbolic regression produces models whose accuracy does not exceed a 6.1 % gain (r2=0.9227). These results suggest that the Schubert-Glänzel's model of φ-index is accurate and robust on the domain-wide bibliographic dataset of computer science. © 2012 Akadémiai Kiadó, Budapest, Hungary.",Scientometrics,10.1007/s11192-012-0862-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879133723&doi=10.1007%2fs11192-012-0862-y&partnerID=40&md5=d849dd852fe5b77c18f1d7590c9b62da,2013,2021-07-20 15:49:05,2021-07-20 15:49:05
4XJIU46N,journalArticle,2021,"Zhuang, J.; Ye, J.; Chen, N.; Fang, W.; Fan, X.; Fu, Y.",Extended Belief Rule-Base Optimization Base on Clustering Tree and Parameter Optimization,"Extended belief rule-based (EBRB) system has a better ability to model complex problems than belief rule-based (BRB) system. However, the storage of rules in EBRB system is out of order, which leads to the low efficiency of rule retrieval during the reasoning process. Therefore, to improve the efficiency of rule retrieval, this study introduces K-means clustering tree algorithm into the construction of rule base, then proposes a multi-layer weighted reasoning approach based on K-means clustering tree. The proposed approach seeks out a path on the tree during the rule retrieval process, and then figures out several reasoning results according to the nodes on the path. These results are weighted and aggregated to obtain the final conclusion of the system, thus ensure both the efficiency of reasoning and the sufficient utilization of information. In addition, the differential evolution (DE) algorithm is used to train the parameters of EBRB system in this study. Several experiments are conducted on commonly used classification datasets from UCI, and the results are compared with some existing works of EBRB system and conventional machine learning methods. The comparison results illustrate that the proposed method can make an obvious improvement in the performance of EBRB system. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3051001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099588101&doi=10.1109%2fACCESS.2021.3051001&partnerID=40&md5=187ff6e3d6a40e9b6c903ad7ff9224ae,2021,2021-07-20 15:49:05,2021-07-20 15:49:05
3NLTZ2RZ,journalArticle,2016,"Jiménez, P.; Corchuelo, R.",Roller: a novel approach to Web information extraction,"The research regarding Web information extraction focuses on learning rules to extract some selected information from Web documents. Many proposals are ad hoc and cannot benefit from the advances in machine learning; furthermore, they are likely to fade away as the Web evolves, and their intrinsic assumptions are not satisfied. Some authors have explored transforming Web documents into relational data and then using techniques that got inspiration from inductive logic programming. In theory, such proposals should be easier to adapt as the Web evolves because they build on catalogues of features that can be adapted without changing the proposals themselves. Unfortunately, they are difficult to scale as the number of documents or features increases. In the general field of machine learning, there are propositio-relational proposals that attempt to provide effective and efficient means to learn from relational data using propositional techniques, but they have seldom been explored regarding Web information extraction. In this article, we present a new proposal called Roller: it relies on a search procedure that uses a dynamic flattening technique to explore the context of the nodes that provide the information to be extracted; it is configured with an open catalogue of features, so that it can adapt to the evolution of the Web; it also requires a base learner and a rule scorer, which helps it benefit from the continuous advances in machine learning. Our experiments confirm that it outperforms other state-of-the-art proposals in terms of effectiveness and that it is very competitive in terms of efficiency; we have also confirmed that our conclusions are solid from a statistical point of view. © 2016, Springer-Verlag London.",Knowledge and Information Systems,10.1007/s10115-016-0921-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960355231&doi=10.1007%2fs10115-016-0921-4&partnerID=40&md5=0d194e0db3f1298e95a16b53f1f94b22,2016,2021-07-20 15:49:06,2021-07-20 15:49:06
B7DZ7PSJ,journalArticle,2021,"Tyutyunnik, A.; Lobaneva, E.; Lazarev, A.",Algorithm for identifying clients based on dynamic MAC addresses in narrowly targeted secure networks using deep learning neural networks,"Existing algorithms for identifying clients on a network segment are based on static client binding by MAC address. MAC address generation is based on pseudo-random sequences of 0–256 characters. With this feature in mind, software was developed based on an algorithm for generating MAC addresses using bidirectional neural networks, followed by integration of a decision support system module. A secondary feature of the developed software is the ability to set a MAC validity timeout, which will limit access to the network segment and increase the security factor. © 2021 Informa UK Limited, trading as Taylor & Francis Group.","International Journal of Parallel, Emergent and Distributed Systems",10.1080/17445760.2021.1941007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108821501&doi=10.1080%2f17445760.2021.1941007&partnerID=40&md5=b40b9f8d40967cebb60dbfaa278c6c9b,2021,2021-07-20 15:49:06,2021-07-20 15:49:06
SESHVYVF,journalArticle,2021,"Sedova, N.; Sedov, V.; Bazhenov, R.; Bogatenkov, S.",Neural network classifier for automatic course-keeping based on fuzzy logic,"The authors continued their research on the development of an intelligent automatic ships pilot containing a controller based on fuzzy logic. Its features are determined by the optimizer based on a genetic algorithm. It also contains a modular unit of neural network models of ship navigation paths, as well as a neural network classifier. This paper is devoted to the description of a neural network classifier designed to classify the movement patterns of marine vessels to identify the peculiarities of the ship depending on its type and sailing conditions. The introduction of such classifier to an autopilot allows for more precise consideration of multivariate and difficult to formalize factors affecting the vessel while operating, such as varying weather conditions, irregular waves, hydrodynamic characteristics of the vessel, draft, water under the keel, rate of the vessel sailing, etc. The article outlines the technique concerning the development of a neural network classifier and the results of its computer modelling on the example of a refrigerated transport vessel type. The authors used such methods for obtaining and processing findings as spectral estimation, machine learning methods, in particular, neural network technology and computer or simulation modelling. © 2021-IOS Press. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-201495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102408954&doi=10.3233%2fJIFS-201495&partnerID=40&md5=2ae36a1ac81c539ca483017d180403b4,2021,2021-07-20 15:49:06,2021-07-20 15:49:06
PEG4JVXZ,journalArticle,2019,"Wang, Y.; Liao, P.-C.; Zhang, C.; Ren, Y.; Sun, X.; Tang, P.",Crowdsourced reliable labeling of safety-rule violations on images of complex construction scenes for advanced vision-based workplace safety,"Construction workplace hazard detection requires engineers to analyze scenes manually against many safety rules, which is time-consuming, labor-intensive, and error-prone. Computer vision algorithms are yet to achieve reliable discrimination of anomalous and benign object relations underpinning safety violation detections. Recently developed deep learning-based computer vision algorithms need tens of thousands of images, including labels of the safety rules violated, in order to train deep-learning networks for acquiring spatiotemporal reasoning capacity in complex workplaces. Such training processes need human experts to label images and indicate whether the relationship between the worker, resource, and equipment in the scenes violate spatiotemporal arrangement rules for safe and productive operations. False alarms in those manual labels (labeling no-violation images as having violations) can significantly mislead the machine learning process and result in computer vision models that produce inaccurate hazard detections. Compared with false alarms, another type of mislabels, false negatives (labeling images having violations as “no violations”), seem to have fewer impacts on the reliability of the trained computer vision models. This paper examines a new crowdsourcing approach that achieves above 95% accuracy in labeling images of complex construction scenes having safety-rule violations, with a focus on minimizing false alarms while keeping acceptable rates of false negatives. The development and testing of this new crowdsourcing approach examine two fundamental questions: (1) How to characterize the impacts of a short safety-rule training process on the labeling accuracy of non-professional image annotators? And (2) How to properly aggregate the image labels contributed by ordinary people to filter out false alarms while keeping an acceptable false negative rate? In designing short training sessions for online image annotators, the research team split a large number of safety rules into smaller sets of six. An online image annotator learns six safety rules randomly assigned to him or her, and then labels workplace images as “no violation” or ‘violation” of certain rules among the six learned by him or her. About one hundred and twenty anonymous image annotators participated in the data collection. Finally, a Bayesian-network-based crowd consensus model aggregated these labels from annotators to obtain safety-rule violation labeling results. Experiment results show that the proposed model can achieve close to 0% false alarm rates while keeping the false negative rate below 10%. Such image labeling performance outdoes existing crowdsourcing approaches that use majority votes for aggregating crowdsourced labels. Given these findings, the presented crowdsourcing approach sheds lights on effective construction safety surveillance by integrating human risk recognition capabilities into advanced computer vision. © 2019 Elsevier Ltd",Advanced Engineering Informatics,10.1016/j.aei.2019.101001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073511412&doi=10.1016%2fj.aei.2019.101001&partnerID=40&md5=4bc1e4274797b0655dfd44b4bf092556,2019,2021-07-20 15:49:06,2021-07-20 15:49:06
9TSJKBGA,journalArticle,2020,"Ding, W.; Li, X.; Li, G.; Wei, Y.",Global relational reasoning with spatial temporal graph interaction networks for skeleton-based action recognition,"With the prevalence of accessible depth sensors, dynamic skeletons have attracted much attention as a robust modality for action recognition. Convolutional neural networks (CNNs) excel at modeling local relations within local receptive fields and are typically inefficient at capturing global relations. In this article, we first view the dynamic skeletons as a spatio-temporal graph (STG) and then learn the localized correlated features that generate the embedded nodes of the STG by message passing. To better extract global relational information, a novel model called spatial–temporal graph interaction networks (STG-INs) is proposed, which perform long-range temporal modeling of human body parts. In this model, human body parts are mapped to an interaction space where graph-based reasoning can be efficiently implemented via a graph convolutional network (GCN). After reasoning, global relation-aware features are distributed back to the embedded nodes of the STG. To evaluate our model, we conduct extensive experiments on three large-scale datasets. The experimental results demonstrate the effectiveness of our proposed model, which achieves the state-of-the-art performance. © 2020 Elsevier B.V.",Signal Processing: Image Communication,10.1016/j.image.2019.115776,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077756177&doi=10.1016%2fj.image.2019.115776&partnerID=40&md5=8dda043740ab333b30d1b479f3c909f8,2020,2021-07-20 15:49:06,2021-07-20 15:49:06
P7XN5PNI,journalArticle,2020,"Low, D.W.W.; Neo, D.W.K.; Kumar, A.S.",A study on automatic fixture design using reinforcement learning,"Fixtures are used to locate and secure workpieces for further machining or measurement process. Design of these fixtures remains a costly process due to the significant technical know-how required. Automated fixture design can mitigate much of these costs by reducing the dependence on skilled labour, making it an attractive endeavour. Historical attempts in achieving automated fixture design solutions predominantly relied on case-based reasoning (CBR) to generate fixtures by extrapolating from previously proven designs. These approaches are limited by their dependence on a fixturing library. Attempts in using rule-based reasoning (RBR) has also shown to be difficult to implement comprehensively. Reinforcement learning, on the other hand, does not require a fixturing library and instead builds experience and learns through interacting with an environment. This paper discusses the use of reinforcement learning to generate optimized fixturing solutions. Through a proposed reinforcement learning driven fixture design (RL-FD) framework, reinforcement learning was used to generate optimized fixturing solutions. In response to the fixturing environment, adjustments to the reinforcement learning process in the exploration phase is studied. A case study is presented, comparing a conventional exploration method with an adjusted one. Both agents show improved average results over time, with the adjusted exploration model exhibiting faster performance. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",International Journal of Advanced Manufacturing Technology,10.1007/s00170-020-05156-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082874532&doi=10.1007%2fs00170-020-05156-6&partnerID=40&md5=60fbf76cdd1a507d11b2283b0e29a359,2020,2021-07-20 15:49:06,2021-07-20 15:49:06
HE8I6226,journalArticle,2014,"Nickles, M.; Rettinger, A.",Interactive relational reinforcement learning of concept semantics,"We present a framework for the machine learning of denotational concept semantics using a simple form of symbolic interaction of machines with human users. The capability of software agents and robots to learn how to communicate verbally with human users would obviously be highly useful in several real-world applications, and our framework is meant to provide a further step towards this goal. Whereas the large majority of existing approaches to the machine learning of word sense and other language aspects focuses on learning using text corpora, our framework allows for the interactive learning of concepts in a dialog of human and agent, using an approach in the area of Relational Reinforcement Learning. Such an approach has a wide range of possible applications, e.g., the interactive acquisition of semantic categories for the Semantic Web, Human-Computer Interaction, (interactive) Information Retrieval, and Natural Language Processing. © 2013 The Author(s).",Machine Learning,10.1007/s10994-013-5344-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894900645&doi=10.1007%2fs10994-013-5344-9&partnerID=40&md5=9370bc497d9fea3c1be4a886ae4c2196,2014,2021-07-20 15:49:06,2021-07-20 15:49:06
LLSCAKRI,journalArticle,2016,"Petrovic, S.; Khussainova, G.; Jagannathan, R.",Knowledge-light adaptation approaches in case-based reasoning for radiotherapy treatment planning,"Objective: Radiotherapy treatment planning aims at delivering a sufficient radiation dose to cancerous tumour cells while sparing healthy organs in the tumour-surrounding area. It is a time-consuming trial-and-error process that requires the expertise of a group of medical experts including oncologists and medical physicists and can take from 2 to 3 h to a few days. Our objective is to improve the performance of our previously built case-based reasoning (CBR) system for brain tumour radiotherapy treatment planning. In this system, a treatment plan for a new patient is retrieved from a case base containing patient cases treated in the past and their treatment plans. However, this system does not perform any adaptation, which is needed to account for any difference between the new and retrieved cases. Generally, the adaptation phase is considered to be intrinsically knowledge-intensive and domain-dependent. Therefore, an adaptation often requires a large amount of domain-specific knowledge, which can be difficult to acquire and often is not readily available. In this study, we investigate approaches to adaptation that do not require much domain knowledge, referred to as knowledge-light adaptation. Methodology: We developed two adaptation approaches: adaptation based on machine-learning tools and adaptation-guided retrieval. They were used to adapt the beam number and beam angles suggested in the retrieved case. Two machine-learning tools, neural networks and naive Bayes classifier, were used in the adaptation to learn how the difference in attribute values between the retrieved and new cases affects the output of these two cases. The adaptation-guided retrieval takes into consideration not only the similarity between the new and retrieved cases, but also how to adapt the retrieved case. Results: The research was carried out in collaboration with medical physicists at the Nottingham University Hospitals NHS Trust, City Hospital Campus, UK. All experiments were performed using real-world brain cancer patient cases treated with three-dimensional (3D)-conformal radiotherapy. Neural networks-based adaptation improved the success rate of the CBR system with no adaptation by 12%. However, naive Bayes classifier did not improve the current retrieval results as it did not consider the interplay among attributes. The adaptation-guided retrieval of the case for beam number improved the success rate of the CBR system by 29%. However, it did not demonstrate good performance for the beam angle adaptation. Its success rate was 29% versus 39% when no adaptation was performed. Conclusions: The obtained empirical results demonstrate that the proposed adaptation methods improve the performance of the existing CBR system in recommending the number of beams to use. However, we also conclude that to be effective, the proposed adaptation of beam angles requires a large number of relevant cases in the case base. © 2016 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2016.01.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958231564&doi=10.1016%2fj.artmed.2016.01.006&partnerID=40&md5=7441cfc22fb60814458db0e75c4d4e74,2016,2021-07-20 15:49:06,2021-07-20 15:49:06
N7KJGKCE,journalArticle,2019,"Aein, M.J.; Aksoy, E.E.; Wörgötter, F.",Library of actions: Implementing a generic robot execution framework by using manipulation action semantics,"When a robot has to imitate an observed action sequence, it must first understand the inherent characteristic features of the individual actions. Such features need to reflect the semantics of the action with a high degree of invariance between different demonstrations of the same action. At the same time the machine needs to be able to execute the action sequence in any appropriate situation. In this study, we introduce a new library of actions, which is a generic framework for executing manipulation actions on robotic systems by combining features that capture action semantics with a framework for execution. We focus on manipulation actions and first create a generic representation consisting of symbolic and sub-symbolic components. To link these two domains we introduce a finite state machine allowing for sequential execution with error handling. The framework is developed from observing humans which provides us with a high degree of grounding. To quantitatively evaluate the scalability of the proposed approach, we conducted a large set of experiments involving different actions performed either individually or sequentially with various types of objects in different scene contexts. © The Author(s) 2019.",International Journal of Robotics Research,10.1177/0278364919850295,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067085299&doi=10.1177%2f0278364919850295&partnerID=40&md5=d4d2a58f7dc57014ae9c726f9c5f6f04,2019,2021-07-20 15:49:06,2021-07-20 15:49:06
3P9VZIGK,journalArticle,2017,"Pounder, G.A.J.; Ellis, R.L.A.; Fernandez-Lopez, G.",Cognitive function synthesis: preliminary results,"Purpose: This paper aims to introduce the cognitive function synthesis (CFS) conceptual framework to artificial general intelligence. CFS posits that at the “core” of intelligence in hybrid architectures, “interdependent” cognitive functions are synthesised through the interaction of various associative memory (AM)-based systems. This synthesis could form an interface layer between deliberative/symbolic and reactive/sub-symbolic layers in hybrid cognitive architectures. Design/methodology/approach: A CFS conceptual framework, specifying an arrangement of AMs, was presented. The framework was executed using sparse distributed memory. Experiments were performed to investigate CFS autonomous extraction, consciousness and imagination. Findings: Autonomous extraction was achieved using data from a Wi-Fi camera with the CFS auto-associative AM handling “Sensor Data”. However, noise reduction degraded the extracted image. An environment, simulated in V-REP 3.3.1, was used to investigate consciousness and imagination. CFS displayed consciousness by successfully tracking/anticipating the object position with over 90 per cent congruence. CFS imagination was seen by its predicting two time steps into the future. Originality/value: Preliminary results demonstrate the plausibility of CFS claims for autonomous extraction, consciousness and imagination. © 2017, © Emerald Publishing Limited.",Kybernetes,10.1108/K-01-2015-0038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012169813&doi=10.1108%2fK-01-2015-0038&partnerID=40&md5=c749b82d8b767d423fb5fcdbc431cd69,2017,2021-07-20 15:49:06,2021-07-20 15:49:06
TUWJMS6C,journalArticle,2019,"Riveret, R.; Gao, Y.; Governatori, G.; Rotolo, A.; Pitt, J.; Sartor, G.",A probabilistic argumentation framework for reinforcement learning agents: Towards a mentalistic approach to agent profiles,"A bounded-reasoning agent may face two dimensions of uncertainty: firstly, the uncertainty arising from partial information and conflicting reasons, and secondly, the uncertainty arising from the stochastic nature of its actions and the environment. This paper attempts to address both dimensions within a single unified framework, by bringing together probabilistic argumentation and reinforcement learning. We show how a probabilistic rule-based argumentation framework can capture Markov decision processes and reinforcement learning agents; and how the framework allows us to characterise agents and their argument-based motivations from both a logic-based perspective and a probabilistic perspective. We advocate and illustrate the use of our approach to capture models of agency and norms, and argue that, in addition to providing a novel method for investigating agent types, the unified framework offers a sound basis for taking a mentalistic approach to agent profiles. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Autonomous Agents and Multi-Agent Systems,10.1007/s10458-019-09404-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062791180&doi=10.1007%2fs10458-019-09404-2&partnerID=40&md5=1e799daa0c28499dc9a6cd1a23bd5ea9,2019,2021-07-20 15:49:06,2021-07-20 15:49:06
CIKGZ948,journalArticle,2012,"Wuillemin, P.-H.; Torti, L.",Structured probabilistic inference,"Probabilistic inference is among the main topics with reasoning in uncertainty in AI. For this purpose, Bayesian Networks (BNs) is one of the most successful and efficient Probabilistic Graphical Model (PGM) so far. Since the mid-90s, a growing number of BNs extensions have been proposed. Object-oriented, entity-relationship and first-order logic are the main representation paradigms used to extend BNs. While entity-relationship and first-order models have been successfully used for machine learning in defining lifted probabilistic inference, object-oriented models have been mostly underused. Structured inference, which exploits the structural knowledge encoded in an object-oriented PGM, is a surprisingly unstudied technique. In this paper we propose a full object-oriented framework for PRM and propose two extensions of the state-of-the-art structured inference algorithm: SPI which removes the major flaws of existing algorithms and SPISBB which largely enhances SPI by using d-separation. © 2012 Elsevier Inc. All rights reserved.",International Journal of Approximate Reasoning,10.1016/j.ijar.2012.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864448085&doi=10.1016%2fj.ijar.2012.04.004&partnerID=40&md5=f253e92f0f363e9d7e8989dcf91d96e0,2012,2021-07-20 15:49:06,2021-07-20 15:49:06
B3ERYYBZ,journalArticle,2021,"Seifoori, Z.; Asadi, H.; Stojilovic, M.",Shrinking FPGA Static Power via Machine Learning-Based Power Gating and Enhanced Routing,"Despite FPGAs rapidly evolving to support the requirements of the most demanding emerging applications, their high static power consumption, concentrated within the routing resources, still presents a major hurdle for low-power applications. Augmenting the FPGAs with power-gating ability is a promising way to effectively address the power-consumption obstacle. However, the main challenge when implementing power gating is in choosing the clusters of resources in a way that would allow the most power-saving opportunities. In this paper, we take advantage of machine learning approaches, such as K-means clustering, to propose efficient algorithms for creating power-gating clusters of FPGA routing resources. In the first group of proposed algorithms, we employ K-means clustering and exploit the utilization pattern of routing resources. In the second group of algorithms, we enhance the power-gating efficiency by minimizing the power overhead introduced by power-gating logic and by taking into account the size of routing multiplexers, which influences the power-gating efficiency. Finally, we enhance and further develop the baseline FPGA routing algorithm to be aware and take advantage of power gating opportunities. The experimental results on Titan benchmark suite and the latest Intel Stratix-IV FPGA architecture in VTR 8.0 show that our approaches achieve an improvement of about 70%, on average, in reducing the FPGA static power consumption over the best power-gating approaches proposed in the previous studies. CCBY",IEEE Access,10.1109/ACCESS.2021.3085005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107363305&doi=10.1109%2fACCESS.2021.3085005&partnerID=40&md5=35cfa707b32d92c2aa3ec3e83d1155ac,2021,2021-07-20 15:49:06,2021-07-20 15:49:06
F4EY43IL,journalArticle,2019,"Chen, L.; Chen, D.; Wang, H.",Fuzzy Kernel Alignment with Application to Attribute Reduction of Heterogeneous Data,"Fuzzy similarity relation is a function to measure the similarity between two samples. It is widely used to learn knowledge under the framework of fuzzy machine learning. The selection of a suitable fuzzy similarity relation is important for the learning task. It has been pointed out that fuzzy similarity relations can be brought into the framework of kernel functions in machine learning. This fact motivates us to study fuzzy similarity relation selection for fuzzy machine learning utilizing kernel selection methods in machine learning. Kernel alignment is a kernel selection method that is effective and has low computational complexity. In this paper, we present novel methods for fuzzy similarity relation selection based on the kernel alignment, and their use in attribution reduction for heterogeneous data. First, we define an ideal kernel for classification problems, based on which a novel fuzzy kernel alignment model is proposed. Second, we present a method for the fuzzy similarity relation selection based on the minimization of the fuzzy alignment between the defined ideal kernel and a kernel for the learning problem at hand. In order to show the correctness of this selection method, we prove that the lower bound of the classification accuracy of a support vector machine will increase with the decrease of the fuzzy alignment value. Furthermore, we apply the proposed fuzzy similarity relation selection to attribute reduction for heterogeneous data. Finally, we present experimental results to show that the proposed method of fuzzy similarity relation selection based on the fuzzy kernel alignment is effective. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2018.2880933,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056310660&doi=10.1109%2fTFUZZ.2018.2880933&partnerID=40&md5=34f86de79af997e69544d7ffda2755f6,2019,2021-07-20 15:49:06,2021-07-20 15:49:06
BEZQABKY,journalArticle,2019,"Tiwari, P.; Melucci, M.",Towards a Quantum-Inspired Binary Classifier,"Machine Learning classification models learn the relation between input as features and output as a class in order to predict the class for the new given input. Several research works have demonstrated the effectiveness of machine learning algorithms but the state-of-the-art algorithms are based on the classical theories of probability and logic. Quantum Mechanics (QM) has already shown its effectiveness in many fields and researchers have proposed several interesting results which cannot be obtained through classical theory. In recent years, researchers have been trying to investigate whether the QM can help to improve the classical machine learning algorithms. It is believed that the theory of QM may also inspire an effective algorithm if it is implemented properly. From this inspiration, we propose the quantum-inspired binary classifier, which is based on quantum detection theory. We used text corpora and image corpora to explore the effect of our proposed model. Our proposed model outperforms the state-of-the-art models in terms of precision, recall, and F-measure for several topics (categories) in the 20 newsgroup text corpora. Our proposed model outperformed all the baselines in terms of recall when the MNIST handwritten image dataset was used; F-measure is also higher for most of the categories and precision is also higher for some categories. Our proposed model suggests that binary classification effectiveness can be achieved by using quantum detection theory. In particular, we found that our Quantum-Inspired Binary Classifier can increase the precision, recall, and F-measure of classification where the state-of-the-art methods cannot. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2904624,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064818887&doi=10.1109%2fACCESS.2019.2904624&partnerID=40&md5=5d067b7b68b9ff3c98bb1a801855852d,2019,2021-07-20 15:49:06,2021-07-20 15:49:06
8RN37DQZ,journalArticle,2014,"Guidi, G.; Pettenati, M.C.; Melillo, P.; Iadanza, E.",A machine learning system to improve heart failure patient assistance,"In this paper, we present a clinical decision support system (CDSS) for the analysis of heart failure (HF) patients, providing various outputs such as an HF severity evaluation, HF-type prediction, as well as a management interface that compares the different patients' follow-ups. The whole system is composed of a part of intelligent core and of an HF special-purpose management tool also providing the function to act as interface for the artificial intelligence training and use. To implement the smart intelligent functions, we adopted a machine learning approach. In this paper, we compare the performance of a neural network (NN), a support vector machine, a system with fuzzy rules genetically produced, and a classification and regression tree and its direct evolution, which is the random forest, in analyzing our database. Best performances in both HF severity evaluation and HF-type prediction functions are obtained by using the random forest algorithm. The management tool allows the cardiologist to populate a ""supervised database"" suitable for machine learning during his or her regular outpatient consultations. The idea comes from the fact that in literature there are a few databases of this type, and they are not scalable to our case. © 2014 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2014.2337752,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909634152&doi=10.1109%2fJBHI.2014.2337752&partnerID=40&md5=3449540a5d7e3cdf3be534f7b97b261a,2014,2021-07-20 15:49:07,2021-07-20 15:49:07
RWQPHS8H,journalArticle,2015,"Wang, Y.; Yu, H.; Ni, L.; Huang, G.-B.; Yan, M.; Weng, C.; Yang, W.; Zhao, J.",An Energy-Efficient Nonvolatile In-Memory Computing Architecture for Extreme Learning Machine by Domain-Wall Nanowire Devices,"The data-oriented applications have introduced increased demands on memory capacity and bandwidth, which raises the need to rethink the architecture of the current computing platforms. The logic-in-memory architecture is highly promising as future logic-memory integration paradigm for high throughput data-driven applications. From memory technology aspect, as one recently introduced nonvolatile memory device, domain-wall nanowire (or race-track) not only shows potential as future power efficient memory, but also computing capacity by its unique physics of spintronics. This paper explores a novel distributed in-memory computing architecture where most logic functions are executed within the memory, which significantly alleviates the bandwidth congestion issue and improves the energy efficiency. The proposed distributed in-memory computing architecture is purely built by domain-wall nanowire, i.e., both memory and logic are implemented by domain-wall nanowire devices. As a case study, neural network-based image resolution enhancement algorithm, called DW-NN, is examined within the proposed architecture. We show that all operations involved in machine learning on neural network can be mapped to a logic-in-memory architecture by nonvolatile domain-wall nanowire. Domain-wall nanowire-based logic is customized for in machine learning within image data storage. As such, both neural network training and processing can be performed locally within the memory. The experimental results show that the domain-wall memory can reduce 92% leakage power and 16% dynamic power compared to main memory implemented by DRAM; and domain-wall logic can reduce 31% both dynamic and 65% leakage power under the similar performance compared to CMOS transistor-based logic. And system throughput in DW-NN is improved by 11.6x and the energy efficiency is improved by 56x when compared to conventional image processing system. © 2002-2012 IEEE.",IEEE Transactions on Nanotechnology,10.1109/TNANO.2015.2447531,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947787452&doi=10.1109%2fTNANO.2015.2447531&partnerID=40&md5=ca4bb5eab6b7c38466998994ec0829d9,2015,2021-07-20 15:49:07,2021-07-20 15:49:07
HMX329PF,journalArticle,2011,"Xu, M.; Petrou, M.",3D Scene interpretation by combining probability theory and logic: The tower of knowledge,"We explore a newly proposed system architecture, called tower of knowledge (ToK), in the context of labelling components of building scenes. The ToK architecture allows the incorporation of statistical feature distributions and logic rules concerning the definition of a component, within a probabilistic framework. The maximum likelihood method of label assignment is modified by being multiplied with a function, called utility function, that expresses the information coming from the logic rules programmed to the system. The logic rules are designed to define an object/component by answering the questions ""why"" and ""how"", referring to the actions in which a particular object may be observed to participate and the characteristics it should have in order to be able to participate in these actions. Two sets of measurements are assumed to be available: those made initially for all components routinely, and which supply the initial statistically based inference of possible labels of each component, and those that are made in order to confirm or deny a particular characteristic of the component that would allow it to participate in a specific action. A recursive version of the architecture is also proposed, in which the distributions of the former types of measurement may be learnt in the process, having no training data at all. Multi-view images are used as input to the system, which uses standard techniques to build the 3D models of the buildings. The system is tested on labelling the components of 10 3D models of buildings. The components are identified either manually, or fully automatically. The results are compared with those obtained by expandable Bayesian networks. The recursive version of ToK proves to be able to cope very well even without any training data, where it learns the characteristics of the various components by simply applying the pre-programmed logic rules that connect labels, actions and attributes. © 2011 Elsevier Inc. All rights reserved.",Computer Vision and Image Understanding,10.1016/j.cviu.2011.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052517382&doi=10.1016%2fj.cviu.2011.08.001&partnerID=40&md5=05960cbce6129cfdf8f3f2455c76c94c,2011,2021-07-20 15:49:07,2021-07-20 15:49:07
P5Y77D3H,journalArticle,2021,"Ong, D.C.; Soh, H.; Zaki, J.; Goodman, N.D.",Applying Probabilistic Programming to Affective Computing,"Affective Computing is a rapidly growing field spurred by advancements in artificial intelligence, but often, held back by the inability to translate psychological theories of emotion into tractable computational models. To address this, we propose a probabilistic programming approach to affective computing, which models psychological-grounded theories as generative models of emotion, and implements them as stochastic, executable computer programs. We first review probabilistic approaches that integrate reasoning about emotions with reasoning about other latent mental states (e.g., beliefs, desires) in context. Recently-developed probabilistic programming languages offer several key desidarata over previous approaches, such as: (i) flexibility in representing emotions and emotional processes; (ii) modularity and compositionality; (iii) integration with deep learning libraries that facilitate efficient inference and learning from large, naturalistic data; and (iv) ease of adoption. Furthermore, using a probabilistic programming framework allows a standardized platform for theory-building and experimentation: Competing theories (e.g., of appraisal or other emotional processes) can be easily compared via modular substitution of code followed by model comparison. To jumpstart adoption, we illustrate our points with executable code that researchers can easily modify for their own models. We end with a discussion of applications and future directions of the probabilistic programming approach © 2010-2012 IEEE.",IEEE Transactions on Affective Computing,10.1109/TAFFC.2019.2905211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107145487&doi=10.1109%2fTAFFC.2019.2905211&partnerID=40&md5=7b8d90ccfd4d31da4a968fee43883202,2021,2021-07-20 15:49:07,2021-07-20 15:49:07
3MYK5YAE,journalArticle,2020,"Tang, C.; Ji, J.; Tang, Y.; Gao, S.; Tang, Z.; Todo, Y.",A novel machine learning technique for computer-aided diagnosis,"The primary motivation of this paper is twofold: first, to employ a heuristic optimization algorithm to optimize the dendritic neuron model (DNM) and second, to design a tidy visual classifier for computer-aided diagnosis that can be easily implemented on a hardware system. Considering that the backpropagation (BP) algorithm is sensitive to the initial conditions and can easily fall into local minima, we propose an evolutionary dendritic neuron model (EDNM), which is optimized by the gbest-guided artificial bee colony (GABC) algorithm. The experiments are performed on the Liver Disorders Data Set, the Wisconsin Breast Cancer Data Set, the Haberman's Survival Data Set, the Diabetic Retinopathy Debrecen Data Set and Hepatitis Data Set, and the effectiveness of our model was rigorously validated in terms of the classification accuracy, the sensitivity, the specificity, the F_measure, Cohen's Kappa, the area under the receiver operating characteristic curve (AUC), convergence speed and the statistical analysis of the Wilcoxon signed-rank test. Moreover, after training, the EDNM can simplify its neural structure by removing redundant synapses and superfluous dendrites by the neuronal pruning mechanism. Finally, the simplified structural morphology of the EDNM can be replaced by a logic circuit (LC) without sacrificing accuracy. It is worth emphasizing that once implemented by an LC, the model has a significant advantage over other classifiers in terms of speed when handling big data. Consequently, our proposed model can serve as an efficient medical classifier with excellent performance. © 2020 Elsevier Ltd",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2020.103627,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082923054&doi=10.1016%2fj.engappai.2020.103627&partnerID=40&md5=7f26f6183e658a4b9c7cb5c60efb46f9,2020,2021-07-20 15:49:07,2021-07-20 15:49:07
VDHPWBD4,journalArticle,2019,"Hernandez, A.; Balasubramanian, A.; Yuan, F.; Mason, S.A.M.; Mueller, T.","Fast, accurate, and transferable many-body interatomic potentials by symbolic regression","The length and time scales of atomistic simulations are limited by the computational cost of the methods used to predict material properties. In recent years there has been great progress in the use of machine-learning algorithms to develop fast and accurate interatomic potential models, but it remains a challenge to develop models that generalize well and are fast enough to be used at extreme time and length scales. To address this challenge, we have developed a machine-learning algorithm based on symbolic regression in the form of genetic programming that is capable of discovering accurate, computationally efficient many-body potential models. The key to our approach is to explore a hypothesis space of models based on fundamental physical principles and select models within this hypothesis space based on their accuracy, speed, and simplicity. The focus on simplicity reduces the risk of overfitting the training data and increases the chances of discovering a model that generalizes well. Our algorithm was validated by rediscovering an exact Lennard-Jones potential and a Sutton-Chen embedded-atom method potential from training data generated using these models. By using training data generated from density functional theory calculations, we found potential models for elemental copper that are simple, as fast as embedded-atom models, and capable of accurately predicting properties outside of their training set. Our approach requires relatively small sets of training data, making it possible to generate training data using highly accurate methods at a reasonable computational cost. We present our approach, the forms of the discovered models, and assessments of their transferability, accuracy and speed. © 2019, The Author(s).",npj Computational Materials,10.1038/s41524-019-0249-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075185034&doi=10.1038%2fs41524-019-0249-1&partnerID=40&md5=5125ad2e925844ed917b5ac602787ca8,2019,2021-07-20 15:49:07,2021-07-20 15:49:07
YX2SCA8W,journalArticle,2018,"Veloso de Melo, V.; Banzhaf, W.",Automatic feature engineering for regression models with machine learning: An evolutionary computation and statistics hybrid,"Symbolic Regression (SR) is a well-studied task in Evolutionary Computation (EC), where adequate free-form mathematical models must be automatically discovered from observed data. Statisticians, engineers, and general data scientists still prefer traditional regression methods over EC methods because of the solid mathematical foundations, the interpretability of the models, and the lack of randomness, even though such deterministic methods tend to provide lower quality prediction than stochastic EC methods. On the other hand, while EC solutions can be big and uninterpretable, they can be created with less bias, finding high-quality solutions that would be avoided by human researchers. Another interesting possibility is using EC methods to perform automatic feature engineering for a deterministic regression method instead of evolving a single model; this may lead to smaller solutions that can be easy to understand. In this contribution, we evaluate an approach called Kaizen Programming (KP) to develop a hybrid method employing EC and Statistics. While the EC method builds the features, the statistical method efficiently builds the models, which are also used to provide the importance of the features; thus, features are improved over the iterations resulting in better models. Here we examine a large set of benchmark SR problems known from the EC literature. Our experiments show that KP outperforms traditional Genetic Programming - a popular EC method for SR - and also shows improvements over other methods, including other hybrids and well-known statistical and Machine Learning (ML) ones. More in line with ML than EC approaches, KP is able to provide high-quality solutions while requiring only a small number of function evaluations. © 2017 Elsevier Inc.",Information Sciences,10.1016/j.ins.2017.11.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85035783891&doi=10.1016%2fj.ins.2017.11.041&partnerID=40&md5=b187844100de50c4720976ff3ad26031,2018,2021-07-20 15:49:07,2021-07-20 15:49:07
BSA4TJKU,journalArticle,2017,"Zhao, J.; Papapetrou, P.; Asker, L.; Boström, H.",Learning from heterogeneous temporal data in electronic health records,"Electronic health records contain large amounts of longitudinal data that are valuable for biomedical informatics research. The application of machine learning is a promising alternative to manual analysis of such data. However, the complex structure of the data, which includes clinical events that are unevenly distributed over time, poses a challenge for standard learning algorithms. Some approaches to modeling temporal data rely on extracting single values from time series; however, this leads to the loss of potentially valuable sequential information. How to better account for the temporality of clinical data, hence, remains an important research question. In this study, novel representations of temporal data in electronic health records are explored. These representations retain the sequential information, and are directly compatible with standard machine learning algorithms. The explored methods are based on symbolic sequence representations of time series data, which are utilized in a number of different ways. An empirical investigation, using 19 datasets comprising clinical measurements observed over time from a real database of electronic health records, shows that using a distance measure to random subsequences leads to substantial improvements in predictive performance compared to using the original sequences or clustering the sequences. Evidence is moreover provided on the quality of the symbolic sequence representation by comparing it to sequences that are generated using domain knowledge by clinical experts. The proposed method creates representations that better account for the temporality of clinical events, which is often key to prediction tasks in the biomedical domain. © 2016 The Author(s)",Journal of Biomedical Informatics,10.1016/j.jbi.2016.11.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006158663&doi=10.1016%2fj.jbi.2016.11.006&partnerID=40&md5=7054733181307b0d4c8bb70b2676143f,2017,2021-07-20 15:49:07,2021-07-20 15:49:07
FHJCBRPD,journalArticle,2021,"Tong, Z.; Xu, P.; Denœux, T.",An evidential classifier based on Dempster-Shafer theory and deep learning,"We propose a new classifier based on Dempster-Shafer (DS) theory and a convolutional neural network (CNN) architecture for set-valued classification. In this classifier, called the evidential deep-learning classifier, convolutional and pooling layers first extract high-dimensional features from input data. The features are then converted into mass functions and aggregated by Dempster's rule in a DS layer. Finally, an expected utility layer performs set-valued classification based on mass functions. We propose an end-to-end learning strategy for jointly updating the network parameters. Additionally, an approach for selecting partial multi-class acts is proposed. Experiments on image recognition, signal processing, and semantic-relationship classification tasks demonstrate that the proposed combination of deep CNN, DS layer, and expected utility layer makes it possible to improve classification accuracy and to make cautious decisions by assigning confusing patterns to multi-class sets. © 2021 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2021.03.066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105043874&doi=10.1016%2fj.neucom.2021.03.066&partnerID=40&md5=abbe08c8d004387fe9c4da2c43f1e1bc,2021,2021-07-20 15:49:07,2021-07-20 15:49:07
GDY8SK4B,journalArticle,2021,"Minaee, S.; Boykov, Y.Y.; Porikli, F.; Plaza, A.J.; Kehtarnavaz, N.; Terzopoulos, D.",Image Segmentation Using Deep Learning: A Survey,"Image segmentation is a key task in computer vision and image processing with important applications such as scene understanding, medical image analysis, robotic perception, video surveillance, augmented reality, and image compression, among others, and numerous segmentation algorithms are found in the literature. Against this backdrop, the broad success of Deep Learning (DL) has prompted the development of new image segmentation approaches leveraging DL models. We provide a comprehensive review of this recent literature, covering the spectrum of pioneering efforts in semantic and instance segmentation, including convolutional pixel-labeling networks, encoder-decoder architectures, multiscale and pyramid-based approaches, recurrent networks, visual attention models, and generative models in adversarial settings. We investigate the relationships, strengths, and challenges of these DL-based segmentation models, examine the widely used datasets, compare performances, and discuss promising research directions. IEEE",IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2021.3059968,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100948197&doi=10.1109%2fTPAMI.2021.3059968&partnerID=40&md5=115a97279a77fb75cf00d20ca1578417,2021,2021-07-20 15:49:07,2021-07-20 15:49:07
HDTCXJVY,journalArticle,2021,"Sayakkara, A.P.; Le-Khac, N.-A.",Forensic insights from smartphones through electromagnetic side-channel analysis,"The increasing use of smartphones has increased their presence in legal and corporate investigations. Unlike desktop and laptop computers, forensic analysis of smartphones is a challenging task due to their limited interfaces to retrieve information of forensic value. Electromagnetic side-channel analysis (EM-SCA) has been recently proposed as an alternative window to acquire forensic insights from computers, in particularly from Internet of Things devices. Along this line, this work experimentally evaluates the potential of extracting information of forensic value from smartphones through their EM radiation. Initially, a group of smartphones representing a diverse set of system-on-chip (SoC) processors were used to acquire EM radiation traces. Later, deep learning models were trained to detect various internal software behaviours running on the SoCs. The results of this work indicates that a wide variety of insights can be extracted from smartphones through EM side-channel, increasing the potential opportunities for digital forensic investigators. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3051921,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099728656&doi=10.1109%2fACCESS.2021.3051921&partnerID=40&md5=499fcf7b20261083a521202c6d4f21ac,2021,2021-07-20 15:49:07,2021-07-20 15:49:07
A9KYJTKS,journalArticle,2020,"Tian, Z.; Shi, W.; Tan, Z.; Qiu, J.; Sun, Y.; Jiang, F.; Liu, Y.",Deep Learning and Dempster-Shafer Theory Based Insider Threat Detection,"Organizations’ own personnel now have a greater ability than ever before to misuse their access to critical organizational assets. Insider threat detection is a key component in identifying rare anomalies in context, which is a growing concern for many organizations. Existing perimeter security mechanisms are proving to be ineffective against insider threats. As a prospective filter for the human analysts, a new deep learning based insider threat detection method that uses the Dempster-Shafer theory is proposed to handle both accidental as well as intentional insider threats via organization’s channels of communication in real time. The long short-term memory (LSTM) architecture together with multi-head attention mechanism is applied in this work to detect anomalous network behavior patterns. Furthermore, belief is updated with Dempster’s conditional rule and utilized to fuse evidence to achieve enhanced prediction. The CERT Insider Threat Dataset v6.2 is used to train the behavior model. Through performance evaluation, our proposed method is proven to be effective as an insider threat detection technique. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Mobile Networks and Applications,10.1007/s11036-020-01656-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092353073&doi=10.1007%2fs11036-020-01656-7&partnerID=40&md5=02fa37160e5eac7cc7df97cc897b506e,2020,2021-07-20 15:49:07,2021-07-20 15:49:07
UAF9VVZ7,journalArticle,2018,"Kim, Y.; Kim, M.; Goo, J.; Kim, H.",Learning Self-Informed Feature Contribution for Deep Learning-Based Acoustic Modeling,"In this paper, we introduce a new feature engineering approach for deep learning-based acoustic modeling, which utilizes input feature contributions. For this purpose, we propose an auxiliary deep neural network (DNN) called a feature contribution network (FCN) whose output layer is composed of sigmoid-based contribution gates. In our framework, the FCN tries to learn element-level discriminative contributions of input features and an acoustic model network (AMN) is trained by gated features generated by element-wise multiplication between contribution gate outputs and input features. In addition, we also propose a regularization method for the FCN, which helps the FCN to activate the minimum number of the gates. The proposed methods were evaluated on the TED-LIUM release 1 corpus. We applied the proposed methods to DNN- and long short-term memory-based AMNs. Experimental results results showed that AMNs with the FCNs consistently improved recognition performance compared with AMN-only frameworks. © 2014 IEEE.",IEEE/ACM Transactions on Audio Speech and Language Processing,10.1109/TASLP.2018.2858923,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050603905&doi=10.1109%2fTASLP.2018.2858923&partnerID=40&md5=91a6fa57094c17bc6437f236b75845e2,2018,2021-07-20 15:49:07,2021-07-20 15:49:07
C4QA6KZT,journalArticle,2018,"Jung, S.; Hwang, S.; Shin, H.; Shim, D.H.","Perception, Guidance, and Navigation for Indoor Autonomous Drone Racing Using Deep Learning","In autonomous drone racing, a drone is required to fly through the gates quickly without any collision. Therefore, it is important to detect the gates reliably using computer vision. However, due to the complications such as varying lighting conditions and gates seen overlapped, traditional image processing algorithms based on color and geometry of the gates tend to fail during the actual racing. In this letter, we introduce a convolutional neural network to estimate the center of a gate robustly. Using the detection results, we apply a line-of-sight guidance algorithm. The proposed algorithm is implemented using low cost, off-the-shelf hardware for validation. All vision processing is performed in real time on the onboard NVIDIA Jetson TX2 embedded computer. In a number of tests our proposed framework successfully exhibited fast and reliable detection and navigation performance in indoor environment. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2018.2808368,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060566396&doi=10.1109%2fLRA.2018.2808368&partnerID=40&md5=617fd17e74f7a053913af5678cf674de,2018,2021-07-20 15:49:07,2021-07-20 15:49:07
N3IWMVH6,journalArticle,2017,"Yin, H.; Wang, W.; Wang, H.; Chen, L.; Zhou, X.",Spatial-Aware Hierarchical Collaborative Deep Learning for POI Recommendation,"Point-of-interest (POI) recommendation has become an important way to help people discover attractive and interesting places, especially when they travel out of town. However, the extreme sparsity of user-POI matrix and cold-start issues severely hinder the performance of collaborative filtering-based methods. Moreover, user preferences may vary dramatically with respect to the geographical regions due to different urban compositions and cultures. To address these challenges, we stand on recent advances in deep learning and propose a Spatial-Aware Hierarchical Collaborative Deep Learning model (SH-CDL). The model jointly performs deep representation learning for POIs from heterogeneous features and hierarchically additive representation learning for spatial-aware personal preferences. To combat data sparsity in spatial-aware user preference modeling, both the collective preferences of the public in a given target region and the personal preferences of the user in adjacent regions are exploited in the form of social regularization and spatial smoothing. To deal with the multimodal heterogeneous features of the POIs, we introduce a late feature fusion strategy into our SH-CDL model. The extensive experimental analysis shows that our proposed model outperforms the state-of-the-art recommendation models, especially in out-of-town and cold-start recommendation scenarios. © 2017 IEEE.",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2017.2741484,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028512678&doi=10.1109%2fTKDE.2017.2741484&partnerID=40&md5=61ed8da3798968509eca6f5746f18eb5,2017,2021-07-20 15:49:07,2021-07-20 15:49:07
UXI3YFZE,journalArticle,2015,"Kaliszyk, C.; Urban, J.",MizAR 40 for Mizar 40,"As a present to Mizar on its 40th anniversary, we develop an AI/ATP system that in 30 seconds of real time on a 14-CPU machine automatically proves 40 % of the theorems in the latest official version of the Mizar Mathematical Library (MML). This is a considerable improvement over previous performance of large-theory AI/ATP methods measured on the whole MML. To achieve that, a large suite of AI/ATP methods is employed and further developed. We implement the most useful methods efficiently, to scale them to the 150000 formulas in MML. This reduces the training times over the corpus to 1–3 seconds, allowing a simple practical deployment of the methods in the online automated reasoning service for the Mizar users (MizAR$\mathbb A\mathbb R$). © 2015, The Author(s).",Journal of Automated Reasoning,10.1007/s10817-015-9330-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942549293&doi=10.1007%2fs10817-015-9330-8&partnerID=40&md5=3665378a4d5749c3f1399977175f353b,2015,2021-07-20 15:49:08,2021-07-20 15:49:08
48I88AWX,journalArticle,2012,"Michalski, R.S.; Wojtusiak, J.","Reasoning with unknown, not-applicable and irrelevant meta-values in concept learning and pattern discovery","This paper describes methods for reasoning with unknown, irrelevant, and not-applicable meta-values when learning concept descriptions from examples or discovering patterns in data. These types of meta-values represent different reasons for which regular values are not available, thus require different treatment in both rule learning and rule testing. The presented methods are handled internally, within the learning and testing algorithms, and not in preprocessing as those widely described in the literature. They have been implemented in the AQ21 multitask learning and knowledge discovery program, and experimentally tested on three real world and one designed datasets. © 2011 Springer Science+Business Media, LLC.",Journal of Intelligent Information Systems,10.1007/s10844-011-0186-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863981395&doi=10.1007%2fs10844-011-0186-z&partnerID=40&md5=09484d322a9e6dde64ea0857423cd33e,2012,2021-07-20 15:49:08,2021-07-20 15:49:08
7KQW8ICA,journalArticle,2019,"Fuller, T.J.","Cognitive Architecture, Holistic Inference and Bayesian Networks","Two long-standing arguments in cognitive science invoke the assumption that holistic inference is computationally infeasible. The first is Fodor’s skeptical argument toward computational modeling of ordinary inductive reasoning. The second advocates modular computational mechanisms of the kind posited by Cosmides, Tooby and Sperber. Based on advances in machine learning related to Bayes nets, as well as investigations into the structure of scientific and ordinary information, I maintain neither argument establishes its architectural conclusion. Similar considerations also undermine Fodor’s decades-long diagnosis of artificial intelligence research as confounded by an inability to circumscribe the amount of information relevant to inferential processes. This diagnosis is particularly inapposite with respect to Bayes nets, since one of their strengths as machine learning systems has been their capacity to reason probabilistically about large data sets whose size overwhelms the capacities of individual human reasoners. A general moral follows from these criticisms: Insights into artificial and human cognitive systems are likely to be cultivated by focusing greater attention on the structure and density of connections among items of information that are available to them. © 2019, Springer Nature B.V.",Minds and Machines,10.1007/s11023-019-09505-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073679712&doi=10.1007%2fs11023-019-09505-7&partnerID=40&md5=8aaf0c92ff8a3adb2183866b036b611e,2019,2021-07-20 15:49:08,2021-07-20 15:49:08
DJWBTRHZ,journalArticle,2020,"Magesh, P.R.; Myloth, R.D.; Tom, R.J.",An Explainable Machine Learning Model for Early Detection of Parkinson's Disease using LIME on DaTSCAN Imagery,"Parkinson's Disease (PD) is a degenerative and progressive neurological condition. Early diagnosis can improve treatment for patients and is performed through dopaminergic imaging techniques like the SPECT DaTSCAN. In this study, we propose a machine learning model that accurately classifies any given DaTSCAN as having Parkinson's disease or not, in addition to providing a plausible reason for the prediction. This kind of reasoning is done through the use of visual indicators generated using Local Interpretable Model-Agnostic Explainer (LIME) methods. DaTSCANs were drawn from the Parkinson's Progression Markers Initiative database and trained on a CNN (VGG16) using transfer learning, yielding an accuracy of 95.2%, a sensitivity of 97.5%, and a specificity of 90.9%. Keeping model interpretability of paramount importance, especially in the healthcare field, this study utilises LIME explanations to distinguish PD from non-PD, using visual superpixels on the DaTSCANs. It could be concluded that the proposed system, in union with its measured interpretability and accuracy may effectively aid medical workers in the early diagnosis of Parkinson's Disease. © 2020 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2020.104041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092534766&doi=10.1016%2fj.compbiomed.2020.104041&partnerID=40&md5=f0923c8ac48df7f6b40ce8fa0afd2cd9,2020,2021-07-20 15:49:08,2021-07-20 15:49:08
R3GCF79Y,journalArticle,2011,"Bascil, M.S.; Temurtas, F.",A study on hepatitis disease diagnosis using multilayer neural network with Levenberg Marquardt training algorithm,"In this study, a hepatitis disease diagnosis study was realized using neural network structure. For this purpose, a multilayer neural network structure was used. Levenberg-Marquardt algorithm was used as training algorithm for the weights update of the neural network. The results of the study were compared with the results of the previous studies reported focusing on hepatitis disease diagnosis and using same UCI machine learning database. We obtained a classification accuracy of 91.87% via tenfold cross validation. © 2009 Springer Science+Business Media, LLC.",Journal of Medical Systems,10.1007/s10916-009-9378-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959751877&doi=10.1007%2fs10916-009-9378-2&partnerID=40&md5=c8b538d96a18f5592a74d629bac992f2,2011,2021-07-20 15:49:08,2021-07-20 15:49:08
2RKWW4CU,journalArticle,2011,"Fan, C.-Y.; Chang, P.-C.; Lin, J.-J.; Hsieh, J.C.",A hybrid model combining case-based reasoning and fuzzy decision tree for medical data classification,"In this research, a hybrid model is developed by integrating a case-based data clustering method and a fuzzy decision tree for medical data classification. Two datasets from UCI Machine Learning Repository, i.e., liver disorders dataset and Breast Cancer Wisconsin (Diagnosis), are employed for benchmark test. Initially a case-based clustering method is applied to preprocess the dataset thus a more homogeneous data within each cluster will be attainted. A fuzzy decision tree is then applied to the data in each cluster and genetic algorithms (GAs) are further applied to construct a decision-making system based on the selected features and diseases identified. Finally, a set of fuzzy decision rules is generated for each cluster. As a result, the FDT model can accurately react to the test data by the inductions derived from the case-based fuzzy decision tree. The average forecasting accuracy for breast cancer of CBFDT model is 98.4% and for liver disorders is 81.6%. The accuracy of the hybrid model is the highest among those models compared. The hybrid model can produce accurate but also comprehensible decision rules that could potentially help medical doctors to extract effective conclusions in medical diagnosis. © 2010 Elsevier B.V. All rights reserved.",Applied Soft Computing Journal,10.1016/j.asoc.2009.12.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957898242&doi=10.1016%2fj.asoc.2009.12.023&partnerID=40&md5=e0954cf56e1a14032ecc27ea7b4d03e8,2011,2021-07-20 15:49:08,2021-07-20 15:49:08
2ASAPCSK,journalArticle,2020,"Ärje, J.; Raitoharju, J.; Iosifidis, A.; Tirronen, V.; Meissner, K.; Gabbouj, M.; Kiranyaz, S.; Kärkkäinen, S.",Human experts vs. machines in taxa recognition,"The step of expert taxa recognition currently slows down the response time of many bioassessments. Shifting to quicker and cheaper state-of-the-art machine learning approaches is still met with expert scepticism towards the ability and logic of machines. In our study, we investigate both the differences in accuracy and in the identification logic of taxonomic experts and machines. We propose a systematic approach utilizing deep Convolutional Neural Nets and extensively evaluate it over a multi-pose taxonomic dataset with hierarchical labels specifically created for this comparison. We also study the prediction accuracy on different ranks of taxonomic hierarchy in detail. We compare the results of Convolutional Neural Networks to human experts and support vector machines. Our results revealed that human experts using actual specimens yield the lowest classification error (CE¯=6.1%). However, a much faster, automated approach using deep Convolutional Neural Nets comes close to human accuracy (CE¯=11.4%) when a typical flat classification approach is used. Contrary to previous findings in the literature, we find that for machines following a typical flat classification approach commonly used in machine learning performs better than forcing machines to adopt a hierarchical, local per parent node approach used by human taxonomic experts (CE¯=13.8%). Finally, we publicly share our unique dataset to serve as a public benchmark dataset in this field. © 2020 Elsevier B.V.",Signal Processing: Image Communication,10.1016/j.image.2020.115917,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086801575&doi=10.1016%2fj.image.2020.115917&partnerID=40&md5=2f2cbf5e5af9ce6cd082dc90bd65f3e0,2020,2021-07-20 15:49:08,2021-07-20 15:49:08
7LHTB7DE,journalArticle,2018,"Shah, S.A.R.; Issac, B.",Performance comparison of intrusion detection systems and application of machine learning to Snort system,"This study investigates the performance of two open source intrusion detection systems (IDSs) namely Snort and Suricata for accurately detecting the malicious traffic on computer networks. Snort and Suricata were installed on two different but identical computers and the performance was evaluated at 10 Gbps network speed. It was noted that Suricata could process a higher speed of network traffic than Snort with lower packet drop rate but it consumed higher computational resources. Snort had higher detection accuracy and was thus selected for further experiments. It was observed that the Snort triggered a high rate of false positive alarms. To solve this problem a Snort adaptive plug-in was developed. To select the best performing algorithm for Snort adaptive plug-in, an empirical study was carried out with different learning algorithms and Support Vector Machine (SVM) was selected. A hybrid version of SVM and Fuzzy logic produced a better detection accuracy. But the best result was achieved using an optimised SVM with firefly algorithm with FPR (false positive rate) as 8.6% and FNR (false negative rate) as 2.2%, which is a good result. The novelty of this work is the performance comparison of two IDSs at 10 Gbps and the application of hybrid and optimised machine learning algorithms to Snort. © 2017 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2017.10.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033218982&doi=10.1016%2fj.future.2017.10.016&partnerID=40&md5=8f4f8be1af7bd0914471d550176f3250,2018,2021-07-20 15:49:08,2021-07-20 15:49:08
DKZCDC5T,journalArticle,2014,"Anifowose, F.; Adeniye, S.; Abdulraheem, A.",Recent advances in the application of computational intelligence techniques in oil and gas reservoir characterisation: A comparative study,"A comparative study of the predictive capabilities of recent advances in computational intelligence (CI) is presented. This study utilised the machine learning paradigm to evaluate the CI techniques while applying them to the prediction of porosity and permeability of heterogeneous petroleum reservoirs using six diverse well data sets. Porosity and permeability are the major petroleum reservoir properties that serve as indicators of reservoir quality and quantity. The results showed that the performance of support vector machines (SVM) and functional networks (FN) is competitively better than that of Type-2 fuzzy logic system (T2FLS) in terms of correlation coefficient. With execution time, FN and SVM were faster than T2FLS, which took the most time for both training and testing. The results also demonstrated the capability of SVM to handle small data sets. This work will assist artificial intelligence practitioners to determine the most appropriate technique to use especially in conditions of limited amount of data and low processing power. © 2014 Taylor & Francis.",Journal of Experimental and Theoretical Artificial Intelligence,10.1080/0952813X.2014.924577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84909955739&doi=10.1080%2f0952813X.2014.924577&partnerID=40&md5=120a3ce08cf6dcc74b26b5ee014a098e,2014,2021-07-20 15:49:08,2021-07-20 15:49:08
B3YKKZVJ,journalArticle,2021,"Ghosh, L.; Saha, S.; Konar, A.",Decoding emotional changes of android-gamers using a fused Type-2 fuzzy deep neural network,"With the fastest growing popularity of gaming applications on android phone, analyzing emotion changes of steadfast android-gamers have become a study of utmost interest among most of the psychologists. Recently, some android games are producing negative impacts to the gamers; even in the worst cases the effect is becoming life-threatening too. Most of the existing research works are based on psychological view-point of exploring the impact (positive/negative) of playing android games for the child and adult age-group. However, the online recognition of emotional state changes of the android-gamers while playing video games may be relatively unexplored. To fill this void, the present study proposes a novel method of identifying the emotional state changes of android-gamers by decoding their brain signals and facial images simultaneously during playing video games. Besides above, the second novelty of the paper lies in designing a multimodal fusion method between brain signals and facial images for the said application. To address this challenge, the paper proposes a fused type-2 fuzzy deep neural network (FT2FDNN) which integrates the brain signal processing approach by a general type-2 fuzzy reasoning algorithm with the flavor of the image/video processing approach using a deep convolutional neural network. FT2FDNN uses multiple modalities to extract the similar information (here, emotional changes) simultaneously from the type-2 fuzzy and deep neural representations. The proposed fused type-2 fuzzy deep learning paradigm demonstrates promising results in classifying the emotional changes of gamers with high classification accuracy. Thus the proposed work explores a new era for future researchers. © 2020 Elsevier Ltd",Computers in Human Behavior,10.1016/j.chb.2020.106640,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097719466&doi=10.1016%2fj.chb.2020.106640&partnerID=40&md5=fe7d9b782c58b35979144c491d1edd42,2021,2021-07-20 15:49:08,2021-07-20 15:49:08
3KMBVK7P,journalArticle,2014,"Otte, C.",Interpretable semi-parametric regression models with defined error bounds,"Unreliable extrapolation of data-driven models hinders their applicability not only in safety-related domains. The paper discusses how model interpretability and uncertainty estimates can address this problem. A new semi-parametric approach is proposed for providing an interpretable model with improved accuracy by combining a symbolic regression model with a residual Gaussian Process. While the learned symbolic model is highly interpretable the residual model usually is not. However, by limiting the output of the residual model to a defined range a worst-case guarantee can be given in the sense that the maximal deviation from the symbolic model is always below a defined limit. The limitation of the residual model can include the uncertainty estimate of the Gaussian Process, thus giving the residual model more impact in high-confidence regions. When ranking the accuracy and interpretability of several different approaches on the SARCOS data benchmark the proposed combination yields the best result. © 2014 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2013.11.042,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904803159&doi=10.1016%2fj.neucom.2013.11.042&partnerID=40&md5=98307e3f95d7bf943a8cdbd460366593,2014,2021-07-20 15:49:08,2021-07-20 15:49:08
2Z2VPSI3,journalArticle,2011,"Papageorgiou, E.I.; Markinos, A.T.; Gemtos, T.A.",Fuzzy cognitive map based approach for predicting yield in cotton crop production as a basis for decision support system in precision agriculture application,"This work investigates the process of yield prediction in cotton crop production using the soft computing technique of fuzzy cognitive maps. Fuzzy cognitive map (FCM) is a fusion of fuzzy logic and cognitive map theories, and is used for modeling and representing experts' knowledge. It is capable of dealing with situations including uncertain descriptions using similar procedure such as human reasoning does. It is a challenging approach for decision making especially in complex processing environments. The FCM approach presented here was chosen to be utilized in agriculture because of the nature of the application. The prediction of yield in cotton production is a complex process with sufficient interacting parameters and FCMs are suitable for this kind of problem. Throughout this proposed method, FCMs designed and developed to represent experts' knowledge for cotton (Gossypium hirsutum L.) yield prediction and crop management. The developed FCM model consists of nodes linked by directed edges, where the nodes represent the main factors affecting cotton crop production such as texture, organic matter, pH, K, P, Mg, N, Ca, Na and cotton yield, and the directed edges show the cause-effect (weighted) relationships between the soil properties and cotton yield. The investigated methodology was evaluated for 360 cases measured during the time of six subsequent years (2001-2006) in a 5 ha experimental cotton field, in predicting the yield class between two possible categories (""low"" and ""high""). The results obtained reveal its comparative advantage over the benchmarking machine learning algorithms tested for the same data set for the years mentioned by providing decisions that match better with the real measured ones. The main advantage of this approach is its simple structure and flexibility, representing knowledge visually and more descriptively. Hence, it might be a convenient tool in predicting cotton yield and improving crop management. © 2011 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2011.01.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79954611831&doi=10.1016%2fj.asoc.2011.01.036&partnerID=40&md5=1639851e004d5502d75561a04227b891,2011,2021-07-20 15:49:08,2021-07-20 15:49:08
PBATN5JV,journalArticle,2020,"Bard, N.; Foerster, J.N.; Chandar, S.; Burch, N.; Lanctot, M.; Song, H.F.; Parisotto, E.; Dumoulin, V.; Moitra, S.; Hughes, E.; Dunning, I.; Mourad, S.; Larochelle, H.; Bellemare, M.G.; Bowling, M.",The Hanabi challenge: A new frontier for AI research,"From the early days of computing, games have been important testbeds for studying how well machines can do sophisticated decision making. In recent years, machine learning has made dramatic advances with artificial agents reaching superhuman performance in challenge domains like Go, Atari, and some variants of poker. As with their predecessors of chess, checkers, and backgammon, these game domains have driven research by providing sophisticated yet well-defined challenges for artificial intelligence practitioners. We continue this tradition by proposing the game of Hanabi as a new challenge domain with novel problems that arise from its combination of purely cooperative gameplay with two to five players and imperfect information. In particular, we argue that Hanabi elevates reasoning about the beliefs and intentions of other agents to the foreground. We believe developing novel techniques for such theory of mind reasoning will not only be crucial for success in Hanabi, but also in broader collaborative efforts, especially those with human partners. To facilitate future research, we introduce the open-source Hanabi Learning Environment, propose an experimental framework for the research community to evaluate algorithmic advances, and assess the performance of current state-of-the-art techniques. © 2019 The Authors",Artificial Intelligence,10.1016/j.artint.2019.103216,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076276822&doi=10.1016%2fj.artint.2019.103216&partnerID=40&md5=e7086c3f9d2a02bc508d363739fce2f9,2020,2021-07-20 15:49:09,2021-07-20 15:49:09
H7BCABJX,journalArticle,2014,"Wong, T.-L.",Learning markov logic networks with limited number of labeled training examples,"Markov Logic Networks (MLN) is a unified framework integrating first-order logic and probabilistic inference. Most existing methods of MLN learning are supervised approaches requiring a large amount of training examples, leading to a substantial amount of human effort for preparing these training examples. To reduce such human effort, we have developed a semi-supervised framework for learning an MLN, in particular structure learning of MLN, from a set of unlabeled data and a limited number of labeled training examples. To achieve this, we aim at maximizing the expected pseudo-log-likelihood function of the observation from the set of unlabeled data, instead of maximizing the pseudo-log-likelihood function of the labeled training examples, which is commonly used in supervised learning of MLN. To evaluate our proposed method, we have conducted experiments on two different datasets and the empirical results demonstrate that our framework is effective, outperforming existing approach which considers labeled training examples alone. © 2014 - IOS Press and the authors.",International Journal of Knowledge-Based and Intelligent Engineering Systems,10.3233/KES-140289,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900413572&doi=10.3233%2fKES-140289&partnerID=40&md5=e7a260fa1db9109cbda6b81e64178f64,2014,2021-07-20 15:49:09,2021-07-20 15:49:09
J5BDQFIM,journalArticle,2019,"Katzouris, N.; Artikis, A.; Paliouras, G.",Parallel online event calculus learning for complex event recognition,"Logic-based event recognition systems infer occurrences of events in time using a set of event definitions in the form of first-order rules. The Event Calculus is a temporal logic that has been used as a basis in event recognition applications, providing, among others, direct connections to machine learning, via Inductive Logic Programming (ILP). OLED is a recently proposed ILP system that learns event definitions in the form of Event Calculus theories, in a single pass over a data stream. We present two strategies for parallel online learning with OLED. We evaluate our proposed approaches on three datasets from the domains of activity recognition and maritime surveillance and show that they can significantly reduce training times, while they are capable of achieving super-linear speed-ups under certain circumstances. © 2018 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2018.11.033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058561210&doi=10.1016%2fj.future.2018.11.033&partnerID=40&md5=394582afdfdc3c6bc80435901c00465f,2019,2021-07-20 15:49:09,2021-07-20 15:49:09
RE66UWH3,journalArticle,2020,"Zhang, J.; Su, Q.; Wang, C.; Gu, H.",Monocular 3D vehicle detection with multi-instance depth and geometry reasoning for autonomous driving,"We propose MCK-NET, a novel monocular framework for 3D target detection, location and pose estimation in autonomous driving scenarios, in which the key challenge is how to effectively mine depth information from a monocular image. To tackle this ill-posed problem, we combine the relative instance-depth of multiple corners in a monocular image to explicitly construct the corresponding depth relations between interest regions, from which MCK-NET learns to detect and locate objects based on geometric reasoning. In addition, there are two significant features existing in MCK-NET: One is to use the relative relationship between the 2D center and the 3D center projection to help locate the 3D center. The other is that the geometric constraints are established, including semantic keypoints, 2D box center and the projection of 3D center, which can enhance the 3D corner detection and improve the estimation accuracy of 3D centers. Experiments on the KITTI datasets show that MCK-NET achieves the most advanced results in all three tasks and outperforms the current monocular methods. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.03.076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084528030&doi=10.1016%2fj.neucom.2020.03.076&partnerID=40&md5=25b15d38f56b279c0e8fc220c196bc77,2020,2021-07-20 15:49:09,2021-07-20 15:49:09
LMXZ46W9,journalArticle,2019,"Rakos, B.",Multiple-valued computing by dipole-dipole coupled proteins,"We demonstrate through a simple model validated by molecular dynamics-based simulations that terahertz-speed, many-valued logic computations, and digital signal processing, functional for several cycles of operations are potentially possible with electric field-effect, dipole-dipole coupled protein architectures with various dipole moment orientations. Many-valued logic can be applied in various areas, such as artificial intelligence, machine learning, and robotics. Furthermore, programmable logic arrays and field programmable gate arrays can also benefit from its implementation. Even top companies like Intel developed circuits based on such logic (eg, StrataFlash and a NOR flash memory). The present study suggests that multivalued logic states can be stored in a protein with the application of proper external electric fields, and digital signal propagation is potentially achievable using dipole-dipole coupled molecules, placed few nanometers (on the order of 10 nm) apart. Furthermore, we propose a Dronpa protein-based ternary logic gate, suitable for universal ternary logic computations. The architectures are potentially operational at room temperature. The proposed operational principle is not restricted to proteins only; it might be applied in case of other types of molecules or artificial structures exhibiting similar behavior. © 2019 The Authors International Journal of Circuit Theory and Applications Published by John Wiley & Sons Ltd",International Journal of Circuit Theory and Applications,10.1002/cta.2649,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070708281&doi=10.1002%2fcta.2649&partnerID=40&md5=856c4ed7c56d31cb881598545986522f,2019,2021-07-20 15:49:09,2021-07-20 15:49:09
SMGRV8YM,journalArticle,2021,"Dasalukunte, D.; Dorrance, R.; Liang, L.; Lu, L.",A Vector Processor for Mean Field Bayesian Channel Estimation,"Physical layer signal processing algorithms in the wireless domain are seeing increased use of machine learning algorithms, especially Bayesian methods. This work presents the hardware architecture and implementation of a vector processor for one such application, Bayesian channel estimation (CE) (BCE). The BCE vector processor supports a generic instruction set with a supplement of specialized instructions to realize Bayesian algorithms in the signal processing context. The vector processor is designed to work as an accelerator in a system-on-chip (SoC) with an AHB/AXI bus interface or as stand-alone unit. The vector processor achieves more than 4x improvement in performance when compared with a traditional CE algorithm running on a commercial vector processor. To the best of authors knowledge, this is a first known hardware implementation of a variational Bayesian inference algorithm for a wireless communication application. IEEE",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2021.3077408,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107182360&doi=10.1109%2fTVLSI.2021.3077408&partnerID=40&md5=116040522c86eedd24f2bfd53cda21c4,2021,2021-07-20 15:49:09,2021-07-20 15:49:09
B8H3TQZ9,journalArticle,2020,"Tu, P.; Pui, C.-W.; Young, E.F.Y.",Simultaneous Reconnection Surgery Technique of Routing with Machine Learning-Based Acceleration,"In global routing, both timing and routability are critical criteria to measure the performance of a design. However, these two objectives naturally conflict with each other during routing. In this paper, we propose reconnection approaches to fix timing. We first formulated a quadratic program (QP), which adjusts routing topologies of all the nets by only reconnecting critical sinks and takes congestion into consideration to tradeoff timing and routability objectives. A machine learning (ML)-based technique is applied to accelerate our algorithm, which offers a fast and effective way to solve the problem. By exploring more reconnection candidates, we then formulated a QP to reconnect any sink of a net and utilized a multilabel classifier to accelerate the process. The experimental results on ICCAD 2015 benchmarks show that our algorithms can achieve timing improvement with no significant degradation in routability and wirelength. With ML-based acceleration, our results can be obtained in almost negligible runtime. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2019.2912930,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065151234&doi=10.1109%2fTCAD.2019.2912930&partnerID=40&md5=3be3684f67324a9bd60440e69a28e04d,2020,2021-07-20 15:49:09,2021-07-20 15:49:09
IFRCPZ2Q,journalArticle,2019,"Guidotti, R.; Monreale, A.; Giannotti, F.; Pedreschi, D.; Ruggieri, S.; Turini, F.",Factual and Counterfactual Explanations for Black Box Decision Making,"The rise of sophisticated machine learning models has brought accurate but obscure decision systems, which hide their logic, thus undermining transparency, trust, and the adoption of artificial intelligence (AI) in socially sensitive and safety-critical contexts. We introduce a local rule-based explanation method, providing faithful explanations of the decision made by a black box classifier on a specific instance. The proposed method first learns an interpretable, local classifier on a synthetic neighborhood of the instance under investigation, generated by a genetic algorithm. Then, it derives from the interpretable classifier an explanation consisting of a decision rule, explaining the factual reasons of the decision, and a set of counterfactuals, suggesting the changes in the instance features that would lead to a different outcome. Experimental results show that the proposed method outperforms existing approaches in terms of the quality of the explanations and of the accuracy in mimicking the black box. © 2001-2011 IEEE.",IEEE Intelligent Systems,10.1109/MIS.2019.2957223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076272618&doi=10.1109%2fMIS.2019.2957223&partnerID=40&md5=e2b958485d1510a355f1cff6dbf0efa2,2019,2021-07-20 15:49:09,2021-07-20 15:49:09
DGEZSSJB,journalArticle,2017,"Nilashi, M.; Ibrahim, O.B.; Ahmadi, H.; Shahmoradi, L.",An analytical method for diseases prediction using machine learning techniques,"The use of medical datasets has attracted the attention of researchers worldwide. Data mining techniques have been widely used in developing decision support systems for diseases prediction through a set of medical datasets. In this paper, we propose a new knowledge-based system for diseases prediction using clustering, noise removal, and prediction techniques. We use Classification and Regression Trees (CART) to generate the fuzzy rules to be used in the knowledge-based system. We test our proposed method on several public medical datasets. Results on Pima Indian Diabetes, Mesothelioma, WDBC, StatLog, Cleveland and Parkinson's telemonitoring datasets show that proposed method remarkably improves the diseases prediction accuracy. The results showed that the combination of fuzzy rule-based, CART with noise removal and clustering techniques can be effective in diseases prediction from real-world medical datasets. The knowledge-based system can assist medical practitioners in the healthcare practice as a clinical analytical method. © 2017 Elsevier Ltd",Computers and Chemical Engineering,10.1016/j.compchemeng.2017.06.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020693969&doi=10.1016%2fj.compchemeng.2017.06.011&partnerID=40&md5=07ab32b48c01c6cb109624e5fcf000d0,2017,2021-07-20 15:49:09,2021-07-20 15:49:09
87HMS7LA,journalArticle,2015,"Chen, D.-R.; Chen, Y.-S.; Chen, L.-C.; Hsu, M.-Y.; Chiang, K.-F.",A Machine Learning Method for Power Prediction on the Mobile Devices,"Energy profiling and estimation have been popular areas of research in multicore mobile architectures. While short sequences of system calls have been recognized by machine learning as pattern descriptions for anomalous detection, power consumption of running processes with respect to system-call patterns are not well studied. In this paper, we propose a fuzzy neural network (FNN) for training and analyzing process execution behaviour with respect to series of system calls, parameters and their power consumptions. On the basis of the patterns of a series of system calls, we develop a power estimation daemon (PED) to analyze and predict the energy consumption of the running process. In the initial stage, PED categorizes sequences of system calls as functional groups and predicts their energy consumptions by FNN. In the operational stage, PED is applied to identify the predefined sequences of system calls invoked by running processes and estimates their energy consumption. © 2015, Springer Science+Business Media New York.",Journal of Medical Systems,10.1007/s10916-015-0320-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84940107821&doi=10.1007%2fs10916-015-0320-5&partnerID=40&md5=870b3f204e9c7ca7e01b096d9a18a873,2015,2021-07-20 15:49:09,2021-07-20 15:49:09
2JUK2NGV,journalArticle,2018,"Sen, S.; Raghunathan, A.",Approximate computing for Long Short Term Memory (LSTM) neural networks,"Long Short Term Memory (LSTM) networks are a class of recurrent neural networks that are widely used for machine learning tasks involving sequences, including machine translation, text generation, and speech recognition. Large-scale LSTMs, which are deployed in many real-world applications, are highly compute intensive. To address this challenge, we propose AxLSTM, an application of approximate computing to improve the execution efficiency of LSTMs. An LSTM is composed of cells, each of which contains a cell state along with multiple gating units that control the addition and removal of information from the state. The LSTM execution proceeds in timesteps, with a new symbol of the input sequence processed at each timestep. AxLSTM consists of two techniques-Dynamic Timestep Skipping (DTS) and Dynamic State Reduction (DSR). DTS identifies, at runtime, input symbols that are likely to have little or no impact on the cell state and skips evaluating the corresponding timesteps. In contrast, DSR reduces the size of the cell state in accordance with the complexity of the input sequence, leading to a reduced number of computations per timestep. We describe how AxLSTM can be applied to the most common application of LSTMs, viz., sequence-to-sequence learning. We implement AxLSTM within the TensorFlow deep learning framework and evaluate it on 3 state-of-the-art sequence-to-sequence models. On a 2.7 GHz Intel Xeon server with 128 GB memory and 32 processor cores, AxLSTM achieves 1.08 ×-1.31 × speedups with minimal loss in quality, and 1.12 ×-1.37 × speedups when moderate reductions in quality are acceptable. © 2018 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2858362,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050397709&doi=10.1109%2fTCAD.2018.2858362&partnerID=40&md5=b85f54db4f748d5bdf2e9e4c14185221,2018,2021-07-20 15:49:09,2021-07-20 15:49:09
7BC6KSYJ,journalArticle,2020,"Kate, R.J.",Automatic full conversion of clinical terms into SNOMED CT concepts,"SNOMED CT is the most comprehensive clinical ontology and is also amenable for automated reasoning. However, in order to unleash its full potential for automated reasoning over clinical text, a mechanism to convert clinical terms into SNOMED CT concepts is necessary. In this paper we present, to the best of our knowledge, the first such complete conversion method that is also capable of converting clinical terms into post-coordinated concepts which are not already listed in SNOMED CT. The method does not require any additional manual annotations and learns only from existing SNOMED CT terms paired with their concepts. The method is based on identifying the defining relations of the clinical concept expressed by a clinical term. We evaluate our method on a large-scale using existing data from SNOMED CT as well as on a small-scale using manually annotated dataset of clinical terms found in clinical text. © 2020 Elsevier Inc.",Journal of Biomedical Informatics,10.1016/j.jbi.2020.103585,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092925337&doi=10.1016%2fj.jbi.2020.103585&partnerID=40&md5=355a5e507e87766690e7348dc0e99b11,2020,2021-07-20 15:49:09,2021-07-20 15:49:09
UBYI8X7I,journalArticle,2011,"Perperis, T.; Giannakopoulos, T.; Makris, A.; Kosmopoulos, D.I.; Tsekeridou, S.; Perantonis, S.J.; Theodoridis, S.",Multimodal and ontology-based fusion approaches of audio and visual processing for violence detection in movies,"In this paper we present our research results towards the detection of violent scenes in movies, employing advanced fusion methodologies, based on learning, knowledge representation and reasoning. Towards this goal, a multi-step approach is followed: initially, automated audio and visual analysis is performed to extract audio and visual cues. Then, two different fusion approaches are deployed: (i) a multimodal one that provides binary decisions on the existence of violence or not, employing machine learning techniques, (ii) an ontological and reasoning one, that combines the audio-visual cues with violence and multimedia ontologies. The latter reasons out not only the existence of violence or not in a video scene, but also the type of violence (fight, screams, gunshots). Both approaches are experimentally tested, validated and compared for the binary decision problem of violence detection. Finally, results for the violence type identification are presented for the ontological fusion approach. For evaluation purposes, a large dataset of real movie data has been populated. © 2011 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2011.04.219,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959926975&doi=10.1016%2fj.eswa.2011.04.219&partnerID=40&md5=12a776fbf371a4091693ae9650bb4952,2011,2021-07-20 15:49:09,2021-07-20 15:49:09
MI4QDPFV,journalArticle,2020,"Dass, J.; Narawane, Y.; Mahapatra, R.N.; Sarin, V.",Distributed training of support vector machine on a multiple-fpga system,"Support Vector Machine (SVM) is a supervised machine learning model for classification tasks. Training SVM on a large number of data samples is challenging due to the high computational cost and memory requirement. Hence, model training is supported on a high-performance server which typically runs a sequential training algorithm on centralized data. However, as we move towards massive workloads, it will be impossible to store all the data in a centralized manner and expect such sequential training algorithms to scale on traditional processors. Moreover, with the growing demands of real-time machine learning for edge analytics, it is imperative to devise an efficient training framework with relatively cheaper computations and limited memory. Therefore, we propose and implement a first-of-its-kind system of multiple FPGAs as a distributed computing framework comprising up to eight FPGA units on Amazon F1 instances with negligible communication overhead to fully parallelize, accelerate, and scale the SVM training on decentralized data. Each FPGA unit has a pipelined SVM training IP logic core operating at 125 MHz with a power dissipation of 39 Watts for accelerating its allocated computations in the overall training process. We evaluate and compare the performance of the proposed system on five real SVM benchmarks. © 1968-2012 IEEE.",IEEE Transactions on Computers,10.1109/TC.2020.2993552,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086587429&doi=10.1109%2fTC.2020.2993552&partnerID=40&md5=b4ed6aacbb0f7f39afcfe992ffdc355a,2020,2021-07-20 15:49:10,2021-07-20 15:49:10
SQI9GEV7,journalArticle,2019,"Mrazek, V.; Sekanina, L.; Dobai, R.; Sys, M.; Svenda, P.",Efficient On-Chip Randomness Testing Utilizing Machine Learning Techniques,"Randomness testing is an important procedure that bit streams, produced by critical cryptographic primitives such as encryption functions and hash functions, have to undergo. In this paper, a new hardware platform for the randomness testing is proposed. The platform exploits the principles of genetic programming, which is a machine learning technique developed for the automated program and circuit design. The platform is capable of evolving efficient randomness distinguishers directly on a chip. Each distinguisher is represented as a Boolean polynomial in the algebraic normal form. The randomness testing is conducted for bit streams that are either stored in an on-chip memory or generated by a circuit placed on the chip. The platform is developed with a Xilinx Zynq-7000 All Programmable System on Chip that integrates a field programmable gate array with on-chip ARM processors. The platform is evaluated in terms of the quality of randomness testing, performance, and resources utilization. With power budget less than 3 W, the platform provides comparable randomness testing capabilities with the standard testing batteries running on a personal computer. © 1993-2012 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2019.2923848,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069509602&doi=10.1109%2fTVLSI.2019.2923848&partnerID=40&md5=49682bf756e8e9bc4c3cea93a39d8c17,2019,2021-07-20 15:49:10,2021-07-20 15:49:10
74QZV8X9,journalArticle,2020,"Wang, Y.; Xia, C.; Si, C.; Yao, B.; Wang, T.",Robust reasoning over heterogeneous textual information for fact verification,"Automatic fact verification (FV) based on artificial intelligence is considered as a promising approach which can be used to identify misinformation distributed on the web. Even though previous FV using deep learning have made great achievements in single dataset (e.g., FEVER), the trained systems are unlikely to be capable of extracting evidence from heterogeneous web-sources and validating claims in accordance with evidence found on the Internet. Nevertheless, the heterogeneity covers abundant semantic information, which will help FV system identify misinformation in a more accurate way. The current work is the first attempt to make the combination of knowledge graph (KG) and graph neural network (GNN) to enhance the robustness of FV systems for heterogeneous information. As a result, it can be generalized to multi-domain datasets after training on a sufficient single one. To make information update and aggregate well on the collaborative graph, the present study proposes a double graph attention network (DGAT) framework which recursively propagates the embeddings from a node's neighbors to refine the node's embedding as well as applies an attention mechanism to classify the importance of the neighbors. We train and evaluate our system on FEVER, a single and benchmark dataset for FV, and then re-evaluate our system on UKP Snopes Corpus, a new richly annotated corpus for FV tasks on the basis of heterogeneous web sources. According to experimental results, although DGAT has no excellent advantages in a single dataset, it shows outstanding performance in more realistic and multi-domain datasets. Moreover, the current study also provides a feasible method for deep learning to have the ability to infer heterogeneous information robustly. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3019586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091207880&doi=10.1109%2fACCESS.2020.3019586&partnerID=40&md5=0dcde93954508fe534c001ed1a88ae22,2020,2021-07-20 15:49:10,2021-07-20 15:49:10
NJFLVCKA,journalArticle,2015,"Petrova, A.; Ma, Y.; Tsatsaronis, G.; Kissa, M.; Distel, F.; Baader, F.; Schroeder, M.",Formalizing biomedical concepts from textual definitions,"Background: Ontologies play a major role in life sciences, enabling a number of applications, from new data integration to knowledge verification. SNOMED CT is a large medical ontology that is formally defined so that it ensures global consistency and support of complex reasoning tasks. Most biomedical ontologies and taxonomies on the other hand define concepts only textually, without the use of logic. Here, we investigate how to automatically generate formal concept definitions from textual ones. We develop a method that uses machine learning in combination with several types of lexical and semantic features and outputs formal definitions that follow the structure of SNOMED CT concept definitions. Results: We evaluate our method on three benchmarks and test both the underlying relation extraction component as well as the overall quality of output concept definitions. In addition, we provide an analysis on the following aspects: (1) How do definitions mined from the Web and literature differ from the ones mined from manually created definitions, e.g., MeSH? (2) How do different feature representations, e.g., the restrictions of relations' domain and range, impact on the generated definition quality?, (3) How do different machine learning algorithms compare to each other for the task of formal definition generation?, and, (4) What is the influence of the learning data size to the task? We discuss all of these settings in detail and show that the suggested approach can achieve success rates of over 90%. In addition, the results show that the choice of corpora, lexical features, learning algorithm and data size do not impact the performance as strongly as semantic types do. Semantic types limit the domain and range of a predicted relation, and as long as relations' domain and range pairs do not overlap, this information is most valuable in formalizing textual definitions. Conclusions: The analysis presented in this manuscript implies that automated methods can provide a valuable contribution to the formalization of biomedical knowledge, thus paving the way for future applications that go beyond retrieval and into complex reasoning. The method is implemented and accessible to the public from: https://github.com/alifahsyamsiyah/learningDL. © 2015 Petrova et al.; licensee BioMed Central.",Journal of Biomedical Semantics,10.1186/s13326-015-0015-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938800329&doi=10.1186%2fs13326-015-0015-3&partnerID=40&md5=03a5419cd4b5acfec04a6cfd3481444b,2015,2021-07-20 15:49:10,2021-07-20 15:49:10
SMHUZFII,journalArticle,2018,"Benabderrahmane, S.; Mellouli, N.; Lamolle, M.",On the predictive analysis of behavioral massive job data using embedded clustering and deep recurrent neural networks,"The recent proliferation of social networks as a main source of information and interaction has led to a huge expansion of automatic e-recruitment systems and by consequence the multiplication of web channels (job boards) that are dedicated to job offers disseminating. In a strategic and economic context where cost control is fundamental, it has become necessary to identify the relevant job board for a given new job offer has become necessary. The purpose of this work is to present the recent results that we have obtained on a new job board recommendation system that is a decision-making tool intended to guide recruiters while they are posting a job on the Internet. Firstly, the Doc2Vec embedded representation is used to analyse the textual content of the job offers, then the job applicant clickstreams history on various job boards are stored in a large learning database, and then represented as time series. Secondly, a deep neural network architecture is used to predict future values of the clicks on the job boards. Third, and in parallel, dimensionality reduction techniques are used to transform the clicks numerical time series into temporal symbolic sequences. Forecasting algorithms are then used to predict future symbols for each sequence. Finally, a list of top ranked job boards are kept by maximizing the clickstreams forecasting in both representations. Our experiments are tested on a real dataset, coming from a job-posting database of an industrial partner. The promising results have shown that using deep learning, the recommendation system outperforms standard multivariate models. © 2018 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2018.03.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044262883&doi=10.1016%2fj.knosys.2018.03.025&partnerID=40&md5=af25a1bd82743947ea3adc6877d61eec,2018,2021-07-20 15:49:10,2021-07-20 15:49:10
6ZFYIRK7,journalArticle,2021,"Lai, K.; Yanushkevich, S.N.; Shmerko, V.; Hou, M.",Capturing causality and bias in human action recognition,"Human action recognition using various sensors is a mandatory component of autonomous vehicles, humanoid robots, and ambient living environments. A particular interest is the detection and recognition of falls. In this paper, we propose the use of temporal convolution networks guided by knowledge distillation for detecting falls and recognizing types of falls using accelerometer data. Tri-axial accelerometers attached to the body measure the acceleration of the body joints when an action occurs. These data are used for pattern analysis and body action recognition. We demonstrate the existence of biases caused by soft biometrics when recognizing human body actions. We introduce a causal network to capture the influences of biases on system performance and illustrate how knowledge distillation can be applied to mitigate the bias effect. © 2021",Pattern Recognition Letters,10.1016/j.patrec.2021.04.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105558316&doi=10.1016%2fj.patrec.2021.04.008&partnerID=40&md5=e4dc9f1bb34aa1ebdb0fb327cd2b92ac,2021,2021-07-20 15:49:10,2021-07-20 15:49:10
9URQK9SB,journalArticle,2019,"Marie, F.; Corbat, L.; Chaussy, Y.; Delavelle, T.; Henriet, J.; Lapayre, J.-C.",Segmentation of deformed kidneys and nephroblastoma using Case-Based Reasoning and Convolutional Neural Network,"Most often, image segmentation is not fully automated and a user is required to lead the process in order to obtain correct results. In a medical context, segmentation can furnish much information to surgeons, but this task is rarely executed. Artificial Intelligence (AI) is a powerful approach for devising a viable solution to fully automated treatment. In this paper, we have focused on kidneys deformed by nephroblastoma. However, a frequent medical constraint is encountered which is a lack of sufficient data with which to train our system. In function of this constraint, two AI approaches were used to segment these structures. First, a Case Based Reasoning (CBR) approach was defined which can enhance the growth of regions for segmentation of deformed kidneys using an adaptation phase to modify coordinates of recovered seeds. This CBR approach was confronted with manual region growing and a Convolutional Neural Network (CNN). The CBR system succeeded in performing the best segmentation for the kidney with a mean Dice of 0.83. Deep Learning was then examined as a possible solution, using the latest performing networks for image segmentation. However, for relevant efficiency, this method requires a large data set. An option would be to manually segment only certain representative slices from a patient and then use them to train a Convolutional Neural Network (CNN) how to segment. In this article the authors propose an evaluation of a CNN for medical image segmentation following different training sets with a variable number of manual segmentations. To choose slices to train the CNN, an Overlearning Vector for Valid Sparse SegmentatIONs (OV2 ASSION) was used, with the notion of gap between two slices from the training set. This protocol made it possible to obtain reliable segmentations of per patient with a small data set and to determine that only 26% of initial segmented slices are required to obtain a complete segmentation of a patient with a mean Dice of 0.897. © 2019",Expert Systems with Applications,10.1016/j.eswa.2019.03.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062915757&doi=10.1016%2fj.eswa.2019.03.010&partnerID=40&md5=9e340be56439b06c3324d4d8aafe16ef,2019,2021-07-20 15:49:10,2021-07-20 15:49:10
452MQ7P7,journalArticle,2016,"Xu, S.; Liu, Z.; Zhang, Y.",Least squares support vector regression and interval type-2 fuzzy density weight for scene denoising,"Support vector machines are the popular machine learning techniques. Its variant least squares support vector regression (LS-SVR) is effective for image denoising. However, the fitting of the samples contaminated by noises in the training phase will result in the fact that LS-SVR cannot work well when noise level is too far from it or noise density is high. Type-2 fuzzy sets and systems have been shown to be a more promising method to manifest the uncertainties. Various noises would be taken as uncertainties in scene images. By integrating the design of learning weights with type-2 fuzzy sets, a systematic design methodology of interval type-2 fuzzy density weighted support vector regression (IT2FDW-SVR) model for scene denoising is presented to address the problem of sample uncertainty in scene images. A novel strategy is used to design the learning weights, which is similar to the selection of human experience. To handle the uncertainty of sample density, interval type-2 fuzzy logic system (IT2FLS) is employed to deduce the fuzzy learning weights (IT2FDW) in the IT2FDW-SVR, which is an extension of the previously weighted SVR. Extensive experimental results demonstrate that the proposed method can achieve better performances in terms of both objective and subjective evaluations than those state-of-the-art denoising techniques. © 2015, Springer-Verlag Berlin Heidelberg.",Soft Computing,10.1007/s00500-015-1598-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961054302&doi=10.1007%2fs00500-015-1598-4&partnerID=40&md5=7cfa31d534c25d6408424ac0281517a5,2016,2021-07-20 15:49:10,2021-07-20 15:49:10
QYVF8259,journalArticle,2012,"Schulte, O.; Khosravi, H.",Learning graphical models for relational data via lattice search,"Many machine learning applications that involve relational databases incorporate first-order logic and probability. Relational extensions of graphical models include Parametrized Bayes Net (Poole in IJCAI, pp. 985-991, 2003), Probabilistic RelationalModels (Getoor et al. in Introduction to statistical relational learning, pp. 129-173, 2007), and Markov Logic Networks (MLNs) (Domingos and Richardson in Introduction to statistical relational learning, 2007). Many of the current state-of-the-art algorithms for learning MLNs have focused on relatively small datasets with few descriptive attributes, where predicates are mostly binary and the main task is usually prediction of links between entities. This paper addresses what is in a sense a complementary problem: learning the structure of a graphical model that models the distribution of discrete descriptive attributes given the links between entities in a relational database. Descriptive attributes are usually nonbinary and can be very informative, but they increase the search space of possible candidate clauses. We present an efficient new algorithm for learning a Parametrized Bayes Net that performs a level-wise search through the table join lattice for relational dependencies. From the Bayes net we obtain an MLN structure via a standard moralization procedure for converting directed models to undirected models. Learning MLN structure by moralization is 200-1000 times faster and scores substantially higher in predictive accuracy than benchmark MLN algorithms on five relational databases. © The Author(s) 2012.",Machine Learning,10.1007/s10994-012-5289-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865227776&doi=10.1007%2fs10994-012-5289-4&partnerID=40&md5=b5ff98260824a25baf0bee6ddfd5eaa3,2012,2021-07-20 15:49:10,2021-07-20 15:49:10
CA82TQD6,journalArticle,2012,"A Santos, J.C.; Nassif, H.; Page, D.; Muggleton, S.H.; E Sternberg, M.J.",Automated identification of protein-ligand interaction features using Inductive Logic Programming: a hexose binding case study,"Background: There is a need for automated methods to learn general features of the interactions of a ligand class with its diverse set of protein receptors. An appropriate machine learning approach is Inductive Logic Programming (ILP), which automatically generates comprehensible rules in addition to prediction. The development of ILP systems which can learn rules of the complexity required for studies on protein structure remains a challenge. In this work we use a new ILP system, ProGolem, and demonstrate its performance on learning features of hexose-protein interactions.Results: The rules induced by ProGolem detect interactions mediated by aromatics and by planar-polar residues, in addition to less common features such as the aromatic sandwich. The rules also reveal a previously unreported dependency for residues cys and leu. They also specify interactions involving aromatic and hydrogen bonding residues. This paper shows that Inductive Logic Programming implemented in ProGolem can derive rules giving structural features of protein/ligand interactions. Several of these rules are consistent with descriptions in the literature.Conclusions: In addition to confirming literature results, ProGolem's model has a 10-fold cross-validated predictive accuracy that is superior, at the 95% confidence level, to another ILP system previously used to study protein/hexose interactions and is comparable with state-of-the-art statistical learners. © 2012 Santos et al.; licensee BioMed Central Ltd.",BMC Bioinformatics,10.1186/1471-2105-13-162,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863562709&doi=10.1186%2f1471-2105-13-162&partnerID=40&md5=19b3fdb0f6c10726088e80e3d8a814ac,2012,2021-07-20 15:49:10,2021-07-20 15:49:10
MS2IJ8C7,journalArticle,2019,"Dezaki, F.T.; Liao, Z.; Luong, C.; Girgis, H.; Dhungel, N.; Abdi, A.H.; Behnami, D.; Gin, K.; Rohling, R.; Abolmaesumi, P.; Tsang, T.",Cardiac Phase Detection in Echocardiograms with Densely Gated Recurrent Neural Networks and Global Extrema Loss,"Accurate detection of end-systolic (ES) and end-diastolic (ED) frames in an echocardiographic cine series can be difficult but necessary pre-processing step for the development of automatic systems to measure cardiac parameters. The detection task is challenging due to variations in cardiac anatomy and heart rate often associated with pathological conditions. We formulate this problem as a regression problem and propose several deep learning-based architectures that minimize a novel global extrema structured loss function to localize the ED and ES frames. The proposed architectures integrate convolution neural networks (CNNs)-based image feature extraction model and recurrent neural networks (RNNs) to model temporal dependencies between each frame in a sequence. We explore two CNN architectures: DenseNet and ResNet, and four RNN architectures: long short-term memory, bi-directional LSTM, gated recurrent unit (GRU), and Bi-GRU, and compare the performance of these models. The optimal deep learning model consists of a DenseNet and GRU trained with the proposed loss function. On average, we achieved 0.20 and 1.43 frame mismatch for the ED and ES frames, respectively, which are within reported inter-observer variability for the manual detection of these frames. © 1982-2012 IEEE.",IEEE Transactions on Medical Imaging,10.1109/TMI.2018.2888807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059000412&doi=10.1109%2fTMI.2018.2888807&partnerID=40&md5=1742425beee10a0d27e1b82cd3f2a747,2019,2021-07-20 15:49:10,2021-07-20 15:49:10
EA7XCDNS,journalArticle,2021,"Refonaa, J.; Lakshmi, M.",Remote sensing based rain fall prediction using big data assisted integrated routing framework,"At present, soft computing technology is the latest, most challenging technology used in prediction analysis of weather parameters. Therefore, the accuracy of the prediction in the area of rainfall analysis is very important. However, present methods in predicting rainfalls have less accuracy. In many cases, weather prediction models function poorly. Machine Learning methods ignore the effect of physical factors that make rainfall forecasting. It improves the accuracy of rainfall prediction and analysis and aims to predict rainfall with the related air density, relative humidity and temperature parameters. The most recent advanced remote sensing techniques with Artificial Neural Network model and Fuzzy Logic Control (FLC) associated with big data Assisted Integrated Routing and Surplus Memory (BIRSM) model has been developed with the available India Meteorological Department (IMD) data for the year from to Data preprocessing techniques are applied to the volume of available data. An appropriate and error-reducing evaluation for rainfall prediction is performed in combination with the proposed BIRSM model and Artificial Neural Network. Accurate and time series prediction can be very helpful for agriculture and flood management etc. © 2021, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-020-02726-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098778828&doi=10.1007%2fs12652-020-02726-0&partnerID=40&md5=7500309a1a9ef1ebac1e7dacc54a3abd,2021,2021-07-20 15:49:10,2021-07-20 15:49:10
9YGPA79Q,journalArticle,2012,"Wu, J.",A framework for learning comprehensible theories in XML document classification,"XML has become the universal data format for a wide variety of information systems. The large number of XML documents existing on the web and in other information storage systems makes classification an important task. As a typical type of semistructured data, XML documents have both structures and contents. Traditional text learning techniques are not very suitable for XML document classification as structures are not considered. This paper presents a novel complete framework for XML document classification. We first present a knowledge representation method for XML documents which is based on a typed higher order logic formalism. With this representation method, an XML document is represented as a higher order logic term where both its contents and structures are captured. We then present a decision-tree learning algorithm driven by precision/recall breakeven point (PRDT) for the XML classification problem which can produce comprehensible theories. Finally, a semi-supervised learning algorithm is given which is based on the PRDT algorithm and the cotraining framework. Experimental results demonstrate that our framework is able to achieve good performance in both supervised and semi-supervised learning with the bonus of producing comprehensible learning theories. © 2011 IEEE.",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2011.158,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82155192311&doi=10.1109%2fTKDE.2011.158&partnerID=40&md5=c242713f3de9c6f29df249f3b6f9b8d0,2012,2021-07-20 15:49:10,2021-07-20 15:49:10
4NTJ3CPI,journalArticle,2021,"Patro, B.N.; Anupriy; Namboodiri, V.P.",Probabilistic framework for solving visual dialog,"In this paper, we propose a probabilistic framework for solving the task of ‘Visual Dialog’. Solving this task requires reasoning and understanding of visual modality, language modality, and common sense knowledge to answer. Various architectures have been proposed to solve this task by variants of multi-modal deep learning techniques that combine visual and language representations. However, we believe that it is crucial to understand and analyze the sources of uncertainty for solving this task. Our approach allows for estimating uncertainty and also aids a diverse generation of answers. The proposed approach is obtained through a probabilistic representation module that provides us with representations for image, question and conversation history, a module that ensures that diverse latent representations for candidate answers are obtained given the probabilistic representations and an uncertainty representation module that chooses the appropriate answer that minimizes uncertainty. We thoroughly evaluate the model with a detailed ablation analysis, comparison with state of the art and visualization of the uncertainty that aids in the understanding of the method. Using the proposed probabilistic framework, we thus obtain an improved visual dialog system that is also more explainable. © 2020",Pattern Recognition,10.1016/j.patcog.2020.107586,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089570260&doi=10.1016%2fj.patcog.2020.107586&partnerID=40&md5=d0f350c9fb879d486ef94ef0c1b5941b,2021,2021-07-20 15:49:10,2021-07-20 15:49:10
MDRB9QX7,journalArticle,2018,"Tchuiev, V.; Indelman, V.",Inference Over Distribution of Posterior Class Probabilities for Reliable Bayesian Classification and Object-Level Perception,"State of the art Bayesian classification approaches typically maintain a posterior distribution over possible classes given available sensor observations (images). Yet, while these approaches fuse all classifier outputs thus far, they do not provide any indication regarding how reliable the posterior classification is, thus limiting its functionality in terms of autonomous systems and robotics. On the other hand, current deep learning based classifiers provide an uncertainty measure, thereby quantifying model uncertainty. However, they do so on a single frame basis and do not consider a sequential framework. In this letter, we develop a novel approach that infers a distribution over posterior class probabilities, while accounting for model uncertainty. This distribution enables reasoning about uncertainty in the posterior classification and, therefore, is of prime importance for robust classification, object-level perception in uncertain and ambiguous scenarios, and for safe autonomy in general. The distribution of the posterior class probability has no known analytical solution; thus, we propose to approximate this distribution via sampling. We evaluate our approach in simulation and using real images fed into a convolutional neural network classifier. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2018.2852844,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063307535&doi=10.1109%2fLRA.2018.2852844&partnerID=40&md5=8e1cf23880d8e5a008694663fddfeb86,2018,2021-07-20 15:49:10,2021-07-20 15:49:10
F4IUDQR9,journalArticle,2018,"Czajka, Ł.; Kaliszyk, C.",Hammer for Coq: Automation for Dependent Type Theory,"Hammers provide most powerful general purpose automation for proof assistants based on HOL and set theory today. Despite the gaining popularity of the more advanced versions of type theory, such as those based on the Calculus of Inductive Constructions, the construction of hammers for such foundations has been hindered so far by the lack of translation and reconstruction components. In this paper, we present an architecture of a full hammer for dependent type theory together with its implementation for the Coq proof assistant. A key component of the hammer is a proposed translation from the Calculus of Inductive Constructions, with certain extensions introduced by Coq, to untyped first-order logic. The translation is “sufficiently” sound and complete to be of practical use for automated theorem provers. We also introduce a proof reconstruction mechanism based on an eauto-type algorithm combined with limited rewriting, congruence closure and some forward reasoning. The algorithm is able to re-prove in the Coq logic most of the theorems established by the ATPs. Together with machine-learning based selection of relevant premises this constitutes a full hammer system. The performance of the whole procedure is evaluated in a bootstrapping scenario emulating the development of the Coq standard library. For each theorem in the library only the previous theorems and proofs can be used. We show that 40.8% of the theorems can be proved in a push-button mode in about 40 s of real time on a 8-CPU system. © 2018, The Author(s).",Journal of Automated Reasoning,10.1007/s10817-018-9458-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042537928&doi=10.1007%2fs10817-018-9458-4&partnerID=40&md5=fa58cc70db865b9d356c25b37b728c49,2018,2021-07-20 15:49:11,2021-07-20 15:49:11
B8H47IUG,journalArticle,2011,"Dehbi, Y.; Plümer, L.",Learning grammar rules of building parts from precise models and noisy observations,"The automatic interpretation of dense three-dimensional (3D) point clouds is still an open research problem. The quality and usability of the derived models depend to a large degree on the availability of highly structured models which represent semantics explicitly and provide a priori knowledge to the interpretation process. The usage of formal grammars for modelling man-made objects has gained increasing interest in the last few years. In order to cope with the variety and complexity of buildings, a large number of fairly sophisticated grammar rules are needed. As yet, such rules mostly have to be designed by human experts. This article describes a novel approach to machine learning of attribute grammar rules based on the Inductive Logic Programming paradigm. Apart from syntactic differences, logic programs and attribute grammars are basically the same language. Attribute grammars extend context-free grammars by attributes and semantic rules and provide a much larger expressive power. Our approach to derive attribute grammars is able to deal with two kinds of input data. On the one hand, we show how attribute grammars can be derived from precise descriptions in the form of examples provided by a human user as the teacher. On the other hand, we present the acquisition of models from noisy observations such as 3D point clouds. This includes the learning of geometric and topological constraints by taking measurement errors into account. The feasibility of our approach is proven exemplarily by stairs, and a generic framework for learning other building parts is discussed. Stairs aggregate an arbitrary number of steps in a manner which is specified by topological and geometric constraints and can be modelled in a recursive way. Due to this recursion, they pose a special challenge to machine learning. In order to learn the concept of stairs, only a small number of examples were required. Our approach represents and addresses the quality of the given observations and the derived constraints explicitly, using concepts from uncertain projective geometry for learning geometric relations and the Wakeby distribution together with decision trees for topological relations. © 2010 International Society for Photogrammetry and Remote Sensing, Inc.(ISPRS).",ISPRS Journal of Photogrammetry and Remote Sensing,10.1016/j.isprsjprs.2010.10.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79951682749&doi=10.1016%2fj.isprsjprs.2010.10.001&partnerID=40&md5=2cb6f0e867771861b9188abe9f80b571,2011,2021-07-20 15:49:11,2021-07-20 15:49:11
J2MLKV49,journalArticle,2019,"Licato, J.; Zhang, Z.",Evaluating representational systems in artificial intelligence,"All artificial reasoners work within representational systems. These systems, which may have varying levels of formality or detail, determine the space of possible representations over which the artificial reasoner can operate, by defining the syntactic and semantic properties of the symbols, structures, and inferences that they manipulate. But we are now seeing an increasing need for the ability to reason over representational systems, rather than just working within them. A prerequisite of performing such reasoning is the ability to evaluate and compare representational objects (and to know the difference between them). We survey the criteria that are used for such evaluations in AI, machine learning, and other AI-related fields. To aid our survey, we introduce a formalism of representations, representational systems, and representational spaces that lends itself nicely to an analysis of the criteria typically used for evaluating them. © 2017, Springer Science+Business Media B.V., part of Springer Nature.",Artificial Intelligence Review,10.1007/s10462-017-9598-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037661853&doi=10.1007%2fs10462-017-9598-7&partnerID=40&md5=65fe7e56115110773c6f7d3c0abdcd7f,2019,2021-07-20 15:49:11,2021-07-20 15:49:11
QQA8KEZY,journalArticle,2016,"Xu, P.; Davoine, F.; Zha, H.; Denœux, T.",Evidential calibration of binary SVM classifiers,"In machine learning problems, the availability of several classifiers trained on different data or features makes the combination of pattern classifiers of great interest. To combine distinct sources of information, it is necessary to represent the outputs of classifiers in a common space via a transformation called calibration. The most classical way is to use class membership probabilities. However, using a single probability measure may be insufficient to model the uncertainty induced by the calibration step, especially in the case of few training data. In this paper, we extend classical probabilistic calibration methods to the evidential framework. Experimental results from the calibration of SVM classifiers show the interest of using belief functions in classification problems. © 2015 Elsevier Inc. All rights reserved.",International Journal of Approximate Reasoning,10.1016/j.ijar.2015.05.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929897940&doi=10.1016%2fj.ijar.2015.05.002&partnerID=40&md5=9d58177104d8f8dcd36035632cfde2af,2016,2021-07-20 15:49:11,2021-07-20 15:49:11
YB9NBAB2,journalArticle,2019,"Ribeiro, R.; Casanova, D.; Teixeira, M.; Wirth, A.; Gomes, H.M.; Borges, A.P.; Enembreck, F.",Generating action plans for poultry management using artificial neural networks,"The fundamental role for poultry farmers to be successful in their activities is to precisely increase, decrease, or maintain, in a short time span, factors that determine poultry growth, such as humidity, temperature, amount of feed ration, ventilation, and others. Although there are modern automatic control technologies supporting these aspects, systems are architected to react to environmental conditions based on predefined programmed control rules, without considering knowledge readings from historical data and, most importantly, the human specialist's reasoning. In practice, when control actions diverge from the specialist's opinion, signals of the automatic controller are immediately intercepted (via the system interface) to recalibrate them for a different control rule to be applied based on human perception. As the set of parameters tends to be large and they are frequently combined, whereas human perception tends to be limited, this intervention of automatic control tends to be an error-prone decision-making option. In this paper, we demonstrate that action plans for poultry management can be derived by systematically collecting data from the production environment. A sensor network is used to register poultry management data, which are then preprocessed using machine-learning techniques. To validate the obtained results, we compare them against action plans generated by a human specialist and baseline results. Analysis suggest that action plans derived from the proposed model follow, with acceptable accuracy, the control actions that should be taken by the controller when considering a knowledge-based perception that absorbs expert reasoning and best practices guidelines. The benefits of the proposed approach are discussed regarding economic factors such as average broiler weight and feed conversion ratio. © 2018 Elsevier B.V.",Computers and Electronics in Agriculture,10.1016/j.compag.2018.02.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044148168&doi=10.1016%2fj.compag.2018.02.017&partnerID=40&md5=865f007215cdf02d28bf346ece43d8a5,2019,2021-07-20 15:49:11,2021-07-20 15:49:11
TRMR26MF,journalArticle,2020,"Nasiri, S.; Helsper, J.; Jung, M.; Fathi, M.",DePicT Melanoma Deep-CLASS: A deep convolutional neural networks approach to classify skin lesion images,"Background: Melanoma results in the vast majority of skin cancer deaths during the last decades, even though this disease accounts for only one percent of all skin cancers' instances. The survival rates of melanoma from early to terminal stages is more than fifty percent. Therefore, having the right information at the right time by early detection with monitoring skin lesions to find potential problems is essential to surviving this type of cancer. Results: An approach to classify skin lesions using deep learning for early detection of melanoma in a case-based reasoning (CBR) system is proposed. This approach has been employed for retrieving new input images from the case base of the proposed system DePicT Melanoma Deep-CLASS to support users with more accurate recommendations relevant to their requested problem (e.g., image of affected area). The efficiency of our system has been verified by utilizing the ISIC Archive dataset in analysis of skin lesion classification as a benign and malignant melanoma. The kernel of DePicT Melanoma Deep-CLASS is built upon a convolutional neural network (CNN) composed of sixteen layers (excluding input and ouput layers), which can be recursively trained and learned. Our approach depicts an improved performance and accuracy in testing on the ISIC Archive dataset. Conclusions: Our methodology derived from a deep CNN, generates case representations for our case base to use in the retrieval process. Integration of this approach to DePicT Melanoma CLASS, significantly improving the efficiency of its image classification and the quality of the recommendation part of the system. The proposed method has been tested and validated on 1796 dermoscopy images. Analyzed results indicate that it is efficient on malignancy detection. © 2020 The Author(s).",BMC Bioinformatics,10.1186/s12859-020-3351-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081908399&doi=10.1186%2fs12859-020-3351-y&partnerID=40&md5=20d7de9278f463c89ed06cb92cf3da3d,2020,2021-07-20 15:49:11,2021-07-20 15:49:11
V7HN2M8V,journalArticle,2018,"Zhou, W.; Li, J.; Zhang, M.; Wang, Y.; Shah, F.",Deep Learning Modeling for Top-N Recommendation with Interests Exploring,"Recommender systems (RS) currently play a crucial role in information filtering and retrieval, and have been ubiquitously applied in many domains, although suffering from such data sparsity and cold start problems. There are plenty of studies that try to make efforts to improve the performance of RS through different aspects, such as traditional matrix factorization technique and deep learning methods in recent years, however, it's still a challenging issue under research. In this paper, motivated by this, a two-stage deep learning-based model for top-N recommendation with interests exploring (DLMR) is proposed: 1) DLMR explores latent interests for each user, captures factors from reviews and contextual information via convolutional neural network, and performs convolutional matrix factorization to generate the candidates list; 2) In order to enhance the recommendation performance, DLMR further conducts candidates ranking through a three-layer denoising autoencoder, with taking account of heterogeneous side information. The DLMR provides a flexible scheme to leverage the available resources for recommendation, which is able to explore user's latent interests, capture the intricate interactions between users and items, and provide accurate and personalized recommendations. Experimental analysis over real world data sets demonstrates that DLMR could provide high performance top-N recommendation in sparse settings and outperform state-of-the-art recommender approaches significantly. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2869924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053340876&doi=10.1109%2fACCESS.2018.2869924&partnerID=40&md5=3a868668307178ab6d22618142b897c1,2018,2021-07-20 15:49:11,2021-07-20 15:49:11
63R2FPMC,journalArticle,2015,"Du, Y.; Guo, Y.",Evidence reasoning method for constructing conditional probability tables in a Bayesian network of multimorbidity,"BACKGROUND: The intrinsic mechanism of multimorbidity is difficult to recognize and prediction and diagnosis are difficult to carry out accordingly. Bayesian networks can help to diagnose multimorbidity in health care, but it is difficult to obtain the conditional probability table (CPT) because of the lack of clinically statistical data. OBJECTIVE: Today, expert knowledge and experience are increasingly used in training Bayesian networks in order to help predict or diagnose diseases, but the CPT in Bayesian networks is usually irrational or ineffective for ignoring realistic constraints especially in multimorbidity. METHODS: In order to solve these problems, an evidence reasoning (ER) approach is employed to extract and fuse inference data from experts using a belief distribution and recursive ER algorithm, based on which evidence reasoning method for constructing conditional probability tables in Bayesian network of multimorbidity is presented step by step. RESULTS: A multimorbidity numerical example is used to demonstrate the method and prove its feasibility and application. Bayesian network can be determined as long as the inference assessment is inferred by each expert according to his/her knowledge or experience. CONCLUSIONS: Our method is more effective than existing methods for extracting expert inference data accurately and is fused effectively for constructing CPTs in a Bayesian network of multimorbidity. © IOS Press and the authors. All rights reserved.",Technology and Health Care,10.3233/thc-150950,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930356739&doi=10.3233%2fthc-150950&partnerID=40&md5=9668df623699b3fb7550b8ebbaba2307,2015,2021-07-20 15:49:11,2021-07-20 15:49:11
U5AE2H6C,journalArticle,2016,"Li, C.; Rusák, Z.; Horváth, I.; Ji, L.",Development of engagement evaluation method and learning mechanism in an engagement enhancing rehabilitation system,"Maintaining and enhancing engagement of patients during stroke rehabilitation exercises are in the focus of current research. There have been various methods and computer supported tools developed for this purpose, which try to avoid mundane exercising that is prone to become a routine or even boring for the patients and leads to ineffective training. This paper proposes a strategy bundle-based smart learning mechanism (SLM) to increase the efficiency of rehabilitation exercises. The underpinning strategy considers motor, perceptive, cognitive and emotional aspects of engagement. Part of a cyber-physical stroke rehabilitation system (CP-SRS), the proposed SLM is able to learn the relationship between the actual engagement levels and applied stimulations. From a computational point of view, the SLM is based on multiplexed signal processing and a machine learning agent. The paper presents the mathematical concepts of signal processing, the reasoning algorithms, and the overall embedding of the SLM in the CP-SRS. Regression and classification are two possible solutions for this learning mechanism. Computer simulation is conducted to investigate the limitations of the proposed learning mechanism and compare the results of different machine learning methods. We simulate regression with artificial neural network (ANN), and classification with ANN and Naive Bayes (NB). Results show that classification with NB is more promising in practice since it is less sensitive to the deviations in the inputs than the applied version of ANN. © 2016 Elsevier Ltd. All rights reserved.",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2016.01.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956866887&doi=10.1016%2fj.engappai.2016.01.021&partnerID=40&md5=8b0547f6785309b19807a9d2e23670ec,2016,2021-07-20 15:49:11,2021-07-20 15:49:11
ZFZWRJLU,journalArticle,2014,"Brown, E.T.; Ottley, A.; Zhao, H.; Lin, Q.; Souvenir, R.; Endert, A.; Chang, R.",Finding waldo: Learning about users from their interactions,"Visual analytics is inherently a collaboration between human and computer. However, in current visual analytics systems, the computer has limited means of knowing about its users and their analysis processes. While existing research has shown that a user's interactions with a system reflect a large amount of the user's reasoning process, there has been limited advancement in developing automated, real-time techniques that mine interactions to learn about the user. In this paper, we demonstrate that we can accurately predict a user's task performance and infer some user personality traits by using machine learning techniques to analyze interaction data. Specifically, we conduct an experiment in which participants perform a visual search task, and apply well-known machine learning algorithms to three encodings of the users' interaction data. We achieve, depending on algorithm and encoding, between 62% and 83% accuracy at predicting whether each user will be fast or slow at completing the task. Beyond predicting performance, we demonstrate that using the same techniques, we can infer aspects of the user's personality factors, including locus of control, extraversion, and neuroticism. Further analyses show that strong results can be attained with limited observation time: in one case 95% of the final accuracy is gained after a quarter of the average task completion time. Overall, our findings show that interactions can provide information to the computer about its human collaborator, and establish a foundation for realizing mixed-initiative visual analytics systems. © 1995-2012 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2014.2346575,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910040498&doi=10.1109%2fTVCG.2014.2346575&partnerID=40&md5=2d35c5f63ed0c84b39fca0044f09b7dd,2014,2021-07-20 15:49:11,2021-07-20 15:49:11
GUQ353EH,journalArticle,2017,"Olier, J.S.; Barakova, E.; Regazzoni, C.; Rauterberg, M.",Re-framing the characteristics of concepts and their relation to learning and cognition in artificial agents,"In this work, the problems of knowledge acquisition and information processing are explored in relation to the definitions of concepts and conceptual processing, and their implications for artificial agents. The discussion focuses on views of cognition as a dynamic property in which the world is actively represented in grounded mental states which only have meaning in the action context. Reasoning is understood as an emerging property consequence of actions-environment couplings achieved through experience, and concepts as situated and dynamic phenomena enabling behaviours. Re-framing the characteristics of concepts is considered crucial to overcoming settled beliefs and reinterpreting new understandings in artificial systems. The first part presents a review of concepts from cognitive sciences. Support is found for views on grounded and embodied cognition, describing concepts as dynamic, flexible, context-dependent, and distributedly coded. That is argued to contrast with many technical implementations assuming concepts as categories, whilst explains limitations when grounding amodal symbols, or in unifying learning, perception and reasoning. The characteristics of concepts are linked to methods of active inference, self-organization, and deep learning to address challenges posed and to reinterpret emerging techniques. In a second part, an architecture based on deep generative models is presented to illustrate arguments elaborated. It is evaluated in a navigation task, showing that sufficient representations are created regarding situated behaviours with no semantics imposed on data. Moreover, adequate behaviours are achieved through a dynamic integration of perception and action in a single representational domain and process. © 2017 Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2017.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017450328&doi=10.1016%2fj.cogsys.2017.03.005&partnerID=40&md5=0fece0df38c76170cacc3d42c6b6dc74,2017,2021-07-20 15:49:11,2021-07-20 15:49:11
FLTMZA7C,journalArticle,2020,"Tuncali, C.E.; Fainekos, G.; Prokhorov, D.; Ito, H.; Kapinski, J.",Requirements-Driven Test Generation for Autonomous Vehicles with Machine Learning Components,"Autonomous vehicles are complex systems that are challenging to test and debug. A requirements-driven approach to the development process can decrease the resources required to design and test these systems, while simultaneously increasing the reliability. We present a testing framework that uses signal temporal logic (STL), which is a precise and unambiguous requirements language. Our framework evaluates test cases against the STL formulae and additionally uses the requirements to automatically identify test cases that fail to satisfy the requirements. One of the key features of our tool is the support for machine learning (ML) components in the system design, such as deep neural networks. The framework allows evaluation of the control algorithms, including the ML components, and it also includes models of CCD camera, lidar, and radar sensors, as well as the vehicle environment. We use multiple methods to generate test cases, including covering arrays, which is an efficient method to search discrete variable spaces. The resulting test cases can be used to debug the controller design by identifying controller behaviors that do not satisfy requirements. The test cases can also enhance the testing phase of development by identifying critical corner cases that correspond to the limits of the system's allowed behaviors. We present STL requirements for an autonomous vehicle system, which capture both component-level and system-level behaviors. Additionally, we present three driving scenarios and demonstrate how our requirements-driven testing framework can be used to identify critical system behaviors, which can be used to support the development process. © 2016 IEEE.",IEEE Transactions on Intelligent Vehicles,10.1109/TIV.2019.2955903,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085714540&doi=10.1109%2fTIV.2019.2955903&partnerID=40&md5=9c55b862e14965c80b1e1fe6173c44b8,2020,2021-07-20 15:49:11,2021-07-20 15:49:11
V9WYKD3I,journalArticle,2017,"Petrangeli, S.; Wu, T.; Wauters, T.; Huysegems, R.; Bostoen, T.; De Turck, F.",A machine learning-based framework for preventing video freezes in HTTP adaptive streaming,"HTTP Adaptive Streaming (HAS) represents the dominant technology to deliver videos over the Internet, due to its ability to adapt the video quality to the available bandwidth. Despite that, HAS clients can still suffer from freezes in the video playout, the main factor influencing users’ Quality of Experience (QoE). To reduce video freezes, we propose a network-based framework, where a network controller prioritizes the delivery of particular video segments to prevent freezes at the clients. This framework is based on OpenFlow, a widely adopted protocol to implement the software-defined networking principle. The main element of the controller is a Machine Learning (ML) engine based on the random undersampling boosting algorithm and fuzzy logic, which can detect when a client is close to a freeze and drive the network prioritization to avoid it. This decision is based on measurements collected from the network nodes only, without any knowledge on the streamed videos or on the clients' characteristics. In this paper, we detail the design of the proposed ML-based framework and compare its performance with other benchmarking HAS solutions, under various video streaming scenarios. Particularly, we show through extensive experimentation that the proposed approach can reduce video freezes and freeze time with about 65% and 45% respectively, when compared to benchmarking algorithms. These results represent a major improvement for the QoE of the users watching multimedia content online. © 2017 Elsevier Ltd",Journal of Network and Computer Applications,10.1016/j.jnca.2017.07.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023613545&doi=10.1016%2fj.jnca.2017.07.009&partnerID=40&md5=da6b1aa64c73b641ebadd8dc82a77d0f,2017,2021-07-20 15:49:12,2021-07-20 15:49:12
MSXLN29Z,journalArticle,2021,"Liang, P.; Fang, Z.; Huang, B.; Zhou, H.; Tang, X.; Zhong, C.",PointFusionNet: Point feature fusion network for 3D point clouds analysis,"The 3D point clouds is an important type of geometric data structure, and the analysis of 3D point clouds based on deep learning is a very challenging task due to the disorder and irregularity. In existing research, RS-CNN provides an effective and promising method to obtain shape features on disordered point clouds directly, which encodes local features effectively. However, RS-CNN fails to consider point-wise features and global features, which are conducive to point clouds better. In this paper, we proposed PointFusionNet, which solves these problems effectively by fusing point-wise features, local features, and global features. We have designed Feature Fusion Convolution (FF-Conv) and Global Relationship Reasoning Module (GRRM) to build PointFusionNet. The point-wise features were fused with their corresponding local features in the FF-Conv and then mapped into a high-dimensional space to extract richer local features. The GRRM inferred the relationship between various parts, in order to capture global features for enriching the content of the feature descriptor. Therefore the PointFusionNet is suitable for point clouds classification and semantic segmentation by using the two distinctive modules. The PointFusionNet has been tested on ModelNet40 and ShapeNet part datasets, and the experiments show that PointFusionNet has a competitive advantage in shape classification and part segmentation tasks. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Applied Intelligence,10.1007/s10489-020-02004-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094167996&doi=10.1007%2fs10489-020-02004-8&partnerID=40&md5=7d9ab33446d202829b9eaf473414f73b,2021,2021-07-20 15:49:12,2021-07-20 15:49:12
D32GU8PR,journalArticle,2021,"Pacheco, A.G.C.; Krohling, R.",An attention-based mechanism to combine images and metadata in deep learning models applied to skin cancer classification,"Computer-aided skin cancer classification systems built with deep neural networks usually yield predictions based only on images of skin lesions. Despite presenting promising results, it is possible to achieve higher performance by taking into account patient demographics, which are important clues that human experts consider during skin lesion screening. In this work, we deal with the problem of combining images and metadata features using deep learning models applied to skin cancer classification. We propose the Metadata Processing Block (MetaBlock), a novel algorithm that uses metadata to support data classification by enhancing the most relevant features extracted from the images throughout the classification pipeline. We compared the proposed method with two other combination approaches: the MetaNet and one based on features concatenation. Results obtained for two different skin lesion datasets show that our method improves classification for all tested models and performs better than the other combination approaches in 6 out of 10 scenarios. IEEE",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2021.3062002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101806310&doi=10.1109%2fJBHI.2021.3062002&partnerID=40&md5=99304d1809d3c0db88828eada6bdb2eb,2021,2021-07-20 15:49:12,2021-07-20 15:49:12
JXBXL4RJ,journalArticle,2020,"Gao, F.; Wang, C.; Li, C.",A combined object detection method with application to pedestrian detection,"Object detection plays an important role in automatic driving systems. Considering the characteristics of classical and deep learning algorithms, a fusion logic is proposed to combine the advantages of these two kinds of object detectors. The relationship of detection performance among different detectors is established theoretically. According to the established theoretical relationship, the improvement of detection performance by fusion is further studied numerically. Furthermore, an optimization method is proposed to guide the design of the sub-detectors to achieve a better comprehensive performance. The effectiveness of this combined approach is validated by application to the detection of pedestrian, in which a support vector machine trained by the HOG feature of pedestrian is adopted as the classical detector and a comparatively simple transfer convolutional neural network (CNN) based on AlexNet structure acts as the deep learning detector. Several comparative tests with the classical and CNN detectors on the training dataset and other totally different dataset have been conducted to show the advantage of the combined one in ensuring detection performance with simpler network and adaptability to new application conditions. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",IEEE Access,10.1109/ACCESS.2020.3031005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102816405&doi=10.1109%2fACCESS.2020.3031005&partnerID=40&md5=ba6362b1be39beb4ee61bfb9c58b6c16,2020,2021-07-20 15:49:12,2021-07-20 15:49:12
URD5RN2Y,journalArticle,2020,"Han, T.; Hao, K.; Tang, X.; Cai, X.; Wang, T.; Liu, X.",A Compressed Sensing Network for Acquiring Human Pressure Information,"In this paper, sparse autoencoder in deep learning is integrated into the compressed sensing (CS) theory, and a reconstruction algorithm is designed based on the biological mechanism of human brain synaptic connections. The compressive sampling process is modeled as a neural network model. Then a biological mechanism-inspired stacked Long Short-Term Memory (LSTM) network model is proposed as a reconstruction algorithm of CS theory. Consequently, a compressed sensing network (ComsensNet) model is introduced, by integrating the compressive sampling process and reconstruction algorithm. ComsensNet can provide a bridge between sparse autoencoder in deep learning, synapses in human brain neurons and the CS theory. A deep neural network is designed based on the synaptic biological mechanism of human brain neurons, and then combine with the theory of compressed sensing. The effectiveness of ComsensNet is investigated by using acquired pressure data from the human body model. Experimental results demonstrate that the biological mechanisminspired stacked LSTM network in ComsensNet can improve the reconstruction accuracy compared to other reconstruction algorithms. IEEE",IEEE Transactions on Cognitive and Developmental Systems,10.1109/TCDS.2020.3041422,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097375587&doi=10.1109%2fTCDS.2020.3041422&partnerID=40&md5=caf10a80eed25b31454b9800798e4ad3,2020,2021-07-20 15:49:12,2021-07-20 15:49:12
KUKEWFVG,journalArticle,2020,"Luo, Y.; Pan, J.; Fan, S.; Du, Z.; Zhang, G.",Retinal Image Classification by Self-Supervised Fuzzy Clustering Network,"Diabetic retinal image classification aims to conduct diabetic retinopathy automatically diagnosing, which has achieved considerable improvement by deep learning models. However, these methods all rely on sufficient network training by large scale annotated data, which is very labor-expensive in medical image labeling. Aiming to overcome these drawbacks, this paper focuses on embedding self-supervised framework into unsupervised deep learning architecture. Specifically, we propose a Self-supervised Fuzzy Clustering Network (SFCN) by a feature learning module, reconstruction module, and a fuzzy self-supervision module. The feature learning and reconstruction modules ensure the representative ability of the network, and fuzzy self-supervision module is in charge of further providing the training direction for the whole network. Furthermore, three losses of reconstruction, self-supervision, and fuzzy supervision jointly optimize the SFCN under an unsupervised manner. To evaluate the effectiveness of the proposed method, we implement the network on three widely used retinal image datasets, which results demonstrate the satisfied performance on unsupervised retinal image classification task. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2994047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085640812&doi=10.1109%2fACCESS.2020.2994047&partnerID=40&md5=0f0fb2dbaef2e60e27f7293d6179a719,2020,2021-07-20 15:49:12,2021-07-20 15:49:12
5TXR7CDH,journalArticle,2019,"Hua, Y.; Zhao, Z.; Li, R.; Chen, X.; Liu, Z.; Zhang, H.",Deep Learning with Long Short-Term Memory for Time Series Prediction,"Time series prediction can be generalized as a process that extracts useful information from historical records and then determines future values. Learning long-range dependencies that are embedded in time series is often an obstacle for most algorithms, whereas LSTM solutions, as a specific kind of scheme in deep learning, promise to effectively overcome the problem. In this article, we first give a brief introduction to the structure and forward propagation mechanism of LSTM. Then, aiming at reducing the considerable computing cost of LSTM, we put forward a RCLSTM model by introducing stochastic connectivity to conventional LSTM neurons. Therefore, RCLSTM exhibits a certain level of sparsity and leads to a decrease in computational complexity. In the field of telecommunication networks, the prediction of traffic and user mobility could directly benefit from this improvement as we leverage a realistic dataset to show that for RCLSTM, the prediction performance comparable to LSTM is available, whereas considerably less computing time is required. We strongly argue that RCLSTM is more competent than LSTM in latency-stringent or power-constrained application scenarios. © 1979-2012 IEEE.",IEEE Communications Magazine,10.1109/MCOM.2019.1800155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062957095&doi=10.1109%2fMCOM.2019.1800155&partnerID=40&md5=514dcf3a1e490789f3333afbd475265d,2019,2021-07-20 15:49:12,2021-07-20 15:49:12
S6GZFTNC,journalArticle,2019,"Zhang, Z.; Singh, J.; Gadiraju, U.; Anand, A.",Dissonance between human and machine understanding,"Complex machine learning models are deployed in several critical domains including healthcare and autonomous vehicles nowadays, albeit as functional blackboxes. Consequently, there has been a recent surge in interpreting decisions of such complex models in order to explain their actions to humans. Models which correspond to human interpretation of a task are more desirable in certain contexts and can help attribute liability, build trust, expose biases and in turn build better models. It is therefore crucial to understand how and which models conform to human understanding of tasks. In this paper we present a large-scale crowdsourcing study that reveals and quantifies the dissonance between human and machine understanding, through the lens of an image classification task. In particular, we seek to answer the following questions: Which (well performing) complex ML models are closer to humans in their use of features to make accurate predictions? How does task difficulty affect the feature selection capability of machines in comparison to humans? Are humans consistently better at selecting features that make image recognition more accurate? Our findings have important implications on human-machine collaboration, considering that a long term goal in the field of artificial intelligence is to make machines capable of learning and reasoning like humans. © 2019 Association for Computing Machinery.",Proceedings of the ACM on Human-Computer Interaction,10.1145/3359158,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075088294&doi=10.1145%2f3359158&partnerID=40&md5=a17ecae3729771c080c653fce47db359,2019,2021-07-20 15:49:12,2021-07-20 15:49:12
BED8VGVH,journalArticle,2019,"Wang, W.; Søndergaard, H.; Stuckey, P.J.",Wombit: A Portfolio Bit-Vector Solver Using Word-Level Propagation,"We develop an idea originally proposed by Michel and Van Hentenryck of how to perform bit-vector constraint propagation on the word level. Most operations are propagated in constant time, assuming the bit-vector fits in a machine word. In contrast, bit-vector SMT solvers usually solve bit-vector problems by (ultimately) bit-blasting, that is, mapping the resulting operations to conjunctive normal form clauses, and using SAT technology to solve them. Bit-blasting generates intermediate variables which can be an advantage, as these can be searched on and learnt about. As each approach has advantages, it makes sense to try to combine them. In this paper, we describe an approach to bit-vector solving using word-level propagation with learning. We have designed alternative word-level propagators to Michel and Van Hentenryck’s, and evaluated different variants of the approach. We have also experimented with different approaches to learning and back-jumping in the solver. Based on the insights gained, we have built a portfolio solver, Wombit, which essentially extends the STP bit-vector solver. Using machine learning techniques, the solver makes a judicious up-front decision about whether to use word-level propagation or fall back on bit-blasting. © 2018, Springer Nature B.V.",Journal of Automated Reasoning,10.1007/s10817-018-9493-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056335383&doi=10.1007%2fs10817-018-9493-1&partnerID=40&md5=ff83aaf2d2f5d0a2f54b928d00949572,2019,2021-07-20 15:49:12,2021-07-20 15:49:12
ZH3BIS2R,journalArticle,2015,"Stein, G.; Gonzalez, A.J.; Barham, C.",Combining NEAT and PSO for learning tactical human behavior,"This article presents and discusses a machine learning algorithm called PIGEON used to build agents capable of displaying tactical behavior in various domains. Such tactical behavior can be relevant in military simulations and video games, as well as in everyday tasks in the physical world, such as driving an automobile. Furthermore, PIGEON displays good performance across two different approaches to learning (observational and experiential) and across multiple domains. PIGEON is a hybrid algorithm, combining NEAT and PSO in two different manners. The investigation described in this paper compares the performance of the two versions of PIGEON to each other as well as to NEAT and to PSO individually. These four machine learning algorithms are applied in two different approaches to learning—through observation of human performance and through experience, as well as in three distinct domain testbeds. The criteria used to compare them were high proficiency in task completion and rapid learning. Results indicate that overall, PIGEON worked best when NEAT and PSO are applied in an alternating manner. This combination was called PIGEON-Alternate, or simply Alternate. The two versions of the PIGEON algorithm, the tests conducted, the results obtained and the conclusions are described in detail. © 2014, The Natural Computing Applications Forum.",Neural Computing and Applications,10.1007/s00521-014-1761-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939957366&doi=10.1007%2fs00521-014-1761-3&partnerID=40&md5=741569379089f8c77fac32877f89a079,2015,2021-07-20 15:49:12,2021-07-20 15:49:12
KHEVR9HV,journalArticle,2018,"Liu, Y.-T.; Pal, N.R.; Marathe, A.R.; Lin, C.-T.",Weighted Fuzzy Dempster-Shafer Framework for Multimodal Information Integration,"This study proposes an architecture based on a weighted fuzzy Dempster-Shafer framework (WFDSF), which can adjust weights associated with inconsistent evidence obtained by different classification approaches, to realize a fusion system for integrating multimodal information. The Dempster-Shafer theory (D-S theory) of evidence enables us to integrate heterogeneous information from multiple sources to obtain collaborative inferences for a given problem. To conquer various uncertainties associated with the collected information, our system assigns beliefs and plausibilities to possible hypotheses of each decision maker and uses a combination rule to fuse multimodal information. For information fusion, an important step in D-S aggregation is to find an appropriate basic probability assignment scheme for allocating support to each possible hypothesis/class, which remains an arduous and unsolved problem. Here, we propose a mathematical structure to aggregate weighted evidence extracted from two different types of approaches: fuzzy Naïve Bayes and nearest mean classification rule. Further, an intuitionistic belief assignment is employed to address uncertainties between hypotheses/classes. Finally, 12 benchmark problems from the UCI machine learning repository for classification are employed to validate the proposed WFDSF-based scheme. In addition, an application of WFDSF to a practical brain-computer interface problem involving multimodal data fusion is demonstrated in this study. The experimental results show that the WFDSF is superior to several existing methods. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2017.2659764,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041489820&doi=10.1109%2fTFUZZ.2017.2659764&partnerID=40&md5=2db77578e534e1a65424946fc5d9b896,2018,2021-07-20 15:49:12,2021-07-20 15:49:12
ZB5F8VVF,journalArticle,2020,"Hughes, J.A.; Houghten, S.; Brown, J.A.",Models of Parkinson's Disease Patient Gait,"Parkinson's Disease is a disorder with diagnostic symptoms that include a change to a walking gait. The disease is problematic to diagnose. An objective method of monitoring the gait of a patient is required to ensure the effectiveness of diagnosis and treatments. We examine the suitability of Extreme Gradient Boosting (XGBoost) and Artificial Neural Network (ANN) Models compared to Symbolic Regression (SR) using genetic programming that was demonstrated to be successful in previous works on gait. The XGBoost and ANN models are found to out-perform SR, but the SR model is more human explainable. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2019.2961808,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095862482&doi=10.1109%2fJBHI.2019.2961808&partnerID=40&md5=ad2339361f80ae5d15ad03086fe1d062,2020,2021-07-20 15:49:12,2021-07-20 15:49:12
7CRLDQK9,journalArticle,2019,"Katz, G.E.; Davis, G.P.; Gentili, R.J.; Reggia, J.A.",A programmable neural virtual machine based on a fast store-erase learning rule,"We present a neural architecture that uses a novel local learning rule to represent and execute arbitrary, symbolic programs written in a conventional assembly-like language. This Neural Virtual Machine (NVM) is purely neurocomputational but supports all of the key functionality of a traditional computer architecture. Unlike other programmable neural networks, the NVM uses principles such as fast non-iterative local learning, distributed representation of information, program-independent circuitry, itinerant attractor dynamics, and multiplicative gating for both activity and plasticity. We present the NVM in detail, theoretically analyze its properties, and conduct empirical computer experiments that quantify its performance and demonstrate that it works effectively. © 2019 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2019.07.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069933034&doi=10.1016%2fj.neunet.2019.07.017&partnerID=40&md5=aec23bf061e58f8783f6f602f6efb3ba,2019,2021-07-20 15:49:13,2021-07-20 15:49:13
HFTHDEN2,journalArticle,2018,"Gall, D.; Frühwirth, T.",An operational semantics for the cognitive architecture ACT-R and its translation to constraint handling rules,"Computational psychology has the aim to explain human cognition by computational models of cognitive processes. The cognitive architecture Adaptive Control of Thought-Rational (ACT-R) is popular to develop such models. Although ACT-R has a well-defined psychological theory and has been used to explain many cognitive processes, there are two problems that make it hard to reason formally about its cognitive models: First, ACT-R lacks a computational formalization of its underlying production rule system, and, second, there are many different implementations and extensions of ACT-R with many technical artifacts complicating formal reasoning even more. This article describes a formal operational semantics-the very abstract semantics-that abstracts from as many technical details as possible, keeping it open to extensions and different implementations of the ACT-R theory. In a second step, this semantics is refined to define some of its abstract features that are found in many implementations of ACT-R-called the abstract semantics. It concentrates on the procedural core of ACT-R and is suitable for analysis of the general transition system, since it still abstracts from details like timing, the sub-symbolic layer of ACT-R or conflict resolution. Furthermore, a translation of ACT-R models to the declarative programming language Constraint Handling Rules (CHR) is defined. This makes the abstract semantics an executable specification of ACT-R. CHR has been used successfully to embed other rule-based formalisms like graph transformation systems or functional programming. There are many theoretical results and practical tools that support formal reasoning about and analysis of CHR programs. The translation of ACT-R models to CHR is proven sound and complete w.r.t. the abstract operational semantics of ACT-R. This paves the way to analysis of ACT-R models through CHR analysis results and tools. Therefore, to the best of our knowledge, our abstract semantics is the first abstract formulation of ACT-R suitable for both analysis and execution. © 2018 ACM.",ACM Transactions on Computational Logic,10.1145/3218818,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053893914&doi=10.1145%2f3218818&partnerID=40&md5=a0645b4a8f4a2de966eab9d808226985,2018,2021-07-20 15:49:13,2021-07-20 15:49:13
DSBCZLXS,journalArticle,2016,"Fang, Y.; Liu, Z.-H.; Min, F.",Multi-objective cost-sensitive attribute reduction on data with error ranges,"In current supervised machine learning research spectrum, there are several attribute reduction methodologies to acquire reducts with low test cost. They can deal with symbolic data, or numeric data with error ranges. In many cases, they consider the situation with only one type of cost; therefore the problem is single-objective. This paper addresses the attribute reduction problem on data with multi-type-costs and error ranges. First, we define the multi-objective attribute reduction problem where multi-type-costs are involved. Second, we propose three metrics to evaluate the quality of a reduct set. Third, we design a backtrack algorithm to compute the Pareto optimal set, and a heuristic algorithm to find a sub-optimal reduct set. Finally, we compare these algorithms on seven UCI (University of California-Irvine) datasets. Experimental results indicate that our heuristic algorithm has good capability of tackling the proposed problem. © 2014, Springer-Verlag Berlin Heidelberg.",International Journal of Machine Learning and Cybernetics,10.1007/s13042-014-0296-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988378413&doi=10.1007%2fs13042-014-0296-3&partnerID=40&md5=8c81287f48147c528c168b8dbe0c3bf3,2016,2021-07-20 15:49:13,2021-07-20 15:49:13
A59DP9ZJ,journalArticle,2014,"Peissig, P.L.; Santos Costa, V.; Caldwell, M.D.; Rottscheit, C.; Berg, R.L.; Mendonca, E.A.; Page, D.",Relational machine learning for electronic health record-driven phenotyping,"Objective: Electronic health records (EHR) offer medical and pharmacogenomics research unprecedented opportunities to identify and classify patients at risk. EHRs are collections of highly inter-dependent records that include biological, anatomical, physiological, and behavioral observations. They comprise a patient's clinical phenome, where each patient has thousands of date-stamped records distributed across many relational tables. Development of EHR computer-based phenotyping algorithms require time and medical insight from clinical experts, who most often can only review a small patient subset representative of the total EHR records, to identify phenotype features. In this research we evaluate whether relational machine learning (ML) using inductive logic programming (ILP) can contribute to addressing these issues as a viable approach for EHR-based phenotyping. Methods: Two relational learning ILP approaches and three well-known WEKA (Waikato Environment for Knowledge Analysis) implementations of non-relational approaches (PART, J48, and JRIP) were used to develop models for nine phenotypes. International Classification of Diseases, Ninth Revision (ICD-9) coded EHR data were used to select training cohorts for the development of each phenotypic model. Accuracy, precision, recall, F-Measure, and Area Under the Receiver Operating Characteristic (AUROC) curve statistics were measured for each phenotypic model based on independent manually verified test cohorts. A two-sided binomial distribution test (sign test) compared the five ML approaches across phenotypes for statistical significance. Results: We developed an approach to automatically label training examples using ICD-9 diagnosis codes for the ML approaches being evaluated. Nine phenotypic models for each ML approach were evaluated, resulting in better overall model performance in AUROC using ILP when compared to PART (p= 0.039), J48 (p= 0.003) and JRIP (p= 0.003). Discussion: ILP has the potential to improve phenotyping by independently delivering clinically expert interpretable rules for phenotype definitions, or intuitive phenotypes to assist experts. Conclusion: Relational learning using ILP offers a viable approach to EHR-driven phenotyping. © 2014 Elsevier Inc.",Journal of Biomedical Informatics,10.1016/j.jbi.2014.07.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919848159&doi=10.1016%2fj.jbi.2014.07.007&partnerID=40&md5=4560b8a03a8abc21fb4e657f3b9e8d4a,2014,2021-07-20 15:49:13,2021-07-20 15:49:13
SBHDPFRP,journalArticle,2014,"Liu, C.; Pontelli, E.",Techniques to enhance efficiency and effectiveness of inductive logic programming systems: The TWEETY approaches,"This paper presents four novel approaches to enhance efficiency and effectiveness of Inductive Logic Programming (ILP) systems, along with their implementation in a new ILP system, called TWEETY. The proposed approaches include (1) a new declaration mechanism, called connection declarations, for bottom clause construction, which is simpler but more expressive than the commonly used mode declarations; (2) a new covering technique, called super-covering, which reduces the examples in such a way that recursion can be learned, independently from the ordering of the examples; (3) a new search heuristics, called neg-coverage heuristics, which guides the search using only the number of negative examples covered by each hypothesis and (4) a new search algorithm, called doubly-guided-search, which searches for best clauses by alternating the use of two search heuristics, i.e. the traditional coverage search heuristics and the new neg-coverage search heuristics. The TWEETY system is shown to be more effective and efficient than the state-of-the-art ILP system ALEPH; the proposed techniques can be used to enhance efficiency and effectiveness of ALEPH and other systems based on the same ILP principles. © 2014 Taylor & Francis.",Journal of Experimental and Theoretical Artificial Intelligence,10.1080/0952813X.2013.808801,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84888641825&doi=10.1080%2f0952813X.2013.808801&partnerID=40&md5=86c85be0f78450156068df11bb30357e,2014,2021-07-20 15:49:13,2021-07-20 15:49:13
MM9TSGYY,journalArticle,2019,"Betere, I.J.; Kinjo, H.; Nakazono, K.; Oshiro, N.",Investigation of multi-layer neural network performance evolved by genetic algorithms,"This paper presents a study on the investigation of multi-layer neural networks (MLNNs) performance evolved with genetic algorithm (GA) for multi-logic training patterns applied to various network functions. Specifically, we have concentrated on the Sigmoid, Step and ReLU functions to evaluate and simulate their performances in the network. We have revealed that GA training gives good training results in evolutionary computation by changing of Sigmoid, ReLU and Step as the activity functions in MLNN performance. Sigmoid function has proved to train all patterns for all outputs without any challenge as compared to ReLU function and Step in this study. We are still trying to see how a ReLU function could be trained with GA for MLNNs performance for the two input and four output training patterns termed as the multi-logic pattern training about multiple training parameters. © 2018, ISAROB.",Artificial Life and Robotics,10.1007/s10015-018-0494-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054535773&doi=10.1007%2fs10015-018-0494-2&partnerID=40&md5=76d07144c99ed1a898d0e87abf5f6874,2019,2021-07-20 15:49:13,2021-07-20 15:49:13
XLND88LE,journalArticle,2020,"Du, Y.; Li, W.; Dai, Z.; Nan, L.",PVHArray: An Energy-Efficient Reconfigurable Cryptographic Logic Array with Intelligent Mapping,"This article presents a coarse-grained reconfigurable cryptographic logic array named PVHArray and an intelligent mapping algorithm for cryptographic algorithms. We propose three techniques to improve energy efficiency without affecting performance. First, the coarse-grained pipeline variable reconfigurable operation units balance the system critical path delay and number of algorithm operations to ensure the best performance. Second, the hierarchical interconnect network overcomes the shortcomings of a single network, providing PVHArray with good interconnectivity and scalability while managing the network hardware resource overhead. Third, the distributed control network supports accurate period-oriented control with a lightweight hardware structure, preserving hardware resources for other performance enhancements. We combine these advances with deep learning to propose a type of smart ant colony optimization mapping algorithm to improve algorithm mapping performance. We implemented our PVHArray on a 12.25 mm2 silicon square with 55-nm CMOS technology, with each algorithm working at its optimum frequency. Experiments show that PVHArray improved performance by about 12.9% per unit area and 13.9% per unit power compared with the reconfigurable cryptographic logic array REMUS-LPP and other state-of-the-art cryptographic structures. For cryptographic algorithm mapping, our smart ant colony optimization (SACO) algorithm reduced compilation time by nearly 38%. Finally, PVHArray supports a variety of types of cryptographic algorithms. © 1993-2012 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2020.2972392,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084184643&doi=10.1109%2fTVLSI.2020.2972392&partnerID=40&md5=c59f342daec126c4121575c188ce0e3b,2020,2021-07-20 15:49:13,2021-07-20 15:49:13
D94QX9C4,journalArticle,2019,"Hwang, M.; Wang, D.; Jiang, W.-C.; Pan, X.; Fu, D.; Hwang, K.-S.; Ding, K.",An Adaptive Regularization Approach to Colonoscopic Polyp Detection Using a Cascaded Structure of Encoder–Decoders,"This research aims to segment colonoscopic images by automatically extracting polyp features by exploiting the strengths of convolution neural networks (CNN). The proposed model employs deep learning and adaptive regularization techniques. The model is structurally composed of two cascaded encoder–decoder networks, each of which is constructed by four CNN layers and two full connection layers. The front model is built on backpropagation learning for segmenting a colonoscopic polyp image. The output images from the precedent hetero-encoder are regarded as corrupted labeled images, especially during the time period close to the end of learning, and are selectively fed into the successive auto-encoder for denoising learning to enhance its discriminative power and relieve the problem of a lack of labeled data for medical image tasks. The performance of the proposed model can be further improved by a simple fuzzy logic approach setting the regularization parameter in the loss function. The proposed method utilizes features learned from some open medical datasets and our own collected dataset. The performance of the proposed architecture is compared with a state-of-the-art network. The evaluation shows the performances of the proposed method are consistent across all the datasets and often outperform the state-of-art model. © 2019, Taiwan Fuzzy Systems Association.",International Journal of Fuzzy Systems,10.1007/s40815-019-00694-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070283867&doi=10.1007%2fs40815-019-00694-y&partnerID=40&md5=ca9577158a0cd72d26ba67d76688fc6a,2019,2021-07-20 15:49:13,2021-07-20 15:49:13
4EF4BXD9,journalArticle,2020,"Zarisfi Kermani, F.; Sadeghi, F.; Eslami, E.",Solving the twitter sentiment analysis problem based on a machine learning-based approach,"Twitter Sentiment Analysis (TSA) as part of a text classification task has been widely attended by researchers in recent years. This paper presents a machine learning approach to solving the TSA problem in three phases. In the second phase, a suitable value for representing each feature in the Vector Space Model is determined through the weighted combination of the values obtained from four methods (i.e., Term Frequency and Inverse Document Frequency, semantic similarity, sentiment scoring using SentiWordNet, and sentiment scoring based on the class of tweets). In this manner, finding the percentage of contributions or weights of each method is defined as an optimization problem and solved using a genetic algorithm. Also, the weighted values obtained from four methods are combined based on the Einstein sum as an important T-conorm method. Finally, the performance of the proposed method is tested based on the accuracy of support vector machine and multinomial naïve Bayes classification algorithms on four famous Twitter datasets, namely the Stanford testing dataset, STS-Gold dataset, Obama-McCain Debate dataset, and Strict Obama-McCain Debate dataset. The obtained results show the high superiority of the proposed method in comparison with the other methods. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Evolutionary Intelligence,10.1007/s12065-019-00301-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074497303&doi=10.1007%2fs12065-019-00301-x&partnerID=40&md5=064426e2a466ab68466559bf866ee630,2020,2021-07-20 15:49:13,2021-07-20 15:49:13
44KCPGAV,journalArticle,2020,"Jia, W.; Liu, X.; Wang, Y.; Pedrycz, W.; Zhou, J.",Semisupervised Learning via Axiomatic Fuzzy Set Theory and SVM,"In this article, we present a semantic semisupervised learning (Semantic SSL) approach targeted at unifying two machine-learning paradigms in a mutually beneficial way, where the classical support vector machine (SVM) learns to reveal primitive logic facts from data, while axiomatic fuzzy set (AFS) theory is utilized to exploit semantic knowledge and correct the wrongly perceived facts for improving the machine-learning model. This novel semisupervised method can easily produce interpretable semantic descriptions to outline different categories by forming a fuzzy set with semantic explanations realized on the basis of the AFS theory. Besides, it is known that disagreement-based semisupervised learning (SSL) can be viewed as an excellent schema so that a co-training approach with SVM and the AFS theory can be utilized to improve the resulting learning performance. Furthermore, an evaluation index is used to prune descriptions to deliver promising performance. Compared with other semisupervised approaches, the proposed approach can build a structure to reflect data-distributed information with unlabeled data and labeled data, so that the hidden information embedded in both labeled and unlabeled data can be sufficiently utilized and can potentially be applied to achieve good descriptions of each category. Experimental results demonstrate that this approach can offer a concise, comprehensible, and precise SSL frame, which strikes a balance between the interpretability and the accuracy. IEEE",IEEE Transactions on Cybernetics,10.1109/TCYB.2020.3032707,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097428882&doi=10.1109%2fTCYB.2020.3032707&partnerID=40&md5=3ba01ff705ac2bcef8bc336542a161db,2020,2021-07-20 15:49:13,2021-07-20 15:49:13
PD2A2ZVV,journalArticle,2019,"Sarwar, S.; Qayyum, Z.U.; García-Castro, R.; Safyan, M.; Munir, R.F.","Ontology based E-learning framework: A personalized, adaptive and context aware model","Enhancing the degree of learner productivity, one of the major challenges in E-Learning systems, may be catered through effective personalization, adaptivity and context awareness while recommending the learning contents to the learners. In this paper, an E-Learning framework has been proposed that profiles the learners, categorizes the learners based on profiles, makes personalized content recommendations and performs assessment based content adaptation. A mathematical model has been proposed for learner categorization using machine learning techniques (a hybrid of case based reasoning and neural networks). The learning contents have been annotated through CourseOntology in which three academic courses (each for language of C++, C# and JAVA) have been modeled for the learners. A dynamic rule based recommender has been presented targeting a ‘relative grading system’ for maximizing the learner’s productivity. Performance of proposed framework has been measured in terms of accurate learner categorization, personalized recommendation of the learning contents, completeness and correctness of ontological model and overall performance improvement of learners in academic sessions of 2015, 2016 and 2017. The comparative analysis of proposed framework exhibits visibly improved results compared to prevalent approaches. These improvements are signified to the comprehensive attribute selection in learner profiling, dynamic techniques for learner categorization and effective content recommendation while ensuring personalization and adaptivity. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-019-08125-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071338621&doi=10.1007%2fs11042-019-08125-8&partnerID=40&md5=04020cef4e95df52dca712468616dd68,2019,2021-07-20 15:49:13,2021-07-20 15:49:13
8S52Z6X6,journalArticle,2011,"Chen, C.-S.; Chen, J.-S.",Rotor fault diagnosis system based on sGA-based individual neural networks,"This paper proposes a robust fault diagnosis system of rotating machine adapting machine learning technology. The kernel of this diagnosis system includes a set of individual neural networks based on structured genetic algorithm (sGAINNs). First, the frequency characteristics from differential signals, including fast Fourier transform (FFT) and full spectrum, are used to feed into the sGAINNs corresponding to assigned faults to emphasize the phenomenon of each fault. Especially, the structured genetic algorithm is applied to get the optimal parameters of the above sGAINNs. In the final step of proposed diagnosis system, the evaluated indexes from sGAINNs are synthesized by a reasoning engine to identify the faults in the rotor system. Finally, six common faults of rotor system, unbalance, bow, misalignment, rub, whirl, and whip, are generated from a rotor kit, produced by Bently Nevada Corporation, to verify the performance of this diagnosis system. The advantage of this diagnosis system is that the optimal sGAINNs parameters can be automatically obtained, the local optimal solutions can be reduced and the diagnosis accuracy can be improved. © 2011 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2011.02.074,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955607648&doi=10.1016%2fj.eswa.2011.02.074&partnerID=40&md5=8744d16e07030a2f887e71bed8d4dc7a,2011,2021-07-20 15:49:13,2021-07-20 15:49:13
JTPEVYT2,journalArticle,2021,"Liu, P.; Liu, K.; Fu, T.; Zhang, Y.; Hu, J.",A privacy-preserving resource trading scheme for Cloud Manufacturing with edge-PLCs in IIoT,"With the development of industrial Internet of things (IIoT), Cloud Manufacturing has been increasingly popular to the manufacturing industry. It can provide resource-sharing and on-demand manufacturing services as well as automatic collaborative manufacturing with the help of edge Programmable Logic Controllers (edge-PLCs). In such a system, there is a high risk of exposing user privacy and trading secret, due to exposure of sensitive transaction data to public servers. We propose a new privacy-preserving resource-trading scheme (PRTS), which leverages the concept of homomorphic cryptography and asymmetric searchable encryption, to simultaneously protect the privacy of the equipment factory and parts factories, while supporting best matching results in terms of parts parameters and price. Furthermore, a random forest-based method is applied to identify abnormal participants. The experimental results and security analysis show that the proposed scheme is accurate, effective, and secure, even under Off-line Keyword Guessing Attacks. Finally, encrypted data can resist analysis from mainstream machine learning techniques. © 2021 Elsevier B.V.",Journal of Systems Architecture,10.1016/j.sysarc.2021.102104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105292603&doi=10.1016%2fj.sysarc.2021.102104&partnerID=40&md5=b68b1093bc296bf8ccaa82d67f488f75,2021,2021-07-20 15:49:14,2021-07-20 15:49:14
R5DP39JK,journalArticle,2019,"Vashishtha, S.; Susan, S.",Fuzzy rule based unsupervised sentiment analysis from social media posts,"In this paper, we compute the sentiment of social media posts using a novel set of fuzzy rules involving multiple lexicons and datasets. The proposed fuzzy system integrates Natural Language Processing techniques and Word Sense Disambiguation using a novel unsupervised nine fuzzy rule based system to classify the post into: positive, negative or neutral sentiment class. We perform a comparative analysis of our method on nine public twitter datasets, three sentiment lexicons, four state-of-the-art approaches for unsupervised Sentiment Analysis and one state-of-the-art method for supervised machine learning. Traditionally, Sentiment Analysis of twitter data is performed using a single lexicon. Our results can give an insight to researchers to choose which lexicon is best for social media. The fusion of fuzzy logic with lexicons for sentiment classification provides a new paradigm in Sentiment Analysis. Our method can be adapted to any lexicon and any dataset (two-class or three-class sentiment). The experiments on benchmark datasets yield higher performance for our approach as compared to the state-of-the-art. © 2019 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2019.112834,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069805425&doi=10.1016%2fj.eswa.2019.112834&partnerID=40&md5=93adfcab9263b61455f694f19e36ec3e,2019,2021-07-20 15:49:14,2021-07-20 15:49:14
6652HST4,journalArticle,2017,"Gupta, M.; Mittal, H.; Singla, P.; Bagchi, A.",Analysis and characterization of comparison shopping behavior in the mobile handset domain,"In this work we characterize the session-level behavior of users on an Indian mobile phone comparison shopping website. We also correlate the popularity of handset on various news sources to its popularity on the shopping website. There are three aspects to our study: data analysis, correlation between news sources of product information and popularity of a handset, and behavior prediction. We have used KL divergence to show that a time-homogeneous Markov chain is observed when the number of clicks varies from 5 to 30. Our results depict that Markov chain model does not hold in entirety for comparison shopping setting but tells us how far the Markov chain model holds for this setting. Our analysis corroborates intuition that increasing price leads to decrease in popularity. After the strong correlation between various variables and user behavior was found, we predict the users macro (the overall sales of handset) and micro behavior (whether a user will convert or exit the site) using Markov logic networks. Our predictive model validates the intuition that past browsing behavior is an important predictor for future behavior. Methodology of combining data analysis with machine learning is, in our opinion, a new approach to the empirical study of such data sets. © 2016, Springer Science+Business Media New York.",Electronic Commerce Research,10.1007/s10660-016-9226-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969786253&doi=10.1007%2fs10660-016-9226-7&partnerID=40&md5=3782682c3985054c5ca67c84428083b0,2017,2021-07-20 15:49:14,2021-07-20 15:49:14
JW8RYKZS,journalArticle,2021,"Ji, S.; Pan, S.; Cambria, E.; Marttinen, P.; Yu, P.S.","A Survey on Knowledge Graphs: Representation, Acquisition, and Applications","Human knowledge provides a formal understanding of the world. Knowledge graphs that represent structural relations between entities have become an increasingly popular research direction toward cognition and human-level intelligence. In this survey, we provide a comprehensive review of the knowledge graph covering overall research topics about: 1) knowledge graph representation learning; 2) knowledge acquisition and completion; 3) temporal knowledge graph; and 4) knowledge-aware applications and summarize recent breakthroughs and perspective directions to facilitate future research. We propose a full-view categorization and new taxonomies on these topics. Knowledge graph embedding is organized from four aspects of representation space, scoring function, encoding models, and auxiliary information. For knowledge acquisition, especially knowledge graph completion, embedding methods, path inference, and logical rule reasoning are reviewed. We further explore several emerging topics, including metarelational learning, commonsense reasoning, and temporal knowledge graphs. To facilitate future research on knowledge graphs, we also provide a curated collection of data sets and open-source libraries on different tasks. In the end, we have a thorough outlook on several promising research directions. IEEE",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2021.3070843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105114155&doi=10.1109%2fTNNLS.2021.3070843&partnerID=40&md5=f1fe35912c009aeea66424df37f54e33,2021,2021-07-20 15:49:14,2021-07-20 15:49:14
AGS6YBSZ,journalArticle,2020,"Xu, L.",Learning deep IA bidirectional intelligence,"There has been a framework sketched for learning deep bidirectional intelligence. The framework has an inbound that features two actions: one is the acquiring action, which gets inputs in appropriate patterns, and the other is A-S cognition, derived from the abbreviated form of words abstraction and self-organization, which abstracts input patterns into concepts that are labeled and understood by self-organizing parts involved in the concept into structural hierarchies. The top inner domain accommodates relations and a priori knowledge with the help of the A-I thinking action that is responsible for the accumulation-amalgamation and induction-inspiration. The framework also has an outbound that comes with two actions. One is called I-S reasoning, which makes inference and synthesis (I-S) and is responsible for performing various tasks including image thinking and problem solving, and the other is called the interacting action, which controls, communicates with, and inspects the environment. Based on this framework, we further discuss the possibilities of design intelligence through synthesis reasoning. © 2020, Zhejiang University and Springer-Verlag GmbH Germany, part of Springer Nature.",Frontiers of Information Technology and Electronic Engineering,10.1631/FITEE.1900541,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083999731&doi=10.1631%2fFITEE.1900541&partnerID=40&md5=c4cc1c083dead1ffaebc239541c63ef2,2020,2021-07-20 15:49:14,2021-07-20 15:49:14
ACDMDN8W,journalArticle,2021,"Bounabi, M.; Elmoutaouakil, K.; Satori, K.",A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case,"Purpose: This paper aims to present a new term weighting approach for text classification as a text mining task. The original method, neutrosophic term frequency – inverse term frequency (NTF-IDF), is an extended version of the popular fuzzy TF-IDF (FTF-IDF) and uses the neutrosophic reasoning to analyze and generate weights for terms in natural languages. The paper also propose a comparative study between the popular FTF-IDF and NTF-IDF and their impacts on different machine learning (ML) classifiers for document categorization goals. Design/methodology/approach: After preprocessing textual data, the original Neutrosophic TF-IDF applies the neutrosophic inference system (NIS) to produce weights for terms representing a document. Using the local frequency TF, global frequency IDF and text N's length as NIS inputs, this study generate two neutrosophic weights for a given term. The first measure provides information on the relevance degree for a word, and the second one represents their ambiguity degree. Next, the Zhang combination function is applied to combine neutrosophic weights outputs and present the final term weight, inserted in the document's representative vector. To analyze the NTF-IDF impact on the classification phase, this study uses a set of ML algorithms. Findings: Practicing the neutrosophic logic (NL) characteristics, the authors have been able to study the ambiguity of the terms and their degree of relevance to represent a document. NL's choice has proven its effectiveness in defining significant text vectorization weights, especially for text classification tasks. The experimentation part demonstrates that the new method positively impacts the categorization. Moreover, the adopted system's recognition rate is higher than 91%, an accuracy score not attained using the FTF-IDF. Also, using benchmarked data sets, in different text mining fields, and many ML classifiers, i.e. SVM and Feed-Forward Network, and applying the proposed term scores NTF-IDF improves the accuracy by 10%. Originality/value: The novelty of this paper lies in two aspects. First, a new term weighting method, which uses the term frequencies as components to define the relevance and the ambiguity of term; second, the application of NL to infer weights is considered as an original model in this paper, which also aims to correct the shortcomings of the FTF-IDF which uses fuzzy logic and its drawbacks. The introduced technique was combined with different ML models to improve the accuracy and relevance of the obtained feature vectors to fed the classification mechanism. © 2021, Emerald Publishing Limited.",International Journal of Web Information Systems,10.1108/IJWIS-11-2020-0067,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103899477&doi=10.1108%2fIJWIS-11-2020-0067&partnerID=40&md5=707e28923b31ac89b6593e7020b37ae9,2021,2021-07-20 15:49:14,2021-07-20 15:49:14
DNTBM6ND,journalArticle,2020,"Jin, S.; Zhang, Z.; Chakrabarty, K.; Gu, X.",Hierarchical Symbol-Based Health-Status Analysis Using Time-Series Data in a Core Router System,"To ensure high reliability and rapid error recovery in commercial core router systems, a health-status analyzer is essential to monitor the different features of core routers. However, traditional health analyzers need to store a large amount of historical data in order to identify health status. The storage requirement becomes prohibitively high when we attempt to carry out long-term health-status analysis for a large number of core routers. We describe the design of a symbol-based health status analyzer that first encodes, as a symbol sequence, the long-term complex time series collected from a number of core routers, and then utilizes the symbol sequence to do health analysis. The symbolic aggregation approximation (SAX), 1d-SAX, moving-average-based trend approximation, and nonparametric symbolic approximation representation methods are implemented to encode complex time series in a hierarchical way. Hierarchical agglomerative clustering and sequitur rule discovery are implemented to learn important global and local patterns. Three classification methods including a vector-space-model-based approach are then utilized to identify the health status of core routers. Data collected from a set of commercial core router systems are used to validate the proposed health-status analyzer. The experimental results show that our symbol-based health status analyzer requires much lower storage than traditional methods, but can still maintain comparable diagnosis accuracy. © 2019 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2890681,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081054539&doi=10.1109%2fTCAD.2018.2890681&partnerID=40&md5=293a4c2b55ed7b06d7f5df0a908d1f05,2020,2021-07-20 15:49:14,2021-07-20 15:49:14
QPIC5NQR,journalArticle,2019,"Shanthi, S.; Rajkumar, N.",Non-small-cell lung cancer prediction using radiomic features and machine learning methods,"One of the primary causes of deaths related to cancer all over the world is Lung cancer. The history of the patient and his histological classification in terms of lung cancer has provided critical information regarding the characteristics of tissues and anatomical locations. There are many different studies that have depicted the radiomic features and their power of prediction in the detection of lung cancer. But its quantitative size in terms of data is large and has been resulting in major challenges in the algorithms of classification. In order to overcome this, symbolic approach to data analysis which employs many different quantitative data is proposed. The work further investigates different techniques of feature selection in order to predict the histologic subtypes of lung cancer by using either symbolic data or the radiomic features. These features have been extracted by using a gray-level co-occurrence matrix (GLCM), the Gabor filter and the fusion that was achieved by making use of concatenation once there is a normalization of the Z score. The results of the experiment have proved that the proposed method had a better performance compared to the other methods. © 2019, © 2019 Informa UK Limited, trading as Taylor & Francis Group.",International Journal of Computers and Applications,10.1080/1206212X.2019.1693723,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075434451&doi=10.1080%2f1206212X.2019.1693723&partnerID=40&md5=8a81b106f1707d3a7b87e65eb5ed502b,2019,2021-07-20 15:49:14,2021-07-20 15:49:14
YL2NV4QJ,journalArticle,2017,"Iqbal, M.; Xue, B.; Al-Sahaf, H.; Zhang, M.",Cross-Domain Reuse of Extracted Knowledge in Genetic Programming for Image Classification,"Genetic programming (GP) is a well-known evolutionary computation technique, which has been successfully used to solve various problems, such as optimization, image analysis, and classification. Transfer learning is a type of machine learning approach that can be used to solve complex tasks. Transfer learning has been introduced to GP to solve complex Boolean and symbolic regression problems with some promise. However, the use of transfer learning with GP has not been investigated to address complex image classification tasks with noise and rotations, where GP cannot achieve satisfactory performance, but GP with transfer learning may improve the performance. In this paper, we propose a novel approach based on transfer learning and GP to solve complex image classification problems by extracting and reusing blocks of knowledge/information, which are automatically discovered from similar as well as different image classification tasks during the evolutionary process. The proposed approach is evaluated on three texture data sets and three office data sets of image classification benchmarks, and achieves better classification performance than the state-of-the-art image classification algorithm. Further analysis on the evolved solutions/trees shows that the proposed approach with transfer learning can successfully discover and reuse knowledge/information extracted from similar or different problems to improve its performance on complex image classification problems. © 2017 IEEE.",IEEE Transactions on Evolutionary Computation,10.1109/TEVC.2017.2657556,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029282037&doi=10.1109%2fTEVC.2017.2657556&partnerID=40&md5=92ee04fd4cfa87b24958d2c507b06613,2017,2021-07-20 15:49:14,2021-07-20 15:49:14
MRW6DUQ6,journalArticle,2019,"Martinez-Gil, J.; Chaves-Gonzalez, J.M.",Automatic design of semantic similarity controllers based on fuzzy logics,"Recent advances in machine learning have been able to make improvements over the state-of-the-art regarding semantic similarity measurement techniques. In fact, we have all seen how classical techniques have given way to promising neural techniques. Nonetheless, these new techniques have a weak point: they are hardly interpretable. For this reason, we have oriented our research towards the design of strategies being able to be accurate enough but without sacrificing their interpretability. As a result, we have obtained a strategy for the automatic design of semantic similarity controllers based on fuzzy logics, which are automatically identified using genetic algorithms (GAs). After an exhaustive evaluation using a number of well-known benchmark datasets, we can conclude that our strategy fulfills both expectations: it is able of achieving reasonably good results, and at the same time, it can offer high degrees of interpretability. © 2019 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2019.04.046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066130076&doi=10.1016%2fj.eswa.2019.04.046&partnerID=40&md5=965b23deab6d16118324176f63145de7,2019,2021-07-20 15:49:14,2021-07-20 15:49:14
BE4WSGRL,journalArticle,2019,"Patil, A.D.; Manipatruni, S.; Nikonov, D.E.; Young, I.A.; Shanbhag, N.R.",Error-Resilient Spintronics via the Shannon- Inspired Model of Computation,"The energy and delay reductions from CMOS scaling have stagnated, motivating the search for a CMOS replacement. Spintronic devices are one of the promising beyond-CMOS alternatives. However, they exhibit high switching error rates of 1% or more when operated at energy and delay comparable to CMOS, rendering them incompatible with the deterministic nature of digital implementations. In this paper, we employ a Shannon-inspired model of computation to enhance the tolerance of all-spin logic (ASL)-based implementations to gate-level switching errors. We develop the logic-level path delay reallocation techniques to shape the output error statistics and propose a novel error compensation scheme to achieve 1000\times higher tolerance to device-level switching errors while maintaining the classification accuracy of an ASL-based support vector machine (SVM) classifier. © 2014 IEEE.",IEEE Journal on Exploratory Solid-State Computational Devices and Circuits,10.1109/JXCDC.2019.2909912,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066767928&doi=10.1109%2fJXCDC.2019.2909912&partnerID=40&md5=8ebb53518a3dfe6f4afc208cf4943255,2019,2021-07-20 15:49:14,2021-07-20 15:49:14
ZAM8XBEB,journalArticle,2016,"Tenorio-González, A.C.; Morales, E.F.",Automatic discovery of relational concepts by an incremental graph-based representation,"Automatic discovery of concepts has been an elusive area in machine learning. In this paper, we describe a system, called ADC, that automatically discovers concepts in a robotics domain, performing predicate invention. Unlike traditional approaches of concept discovery, our approach automatically finds and collects instances of potential relational concepts. An agent, using ADC, creates an incremental graph-based representation with the information it gathers while exploring its environment, from which common sub-graphs are identified. The subgraphs discovered are instances of potential relational concepts which are induced with Inductive Logic Programming and predicate invention. Several concepts can be induced concurrently and the learned concepts can form arbitrarily hierarchies. The approach was tested for learning concepts of polygons, furniture, and floors of buildings with a simulated robot and compared with concepts suggested by users. © 2016 Elsevier B.V.",Robotics and Autonomous Systems,10.1016/j.robot.2016.06.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028619966&doi=10.1016%2fj.robot.2016.06.012&partnerID=40&md5=1625f7e11b6201cc9eb60ea36a9e11b6,2016,2021-07-20 15:49:14,2021-07-20 15:49:14
5ERBF464,journalArticle,2020,"da Fonseca, F.N.; Abe, J.M.; de Alencar Nääs, I.; da Silva Cordeiro, A.F.; do Amaral, F.V.; Ungaro, H.C.",Automatic prediction of stress in piglets (Sus Scrofa) using infrared skin temperature,"Pork consumption grows about 5% per year in developing countries. Ensuring food safety within ethical standards of meat production is a growing consumer’ demand. The present study aimed to develop a model to predict stress in piglets based on the infrared skin temperature (IST) using machine learning and the paraconsistent logic. A total of 72 piglets (32 males and 40 females) from 1 to 52 days old had the infrared skin temperature recorded during the farrowing and nursery phases under different stress conditions (pain, cold/heat, hunger, and thirst). The assessment of the thermal images was done using an infrared thermography camera. Thermograms were taken at ambient air temperatures ranging from 24 to 30 °C. The minimum infrared skin temperature (IST min) and the maximum infrared skin temperature (ISTmax) and the piglet sex were used as attributes to find the stress conditions (target). The attributes considered in the analysis were classified using the data mining method. The imaging technique is subject to certain contradictions and uncertainties that require mathematical modeling. The paraconsistent logic was applied to extract the contradiction from the data. The stress condition that had higher accuracy in the detection was that predicted by the cold (100%) using the ISTmin, and ISTmin plus the piglet sex, and thirst (91%) using ISTmax and ISTmax plus the piglet sex. The highest prediction of hunger was found using ISTmin (86%). Although the model was precise in detecting those stresses, the other stressful conditions in piglets such as pain that had an accuracy equal or less than 50%. Results indicate a promising assessment of stress condition in piglets using infrared skin temperature. We suggest the inclusion of other attributes in the machine learning process to amplify the use of the model. © 2019",Computers and Electronics in Agriculture,10.1016/j.compag.2019.105148,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076000775&doi=10.1016%2fj.compag.2019.105148&partnerID=40&md5=0ff6ee4a1f06480b49b57344767dd541,2020,2021-07-20 15:49:14,2021-07-20 15:49:14
MRPX5C87,journalArticle,2013,"Hedjazi, L.; Le Lann, M.-V.; Kempowsky, T.; Dalenc, F.; Aguilar-Martin, J.; Favre, G.",Symbolic data analysis to defy low signal-to-noise ratio in microarray data for breast cancer prognosis,"Microarray profiling has recently generated the hope to gain new insights into breast cancer biology and thereby improve the performance of current prognostic tools. However, it also poses several serious challenges to classical data analysis techniques related to the characteristics of resulting data, mainly high dimensionality and low signal-to-noise ratio. Despite the tremendous research work performed to handle the first challenge in the feature selection framework, very little attention has been directed to address the second one. We propose in this article to address both issues simultaneously based on symbolic data analysis capabilities in order to derive more accurate genetic marker-based prognostic models. In particular, interval data representation is employed to model various uncertainties in microarray measurements. A recent feature selection algorithm that handles symbolic interval data is used then to derive a genetic signature. The predictive value of the derived signature is then assessed by following a rigorous experimental setup and compared with existing prognostic approaches in terms of predictive performance and estimated survival probability. It is shown that the derived signature (GenSym) performs significantly better than other prognostic models, including the 70-gene signature, St. Gallen, and National Institutes of Health criteria. © Copyright 2013, Mary Ann Liebert, Inc. 2013.",Journal of Computational Biology,10.1089/cmb.2012.0249,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881143116&doi=10.1089%2fcmb.2012.0249&partnerID=40&md5=372bcca305bad707b823b1339f188b97,2013,2021-07-20 15:49:14,2021-07-20 15:49:14
W343HNE7,journalArticle,2019,"Silva de Oliveira, C.; Sanin, C.; Szczerbicki, E.",Visual Content Learning in a Cognitive Vision Platform for Hazard Control (CVP-HC),"This work is part of an effort for the development of a Cognitive Vision Platform for Hazard Control (CVP-HC) for applications in industrial workplaces, adaptable to a wide range of environments. The paper focuses on hazards resulted from the nonuse of personal protective equipment (PPE). Given the results of previous analysis of supervised techniques for the problem of classification of a few PPE (boots, hard hats, and gloves extracted from frames of low resolution videos), which found the Deep Learning (DL) methods as the most suitable ones to integrate our platform, the objective of this paper is to test two DL algorithms: Single Shot Detector (SSD) and Faster Region-based Convolutional Network (Faster R-CNN). The testing uses pretrained models on a second version of our PPE dataset (containing 11 classes of objects) and evaluates which of examined algorithms is more appropriate to compose our system reasoning. © 2019, © 2019 Taylor & Francis Group, LLC.",Cybernetics and Systems,10.1080/01969722.2019.1565116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061229376&doi=10.1080%2f01969722.2019.1565116&partnerID=40&md5=e1e7b8777dd628ee7dc85d650c409337,2019,2021-07-20 15:49:15,2021-07-20 15:49:15
PN8G2HTP,journalArticle,2019,"Mitash, C.; Boularias, A.; Bekris, K.",Physics-based scene-level reasoning for object pose estimation in clutter,"This paper focuses on vision-based pose estimation for multiple rigid objects placed in clutter, especially in cases involving occlusions and objects resting on each other. Progress has been achieved recently in object recognition given advancements in deep learning. Nevertheless, such tools typically require a large amount of training data and significant manual effort to label objects. This limits their applicability in robotics, where solutions must scale to a large number of objects and variety of conditions. Moreover, the combinatorial nature of the scenes that could arise from the placement of multiple objects is difficult to capture in the training dataset. Thus, the learned models might not produce the desired level of precision required for tasks, such as robotic manipulation. This work proposes an autonomous process for pose estimation that spans from data generation to scene-level reasoning and self-learning. In particular, the proposed framework first generates a labeled dataset for training a convolutional neural network (CNN) for object detection in clutter. These detections are used to guide a scene-level optimization process, which considers the interactions between the different objects present in the clutter to output pose estimates of high precision. Furthermore, confident estimates are used to label online real images from multiple views and re-train the process in a self-learning pipeline. Experimental results indicate that this process is quickly able to identify in cluttered scenes physically consistent object poses that are more precise than those found by reasoning over individual instances of objects. Furthermore, the quality of pose estimates increases over time given the self-learning process. © The Author(s) 2019.",International Journal of Robotics Research,10.1177/0278364919846551,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065729149&doi=10.1177%2f0278364919846551&partnerID=40&md5=c8c4769acb924c2d1a3d593706f3e476,2019,2021-07-20 15:49:15,2021-07-20 15:49:15
2FE2GRPE,journalArticle,2020,"Deponte, H.; Tonda, A.; Gottschalk, N.; Bouvier, L.; Delaplace, G.; Augustin, W.; Scholl, S.",Two complementary methods for the computational modeling of cleaning processes in food industry,"Insufficient cleaning in the food industry can create serious hygienic risks. However, when attempting to avoid these risks, food-processing plants frequently tend to clean for too long, at extremely high temperatures, or with too many chemicals, resulting in high cleaning costs and severe environmental impacts. Therefore, the optimization of cleaning processes in the food industry has significant economic and ecological potential. Unfortunately, in-situ assessments of cleaning processes are difficult, and the multitude of different cleaning situations complicates the definition of a comprehensive approach. In this study, two methodological approaches for the comprehensive modeling of cleaning processes are introduced. The resulting models facilitate comparisons of different cleaning processes and they can be scaled up for processes with similar conditions, using cleaning time as a response. A dimensional analysis is performed to obtain general results and to allow transfer of the approaches to other cleaning situations. The models are established according to the statistical rules for the deduction of multiple regression equations for the prediction of the response based on the input parameters. The terms of the model equation are confirmed with a significance analysis. A machine learning approach is also used to create model equations with symbolic regression. Both methods and the obtained model equations are validated. The two applied approaches reveal similar significant terms and models. Significant dimensionless numbers are the Reynolds number, the density number that describes the ratio of the density of the soil to the density of the cleaning agent, and the soil number, which is a new dimensionless number that characterizes the properties of food soils. The methodology of both approaches is transparent; therefore, the resulting equations can be compared and similarities are found. Both methods are deemed applicable for the computational modeling of cleaning processes in food industry. © 2020",Computers and Chemical Engineering,10.1016/j.compchemeng.2020.106733,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078057056&doi=10.1016%2fj.compchemeng.2020.106733&partnerID=40&md5=01844870bcfa03045c456f1ee5cf625b,2020,2021-07-20 15:49:15,2021-07-20 15:49:15
L5FKAPGN,journalArticle,2021,"Mathisen, B.M.; Bach, K.; Aamodt, A.",Using extended siamese networks to provide decision support in aquaculture operations,"Aquaculture as an industry is quickly expanding. As a result, new aquaculture sites are being established at more exposed locations previously deemed unfit because they are more difficult and resource demanding to safely operate than are traditional sites. To help the industry deal with these challenges, we have developed a decision support system to support decision makers in establishing better plans and make decisions that facilitate operating these sites in an optimal manner. We propose a case-based reasoning system called aquaculture case-based reasoning (AQCBR), which is able to predict the success of an aquaculture operation at a specific site, based on previously applied and recorded cases. In particular, AQCBR is trained to learn a similarity function between recorded operational situations/cases and use the most similar case to provide explanation-by-example information for its predictions. The novelty of AQCBR is that it uses extended Siamese neural networks to learn the similarity between cases. Our extensive experimental evaluation shows that extended Siamese neural networks outperform state-of-the-art methods for similarity learning in this task, demonstrating the effectiveness and the feasibility of our approach. © 2021, The Author(s).",Applied Intelligence,10.1007/s10489-021-02251-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103348194&doi=10.1007%2fs10489-021-02251-3&partnerID=40&md5=991d158cc017428b6f34ac436540dbcf,2021,2021-07-20 15:49:15,2021-07-20 15:49:15
JS7C6KIH,journalArticle,2020,"Cheng, P.; Chen, Z.; Ding, M.; Li, Y.; Vucetic, B.; Niyato, D.","Spectrum Intelligent Radio: Technology, Development, and Future Trends","The advent of Industry 4.0 with massive connectivity places significant strains on the current spectrum resources, and challenges the industry and regulators to respond promptly with new disruptive spectrum management strategies. The envisioned spectrum intelligent radio has long been promised to unlock the full potential of spectrum resource. However, the current radio development, with certain elements of intelligence, is nowhere near showing an agile response to the complex radio environments. Following the line of intelligence, we propose to classify spectrum intelligent radio into three streams: classical signal processing, machine learning (ML), and contextual adaptation. We focus on the ML approach, and propose a new intelligent radio architecture with three hierarchical forms: perception, understanding, and reasoning. For each form, we propose some preliminary methods. The proposed perception method achieves fully blind multi-level spectrum sensing. The understanding method accurately predicts the primary users' coverage across a large area, and the reasoning method performs near-optimal idle channel selection. Opportunities, challenges, and future visions are also discussed for the realization of a fully intelligent radio. © 1979-2012 IEEE.",IEEE Communications Magazine,10.1109/MCOM.001.1900200,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078760189&doi=10.1109%2fMCOM.001.1900200&partnerID=40&md5=cf72b4083581083bce56a40952e89263,2020,2021-07-20 15:49:15,2021-07-20 15:49:15
F6CE42MS,journalArticle,2019,"Quemy, A.",Binary classification in unstructured space with hypergraph case-based reasoning,"Binary classification is one of the most common problem in machine learning. It consists in predicting whether a given element belongs to a particular class. In this paper, a new algorithm for binary classification is proposed using a hypergraph representation. The method is agnostic to data representation, can work with multiple data sources or in non-metric spaces, and accommodates with missing values. As a result, it drastically reduces the need for data preprocessing or feature engineering. Each element to be classified is partitioned according to its interactions with the training set. For each class, a seminorm over the training set partition is learnt to represent the distribution of evidence supporting this class. Empirical validation demonstrates its high potential on a wide range of well-known datasets and the results are compared to the state-of-the-art. The time complexity is given and empirically validated. Its robustness with regard to hyperparameter sensitivity is studied and compared to standard classification methods. Finally, the limitation of the model space is discussed, and some potential solutions proposed. © 2019 Elsevier Ltd",Information Systems,10.1016/j.is.2019.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063091499&doi=10.1016%2fj.is.2019.03.005&partnerID=40&md5=b882bfef650ca7b7908701926cb4cb56,2019,2021-07-20 15:49:15,2021-07-20 15:49:15
7DRQNQ69,journalArticle,2018,"Smiti, A.; Elouedi, Z.",SCBM: soft case base maintenance method based on competence model,"This paper concerns one of intelligent computational techniques which is case based reasoning (CBR) more particularly the case base maintenance (CBM). It aims to ensure the CBR systems quality. Throughout this paper, we were faced to a problematic question: how to shrink the size of the case base while preserving as much as possible the performance and the competence of the CBR system in soft context. To answer this question, we have first analyzed and revised the theoretical foundations of the existing CBM methods. Then, we have proposed a novel soft case base maintenance (SCBM) method based on a soft competence model (SCM) and a fuzzy clustering technique. Our method has the objective to guarantee the CBR systems efficiency in terms of improving the competence, and reducing both the storage requirements and search time. We support our approach with empirical evaluation using different benchmark data sets to show the effectiveness of our method in terms of improving the competence and the performance of the system. © 2017 Elsevier B.V.",Journal of Computational Science,10.1016/j.jocs.2017.09.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030647221&doi=10.1016%2fj.jocs.2017.09.013&partnerID=40&md5=0f7a290f1d5685e7ef3289369fc1ee4c,2018,2021-07-20 15:49:15,2021-07-20 15:49:15
948BANJ5,journalArticle,2016,"Gori, M.; Lippi, M.; Maggini, M.; Melacci, S.",Semantic video labeling by developmental visual agents,"In the recent years, computer vision has been undergoing a period of great development, testified by the many successful applications that are currently available in a variety of industrial products. Yet, when we come to the most challenging and foundational problem of building autonomous agents capable of performing scene understanding in unrestricted videos, there is still a lot to be done. In this paper we focus on semantic labeling of video streams, in which a set of semantic classes must be predicted for each pixel of the video. We propose to attack the problem from bottom to top, by introducing Developmental Visual Agents (DVAs) as general purpose visual systems that can progressively acquire visual skills from video data and experience, by continuously interacting with the environment and following lifelong learning principles. DVAs gradually develop a hierarchy of architectural stages, from unsupervised feature extraction to the symbolic level, where supervisions are provided by external users, pixel-wise. Differently from classic machine learning algorithms applied to computer vision, which typically employ huge datasets of fully labeled images to perform recognition tasks, DVAs can exploit even a few supervisions per semantic category, by enforcing coherence constraints based on motion estimation. Experiments on different vision tasks, performed on a variety of heterogeneous visual worlds, confirm the great potential of the proposed approach. © 2016 Elsevier Inc. All rights reserved.",Computer Vision and Image Understanding,10.1016/j.cviu.2016.02.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959880509&doi=10.1016%2fj.cviu.2016.02.011&partnerID=40&md5=84205171cd3e18b3a57bc89e789a3397,2016,2021-07-20 15:49:15,2021-07-20 15:49:15
J5UZZCST,journalArticle,2020,"Li, X.; Zeng, F.; Yao, C.",A Semi-Supervised Paraphrase Identification Model Based on Multi-Granularity Interaction Reasoning,"Conventional paraphrase identification (PI) models based on deep learning usually focus on text representation and ignore the mining and matching of multi-granular interaction features. In addition, supervised learning relies on a large labeled data. However, labeled training set for PI is small in comparison with the high complexity of the task. To solve the problems, we propose a semi-supervised deep learning framework for PI. We use a neural encoder with word-by-word attention mechanism to reason equivalence or contradiction over pairs of words, phrases and sentences. We employ a two-stage training procedure. First, we use a language modeling objective to learn the initial parameters on the unlabeled corpora of more than one million pairs of sentences. This is followed by a supervised training, where we adapt these parameters to a specific classification task with labeled data. Experimental results on MRPC (Microsoft Research Paraphrase Corpus) and SICK (Sentences Involving Compositional Knowledge) datasets demonstrate the effectiveness of our approach. Compared with the previous neural network models, we achieve absolute improvements in accuracy of 7.6% and F1 of 5.4% on MRPC, Pearson's r of 4.5% and Spearman's\rho of 5.1% on SICK. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2984009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083345177&doi=10.1109%2fACCESS.2020.2984009&partnerID=40&md5=aaef96d91e443650f9e7e63cf2631aca,2020,2021-07-20 15:49:15,2021-07-20 15:49:15
ZPVH4GZ4,journalArticle,2020,"Shen, T.; Wang, J.; Gou, C.; Wang, F.-Y.",Hierarchical Fused Model with Deep Learning and Type-2 Fuzzy Learning for Breast Cancer Diagnosis,"Breast cancer diagnosis based on medical imaging necessitates both fine-grained lesion segmentation and disease grading. Although deep learning (DL) offers an emerging and powerful paradigm of feature learning for these two tasks, it is hampered from popularizing in practical application due to the lack of interpretability, generalization ability, and large labeled training sets. In this article, we propose a hierarchical fused model based on DL and fuzzy learning to overcome the drawbacks for pixelwise segmentation and disease grading of mammography breast images. The proposed system consists of a segmentation model (ResU-segNet) and a hierarchical fuzzy classifier (HFC) that is a fusion of interval type-2 possibilistic fuzzy c-means and fuzzy neural network. The ResU-segNet segments the masks of mass regions from the images through convolutional neural networks, while the HFC encodes the features from mass images and masks to obtain the disease grading through fuzzy representation and rule-based learning. Through the integration of feature extraction aided by domain knowledge and fuzzy learning, the system achieves favorable performance in a few-shot learning manner, and the deterioration of cross-dataset generalization ability is alleviated. In addition, the interpretability is further enhanced. The effectiveness of the proposed system is analyzed on the publicly available mammogram database of INbreast and a private database through cross-validation. Thorough comparative experiments are also conducted and demonstrated. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2020.3013681,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097342886&doi=10.1109%2fTFUZZ.2020.3013681&partnerID=40&md5=4256dd19130ec810aa87cb6df3ef7000,2020,2021-07-20 15:49:15,2021-07-20 15:49:15
NWG2B2ZI,journalArticle,2020,"Perkusich, M.; Chaves e Silva, L.; Costa, A.; Ramos, F.; Saraiva, R.; Freire, A.; Dilorenzo, E.; Dantas, E.; Santos, D.; Gorgônio, K.; Almeida, H.; Perkusich, A.",Intelligent software engineering in the context of agile software development: A systematic literature review,"CONTEXT: Intelligent Software Engineering (ISE) refers to the application of intelligent techniques to software engineering. We define an “intelligent technique” as a technique that explores data (from digital artifacts or domain experts) for knowledge discovery, reasoning, learning, planning, natural language processing, perception or supporting decision-making. OBJECTIVE: The purpose of this study is to synthesize and analyze the state of the art of the field of applying intelligent techniques to Agile Software Development (ASD). Furthermore, we assess its maturity and identify adoption risks. METHOD: Using a systematic literature review, we identified 104 primary studies, resulting in 93 unique studies. RESULTS: We identified that there is a positive trend in the number of studies applying intelligent techniques to ASD. Also, we determined that reasoning under uncertainty (mainly, Bayesian network), search-based solutions, and machine learning are the most popular intelligent techniques in the context of ASD. In terms of purposes, the most popular ones are effort estimation, requirements prioritization, resource allocation, requirements selection, and requirements management. Furthermore, we discovered that the primary goal of applying intelligent techniques is to support decision making. As a consequence, the adoption risks in terms of the safety of the current solutions are low. Finally, we highlight the trend of using explainable intelligent techniques. CONCLUSION: Overall, although the topic area is up-and-coming, for many areas of application, it is still in its infancy. So, this means that there is a need for more empirical studies, and there are a plethora of new opportunities for researchers. © 2019 Elsevier B.V.",Information and Software Technology,10.1016/j.infsof.2019.106241,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077492168&doi=10.1016%2fj.infsof.2019.106241&partnerID=40&md5=3bfc8d9fee407e5f7e2d15a745c97341,2020,2021-07-20 15:49:15,2021-07-20 15:49:15
MGCPJT4Y,journalArticle,2016,"Horita, H.; Hirayama, H.; Hayase, T.; Tahara, Y.; Ohsuga, A.",Business process verification and restructuring LTL formula based on machine learning approach,"It is important to deal with rapidly changing environments (regulations, customer behavior change, and process improvement etc.) to keep achieving business goals. Therefore, verification for business process in various phases are needed to make sure of goal achievements. LTL (Linear Temporal Logic) verification is an important method for checking a specific property to be satisfied with business processes, but correctly writing formal language like LTL is difficult. Lacks of domain knowledge and knowledge of mathematical logics have bad influence on writing LTL formulas. In this paper, we use LTL verification and prediction based on decision tree learning for verification of specific properties. Furthermore, we helps writing properly LTL formula for representing the correct desirable property using decision tree constrction. We conducted a case study for evaluations. © Springer International Publishing Switzerland 2016.",Studies in Computational Intelligence,10.1007/978-3-319-40171-3_7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991811269&doi=10.1007%2f978-3-319-40171-3_7&partnerID=40&md5=28ba8449effd69e39215d8bc9fff5785,2016,2021-07-20 15:49:15,2021-07-20 15:49:15
95SRWHND,journalArticle,2015,"Pejovic, V.; Musolesi, M.",Anticipatory mobile computing: A survey of the state of the art and research challenges,"Today's mobile phones are far from the mere communication devices they were 10 years ago. Equipped with sophisticated sensors and advanced computing hardware, phones can be used to infer users' location, activity, social setting, and more. As devices become increasingly intelligent, their capabilities evolve beyond inferring context to predicting it, and then reasoning and acting upon the predicted context. This article provides an overview of the current state of the art in mobile sensing and context prediction paving the way for full-fledged anticipatory mobile computing. We present a survey of phenomena that mobile phones can infer and predict, and offer a description of machine learning techniques used for such predictions. We then discuss proactive decision making and decision delivery via the user-device feedback loop. Finally, we discuss the challenges and opportunities of anticipatory mobile computing. © 2015 ACM.",ACM Computing Surveys,10.1145/2693843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920129025&doi=10.1145%2f2693843&partnerID=40&md5=6ad1e0ca21355613027b5e6c97280d0d,2015,2021-07-20 15:49:16,2021-07-20 15:49:16
7UVV8IEJ,journalArticle,2020,"Correll, J.",A discretely adaptive connection logic network,"This paper proposes a new model for a theoretical neural network that can be used as a guide for the design of future (quantum or optical) computational technologies. The model utilizes a uniformly connected nodal structure, where the connections are discretely adaptive and the nodes act as simple gatekeepers. The model replicates all known logics used in current electronics, such as AND, OR, XNOR, XOR, NOR, XNOR, NAND and NOT. Additionally, by using recurrent negating connections the model easily creates XOR gates, and adds novel sided and favoured gates. This model also facilitates the creation of ternary to n-ary gates, and it simplifies the creation of a number of majority functions (especially for an odd number of inputs). Also, as a multi-layered neural network, the model allows learning back propagation through the use of its negating connections. Finally, because of its adaptive connections, parts of the network can be used as internal memory. Overall, the model provides backward compatibility to existing CMOS circuitry, while opening up a number of new logics and architectures for neural computing. © 2019",Neurocomputing,10.1016/j.neucom.2019.10.099,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076001333&doi=10.1016%2fj.neucom.2019.10.099&partnerID=40&md5=e68112b53f451f39f954f4cbb0be03f3,2020,2021-07-20 15:49:16,2021-07-20 15:49:16
9MZYR2WH,journalArticle,2019,"Zhang, H.; Hong, X.",Recent progresses on object detection: a brief review,"Object detection, aiming at locating objects from a large number of specific categories in natural images, is a fundamental but challenging task in the field of computer vision. Recent years have seen significant progress of object detection using deep CNN mainly due to its robust feature representation ability. The goal of this paper is to provide a simple but comprehensive survey of the recent improvements in object detection in the era of deep learning. More than 100 key contributions are investigated mainly from five directions: architecture diagram, contextual reasoning, multi-layer exploiting, training strategy, and others which includes some other progress like real-time object detectors and works borrowing the idea from RNN and GAN. We discuss comprehensive but straightforward experimental comparisons under widely used benchmarks and metrics. This review finishes by providing promising trends for future research. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-019-07898-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068336290&doi=10.1007%2fs11042-019-07898-2&partnerID=40&md5=78dc6434be702aaf317d00ba6ad75bcf,2019,2021-07-20 15:49:16,2021-07-20 15:49:16
4XQVNZV2,journalArticle,2020,"Cui, H.; Chang, C.",Deep Learning Based Advanced Spatiooral Extraction Model in Medical Sports Rehabilitation for Motion Analysis and Data Processing,"Presently, A wide range of unlabeled and minimal style data significantly decreases the current motion sequence's reuse ability. An important method of data reuse has a successful classification and fragment separation, which has been discussed in this research. This paper focuses on these particular problems and the tremendous progress of deep learning in design and symbolic fields. A Limited Boltzmann Model (LBM) theory is based on the Advanced Spatiooral Extraction Model (ASTEM), which has been used for analyzing the physiological motion of human skeletons. There are primarily three aspects to the results of the study. (1) For constructing a semi-combination model, the stack factor decomposition is used as a spatiotemporal model function and LBM discrimination. (2), Optimized algorithm used to create the three-channel generative LBM model using the weight decomposition idea and then extract the time and space-based abstract properties of the original motion series. (3) The unsupervised related model of frame detection is built using the perception of human interaction through 3D convolution LBM. A significant research direction of the medical analysis and extraction of sports data is used appropriately to interpret and gain valuable information and knowledge from motion analyses. Experimental outcomes show that this technique offers technical assistance and guidance for implementing a real cloud-based fusion system. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3003652,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087844719&doi=10.1109%2fACCESS.2020.3003652&partnerID=40&md5=6d215d56b8115f3a587db82f488fcef1,2020,2021-07-20 15:49:16,2021-07-20 15:49:16
TBJ6EUJC,journalArticle,2020,"Sachan, S.; Yang, J.-B.; Xu, D.-L.; Benavides, D.E.; Li, Y.",An explainable AI decision-support-system to automate loan underwriting,"Widespread adoption of automated decision making by artificial intelligence (AI) is witnessed due to specular advances in computation power and improvements in optimization algorithms especially in machine learning (ML). Complex ML models provide good prediction accuracy; however, the opacity of ML models does not provide sufficient assurance for their adoption in the automation of lending decisions. This paper presents an explainable AI decision-support-system to automate the loan underwriting process by belief-rule-base (BRB). This system can accommodate human knowledge and can also learn from historical data by supervised learning. The hierarchical structure of BRB can accommodates factual and heuristic rules. The system can explain the chain of events leading to a decision for a loan application by the importance of an activated rule and the contribution of antecedent attributes in the rule. A business case study on automation of mortgage underwriting is demonstrated to show that the BRB system can provide a good trade-off between accuracy and explainability. The textual explanation produced by the activation of rules could be used as a reason for denial of a loan. The decision-making process for an application can be comprehended by the significance of rules in providing the decision and contribution of its antecedent attributes. © 2019",Expert Systems with Applications,10.1016/j.eswa.2019.113100,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075989564&doi=10.1016%2fj.eswa.2019.113100&partnerID=40&md5=5ec482b90ba8e4eee4ae693fa8d34dd6,2020,2021-07-20 15:49:16,2021-07-20 15:49:16
2U4V9L7M,journalArticle,2018,"Liu, L.; Yu, Y.; Fei, Z.; Li, M.; Wu, F.-X.; Li, H.-D.; Pan, Y.; Wang, J.",An interpretable boosting model to predict side effects of analgesics for osteoarthritis,"Background: Osteoarthritis (OA) is the most common disease of arthritis. Analgesics are widely used in the treat of arthritis, which may increase the risk of cardiovascular diseases by 20% to 50% overall.There are few studies on the side effects of OA medication, especially the risk prediction models on side effects of analgesics. In addition, most prediction models do not provide clinically useful interpretable rules to explain the reasoning process behind their predictions. In order to assist OA patients, we use the eXtreme Gradient Boosting (XGBoost) method to balance the accuracy and interpretability of the prediction model. Results: In this study we used the XGBoost model as a classifier, which is a supervised machine learning method and can predict side effects of analgesics for OA patients and identify high-risk features (RFs) of cardiovascular diseases caused by analgesics. The Electronic Medical Records (EMRs), which were derived from public knee OA studies, were used to train the model. The performance of the XGBoost model is superior to four well-known machine learning algorithms and identifies the risk features from the biomedical literature. In addition the model can provide decision support for using analgesics in OA patients. Conclusion: Compared with other machine learning methods, we used XGBoost method to predict side effects of analgesics for OA patients from EMRs, and selected the individual informative RFs. The model has good predictability and interpretability, this is valuable for both medical researchers and patients. © 2018 The Author(s).",BMC Systems Biology,10.1186/s12918-018-0624-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056929660&doi=10.1186%2fs12918-018-0624-4&partnerID=40&md5=db9e054c26a6a318c0d57cfa0cdc8ff4,2018,2021-07-20 15:49:16,2021-07-20 15:49:16
8ATUXH3H,journalArticle,2021,"Deshmukh, R.; Sun, D.; Kim, K.; Hwang, I.",Temporal logic learning-based anomaly detection in metroplex terminal airspace operations,"The airspace system is a complex dynamical system with complicated controlled interactions between its constituent subsystems – terminal airspace, en-route airspace, and ground. Of these, air traffic management in the multi-airport (metroplex) terminal airspace is one of the most complicated subsystems to manage, especially due to the interactions between proximal airports. Analyzing anomalous behaviors in the metroplex is emerging as a key problem in understanding air traffic management complexity and safety. Although physics-based approaches have been studied in-depth for this application, newfound interest has been observed to use recorded time-series air traffic surveillance and airport operations datasets for this purpose. In this paper, we propose a machine learning-based anomaly detection algorithm that generates mathematical models to detect anomalies in metroplex operations. Several machine learning algorithms have been developed to detect anomalies using only air traffic surveillance data, but there is a significant scope of improvement by including airport operational characteristics as well, since integrating such closely-controlled metroplex operational datasets allows the developed models to effectively detect anomalies. The key contribution of this paper is in allowing anomaly detection models to recursively update so as to adapt to changes in metroplex operations. The proposed algorithm is demonstrated with real air traffic surveillance and airport operations datasets at LaGuardia, John F. Kennedy, and Newark airports, thereby detecting anomalies for operations in the New York metroplex. © 2021 Elsevier Ltd",Transportation Research Part C: Emerging Technologies,10.1016/j.trc.2021.103036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102894653&doi=10.1016%2fj.trc.2021.103036&partnerID=40&md5=83721856415e3b289e3e368821e5ee0f,2021,2021-07-20 15:49:16,2021-07-20 15:49:16
LN3B9I9X,journalArticle,2021,"Muthamil Sudar, K.; Deepalakshmi, P.",An intelligent flow-based and signature-based IDS for SDNs using ensemble feature selection and a multi-layer machine learning-based classifier,"Software-defined networking is a new paradigm that overcomes problems associated with traditional network architecture by separating the control logic from data plane devices. It also enhances performance by providing a highly-programmable interface that adapts to dynamic changes in network policies. As software-defined networking controllers are prone to single-point failures, providing security is one of the biggest challenges in this framework. This paper intends to provide an intrusion detection mechanism in both the control plane and data plane to secure the controller and forwarding devices respectively. In the control plane, we imposed a flow-based intrusion detection system that inspects every new incoming flow towards the controller. In the data plane, we assigned a signature-based intrusion detection system to inspect traffic between Open Flow switches using port mirroring to analyse and detect malicious activity. Our flow-based system works with the help of trained, multi-layer machine learning-based classifier, while our signature-based system works with rule-based classifiers using the Snort intrusion detection system. The ensemble feature selection technique we adopted in the flow-based system helps to identify the prominent features and hasten the classification process. Our proposed work ensures a high level of security in the Software-defined networking environment by working simultaneously in both control plane and data plane. © 2021-IOS Press. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-200850,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102409114&doi=10.3233%2fJIFS-200850&partnerID=40&md5=96f33f1077fb7b940c1ffc248e55e196,2021,2021-07-20 15:49:16,2021-07-20 15:49:16
ESSZJL2U,journalArticle,2015,"Gerevini, A.E.; Saetti, A.; Vallati, M.",Exploiting macro-actions and predicting plan length in planning as satisfiability,"The use of automatically learned knowledge for a planning domain can significantly improve the performance of a generic planner when solving a problem in this domain. In this work, we focus on the well-known SAT-based approach to planning and investigate two types of learned knowledge: macro-actions and planning horizon. Macro-actions are sequences of actions that typically occur in the solution plans, while a planning horizon of a problem is the length of a (possibly optimal) plan solving it. We propose a method that uses a machine learning tool for building a predictive model of the optimal planning horizon, and variants of the well-known planner SatPlan and solver MiniSat that can exploit macro actions and learned planning horizons to improve their performance. An experimental analysis illustrates the effectiveness of the proposed techniques demonstrating that significant speedups can be obtained. © 2015-IOS Press and the authors. All rights reserved.",AI Communications,10.3233/AIC-140641,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922570357&doi=10.3233%2fAIC-140641&partnerID=40&md5=b4df36c0ba0fcf91ff314be58dfaddb7,2015,2021-07-20 15:49:16,2021-07-20 15:49:16
G6T5N4DJ,journalArticle,2021,"Holla, M.R.; Pais, A.R.",An effective secret image sharing using quantum logic and GPGPU based EDNN super-resolution,"This paper presented an effective secret image sharing with super-resolution utilizing quantum logic and enthalpy based adaptive deep neural network. The proposed technique is processed as; at the sender side, initially secret input image is converted into a halftone image format by utilizing Error diffusion with varying thresholds (EDVT) method. Then in share generation phase, shares are produced with the basis matrix. Here, the basis matrix is created utilizing the quantum logic methodology. Then in embedding phase, discrete wavelet transform (DWT) is utilized for encoding shares. At the receiver side, encoded image is reconstructed using XOR operation and results the low-resolution image. Finally, enthalpy based adaptive deep neural network (EDNN) is designed with the General Purpose Graphic Processing Unit (GPGPU) to enhance the resolution of the reconstructed images and to lessen the time complexity of deep learning. Here, the EDNN is adapted with the enthalpy based normalization to mitigate the over fitting in layers of deep neural network. Furthermore, the proficiency of the proposed work improved in terms of normalized cross correlation, normalized absolute error, peak signal to noise ratio, mean square error and execution time by deploying images among CPU and GPGPU in an enhanced manner. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-020-10065-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096102609&doi=10.1007%2fs11042-020-10065-7&partnerID=40&md5=5ee20543c162dcdb913fd139ce760eb5,2021,2021-07-20 15:49:16,2021-07-20 15:49:16
A33EPND6,journalArticle,2020,"Hosseinzadeh, M.; Tho, Q.T.; Ali, S.; Rahmani, A.M.; Souri, A.; Norouzi, M.; Huynh, B.",A Hybrid Service Selection and Composition Model for Cloud-Edge Computing in the Internet of Things,"Cloud-edge computing is a hybrid model of computing where resources and services provided via the Internet of Things (IoT) between large-scale and long-term data informs of the cloud layer and small-scale and short-term data as edge layer. The main challenge of the cloud service providers is to select the optimal candidate services that are doing the same work but offer different Quality of Service (QoS) values in IoT applications. Service composition in cloud-edge computing is an NP-hard problem; therefore, many meta-heuristic methods introduced to solve this issue. Also, the correctness of meta-heuristic and machine learning algorithms for evaluating service composition problem should be proven using formal methods to guarantee functional and non-functional specifications. In this paper, a hybrid Artificial Neural Network-based Particle Swarm Optimization (ANN-PSO) Algorithm presented to enhance the QoS factors in cloud-edge computing. To illustrate the correctness and improve the reachability rate of candidate composited services and QoS factors for the proposed hybrid algorithm, we present a formal verification method based on a labeled transition system to check some critical Linear Temporal Logics (LTL) formulas. The experimental results illustrated the high performance of the proposed model in terms of minimum verification time, memory consumption, and guaranteeing critical specifications rules as the Linear Temporal Logic (LTL) formulas. Also, we observed that the proposed model has optimal response time, availability, and price with maximum fitness function value than other service composition algorithms. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2992262,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085254747&doi=10.1109%2fACCESS.2020.2992262&partnerID=40&md5=80a400ed7f652533e84e0acf8e37ab97,2020,2021-07-20 15:49:16,2021-07-20 15:49:16
EQR62SUA,journalArticle,2018,"Hela, S.; Amel, B.; Badran, R.",Early anomaly detection in smart home: A causal association rule-based approach,"As the world's population grows older, an increasing number of people are facing health issues. For the elderly, living alone can be difficult and dangerous. Consequently, smart homes are becoming increasingly popular. A sensor-rich environment can be exploited for healthcare applications, in particular, anomaly detection (AD). The literature review for this paper showed that few works consider environmental factors to detect anomalies. Instead, the focus is on user activity and checking whether it is abnormal, i.e., does not conform to expected behavior. Furthermore, reducing the number of anomalies using early detection is a major issue in many applications. In this context, anomaly-cause discovery may be helpful in recommending actions that may prevent risk. In this paper, we present a novel approach for detecting the risk of anomalies occurring in the environment regarding user activities. The method relies on anomaly-cause extraction from a given dataset using causal association rules mining. These anomaly causes are utilized afterward for real-time analysis to detect the risk of anomalies using the Markov logic network machine learning method. The detected risk allows the method to recommend suitable actions to perform in order to avoid the occurrence of an actual anomaly. The proposed approach is implemented, tested, and evaluated for each contribution using real data obtained from an intelligent environment platform and real data from a clinical datasets. Experimental results prove our approach to be efficient in terms of recognition rate. © 2018 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2018.06.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049319216&doi=10.1016%2fj.artmed.2018.06.001&partnerID=40&md5=fa0cc2353b9719fbac3695613670b08e,2018,2021-07-20 15:49:17,2021-07-20 15:49:17
RWSZRW6S,journalArticle,2017,"Kardas, K.; Cicekli, N.K.",SVAS: Surveillance Video Analysis System,"This paper introduces a Surveillance Video Analysis System, called SVAS, for surveillance domain, in which the semantic rules and the definition of event models can be learned or defined by the user for automatic detection and inference of complex video events. In the scope of SVAS, an event model method named Interval-Based Spatio-Temporal Model (IBSTM) is proposed. SVAS can learn action models and event models without any predefined threshold values and generates understandable and manageable IBSTM event models. Hybrid machine learning methods are proposed and used. A set of feature models named Threshold Model, which reflects the spatio-temporal motion analysis of an event, is kept as the first model. As the second model, Bag of Actions (BoA) model is used in order to reduce the search space in the detection phase. Markov Logic Network (MLN) model, which provides understandable and manageable logic predicates for users, is kept as the third model. SVAS has high performance event detection capability due to its interval-based hierarchical manner. It determines related candidate intervals for each main model of IBSTM and uses the related main model when needed rather than using all models as a whole. The main contribution of this study is to fill the semantic gap between humans and video computer systems such that, on the one hand it decreases human intervention through its learning capabilities, but on the other hand it also enables human intervention when necessary through its manageable event model method. The study achieves all of them in the most efficient way through its machine learning methods. The proposed system is applied to different event datasets from CAVIAR, BEHAVE and our synthetic datasets. The experimental results show that our approach improves the event recognition performance and precision as compared to the current state-of-the-art approaches. © 2017 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2017.07.051,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026764328&doi=10.1016%2fj.eswa.2017.07.051&partnerID=40&md5=939512d863628d224f8ae8bb5757bff5,2017,2021-07-20 15:49:17,2021-07-20 15:49:17
9VUI3ZWE,journalArticle,2020,"Csiszár, O.; Csiszár, G.; Dombi, J.",Interpretable neural networks based on continuous-valued logic and multicriteria decision operators,"Combining neural networks with continuous logic and multicriteria decision-making tools can reduce the black-box nature of neural models. In this study, we show that nilpotent logical systems offer an appropriate mathematical framework for hybridization of continuous nilpotent logic and neural models, helping to improve the interpretability and safety of machine learning. In our concept, perceptrons model soft inequalities; namely membership functions and continuous logical operators. We design the network architecture before training, using continuous logical operators and multicriteria decision tools with given weights working in the hidden layers. Designing the structure appropriately leads to a drastic reduction in the number of parameters to be learned. The theoretical basis offers a straightforward choice of activation functions (the cutting function or its differentiable approximation, the squashing function), and also suggests an explanation to the great success of the rectified linear unit (ReLU). In this study, we focus on the architecture of a hybrid model and introduce the building blocks for future applications in deep neural networks. © 2020 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2020.105972,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083895664&doi=10.1016%2fj.knosys.2020.105972&partnerID=40&md5=788ad14e7d99045f096f71934fb40166,2020,2021-07-20 15:49:17,2021-07-20 15:49:17
KKLQLDQG,journalArticle,2017,"Drole, M.; Kononenko, I.",Pairwise saturations in inductive logic programming,"One of the main issues when using inductive logic programming (ILP) in practice remain the long running times that are needed by ILP systems to induce the hypothesis. We explore the possibility of reducing the induction running times of systems that use asymmetric relative minimal generalisation (ARMG) by analysing the bottom clauses of examples that serve as inputs into the generalisation operator. Using the fact that the ARMG covers all of the examples and that it is a subset of the variabilization of one of the examples, we identify literals that cannot appear in the ARMG and remove them prior to computing the generalisation. We apply this procedure to the ProGolem ILP system and test its performance on several real world data sets. The experimental results show an average speedup of 36% compared to the base ProGolem system and 12% compared to ProGolem extended with caching, both without a decrease in the accuracy of the produced hypotheses. We also observe that the gain from using the proposed method varies greatly, depending on the structure of the data set. © 2016, Springer Science+Business Media Dordrecht.",Artificial Intelligence Review,10.1007/s10462-016-9487-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975125141&doi=10.1007%2fs10462-016-9487-5&partnerID=40&md5=64192cb7ad3bf6ecac1292fcab5e2cd3,2017,2021-07-20 15:49:17,2021-07-20 15:49:17
QVDA2MWU,journalArticle,2014,"Ferilli, S.",WoMan: Logic-based workflow learning and management,"Workflow management is fundamental to efficiently, effectively, and economically carry out complex working and domestic activities. Manual engineering of workflow models is a complex, costly, and error-prone task. The WoMan framework for workflow management is based on first-order logic. Its core is an automatic procedure that learns and refines workflow models from observed cases of process execution. Its innovative peculiarities include incrementality (allowing quick learning even in the presence of noise and changed behavior), strict adherence to the observed practices, ability to learn complex conditions for the workflow components, and improved expressive power compared to the state of the art. This paper presents the entire algorithmic apparatus of WoMan, including translation and learning from a standard log format for case representation, import/export of workflow models from/into standard formalisms (Petri nets), and exploitation of the learned models for process simulation and monitoring. Qualitative and quantitative experimental evaluation shows the power and efficiency of WoMan, both in controlled and in real-world domains. © 2013 IEEE.","IEEE Transactions on Systems, Man, and Cybernetics: Systems",10.1109/TSMC.2013.2273310,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901280175&doi=10.1109%2fTSMC.2013.2273310&partnerID=40&md5=a4edc82243924a40eb0b200ef14c998d,2014,2021-07-20 15:49:17,2021-07-20 15:49:17
2RBV2J46,journalArticle,2019,"Jeridi, M.H.; Khalaifi, H.; Bouatay, A.; Ezzedine, T.",Targets Classification Based on Multi-sensor Data Fusion and Supervised Learning for Surveillance Application,"In surveillance application scenarios, like border security and area monitoring, potential targets to be detected may be either an unarmed person, a soldier carrying ferrous weapon or a vehicle. Detection is the first phase of a monitoring process, followed by the target classification phase and finally their tracking if required. This work focuses on classification step, where we introduce our classification approach not too resource-intensive, easy to implement and suitable for large scale environment. For that, we used probabilistic reasoning techniques to address multi sensing data correlation and take advantage of multi-sensor data fusion, then, based on adopted fusion architecture, we implemented our trained classification model in a fusion node, to make the classification more accurate. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Wireless Personal Communications,10.1007/s11277-018-6114-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058794854&doi=10.1007%2fs11277-018-6114-6&partnerID=40&md5=9668f6a5077c101469e71727f0a6a86c,2019,2021-07-20 15:49:17,2021-07-20 15:49:17
DIXVTU9Z,journalArticle,2018,"Ploennigs, J.; Ba, A.; Barry, M.",Materializing the promises of cognitive IoT: How cognitive buildings are shaping the way,"Relatively tiny examples have demonstrated the potential of cognitive IoT (CIoT) in its full-stack, namely, semantic modeling, learning and reasoning over sensors data, and machine learning, to uncover and expose actionable insights via advanced user interfaces. In this paper, we make the case for the feasibility of CIoT in all of its dimensions. We devise a CIoT architecture that integrates thousands of sensors present in our buildings in order to learn the buildings’ behavior and intuitively assist users in diagnosing and mitigating undesired events. With our architecture, we place emphasis on the scalability and flexibility that reduce the configuration effort. The solution shows the potential of CIoT to create highly scalable, adaptable and interactive IoT systems functioning for buildings and capable of addressing the challenges encountered in the realm of homes, Smart Cities and Industry 4.0. © 2017 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2017.2755376,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030636141&doi=10.1109%2fJIOT.2017.2755376&partnerID=40&md5=802e1fe672a6aeafa9e397abb0834e4b,2018,2021-07-20 15:49:17,2021-07-20 15:49:17
BSPHEXCD,journalArticle,2012,"Szwabe, A.; Misiorek, P.; Walkowiak, P.",Tensor-based relational learning for ontology matching,"In this paper we propose the Tensor-based Reective Relational Learning System (TRRLS) as a first tensor-based approach to decision support in the area of ontology alignment. The system may be seen as realizing a probabilistic inference with regard to the relation representing the 'semantic equivalence' of ontology classes or their properties. Despite the fact that TRRLS is based on the new idea of algebraic modeling of multi-relational data, it provides similar results to the best approaches of the Ontology Alignment Evaluation Initiative (OAEI) competitors to the task of matching concepts of Adult Mouse Anatomy ontology and NCI Thesaurus ontology on the basis of partially known expert matches. © 2012 The authors and IOS Press. All rights reserved.",Frontiers in Artificial Intelligence and Applications,10.3233/978-1-61499-105-2-509,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873449067&doi=10.3233%2f978-1-61499-105-2-509&partnerID=40&md5=5eefe2f6f879533e44a4d02b9493ba3c,2012,2021-07-20 15:49:17,2021-07-20 15:49:17
CIDIIPGP,journalArticle,2021,"Pustokhina, I.V.; Pustokhin, D.A.; Kumar Pareek, P.; Gupta, D.; Khanna, A.; Shankar, K.",Energy-efficient cluster-based unmanned aerial vehicle networks with deep learning-based scene classification model,"In present days, unmanned aerial vehicles (UAVs) have gained significant interest among researchers and academicians. The UAVs were found useful in diverse application areas, namely, intelligent transportation system, disaster management, surveillance, and wildlife monitoring. Clustering is a well-known energy-efficient technique, which elects a cluster head (CH) among other nodes. At the same time, scene classification from the high-resolution remote sensing images captured by UAV is also a major issue in the UAV networks. In order to resolve these problems, this paper projects novel energy-efficient cluster-based UAV networks with deep learning (DL)-based scene classification method. The proposed model involves a clustering with parameter tuned residual network (C-PTRN) model, which operates on two major phases such as cluster construction and scene classification. Initially, the UAVs are clustered using the type II fuzzy logic (T2FL) technique on the basis of residual energy, distance to nearby UAVs, and UAV degree. Next, the chosen CHs transmit the captured images to the base station (BS). At the second level, a DL-based ResNet50 technique is employed for scene classification. To tune the hyperparameters of the ResNet50 model, water wave optimization (WWO) algorithm is used. At last, kernel extreme learning machine (KELM) model is used to perform the scene classification process. In order to ensure the performance of the proposed method, a detailed set of simulations takes place under different dimensions. The obtained results ensured that the C-PTRN model has showcased supreme outcome with the maximum precision of 95.89%, recall of 98.91%, and F score of 96.54%. © 2021 John Wiley & Sons Ltd.",International Journal of Communication Systems,10.1002/dac.4786,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102779552&doi=10.1002%2fdac.4786&partnerID=40&md5=09c225893d3e88a0de1fe2a06c40570f,2021,2021-07-20 15:49:18,2021-07-20 15:49:18
R7GRYRYP,journalArticle,2020,"Fu, X.; Yu, F.R.; Wang, J.; Qi, Q.; Liao, J.",Dynamic Service Function Chain Embedding for NFV-Enabled IoT: A Deep Reinforcement Learning Approach,"The Internet of things (IoT) is becoming more and more flexible and economical with the advancement in information and communication technologies. However, IoT networks will be ultra-dense with the explosive growth of IoT devices. Network function virtualization (NFV) emerges to provide flexible network frameworks and efficient resource management for the performance of IoT networks. In NFV-enabled IoT infrastructure, service function chain (SFC) is an ordered combination of virtual network functions (VNFs) that are related to each other based on the logic of IoT applications. However, the embedding process of SFC to IoT networks is becoming a big challenge due to the dynamic nature of IoT networks and the abundance of IoT terminals. In this paper, we decompose the complex VNFs into smaller virtual network function components (VNFCs) to make more effective decisions since VNF nodes and IoT network devices are usually heterogeneous. In addition, a deep reinforcement learning (DRL) based scheme with experience replay and target network is proposed as a solution that can efficiently handle complex and dynamic SFC embedding scenarios in IoT. Our simulations consider different types of IoT network topologies. The simulation results present the efficiency of the proposed dynamic SFC embedding scheme. © 2002-2012 IEEE.",IEEE Transactions on Wireless Communications,10.1109/TWC.2019.2946797,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078348149&doi=10.1109%2fTWC.2019.2946797&partnerID=40&md5=c7d477a167e62fc1f1881a5b0eb54b52,2020,2021-07-20 15:49:18,2021-07-20 15:49:18
K2KI8WVH,journalArticle,2020,"Glatt, R.; Da Silva, F.L.; da Costa Bianchi, R.A.; Costa, A.H.R.",DECAF: Deep Case-based Policy Inference for knowledge transfer in Reinforcement Learning,"Having the ability to solve increasingly complex problems using Reinforcement Learning (RL) has prompted researchers to start developing a greater interest in systematic approaches to retain and reuse knowledge over a variety of tasks. With Case-based Reasoning (CBR) there exists a general methodology that provides a framework for knowledge transfer which has been underrepresented in the RL literature so far. We formulate a terminology for the CBR framework targeted towards RL researchers with the goal of facilitating communication between the respective research communities. Based on this framework, we propose the Deep Case-based Policy Inference (DECAF) algorithm to accelerate learning by building a library of cases and reusing them if they are similar to a new task when training a new policy. DECAF guides the training by dynamically selecting and blending policies according to their usefulness for the current target task, reusing previously learned policies for a more effective exploration but still enabling the adaptation to particularities of the new task. We show an empirical evaluation in the Atari game playing domain depicting the benefits of our algorithm with regards to sample efficiency, robustness against negative transfer, and performance increase when compared to state-of-the-art methods. © 2020 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2020.113420,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083423326&doi=10.1016%2fj.eswa.2020.113420&partnerID=40&md5=ea3baaa7d8733ce088732933c1332aaa,2020,2021-07-20 15:49:18,2021-07-20 15:49:18
PYM7X9RY,journalArticle,2019,"Xing, F.Z.; Cambria, E.; Zhang, Y.",Sentiment-aware volatility forecasting,"Recent advances in the integration of deep recurrent neural networks and statistical inferences have paved new avenues for joint modeling of moments of random variables, which is highly useful for signal processing, time series analysis, and financial forecasting. However, introducing explicit knowledge as exogenous variables has received little attention. In this paper, we propose a novel model termed sentiment-aware volatility forecasting (SAVING), which incorporates market sentiment for stock return fluctuation prediction. Our framework provides an ensemble of symbolic and sub-symbolic AI approaches, that is, including grounded knowledge into a connectionist neural network. The model aims at producing a more accurate estimation of temporal variances of asset returns by better capturing the bi-directional interaction between movements of asset price and market sentiment. The interaction is modeled using Variational Bayes via the data generation and inference operations. We benchmark our model with 9 other popular ones in terms of the likelihood of forecasts given the observed sequence. Experimental results suggest that our model not only outperforms pure statistical models, e.g., GARCH and its variants, Gaussian-process volatility model, but also outperforms the state-of-the-art autoregressive deep neural nets architectures, such as the variational recurrent neural network and the neural stochastic volatility model. © 2019 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2019.03.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063733190&doi=10.1016%2fj.knosys.2019.03.029&partnerID=40&md5=76fd451e919da1400fbff4fcdb82e3cb,2019,2021-07-20 15:49:18,2021-07-20 15:49:18
3C8EP7K7,journalArticle,2014,"Hodge, V.J.; Krishnan, R.; Austin, J.; Polak, J.; Jackson, T.",Short-term prediction of traffic flow using a binary neural network,"This paper introduces a binary neural network-based prediction algorithm incorporating both spatial and temporal characteristics into the prediction process. The algorithm is used to predict short-term traffic flow by combining information from multiple traffic sensors (spatial lag) and time series prediction (temporal lag). It extends previously developed Advanced Uncertain Reasoning Architecture (AURA) k-nearest neighbour (k-NN) techniques. Our task was to produce a fast and accurate traffic flow predictor. The AURA k-NN predictor is comparable to other machine learning techniques with respect to recall accuracy but is able to train and predict rapidly. We incorporated consistency evaluations to determine whether the AURA k-NN has an ideal algorithmic configuration or an ideal data configuration or whether the settings needed to be varied for each data set. The results agree with previous research in that settings must be bespoke for each data set. This configuration process requires rapid and scalable learning to allow the predictor to be set-up for new data. The fast processing abilities of the AURA k-NN ensure this combinatorial optimisation will be computationally feasible for real-world applications. We intend to use the predictor to proactively manage traffic by predicting traffic volumes to anticipate traffic network problems. © 2014, The Natural Computing Applications Forum.",Neural Computing and Applications,10.1007/s00521-014-1646-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920249613&doi=10.1007%2fs00521-014-1646-5&partnerID=40&md5=029fc67e796402f013fb8269d3a2e81b,2014,2021-07-20 15:49:18,2021-07-20 15:49:18
IGP9Z8ZP,journalArticle,2013,"Khan, A.; Doucette, J.A.; Cohen, R.",Validation of an ontological medical decision support system for patient treatment using a repository of patient data: Insights into the value of machine learning,"In this article, we begin by presenting OMeD, a medical decision support system, and argue for its value over purely probabilistic approaches that reason about patients for time-critical decision scenarios. We then progress to present Holmes, a Hybrid Ontological and Learning MEdical System which supports decision making about patient treatment. This system is introduced in order to cope with the case of missing data. We demonstrate its effectiveness by operating on an extensive set of real-world patient health data from the CDC, applied to the decision-making scenario of administering sleeping pills. In particular, we clarify how the combination of semantic, ontological representations, and probabilistic reasoning together enable the proposal of effective patient treatments. Our focus is thus on presenting an approach for interpreting medical data in the context of real-time decision making. This constitutes a comprehensive framework for the design of medical recommendation systems for potential use by medical professionals and patients both, with the end result being personalized patient treatment. We conclude with a discussion of the value of our particular approach for such diverse considerations as coping with misinformation provided by patients, performing effectively in time-critical environments where real-time decisions are necessary, and potential applications facilitating patient information gathering. © 2013 ACM.",ACM Transactions on Intelligent Systems and Technology,10.1145/2508037.2508049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84885623290&doi=10.1145%2f2508037.2508049&partnerID=40&md5=1b9cf208dde32df37fe8622e66f32359,2013,2021-07-20 15:49:18,2021-07-20 15:49:18
QYWG8INU,journalArticle,2019,"Zhang, W.; Zhang, Y.; Xu, L.; Zhou, J.; Liu, Y.; Guis, M.; Liu, X.; Yang, S.",Modeling IoT Equipment with Graph Neural Networks,"Traditional neural networks usually concentrate on temporal data in system simulation, and lack of capabilities to reason inner logic relations between different dimensions of data collected from embedded sensors. This paper proposes a graph neural network-based modeling approach for IoT equipment (called GNNM-IoT), which considers both temporal and inner logic relations of data, in which vertices denote sensor data and edges denote relationships between vertices. The GNNM-IoT model's relationships between sensors with neural networks to produce nonlinear complex relationships. We have evaluated the GNNM-IoT using air-conditioner data from a world leading IoT company, which demonstrates that it is effective and outperforms ARIMA and LSTM methods. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2902865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063586110&doi=10.1109%2fACCESS.2019.2902865&partnerID=40&md5=af52442b01a0d9ecf5c7217f8b941acd,2019,2021-07-20 15:49:18,2021-07-20 15:49:18
4MLWKT3P,journalArticle,2019,"Long, Z.; Lu, Y.; Dong, B.",PDE-Net 2.0: Learning PDEs from data with a numeric-symbolic hybrid deep network,"Partial differential equations (PDEs) are commonly derived based on empirical observations. However, recent advances of technology enable us to collect and store massive amount of data, which offers new opportunities for data-driven discovery of PDEs. In this paper, we propose a new deep neural network, called PDE-Net 2.0, to discover (time-dependent) PDEs from observed dynamic data with minor prior knowledge on the underlying mechanism that drives the dynamics. The design of PDE-Net 2.0 is based on our earlier work [1] where the original version of PDE-Net was proposed. PDE-Net 2.0 is a combination of numerical approximation of differential operators by convolutions and a symbolic multi-layer neural network for model recovery. Comparing with existing approaches, PDE-Net 2.0 has the most flexibility and expressive power by learning both differential operators and the nonlinear response function of the underlying PDE model. Numerical experiments show that the PDE-Net 2.0 has the potential to uncover the hidden PDE of the observed dynamics, and predict the dynamical behavior for a relatively long time, even in a noisy environment. © 2019 Elsevier Inc.",Journal of Computational Physics,10.1016/j.jcp.2019.108925,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072691481&doi=10.1016%2fj.jcp.2019.108925&partnerID=40&md5=107070c8aeff3fb79a75c50c84acc172,2019,2021-07-20 15:49:18,2021-07-20 15:49:18
7DXG8V9R,journalArticle,2014,"Celebi, M.E.; Zornberg, A.",Automated quantification of clinically significant colors in dermoscopy images and its application to skin lesion classification,"Dermoscopy is a noninvasive skin imaging technique, which permits visualization of features of pigmented melanocytic neoplasms that are not discernable by examination with the naked eye. Color information is indispensable for the clinical diagnosis malignant melanoma, the most deadly form of skin cancer. For this reason, most of the currently accepted dermoscopic scoring systems either directly or indirectly incorporate color as a diagnostic criterion. For example, both the asymmetry, border, colors, and dermoscopic (ABCD) rule of dermoscopy and the more recent color, architecture, symmetry, and homogeneity (CASH) algorithm include the number of clinically significant colors in their calculation of malignancy scores. In this paper, we present a machine learning approach to the automated quantification of clinically significant colors in dermoscopy images. Given a true-color dermoscopy image with N colors, we first reduce the number of colors in this image to a small number K, i.e., K N, using the K-means clustering algorithm incorporating a spatial term. The optimal K value for the image is estimated separately using five commonly used cluster validity criteria. We then train a symbolic regression algorithm using the estimates given by these criteria, which are calculated on a set of 617 images. Finally, the mathematical equation given by the regression algorithm is used for two-class (benign versus malignant) classification. The proposed approach yields a sensitivity of 62% and a specificity of 76% on an independent test set of 297 images. © 2014 IEEE.",IEEE Systems Journal,10.1109/JSYST.2014.2313671,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907599302&doi=10.1109%2fJSYST.2014.2313671&partnerID=40&md5=c25db3df6f987144fd353940d81e9897,2014,2021-07-20 15:49:18,2021-07-20 15:49:18
EJGV4D23,journalArticle,2014,"Willem, L.; Stijven, S.; Vladislavleva, E.; Broeckhove, J.; Beutels, P.; Hens, N.",Active Learning to Understand Infectious Disease Models and Improve Policy Making,"Modeling plays a major role in policy making, especially for infectious disease interventions but such models can be complex and computationally intensive. A more systematic exploration is needed to gain a thorough systems understanding. We present an active learning approach based on machine learning techniques as iterative surrogate modeling and model-guided experimentation to systematically analyze both common and edge manifestations of complex model runs. Symbolic regression is used for nonlinear response surface modeling with automatic feature selection. First, we illustrate our approach using an individual-based model for influenza vaccination. After optimizing the parameter space, we observe an inverse relationship between vaccination coverage and cumulative attack rate reinforced by herd immunity. Second, we demonstrate the use of surrogate modeling techniques on input-response data from a deterministic dynamic model, which was designed to explore the cost-effectiveness of varicella-zoster virus vaccination. We use symbolic regression to handle high dimensionality and correlated inputs and to identify the most influential variables. Provided insight is used to focus research, reduce dimensionality and decrease decision uncertainty. We conclude that active learning is needed to fully understand complex systems behavior. Surrogate models can be readily explored at no computational expense, and can also be used as emulator to improve rapid policy making in various settings. © 2014 Willem et al.",PLoS Computational Biology,10.1371/journal.pcbi.1003563,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901335905&doi=10.1371%2fjournal.pcbi.1003563&partnerID=40&md5=00e6914cf51b1e5c16677335f4dd9a10,2014,2021-07-20 15:49:18,2021-07-20 15:49:18
2TWEX8A6,journalArticle,2020,"Khan, N.; Arya, K.V.",A new fuzzy rule based pixel organization scheme for optimal edge detection and impulse noise removal,"Fuzzy sets provide a framework for incorporating human knowledge as an efficient unsupervised machine learning tool for problem solving. The approach discussed in this paper introduces a generalized transfer learning scheme using rule based fuzzy logic for edge detection in digital images. The spatial domain statistical properties of the image are explored as training data set and expressed in fuzzy format to obtain a decision function for optimal edge detection along with reduction of impulse noise. During fuzzy inference process, a specific linguistic value in input fuzzy set is selected in order to obtain an optimal range of second order difference which discriminates the edge pixels from the non-edge pixels. The proposed fuzzy rule based optimal edge pixel detection method in the presence of random valued impulse noise tends to sufficiently extract the edge pixels with out boosting the noisy pixels. The effectiveness of the proposed fuzzy rule based edge detection scheme is verified by testing it on various standard test images and comparing with existing edge detection techniques at different noise densities. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-020-08707-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079647070&doi=10.1007%2fs11042-020-08707-x&partnerID=40&md5=3a24e4059224fcd1695081a8d91517c4,2020,2021-07-20 15:49:18,2021-07-20 15:49:18
SAYJMQTT,journalArticle,2019,"Li, X.; Serlin, Z.; Yang, G.; Belta, C.",A formal methods approach to interpretable reinforcement learning for robotic planning,"Growing interest in reinforcement learning approaches to robotic planning and control raises concerns of predictability and safety of robot behaviors realized solely through learned control policies. In addition, formally defining reward functions for complex tasks is challenging, and faulty rewards are prone to exploitation by the learning agent. Here, we propose a formal methods approach to reinforcement learning that (i) provides a formal specification language that integrates high-level, rich, task specifications with a priori, domain-specific knowledge; (ii) makes the reward generation process easily interpretable; (iii) guides the policy generation process according to the specification; and (iv) guarantees the satisfaction of the (critical) safety component of the specification. The main ingredients of our computational framework are a predicate temporal logic specifically tailored for robotic tasks and an automaton-guided, safe reinforcement learning algorithm based on control barrier functions. Although the proposed framework is quite general, we motivate it and illustrate it experimentally for a robotic cooking task, in which two manipulators worked together to make hot dogs. Copyright © 2019 The Authors, some rights reserved; exclusive licensee American Association for the Advancement of Science. No claim to original U.S. Government Works",Science Robotics,10.1126/scirobotics.aay6276,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077808398&doi=10.1126%2fscirobotics.aay6276&partnerID=40&md5=06bbe34ebd6f6066e54f657f8610dcac,2019,2021-07-20 15:49:18,2021-07-20 15:49:18
72K6I3AK,journalArticle,2019,"Wang, H.; He, K.",Improving test and diagnosis efficiency through ensemble reduction and learning,"Machine learning is a powerful lever for developing, improving, and optimizing test methodologies to cope with the demand from the advanced nodes. Ensemble methods are a particular learning paradigm that uses multiple models to boost performance. In this work, ensemble reduction and learning is explored for integrated circuit test and diagnosis. For testing, the proposed method is able to reduce the number of system-level tests without incurring substantial increase in defect escapes or yield losses. Significant cost from test execution and set-up preparation can thereby be saved. Experiments are performed on two designs of commercially fabricated chips, for an overall population of >264,000 chips. The results demonstrate that our method is able to reduce 29.27% and 21.74% of the number of tests for the two chips, respectively, at the cost of very low defect escapes. For failure diagnosis, the framework is able to predict an adequate amount of test data necessary for accurate failure diagnosis. Experiments performed on five standard benchmarks demonstrate that our method outperforms a state-of-the-art work in terms of data-volume reduction. The proposed ensemble-based methodology creates opportunities for improving test and diagnosis efficiency. © 2019 Association for Computing Machinery.",ACM Transactions on Design Automation of Electronic Systems,10.1145/3328754,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067199200&doi=10.1145%2f3328754&partnerID=40&md5=5a12540fbe8e5fc6e13820fd27c8d2ab,2019,2021-07-20 15:49:18,2021-07-20 15:49:18
8VBYRUUJ,journalArticle,2019,"Tang, W.; Ding, Z.; Zhou, M.",A Spammer Identification Method for Class Imbalanced Weibo Datasets,"Nowadays, Weibo has become a significant and popular information sharing platform in China. Meanwhile, spammer identification has been a big challenge for it. To mitigate the damage caused by spammers, classification algorithms from machine learning have been applied to distinguish spammers and non-spammers. However, most of the previous studies overlook the class imbalance problem of real-world data. In this paper, by analyzing the characteristics of spammers in Weibo, we select microblog content similarity, the average number of links, and the other 12 features to construct a comprehensive feature vector never seen before. Considering the existence of imbalance problems in spammer identification, an ensemble learning method is used to combine multiple base classifiers for improving the learning performance. During the training stage of base learners, fuzzy-logic-based oversampling and cost-sensitive support vector machine are considered to tackle imbalanced data at both data and algorithmic levels. The experimental results demonstrate that compared with the existing state-of-the-art methods, the recall rate of our proposed approach increases by 6.5% and reaches the precision value of 87.53% when used to deal with real-world Weibo datasets we collected. © 2019 IEEE.",IEEE Access,10.1109/ACCESS.2019.2901756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064705420&doi=10.1109%2fACCESS.2019.2901756&partnerID=40&md5=e569aab48775c8a962631526d169f656,2019,2021-07-20 15:49:18,2021-07-20 15:49:18
4EBT8YAQ,journalArticle,2020,"Zhu, H.; Xiao, M.; Yang, L.; Tang, X.; Liang, Y.; Li, J.",A minimum centre distance rule activation method for extended belief rule-based classification systems,"Originating from the belief-rule-based (BRB) system, the extended belief rule-based (EBRB) system combined the advantages of the rule-based method and those of data-driven methods. By transforming the data set into extended belief rules and using evidential reasoning (ER), the EBRB system has expanded the application of BRB systems and demonstrated their capability in addressing classification problems. Nevertheless, the problem of activating nearly the entire rule base in every classification process is embedded in the EBRB scheme. There have been advances in rule activation for the EBRB system; however, the introduction of subjective information into the classification, high computational costs and long response times are common problems facing existing rule activation methods. To solve the problems facing rule activation for EBRB systems, a minimum centre distance rule activation (MCDRA) method for EBRB systems is proposed. In MCDRA, no subjective information is required, and no time-consuming iteration procedure is necessary. Two components of the proposed MCDRA, i.e., the filtering procedure and the selection procedure, are designed to eliminate unrelated samples of input query data and to select and activate the highly related samples to the input query data. A total of 12 benchmark data sets are used to test the performance of EBRB with MCDRA (M-EBRB). The experimental results show that compared with other rule activation methods, the proposed method obtains satisfactory rule activation ratios, accuracies and response times. Additionally, M-EBRB performs well on noisy data and comparatively with both the fuzzy-rule-based classification system (FRBCS) and several machine learning classification algorithms. In addition, MCDRA can be utilized as a generic rule activation method and can be used to optimize other rule-based classification systems. © 2020 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2020.106214,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081134887&doi=10.1016%2fj.asoc.2020.106214&partnerID=40&md5=e7247820c6c192343182ddb9e00c796e,2020,2021-07-20 15:49:19,2021-07-20 15:49:19
SPU9STDS,journalArticle,2020,"Franch, D.K.; Chaves, D.P.B.; Pimentel, C.; Hamilton, D.M.",Markov modeling of dynamical systems via clustering and graph minimization,"Discrete dynamical systems are widely used in a variety of scientific and engineering applications. Modeling these systems involves performing statistical analysis of the system output to estimate the parameters of a model so it can behave statistically similar to the original system. These models can be used for simulation, performance analysis, fault detection, among other applications. This work presents a new algorithm to model discrete dynamical systems using probabilistic finite state automata (PFSA) by initially finding a special class of PFSA called D-Markov machines (a D-ary Markov process) and then applying machine learning algorithms and graph minimization techniques to obtain compact and precise PFSA models. Modeling examples are provided to discuss the trade-off between accuracy and the number of PFSA states. Results using simulated datasets show that the proposed FPSA construction achieves good sensitivity and precision for anomaly detection. © 2020 Elsevier Inc.",Digital Signal Processing: A Review Journal,10.1016/j.dsp.2020.102769,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086499042&doi=10.1016%2fj.dsp.2020.102769&partnerID=40&md5=8e362bb48ef61cceef0fb96bed004d2e,2020,2021-07-20 15:49:19,2021-07-20 15:49:19
I8IZECX6,journalArticle,2017,"Veloso de Melo, V.; Banzhaf, W.",Improving the prediction of material properties of concrete using Kaizen Programming with Simulated Annealing,"Predicting the properties of materials like concrete has been proven a difficult task given the complex interactions among its components. Over the years, researchers have used Statistics, Machine Learning, and Evolutionary Computation to build models in an attempt to accurately predict such properties. High-quality models are often non-linear, justifying the study of nonlinear regression tools. In this paper, we employ a traditional multiple linear regression method by ordinary least squares to solve the task. However, the model is built upon nonlinear features automatically engineered by Kaizen Programming, a recently proposed hybrid method. Experimental results show that Kaizen Programming can find low-correlated features in an acceptable computational time. Such features build high-quality models with better predictive quality than results reported in the literature. © 2017 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2016.12.077,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014148577&doi=10.1016%2fj.neucom.2016.12.077&partnerID=40&md5=55166f2f73059fd3b5ec2b70fb383448,2017,2021-07-20 15:49:19,2021-07-20 15:49:19
QDA52ZBD,journalArticle,2019,"Liu, H.; Chen, S.-M.",Multi-stage mixed rule learning approach for advancing performance of rule-based classification,"Rule learning is a special type of machine learning approaches, and its key advantage is the generation of interpretable models, which provides a transparent process of showing how an input is mapped to an output. Traditional rule learning algorithms are typically based on Boolean logic for inducing rule antecedents, which are very effective for training models on data sets that involve discrete attributes only. When continuous attributes are present in a data set, traditional rule learning approaches need to employ crisp intervals. However, in reality, problems usually show shades of grey, which motivated the development of fuzzy rule learning approaches by employing fuzzy intervals for handling continuous attributes. While a data set contains a large portion of discrete attributes or even no continuous attributes, fuzzy approaches cannot be used to learn rules effectively, leading to a drop in the performance. In this paper, a multi-stage approach of mixed rule learning is proposed, which involves strategic combination of both traditional and fuzzy approaches to handle effectively various types of attributes. We compare our proposed approach with existing algorithms of rule learning. Our experimental results show that our proposed approach leads to significant advances in the performance compared with the existing algorithms. © 2019 Elsevier Inc.",Information Sciences,10.1016/j.ins.2019.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065209026&doi=10.1016%2fj.ins.2019.05.008&partnerID=40&md5=d0fc7102d8dfb955fdd43cd808b4a698,2019,2021-07-20 15:49:19,2021-07-20 15:49:19
L7SY5CHQ,journalArticle,2018,"Li, W.; Logenthiran, T.; Phan, V.-T.; Woo, W.L.",Implemented IoT-based self-learning home management system (SHMS) for Singapore,"Internet of Things makes deployment of smart home concept easy and real. Smart home concept ensures residents to control, monitor, and manage their energy consumption without any wastage. This paper presents a self-learning home management system. In the proposed system, a home energy management system, demand side management system, and supply side management system were developed and integrated for real time operation of a smart home. This integrated system has some capabilities such as price forecasting, price clustering, and power alert system to enhance its functions. These enhancing capabilities were developed and implemented using computational and machine learning technologies. In order to validate the proposed system, real-time power consumption data was collected from a Singapore smart home and a realistic experimental case study was carried out. The case study has shown that the developed system has performed well and created energy awareness to the residents. This proposed system also displays its ability to customize the model for different types of environments compared to traditional smart home models. © 2014 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2018.2828144,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045735953&doi=10.1109%2fJIOT.2018.2828144&partnerID=40&md5=352a867231ed5740c9779380e52838c0,2018,2021-07-20 15:49:19,2021-07-20 15:49:19
XVPC5VNR,journalArticle,2020,"Soda, P.; Sicilia, R.; Acciai, L.; Iannello, G.",Grasping inter-attribute and temporal variability in multivariate time series,"The rising capabilities of storing and registering data has increased the number of temporal datasets, boosting the attention on time series classification and forecasting. In case of multivariate time series, symbolic methods that try to predict phenomena transform the data into a more compact format to produce a representation of the time series easy to be handled in a machine learning framework. However, up to now these representations do not grasp information on both inter-attribute variability and temporal variability. In this work we present an approach that, taking into account the relationships between attributes and their periodicity, reduces the multivariate time series to a collection of symbols, whose distribution is represented by histograms. The approach has been successfully tested on a publicly available dataset, the Telecom Italia Big Data Challenge 2014 dataset, reporting also the results attained by other methods available in the literature. © 2019 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",IEEE Transactions on Big Data,10.1109/TBDATA.2019.2918807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103657858&doi=10.1109%2fTBDATA.2019.2918807&partnerID=40&md5=1feb122be9cb523cb8e04d4ba934568e,2020,2021-07-20 15:49:19,2021-07-20 15:49:19
4TEYGEW3,journalArticle,2020,"Lalouani, W.; Younis, M.",Multi-observable reputation scoring system for flagging suspicious user sessions,"Conventionally, network and cloud infrastructure security is handled by firewalls which monitor traffic and block malicious access by matching certain observables, e.g., IP, and DNS, to blacklisted entries in intelligence databases. Therefore, such an approach fails to deal with emerging threats that utilize unclassified observables, and to report suspicious activities of individual users. In this paper we propose MuSeR, a novel approach to assign reputation scores for observables, even when no prior information is available, and flag suspicious sessions by conducting inter-observable analysis of user requests. In essence, MuSeR opts to assist network and cloud administrators mitigate attacks while avoiding unwarranted blocking of benign access. MuSeR achieves such an objective by associating session reputation scores based on the trustworthiness of the user navigation pattern, and conducting dynamic analysis of individual observables involved within requests. Specifically, MuSeR employs a new machine learning model for classifying observables using features specifically chosen to factor in evidence provided by blacklists, and access patterns of known attacks. To determine a request score, MuSeR maps the classifier probabilities to adaptive subjective logic and then uses multinomial fusion to leverage evidence from the different observables. Given the request scores, MuSeR further promotes a novel session reputation scoring model that uses three-valued subjective logic to handle trust propagation and aggregation over user requests. The effectiveness ofMuSeR is validated using a large dataset obtained from popular databases such as WHOIS, CYMUS, and passive DNS databases. © 2020",Computer Networks,10.1016/j.comnet.2020.107474,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089747214&doi=10.1016%2fj.comnet.2020.107474&partnerID=40&md5=8ceaf98d14dea582562d551b24f7d826,2020,2021-07-20 15:49:19,2021-07-20 15:49:19
FM84GNCJ,journalArticle,2020,"Lam, T.L.",Low-cost Non-contact PCBs Temperature Monitoring and Control in a Hot Air Reflow Process based on Multiple Thermocouples Data Fusion,"As the environmental concern is being raised over residues of lead, the trend of transferring from the conventional lead-based soldering to lead-free soldering is overwhelming. Lead-free solders require the peak temperature to be about 30 degrees Celsius higher than lead-based solders, which induce a narrower margin between the highest melting temperatures of lead-free solders and the heat-resistant temperatures of electronic components. As a result, the accuracy of temperature control of reflow systems needs to meet a higher standard to maintain the solder quality. Whereas, the conventional control process of the onboard temperature is open-loop, which cannot achieve the required accuracy. A closed-loop method by using an array of thermal image cameras for temperature monitoring is too expensive. In order to provide a low-cost and accurate temperature control solution for reflow systems, a cost-effective non-contact temperature approximation and control system is proposed in this paper. The proposed temperature approximation is achieved based on a machine learning method with multiple-input single-output strategies to get a relationship between the temperatures near the PCBs and the onboard temperature. The proposed system controls the temperature in a real-time fuzzy logic algorithm to achieve a more accurate control result. The experiment results reveal the feasibility of the proposed temperature approximation and control system. CCBYNCND",IEEE Access,10.1109/ACCESS.2020.3036527,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097923350&doi=10.1109%2fACCESS.2020.3036527&partnerID=40&md5=dea9a1c0645903509567f2c2e4e62a9e,2020,2021-07-20 15:49:19,2021-07-20 15:49:19
HMEIIUQS,journalArticle,2017,"Khammassi, C.; Krichen, S.",A GA-LR wrapper approach for feature selection in network intrusion detection,"Intrusions constitute one of the main issues in computer network security. Through malicious actions, hackers can have unauthorised access that compromises the integrity, the confidentiality, and the availability of resources or services. Intrusion detection systems (IDSs) have been developed to monitor and filter network activities by identifying attacks and alerting network administrators. Different IDS approaches have emerged using data mining, machine learning, statistical analysis, and artificial intelligence techniques such as genetic algorithms, artificial neural networks, fuzzy logic, swarm intelligence, etc. Due to the high dimensionality of the exchanged data, applying those techniques will be extremely time consuming. Feature selection is needed to select the optimal subset of features that represents the entire dataset to increase the accuracy and the classification performance of the IDS. In this work, we apply a wrapper approach based on a genetic algorithm as a search strategy and logistic regression as a learning algorithm for network intrusion detection systems to select the best subset of features. The experiment will be conducted on the KDD99 dataset and the UNSW-NB15 dataset. Three different decision tree classifiers are used to measure the performance of the selected subsets of features. The obtained results are compared with other feature selection approaches to verify the efficiency of our proposed approach. © 2017 Elsevier Ltd",Computers and Security,10.1016/j.cose.2017.06.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021106059&doi=10.1016%2fj.cose.2017.06.005&partnerID=40&md5=b6fd1bb689560f04290680f53ad16803,2017,2021-07-20 15:49:19,2021-07-20 15:49:19
8WI3IY7W,journalArticle,2012,"Coffi, J.-R.; Marsala, C.; Museux, N.",Adaptive complex event processing for harmful situation detection,"In the field of infrastructures' surveillance and protection, it is important to make decisions based on activities occurring in the environment and its local context and conditions. In this paper we use an active rule based event processing architecture in order to make sense of situations from the combination of different signals received by the rule engine. However obtaining some high level information automatically is not without risks, especially in sensitive environments, and detection mistakes can happen for various reasons: the signal's source can be defective, whether it is human-miss-interpretation of the signal-or computed-material malfunction; the aggregation rules can be wrong syntaxically, for example when a rule will never be triggered or a situation never detected; the interpretation given to the combination of signals does not correspond to the reality on the field-because the knowledge of the rule designer is subjective or because the environment evolves over-time-the rules are therefore incorrect semantically. In this paper, a new approach is proposed to avoid the third kind of error sources. We present a hybrid machine learning technique adapted to the complexity of the rules' representation, in order to create a system more conform to reality. The proposed approach uses a combination of an Association Rule Mining algorithm and Inductive Logic Programming for rule induction. Empirical studies on simulated datasets demonstrate how our method can contribute to sensible systems such as the security of a public or semi-public place. © 2012 Springer-Verlag.",Evolving Systems,10.1007/s12530-012-9052-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865758037&doi=10.1007%2fs12530-012-9052-7&partnerID=40&md5=55433851d46f8055617f7ed8a1714f94,2012,2021-07-20 15:49:19,2021-07-20 15:49:19
7KXVGAIK,journalArticle,2021,"Ayed, S.B.; Elouedi, Z.; Lefevre, E.",CIMMEP: constrained integrated method for CBR maintenance based on evidential policies,"The quality of the proposed solutions by Case-Based Reasoning (CBR) systems is highly dependent on recorded experiences and their describing attributes. Hence, to keep them offering accurate and efficient responses for a long time frame, the maintenance of Case Bases (CB) and Vocabulary knowledge is required. However, maintenance operations are usually unable to exploit provided domain-experts knowledge although this kind of systems are widely applied in several real-life contexts. This offered prior knowledge is handled, in our work, in form of pairwise constraints: Regarding cases, Must-Link (ML) affirms that two given problems should have the same solution, and Cannot-Link (CL) informs that two problems cannot have the same solution. These constraints may also regard vocabulary knowledge in such a way that ML is generated when prior knowledge affirm that two given features offer correlated values, therefore, similar information, and CL is built when they provide different information. This paper proposes a new constrained & integrated method, named CIMMEP, encoding Constrained & Integrated Maintaining Method based on Evidential Policies, for maintaining both vocabulary and CB through eliminating redundancy and noisiness. Since CBR systems handle real-world experiences, which are full of uncertainty, CIMMEP manages this imperfection using a powerful tool called the belief function theory. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Applied Intelligence,10.1007/s10489-020-02159-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100629592&doi=10.1007%2fs10489-020-02159-4&partnerID=40&md5=10619f419121d42e2d827fa364da4e3c,2021,2021-07-20 15:49:19,2021-07-20 15:49:19
CHIBCJTD,journalArticle,2020,"Ben Ayed, S.; Elouedi, Z.; Lefevre, E.",An evidential integrated method for maintaining case base and vocabulary containers within CBR systems,"Cases and vocabulary maintenance presents a crucial task to preserve high competent Case-Based Reasoning (CBR) systems, since the accuracy of their offered solutions are strongly dependent on stored cases and their describing attributes quality. The maintenance aims generally at eliminating two types of undesirable knowledge which are noisy and redundant data. However, inexpedient Case Base Maintenance (CBM) or vocabulary maintenance may not only greatly decrease CBR competence in solving new problems, but also reduce its performance in term of retrieval time. Besides, to provide a high maintenance quality, it is necessary to manage uncertainty within knowledge since “real-world data are never perfect” and stored cases within a CBR system's Case Base (CB) describe real-world experiences. Hence, we propose, in this paper, a new integrated method that maintains both of the CB and the vocabulary knowledge containers of CBR systems by offering a new alternating technique to properly detect noisiness and redundancy whether in cases or features. During the learning steps of our new integrated maintenance policy, which drives the decision making about cases and attributes selection, we manage uncertainty using one among the most powerful tools called the Belief Function Theory. © 2019 Elsevier Inc.",Information Sciences,10.1016/j.ins.2019.11.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076055659&doi=10.1016%2fj.ins.2019.11.009&partnerID=40&md5=5779290350b1d807529130507a98f63a,2020,2021-07-20 15:49:19,2021-07-20 15:49:19
GQ4IILGA,journalArticle,2018,"Yang, Y.; Xu, D.-L.; Yang, J.-B.; Chen, Y.-W.",An evidential reasoning-based decision support system for handling customer complaints in mobile telecommunications,"Handling customer complaints is a decision-making process that inherently involves a classification problem where each complaint should be classified exclusively to one of the complaint categories before a resolution is communicated to customers. Previous studies focus extensively on decision support systems (DSSs) to automate complaint handling, while few addresses the issue of classification imprecision when inaccurate or inconsistent information exists in customer complaint narratives. This research presents a novel DSS for handling customer complaints and develops an evidential reasoning (ER) rule-based classifier as the core component of the system to classify customer complaints with uncertain information. More specifically, textual and numeric features are firstly combined to generate evidence for formulating the relationship between customer complaint features and classification results. The ER rule is then applied to combine multiple pieces of evidence and classify customer complaints into different categories with probabilities. An empirical study is conducted in a telecommunication company. Results show that the proposed ER rule-based classification model provides high performance in comparison with other machine learning algorithms. The developed system offers telecommunication companies an informative and data-driven method for handling customer complaints in a systematic and automatic manner. © 2018 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2018.09.029,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054088729&doi=10.1016%2fj.knosys.2018.09.029&partnerID=40&md5=89e4ff12455c3a700237efe0bf8cee8b,2018,2021-07-20 15:49:19,2021-07-20 15:49:19
S8Z4DGTN,journalArticle,2021,"Fuchs, C.; Nobile, M.S.; Zamora, G.; Degeneffe, A.; Kubben, P.; Kaymak, U.",Tremor assessment using smartphone sensor data and fuzzy reasoning,"Background: Tremor severity assessment is an important step for the diagnosis and treatment decision-making of essential tremor (ET) patients. Traditionally, tremor severity is assessed by using questionnaires (e.g., ETRS and QUEST surveys). In this work we assume the possibility of assessing tremor severity using sensor data and computerized analyses. The goal of this work is to assess severity of tremor objectively, to be better able to asses improvement in ET patients due to deep brain stimulation or other treatments. Methods: We collect tremor data by strapping smartphones to the wrists of ET patients. The resulting raw sensor data is then pre-processed to remove any artifact due to patient’s intentional movement. Finally, this data is exploited to automatically build a transparent, interpretable, and succinct fuzzy model for the severity assessment of ET. For this purpose, we exploit pyFUME, a tool for the data-driven estimation of fuzzy models. It leverages the FST-PSO swarm intelligence meta-heuristic to identify optimal clusters in data, reducing the possibility of a premature convergence in local minima which would result in a sub-optimal model. pyFUME was also combined with GRABS, a novel methodology for the automatic simplification of fuzzy rules. Results: Our model is able to assess tremor severity of patients suffering from Essential Tremor, notably without the need for subjective questionnaires nor interviews. The fuzzy model improves the mean absolute error (MAE) metric by 78–81% compared to linear models and by 71–74% compared to a model based on decision trees. Conclusion: This study confirms that tremor data gathered using the smartphones is useful for the constructing of machine learning models that can be used to support the diagnosis and monitoring of patients who suffer from Essential Tremor. The model produced by our methodology is easy to inspect and, notably, characterized by a lower error with respect to approaches based on linear models or decision trees. © 2021, The Author(s).",BMC Bioinformatics,10.1186/s12859-021-03961-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104869763&doi=10.1186%2fs12859-021-03961-8&partnerID=40&md5=93ba770c76ce871448f1a300d0c4ab72,2021,2021-07-20 15:49:20,2021-07-20 15:49:20
63UCY4MH,journalArticle,2020,"Boccia, M.; Sforza, A.; Sterle, C.",Simple pattern minimality problems: Integer linear programming formulations and covering-based heuristic solving approaches,"The simple pattern minimality problem (SPMP) represents a central problem in the logical analysis of data and association rules mining, and it finds applications in several fields as logic synthesis, reliability analysis, and automated reasoning. It consists of determining the minimum number of patterns explaining all the observations of a data set, that is, a Boolean logic formula that is true for all the elements of the data set and false for all the unseen observations. We refer to this problem as covering SPMP (C-SPMP), because each observation can be explained (covered) by more than one pattern. Starting from a real industrial application, we also define a new version of the problem, and we refer to it as partitioning SPMP (P-SPMP), because each observation has to be covered just once. Given a propositional formula or a truth table, C-SPMP and P-SPMP coincide exactly with the problem of determining the minimum disjunctive and minimum exclusive disjunctive normal form, respectively. Both problems are known to be NP-hard and have been generally tackled by heuristic methods. In this context, the contribution of this work is twofold. On one side, it provides two original integer linear programming formulations for the two variants of the SPMP. These formulations exploit the concept of Boolean hypercube to build a graph representation of the problems and allow to exactly solve instances with more than 1,000 observations by using an MIP solver. On the other side, two effective and fast heuristics are proposed to solve relevant size instances taken from literature (SeattleSNPs) and from the industrial database. The proposed methods do not suffer from the same dimensional drawbacks of the methods present in the literature and outperform either existing commercial and freeware logic tools or the available industrial solutions in the number of generated patterns and/or in the computational burden. Copyright: © 2020 INFORMS",INFORMS Journal on Computing,10.1287/ijoc.2019.0940,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087746164&doi=10.1287%2fijoc.2019.0940&partnerID=40&md5=b01ca231f2fa20d5f839cb489513c598,2020,2021-07-20 15:49:20,2021-07-20 15:49:20
D84YKIEW,journalArticle,2021,"Okudan, O.; Budayan, C.; Dikmen, I.",A knowledge-based risk management tool for construction projects using case-based reasoning,"Construction projects are often deemed as complex and high-risk endeavours, mostly because of their vulnerability to external conditions as well as project-related uncertainties. Risk management (RM) is a critical success factor for companies operating in the construction industry. RM is a knowledge-intensive process that requires effective management of risk-related knowledge. Although some research has already been conducted to develop tools to support knowledge-based RM processes, most of these tools ignore some critical features, such as live knowledge capture, web-based platform for knowledge sharing and effective case retrieval for learning from past projects. Moreover, several RM phases, such as risk identification, analysis, response and monitoring are not usually integrated. Thus, this study aims to bridge these gaps by developing a knowledge-based RM tool (namely, CBRisk) via case-based reasoning (CBR). CBRisk has been developed as a web-based tool that supports the cyclic RM process and utilises an effective case retrieval method considering a comprehensive list of project similarity features in the form of fuzzy linguistic variables. Finally, the developed tool was evaluated and validated by conducting black-box testing and expert review meeting. Results demonstrated that CBRisk has a considerable potential to enhance the effectiveness of RM in construction projects and may be used in other project-based industries with minimal modifications. © 2021 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2021.114776,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101822304&doi=10.1016%2fj.eswa.2021.114776&partnerID=40&md5=36ae006d0999d18fe33e8b046a162f61,2021,2021-07-20 15:49:20,2021-07-20 15:49:20
STHTFNSV,journalArticle,2020,"Bettini, C.; Civitarese, G.; Presotto, R.",CAVIAR: Context-driven Active and Incremental Activity Recognition,"Activity recognition on mobile device sensor data has been an active research area in mobile and pervasive computing for several years. While the majority of the proposed techniques are based on supervised learning, semi-supervised approaches are being considered to reduce the size of the training set required to initialize the model. These approaches usually apply self-training or active learning to incrementally refine the model, but their effectiveness seems to be limited to a restricted set of physical activities. We claim that the context which surrounds the user (e.g., time, location, proximity to transportation routes) combined with common knowledge about the relationship between context and human activities could be effective in significantly increasing the set of recognized activities including those that are difficult to discriminate only considering inertial sensors, and the highly context-dependent ones. In this paper, we propose CAVIAR, a novel hybrid semi-supervised and knowledge-based system for real-time activity recognition. Our method applies semantic reasoning on context-data to refine the predictions of an incremental classifier. The recognition model is continuously updated using active learning. Results on a real dataset obtained from 26 subjects show the effectiveness of our approach in increasing the recognition rate, extending the number of recognizable activities and, most importantly, reducing the number of queries triggered by active learning. In order to evaluate the impact of context reasoning, we also compare CAVIAR with a purely statistical version, considering features computed on context-data as part of the machine learning process. © 2020 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2020.105816,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082534286&doi=10.1016%2fj.knosys.2020.105816&partnerID=40&md5=ea85a98755d05968e90c397953170d7f,2020,2021-07-20 15:49:20,2021-07-20 15:49:20
J2LCPFWH,journalArticle,2013,"Dechter, R.",Reasoning with probabilistic and deterministic graphical models: Exact Algorithms,"Graphical models (e.g., Bayesian and constraint networks, influence diagrams, and Markov decision processes) have become a central paradigm for knowledge representation and reasoning in both artificial intelligence and computer science in general. These models are used to perform many reasoning tasks, such as scheduling, planning and learning, diagnosis and prediction, design, hardware and software verification, and bioinformatics. These problems can be stated as the formal tasks of constraint satisfaction and satisfiability, combinatorial optimization, and probabilistic inference. It is well known that the tasks are computationally hard, but research during the past three decades has yielded a variety of principles and techniques that significantly advanced the state of the art. In this book we provide comprehensive coverage of the primary exact algorithms for reasoning with such models. The main feature exploited by the algorithms is the model's graph. We present inference-based, message-passing schemes (e.g., variable-elimination) and search-based, conditioning schemes (e.g., cycle-cutset conditioning and AND/OR search). Each class possesses distinguished characteristics and in particular has different time vs. space behavior. We emphasize the dependence of both schemes on few graph parameters such as the treewidth, cycle-cutset, and (the pseudo-tree) height. We believe the principles outlined here would serve well in moving forward to approximation and anytime-based schemes. The target audience of this book is researchers and students in the artificial intelligence and machine learning area, and beyond. © 2013 by Morgan and Claypool.",Synthesis Lectures on Artificial Intelligence and Machine Learning,10.2200/S00529ED1V01Y201308AIM023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891808231&doi=10.2200%2fS00529ED1V01Y201308AIM023&partnerID=40&md5=5e94fd63d8f56e4427d8544e608dd024,2013,2021-07-20 15:49:20,2021-07-20 15:49:20
KKVQQ3KI,journalArticle,2012,"Mørk, S.; Holmes, I.",Evaluating bacterial gene-finding hmm structures as probabilistic logic programs,"Motivation: Probabilistic logic programming offers a powerful way to describe and evaluate structured statistical models. To investigate the practicality of probabilistic logic programming for structure learning in bioinformatics, we undertook a simplified bacterial gene-finding benchmark in PRISM, a probabilistic dialect of Prolog.Results: We evaluate Hidden Markov Model structures for bacterial protein-coding gene potential, including a simple null model structure, three structures based on existing bacterial gene finders and two novel model structures. We test standard versions as well as ADPH length modeling and three-state versions of the five model structures. The models are all represented as probabilistic logic programs and evaluated using the PRISM machine learning system in terms of statistical information criteria and gene-finding prediction accuracy, in two bacterial genomes. Neither of our implementations of the two currently most used model structures are best performing in terms of statistical information criteria or prediction performances, suggesting that better-fitting models might be achievable. © The Author 2012. Published by Oxford University Press. All rights reserved.",Bioinformatics,10.1093/bioinformatics/btr698,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857869765&doi=10.1093%2fbioinformatics%2fbtr698&partnerID=40&md5=308168f2a09772b907719c8c2448ef4e,2012,2021-07-20 15:49:20,2021-07-20 15:49:20
RCU8TBWV,journalArticle,2015,"Jungmann, A.; Mohr, F.",An approach towards adaptive service composition in markets of composed services,"On-the-fly composition of service-based software solutions is still a challenging task. Even more challenges emerge when facing automatic service composition in markets of composed services for end users. In this paper, we focus on the functional discrepancy between “what a user wants” specified in terms of a request and “what a user gets” when executing a composed service. To meet the challenge of functional discrepancy, we propose the combination of existing symbolic composition approaches with machine learning techniques. We developed a learning recommendation system that expands the capabilities of existing composition algorithms to facilitate adaptivity and consequently reduces functional discrepancy. As a representative of symbolic techniques, an Artificial Intelligence planning based approach produces solutions that are correct with respect to formal specifications. Our learning recommendation system supports the symbolic approach in decision-making. Reinforcement Learning techniques enable the recommendation system to adjust its recommendation strategy over time based on user ratings. We implemented the proposed functionality in terms of a prototypical composition framework. Preliminary results from experiments conducted in the image processing domain illustrate the benefit of combining both complementary techniques. © 2015, Jungmann and Mohr; licensee Springer.",Journal of Internet Services and Applications,10.1186/s13174-015-0022-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938836676&doi=10.1186%2fs13174-015-0022-8&partnerID=40&md5=0b348b595890f8caee8033aa08b1524a,2015,2021-07-20 15:49:20,2021-07-20 15:49:20
WQPZWUPW,journalArticle,2017,"Gedeon, T.",Bio-inspired computing tools and applications: position paper,"The confluence of significant computational power and inexpensive sensors provides the opportunity to reliably collect large volumes of information from the world and extract humanly useful information resources. This paper reviews a coherent body of work over the last 20+ years focused on development of advanced bio-inspired computing techniques, and their applications primarily for human related data in behaviour and human centered computing. We close with a synthesis proposing an experiment analysis methodology combining these tools. © 2017, Bharati Vidyapeeth's Institute of Computer Applications and Management.",International Journal of Information Technology (Singapore),10.1007/s41870-017-0006-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091883538&doi=10.1007%2fs41870-017-0006-y&partnerID=40&md5=89339a80b67671c98d25e2785ce6eb5d,2017,2021-07-20 15:49:20,2021-07-20 15:49:20
HZ6UUSVK,journalArticle,2020,"Das, S.; Mukherjee, H.; Obaidullah, S.M.; Roy, K.; Saha, C.K.",Ensemble based technique for the assessment of fetal health using cardiotocograph – a case study with standard feature reduction techniques,"Intrauterine fetal hypoxia is one of the leading cause of perinatal mortality and morbidity. This can eventually lead to severe neurological damage like cerebral palsy and in extreme cases to fetal demise. It is thus necessary to monitor the fetus during intrapartum and antepartum period. Cardiotocograph (CTG) as a method of assessing the status of the fetus had been in use for last six decades. Nowadays it is the most widely used non-invasive technique for the continuous monitoring of the fetal heart rate (FHR) and the uterine contraction pressure (UCP). Though its introduction limited the birth related problems, the accuracy of interpretation was hindered by quite a few factors. Different guidelines that are provided for the interpretation are based on crisp logic which fails to capture the inherent uncertainty present in the medical diagnosis. Misinterpretations had led to inaccurate diagnosis which resulted in many medico-legal litigations. The vagueness present in the physician’s evaluation is best modeled using soft-computing based techniques. In this paper authors used the CTG dataset from UCI Irvine Machine Learning Data Repository which contains 2126 data and each data-point is represented by 37 features. Dimensionality of the feature set was reduced using different automated methods as well as manually by the physicians. The resulting data sets were classified using various machine learning algorithms. Aim of this study is to establish which set of features is best suited to give good insight into the status of the fetus and also determine the most effective machine learning technique for this purpose. The accuracy of the outcomes were measured using statistical methods such as sensitivity, specificity, precision, F-Measure, confusion matrix and kappa value. We obtained an accuracy of 99.91% and kappa measure of 0.997 when the feature set was reduced using MRMR. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-020-08853-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083717979&doi=10.1007%2fs11042-020-08853-2&partnerID=40&md5=4a9528ad6061fac54839e45fe3f8a5ed,2020,2021-07-20 15:49:20,2021-07-20 15:49:20
25BNTAG7,journalArticle,2015,"Racoceanu, D.; Capron, F.",Towards semantic-driven high-content image analysis: An operational instantiation for mitosis detection in digital histopathology,"This study concerns a novel symbolic cognitive vision framework emerged from the Cognitive Microscopy (MICO. 11MICO - a French National Research Agency, Technologies for health and autonomy (ANR TecSan) project: https://www.comp.nus.edu.sg/ danielr/projects.htm.) initiative. MICO aims at supporting the evolution towards digital pathology, by studying cognitive clinical-compliant protocols involving routine virtual microscopy. We instantiate this paradigm in the case of mitotic count as a component of breast cancer grading in histopathology. The key concept of our approach is the role of the semantics as driver of the whole slide image analysis protocol. All the decisions being taken into a semantic and formal world, MICO represents a knowledge-driven platform for digital histopathology. Therefore, the core of this initiative is the knowledge representation and the reasoning. Pathologists' knowledge and strategies are used to efficiently guide image analysis algorithms. In this sense, hard-coded knowledge, semantic and usability gaps are to be reduced by a leading, active role of reasoning and of semantic approaches. Integrating ontologies and reasoning in confluence with modular imaging algorithms, allows the emergence of new clinical-compliant protocols for digital pathology. This represents a promising way to solve decision reproducibility and traceability issues in digital histopathology, while increasing the flexibility of the platform and pathologists' acceptance, the one always having the legal responsibility in the diagnosis process. The proposed protocols open the way to increasingly reliable cancer assessment (i.e. multiple slides per sample analysis), quantifiable and traceable second opinion for cancer grading, and modern capabilities for cancer research support in histopathology (i.e. content and context-based indexing and retrieval). Last, but not least, the generic approach introduced here is applicable for number of additional challenges, related to molecular imaging and, in general, to high-content image exploration. © 2014 Elsevier Ltd.",Computerized Medical Imaging and Graphics,10.1016/j.compmedimag.2014.09.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926410392&doi=10.1016%2fj.compmedimag.2014.09.004&partnerID=40&md5=53c2fc5b8edf8255679acbaba391882f,2015,2021-07-20 15:49:20,2021-07-20 15:49:20
XN6ZFIKC,journalArticle,2015,"Ye, F.; Zhang, Z.; Chakrabarty, K.; Gu, X.","Information-theoretic syndrome evaluation, statistical root-cause analysis, and correlation-based feature selection for guiding board-level fault diagnosis","Reasoning-based functional-fault diagnosis has recently been advocated to achieve high diagnosis accuracy, low defect escapes, and reducing manufacturing cost. However, such diagnosis method requires a rich set of test items (syndromes) and a sizable database of faulty boards to learn from. An insufficient number of failed boards, ambiguous root-cause identification, and redundant or irrelevant syndromes can render reasoning-based diagnosis ineffective. Periodic evaluation and analysis can help locate weaknesses in a diagnosis system and thereby provide guidelines for redesigning the tests, which facilitates better diagnosis. We propose an information-theoretic framework for evaluating the effectiveness of and providing guidance to a reasoning-based functional-fault diagnosis system. Syndrome analysis based on feature selection methods provides a representative set of syndromes and suggests irrelevant syndromes in diagnosis. Root-cause analysis measures the discriminative ability of differentiating a given root cause from others. Results are presented for four types of diagnosis systems for three complex boards that are in volume production. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2015.2399438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930505340&doi=10.1109%2fTCAD.2015.2399438&partnerID=40&md5=192c7f70f130896633652405d9daf392,2015,2021-07-20 15:49:20,2021-07-20 15:49:20
4N4ETL7N,journalArticle,2019,"Qazi, N.; Wong, B.L.W.",An interactive human centered data science approach towards crime pattern analysis,"The traditional machine learning systems lack a pathway for a human to integrate their domain knowledge into the underlying machine learning algorithms. The utilization of such systems, for domains where decisions can have serious consequences (e.g. medical decision-making and crime analysis), requires the incorporation of human experts' domain knowledge. The challenge, however, is how to effectively incorporate domain expert knowledge with machine learning algorithms to develop effective models for better decision making. In crime analysis, the key challenge is to identify plausible linkages in unstructured crime reports for the hypothesis formulation. Crime analysts painstakingly perform time-consuming searches of many different structured and unstructured databases to collate these associations without any proper visualization. To tackle these challenges and aiming towards facilitating the crime analysis, in this paper, we examine unstructured crime reports through text mining to extract plausible associations. Specifically, we present associative questioning based searching model to elicit multi-level associations among crime entities. We coupled this model with partition clustering to develop an interactive, human-assisted knowledge discovery and data mining scheme. The proposed human-centered knowledge discovery and data mining scheme for crime text mining is able to extract plausible associations between crimes, identifying crime pattern, grouping similar crimes, eliciting co-offender network and suspect list based on spatial-temporal and behavioral similarity. These similarities are quantified through calculating Cosine, Jacquard, and Euclidean distances. Additionally, each suspect is also ranked by a similarity score in the plausible suspect list. These associations are then visualized through creating a two-dimensional re-configurable crime cluster space along with a bipartite knowledge graph. This proposed scheme also inspects the grand challenge of integrating effective human interaction with the machine learning algorithms through a visualization feedback loop. It allows the analyst to feed his/her domain knowledge including choosing of similarity functions for identifying associations, dynamic feature selection for interactive clustering of crimes and assigning weights to each component of the crime pattern to rank suspects for an unsolved crime. We demonstrate the proposed scheme through a case study using the Anonymized burglary dataset. The scheme is found to facilitate human reasoning and analytic discourse for intelligence analysis. © 2019 Elsevier Ltd",Information Processing and Management,10.1016/j.ipm.2019.102066,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068542037&doi=10.1016%2fj.ipm.2019.102066&partnerID=40&md5=0fd278b65330d57901c2cafac2d6a79e,2019,2021-07-20 15:49:21,2021-07-20 15:49:21
QJ2SS2BK,journalArticle,2021,"Kolomvatsos, K.; Anagnostopoulos, C.","Proactive, uncertainty-driven queries management at the edge","Research community has already revealed the challenges of data processing when performed at the Cloud that may affect the performance of any desired application. The main challenge is the increased latency observed when the data should ‘travel’ to the Cloud from the location they are collected and the waiting time for getting the final response. In an Internet of Things (IoT) scenario, this time could be critical for supporting real time applications. A solution to the discussed problem is the adoption of an Edge Computing (EC) approach where data can be processed close to their collection point. IoT devices could report data to a number of edge nodes that behave as distributed data repositories having the capability of processing them and producing analytics. Analytics should match the requirements of queries defined by end users or applications with the collected data and the characteristics of every edge node. However, when a query is defined, we should identify the appropriate edge node(s) to process it. In this paper, we propose an uncertainty management model to efficiently allocate every incoming query to the available edge nodes. Our scheme adopts the principles of the Fuzzy Logic (FL) theory and provides a decision making mechanism for the entity having the responsibility of the envisioned allocations. We combine the proposed uncertainty management scheme with a machine learning model based on a Support Vector Machine (SVM) to enhance the FL reasoning. Our aim is to manage all the hidden aspects of the problem combining two different technologies with different orientations. We also propose a methodology for the automated generation of the Footprint of Uncertainty (FoU) of membership functions involved in our interval Type-2 FL model. Our experimental evaluation aims at revealing the pros and cons of our mechanism presenting the results of extensive simulations adopting datasets found in the literature and a comparative analysis with other efforts in the domain. © 2021",Future Generation Computer Systems,10.1016/j.future.2020.12.028,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099225073&doi=10.1016%2fj.future.2020.12.028&partnerID=40&md5=7dbb2f56bbd9b5169045c9f63b554b54,2021,2021-07-20 15:49:21,2021-07-20 15:49:21
KTEHK4MZ,journalArticle,2021,"Wang, Q.; Hao, Y.; Chen, F.",Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture,"Inferring missing links in Knowledge Graphs (KGs) is a key evaluation task for KG reasoning, which aims to find relations for a given entity pair. Existing research often employs the IDA* (Iterative Deepening A*) algorithm for the path discovery task owing to its efficiency and accuracy. However, it relies on heuristics to set cost functions and is also difficult to utilize useful context information in the search process. In this paper, we propose the Deep-IDA* framework which applies neural networks and reinforcement learning (RL) to empower the IDA* algorithm to tackle the path discovery problem in KG reasoning. We model KG reasoning as a Markov Decision Process (MDP) and divide our Deep-IDA* framework and the resulting path into two parts: path-finding and path-reasoning. For path-finding, we propose a policy network to model the cost from the source to a candidate location. In this process, we employ the GCN (Graph Convolutional Network) to embed the observable sub-track, then employ the LSTM (Long Short-Term Memory) to record the historical trajectory, and introduce the attention to utilize the context information, and finally form policy. For path-reasoning with the searched candidate paths passed from the former process, we employ a value network to estimate the cost from the candidate to the destination entity, using the GNN (Graph Neural Networks) to learn a message-passing algorithm that solves the path inference problem, and using the GRU (Gated Recurrent Unit) to update the historical information. Finally, the actor-learner algorithm is utilized to minimize the sum of the losses of the two parts. Experiment results on three datasets demonstrate the effectiveness and efficiency of our framework. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.12.040,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098717859&doi=10.1016%2fj.neucom.2020.12.040&partnerID=40&md5=1664fc555b7f44fd3fadab33ad9243f8,2021,2021-07-20 15:49:21,2021-07-20 15:49:21
WURW9YIW,journalArticle,2011,"Magdalinos, P.; Kousaridas, A.; Spapis, P.; Katsikas, G.; Alonistioti, N.",Enhancing a fuzzy logic inference engine through machine learning for a self- Managed network,"Existing network management systems have static and predefined rules or parameters, while human intervention is usually required for their update. However, an autonomic network management system that operates in a volatile network environment should be able to adapt continuously its decision making mechanism through learning from the system's behavior. In this paper, a novel learning scheme based on the network wide collected experience is proposed targeting the enhancement of network elements' decision making engine. The algorithm employs a fuzzy logic inference engine in order to enable self-managed network elements to identify faults or optimization opportunities. The fuzzy logic engine is periodically updated through the use of two well known data mining techniques, namely k-Means and k-Nearest Neighbor. The proposed algorithm is evaluated in the context of a load identification problem. The acquired results prove that the proposed learning mechanism improves the deduction capability, thus promoting our algorithm as an attractive approach for enhancing the autonomic capabilities of network elements. © 2011 Springer Science+Business Media, LLC.",Mobile Networks and Applications,10.1007/s11036-011-0327-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052630217&doi=10.1007%2fs11036-011-0327-1&partnerID=40&md5=77b0faa4d6851537307b5dd22572020a,2011,2021-07-20 15:49:21,2021-07-20 15:49:21
RDSQSBN7,journalArticle,2021,"Zhou, M.; Ji, D.; Li, F.",Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text,"Relation extraction from dialogue text is an innovative task in natural language processing. In addition to the general characteristics of general relation extraction from news or scientific publication text, the task is of certain special features. For example, the context in dialogues frequently switches between speakers, and there exist rich pronoun anaphora in the dialogue text. Thus, it is important for the model to be aware of such features to improve the performance. Taking these factors together, we propose an end to-end neural model for dialogue-based relation extraction, which includes four modules to handle the problems existing in the task from different aspects: (1) the word-relation attention to model a natural intuition that different words contribute differently for the identification of different relations; (2) the graph reasoning to consider the global context information in the dialogue that contains many inter-sentence relations; (3) the speaker embeddings to incorporate speaker information into our model; (4) the speaker coreference to associate pronouns with speakers and enrich the information of graph reasoning. Our model was evaluated on a recently-proposed dataset for dialogue-based relation extraction, and achieved the state of the-art performance. We show that our proposed modules are effective through ablation studies. Our work can be a competitive benchmark for the study of dialogue based relation extraction. IEEE",IEEE/ACM Transactions on Audio Speech and Language Processing,10.1109/TASLP.2021.3082295,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107200162&doi=10.1109%2fTASLP.2021.3082295&partnerID=40&md5=95e3b167fd71613270903576b8ac1557,2021,2021-07-20 15:49:21,2021-07-20 15:49:21
WATJLW3E,journalArticle,2020,"Chen, C.; Zhang, M.; Zhang, Y.; Liu, Y.; Ma, S.",Efficient neural matrix factorization without sampling for recommendation,"Recommendation systems play a vital role to keep users engaged with personalized contents in modern online platforms. Recently, deep learning has revolutionized many research fields and there is a surge of interest in applying it for recommendation. However, existing studies have largely focused on exploring complex deeplearning architectures for recommendation task, while typically applying the negative sampling strategy for model learning. Despite effectiveness, we argue that these methods suffer from two important limitations: (1) the methods with complex network structures have a substantial number of parameters, and require expensive computations even with a sampling-based learning strategy; (2) the negative sampling strategy is not robust,making sampling-based methods difficult to achieve the optimal performance in practical applications. In this work, we propose to learn neural recommendation models from the whole training data without sampling. However, such a non-sampling strategy poses strong challenges to learning efficiency. To address this, we derive three new optimization methods through rigorous mathematical reasoning, which can efficiently learn model parameters from the whole data (including all missing data) with a rather low time complexity. Moreover, based on a simple Neural Matrix Factorization architecture, we present a general framework named ENMF, short for Efficient Neural Matrix Factorization. Extensive experiments on three real-world public datasets indicate that the proposed ENMF framework consistently and significantly outperforms the state-of-the-art methods on the Top-K recommendation task. Remarkably, ENMF also shows significant advantages in training efficiency, which makes it more applicable to real-world large-scale systems. © 2020 Association for Computing Machinery.",ACM Transactions on Information Systems,10.1145/3373807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078275737&doi=10.1145%2f3373807&partnerID=40&md5=0df13e9078923af43637b385aa56025a,2020,2021-07-20 15:49:21,2021-07-20 15:49:21
ZWSKQQLU,journalArticle,2021,"Li, S.; Wang, H.; Pan, R.; Mao, M.",MemoryPath: A deep reinforcement learning framework for incorporating memory component into knowledge graph reasoning,"Knowledge Graph (KG) is identified as a major area in artificial intelligence, which is used for many real-world applications. The task of knowledge graph reasoning has been widely used and proven to be effective, which aims to find these reasonable paths for various relations to solve the issue of incompleteness in KGs. However, many previous works on KG reasoning, such as path-based or reinforcement learning-based methods, are too reliant on the pre-training, where the paths from the head entity and the target entity must be given to pre-train the model, which would easily lead the model to overfit on the given paths seen in the pre-training. To address this issue, we propose a novel reasoning model named MemoryPath with a deep reinforcement learning framework, which incorporates Long Short Term Memory (LSTM) and graph attention mechanism to form the memory component. The well-designed memory component can get rid of the pre-training so that the model doesn't depend on the given target entity for training. A tailored mechanism of reinforcement learning is presented in this proposed deep reinforcement framework to optimize the training procedure, where two metrics, Mean Selection Rate (MSR) and Mean Alternative Rate (MAR), are defined to quantitatively measure the complexities of the query relations. Meanwhile, three different training mechanisms, Action Dropout, Reward Shaping and Force Forward, are proposed to optimize the training process of the proposed MemoryPath. The proposed MemoryPath is validated on two datasets from FB15K-237 and NELL-995 on different tasks including fact prediction, link prediction and success rate in finding paths. The experimental results demonstrate that the tailored mechanism of reinforcement learning make the MemoryPath achieves state-of-the-art performance comparing with the other models. Also, the qualitative analysis indicates that the MemoryPath can store the learning process and automatically find the promising paths for a reasoning task during the training, and shows the effectiveness of the memory component. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.08.032,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091334260&doi=10.1016%2fj.neucom.2020.08.032&partnerID=40&md5=376fee3b8d1c2366c8f51c0bf660bd3e,2021,2021-07-20 15:49:21,2021-07-20 15:49:21
CFA6YMU5,journalArticle,2019,"Xing, W.; Popov, V.; Zhu, G.; Horwitz, P.; McIntyre, C.",The effects of transformative and non-transformative discourse on individual performance in collaborative-inquiry learning,"The effectiveness of computer-supported collaborative inquiry learning in STEM education is well-documented in the literature. At the same time, research indicates that some students struggle to articulate relevant concepts, to make their reasoning explicit, and to regulate their learning—all of which are necessary for effective collaboration. In this study, 106 college students completed tasks related to Ohm's Law in a simulation-based, collaborative-inquiry learning environment. Using qualitative analysis, multilevel modelling, and data-mining techniques, we investigated the relationship between student engagement in transformative and non-transformative learning processes and learning outcomes. The results revealed that by using the appropriate feature engineering and algorithms, we could build accurate machine-learning models that could automatically identify transformative and non-transformative discussions on a large scale. Additional qualitative and quantitative analyses indicated that when groups engaged in additional interpretation and sustained mutual understanding, their members tended to have statistically better individual-learning outcomes. These analyses also indicated that when groups engaged in additional orientation and proposition generation, their learning outcomes were statistically lower. Approximately two-thirds of the students considered their group work helpful in completing inquiry tasks. Explanations of these results and research recommendations are provided. © 2019",Computers in Human Behavior,10.1016/j.chb.2019.04.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065835961&doi=10.1016%2fj.chb.2019.04.022&partnerID=40&md5=b102f2f70c980d6b611f8350c51817b3,2019,2021-07-20 15:49:21,2021-07-20 15:49:21
7SWEDW5Q,journalArticle,2014,"Rovcanin, M.; Poorter, E.D.; Moerman, I.; Demeester, P.","A reinforcement learning based solution for cognitive network cooperation between co-located, heterogeneous wireless sensor networks","Due to a drastic increase in the number of wireless communication devices, these devices are forced to interfere or interact with each other. This raises the issue of possible effects this coexistence might have on the performance of these networks. Negative effects are a consequence of contention for network resources (such as free wireless communication frequencies) between different devices, which can be avoided if co-located networks cooperate with each other and share the available resources. This paper presents a self-learning, cognitive cooperation approach for heterogeneous co-located networks. Cooperation is performed by activating or deactivating services such as interference avoidance, packet sharing, various MAC protocols, etc. Activation of a cooperative service might have both positive and negative effects on a network's performance, regarding its high level goals. Such a cooperation approach has to incorporate a reasoning mechanism, centralized or distributed, capable of determining the influence of each symbiotic service on the performance of all the participating sub-networks, taking into consideration their requirements. In this paper, a cooperation method incorporating a machine learning technique, known as the Least Squares Policy Iteration (LSPI), is proposed and discussed as a novel network cooperation paradigm. © 2014 Elsevier B.V. All rights reserved.",Ad Hoc Networks,10.1016/j.adhoc.2014.01.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896715082&doi=10.1016%2fj.adhoc.2014.01.009&partnerID=40&md5=ef8c5c972b706dc17153656fa5f40a77,2014,2021-07-20 15:49:21,2021-07-20 15:49:21
ZPT2RZFB,journalArticle,2021,"Xu, J.; Zhao, J.; Zhou, R.; Liu, C.; Zhao, P.; Zhao, L.",Predicting Destinations by a Deep Learning based Approach,"Destination prediction is known as an important problem for many location based services (LBSs). Existing solutions generally apply probabilistic models or neural network models to predict destinations over a subtrajectory, and adopt the standard attention mechanism to improve the prediction accuracy. However, the standard attention mechanism uses fixed feature representations, and has a limited ability to represent distinct features of locations. Besides, existing methods rarely take the impact of spatial and temporal characteristics of the trajectory into account. Their accuracies in fine-granularity prediction are always not satisfactory due to the data sparsity problem. Thus, in this paper, a carefully designed deep learning model called LATL model is presented. It not only adopts an adaptive attention network to model the distinct features of locations, but also implements time gates and distance gates into the Long Short-Term Memory (LSTM) network to capture the spatialoral relation between consecutive locations. Furthermore, to better understand the mobility patterns in different spatial granularities, and explore the fusion of multi-granularity learning capability, a hierarchical model that utilizes tailored combination of different neural networks under multiple spatial granularities is further proposed. Extensive empirical studies verify that the newly proposed models perform effectively and settle the problem nicely. © 1989-2012 IEEE.",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2019.2932984,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070685230&doi=10.1109%2fTKDE.2019.2932984&partnerID=40&md5=99a2a477fe07fc9d99b0e291f8d4872b,2021,2021-07-20 15:49:21,2021-07-20 15:49:21
XEQSDFKG,journalArticle,2019,"Lee, S.; Kim, D.; Nguyen, D.; Lee, J.",Double MAC on a DSP: Boosting the performance of convolutional neural networks on FPGAs,"Deep learning workloads, such as convolutional neural networks (CNNs) are important due to increasingly demanding high-performance hardware acceleration. One distinguishing feature of a deep learning workload is that it is inherently resilient to small numerical errors and thus works very well with low precision hardware. We propose a novel method called double multiply-and-accumulate (MAC) to theoretically double the computation rate of CNN accelerators by packing two MAC operations into one digital signal processing block of off-the-shelf field-programmable gate arrays (FPGAs). We overcame several technical challenges by exploiting the mode of operation in the CNN accelerator. We have validated our method through FPGA synthesis and Verilog simulation, and evaluated our method by applying it to the state-of-the-art CNN accelerator. The double MAC approach used can double the computation throughput of a CNN layer. On the network level (all convolution layers combined), the performance improvement varies depending on the CNN application and FPGA size, from 14% to more than 80% over a highly optimized state-of-the-art accelerator solution, without sacrificing the output quality significantly. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2824280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045193712&doi=10.1109%2fTCAD.2018.2824280&partnerID=40&md5=24bd48bc2bfa50f997cd08ec0b950bb4,2019,2021-07-20 15:49:21,2021-07-20 15:49:21
PPUFZLVI,journalArticle,2021,"Li, C.-T.; Siu, W.-C.",Fast Monocular Visual Place Recognition for Non-Uniform Vehicle Speed and Varying Lighting Environment,"This paper presents a novel Fast Monocular Visual Place Recognition (FMPR) with a shallow path-oriented offline learning stage and an online place recognition and tracking stage. FMPR uses a tube of frames with a humanlike key frame recognition to solve place recognition for situations with varying speeds and changing lighting conditions, which are two most commonly encountered situations in real life. We propose an offline learning to analyze the correlation of all video frames in a reference path and to extract effective feature patches of key frames with an offline feature-shifts approach to achieve real-time place recognition. Our recognition results are on the basis of both the instant feature matching of frames and the historical recognition results which impose temporal logic constraints on the movement of a vehicle. Experimental results demonstrate that our proposed method can achieve comparable or even better performance compared with the state-of-the-art methods on different challenging datasets, especially for the case which requires a trade-off between the performance and the processing time. We believe that our FMPR offers a useful alternative to computationally expensive deep learning-based methods especially for applications with battery-powered or resource-limited devices. © 2000-2011 IEEE.",IEEE Transactions on Intelligent Transportation Systems,10.1109/TITS.2020.2975710,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102370083&doi=10.1109%2fTITS.2020.2975710&partnerID=40&md5=97def7c0fffb5f4e7a7f659163689e90,2021,2021-07-20 15:49:21,2021-07-20 15:49:21
H8M7JDDK,journalArticle,2021,"Hernandez-Aguila, A.; Garcia-Valdez, M.; Merelo-Guervos, J.-J.; Castanon-Puga, M.; Lopez, O.C.",Using Fuzzy Inference Systems for the Creation of Forex Market Predictive Models,"This paper presents a method for creating Forex market predictive models using multi-agent and fuzzy systems, which have the objective of simulating the interactions that provoke changes in the price. Agents in the system represent traders performing buy and sell orders in a market, and fuzzy systems are used to model the rules followed by traders performing trades in a live market and intuitionistic fuzzy logic to model their decisions' indeterminacy. We use functions to restrict the agents' decisions, which make the agents become specialized at particular market conditions. These 'specialization' functions use the grades of membership obtained from an agent's fuzzy system and thresholds obtained from training data sets, to determine if that agent is specialized enough to handle a market's current conditions. We have performed experiments and compared against the state of the art. Results demonstrate that our method obtains predictive errors (using mean absolute error) that are in the same order of magnitude than those errors obtained by models generated using deep learning and models generated by random forest, AdaBoost, XGBoost, and support-vector machines. Furthermore, we performed experiments that show that identifying specialized agents yields better results. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3077910,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107237092&doi=10.1109%2fACCESS.2021.3077910&partnerID=40&md5=ea913016b3a6a5707a06ecbc8f283a28,2021,2021-07-20 15:49:21,2021-07-20 15:49:21
8CBUEYRZ,journalArticle,2020,"Zhang, Z.; Jiang, W.; Geng, J.; Deng, X.; Li, X.",Fault diagnosis based on non-negative sparse constrained deep neural networks and dempster-shafer theory,"Fault diagnosis is an important technology to ensure the safe and reliable operation of equipment. Deep learning driven by big data brings new opportunities for fault diagnosis. Due to the diversity and complexity of the actual fault data distribution, a fault diagnosis algorithm based on non-negative sparse constrained deep neural networks (NSCDNN) and Dempster-Shafer theory (DST) is proposed in this paper. The deep neural network is trained by non-negative constraint and sparse constraint, which can learn part-based representation of fault data. The improved DST is combined with the classification confidence and accuracy of NSCDNN model, which can deal with the uncertainty of information from different sensors. Experimental results of the data provided by Case Western Reserve University Bearing Data Center show that the proposed NSCDNN-DST algorithm can improve the accuracy of fault diagnosis effectively. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2966260,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079822490&doi=10.1109%2fACCESS.2020.2966260&partnerID=40&md5=b313eaf0bb73b18a82f6d63470cda443,2020,2021-07-20 15:49:21,2021-07-20 15:49:21
MAVG55KV,journalArticle,2017,"Deng, Y.; Ren, Z.; Kong, Y.; Bao, F.; Dai, Q.",A Hierarchical Fused Fuzzy Deep Neural Network for Data Classification,"Deep learning (DL) is an emerging and powerful paradigm that allows large-scale task-driven feature learning from big data. However, typical DL is a fully deterministic model that sheds no light on data uncertainty reductions. In this paper, we show how to introduce the concepts of fuzzy learning into DL to overcome the shortcomings of fixed representation. The bulk of the proposed fuzzy system is a hierarchical deep neural network that derives information from both fuzzy and neural representations. Then, the knowledge learnt from these two respective views are fused altogether forming the final data representation to be classified. The effectiveness of the model is verified on three practical tasks of image categorization, high-frequency financial data prediction and brain MRI segmentation that all contain high level of uncertainties in the raw data. The fuzzy dDL paradigm greatly outperforms other nonfuzzy and shallow learning approaches on these tasks. © 2016 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2016.2574915,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986263974&doi=10.1109%2fTFUZZ.2016.2574915&partnerID=40&md5=01ad822b0e85c6891f3042c8f80d571e,2017,2021-07-20 15:49:21,2021-07-20 15:49:21
KT9B5ZG6,journalArticle,2021,"Qu, Y.; Pokhrel, S.R.; Garg, S.; Gao, L.; Xiang, Y.",A Blockchained Federated Learning Framework for Cognitive Computing in Industry 4.0 Networks,"Cognitive computing, a revolutionary AI concept emulating human brain's reasoning process, is progressively flourishing in the Industry 4.0 automation. With the advancement of various AI and machine learning technologies the evolution toward improved decision making as well as data-driven intelligent manufacturing has already been evident. However, several emerging issues, including the poisoning attacks, performance, and inadequate data resources, etc., have to be resolved. Recent research works studied the problem lightly, which often leads to unreliable performance, inefficiency, and privacy leakage. In this article, we developed a decentralized paradigm for big data-driven cognitive computing (D2C), using federated learning and blockchain jointly. Federated learning can solve the problem of 'data island' with privacy protection and efficient processing while blockchain provides incentive mechanism, fully decentralized fashion, and robust against poisoning attacks. Using blockchain-enabled federated learning help quick convergence with advanced verifications and member selections. Extensive evaluation and assessment findings demonstrate D2C's effectiveness relative to existing leading designs and models. © 2005-2012 IEEE.",IEEE Transactions on Industrial Informatics,10.1109/TII.2020.3007817,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099507209&doi=10.1109%2fTII.2020.3007817&partnerID=40&md5=a2660a6840ba4b01a324dd3ac8460d80,2021,2021-07-20 15:49:21,2021-07-20 15:49:21
R7F7LCMV,journalArticle,2020,"Hogan, F.R.; Rodriguez, A.",Reactive planar non-prehensile manipulation with hybrid model predictive control,"This article presents an offline solution and online approximation to the hybrid control problem of planar non-prehensile manipulation. Hybrid dynamics and underactuation are key characteristics of this task that complicate the design of feedback controllers. We show that a model predictive control approach used in tandem with integer programming offers a powerful solution to capture the dynamic constraints associated with the friction cone as well as the hybrid nature of contact. We introduce the Model Predictive Controller with Learned Mode Scheduling (MPC-LMS), which leverages integer programming and machine learning techniques to effectively deal with the combinatorial complexity associated with determining sequences of contact modes. We validate the controller design through a numerical simulation study and with experiments on a planar manipulation setup using an industrial ABB IRB 120 robotic arm. Results show that the proposed algorithm achieves closed-loop tracking of a nominal trajectory by reasoning in real-time across multiple contact modalities. © The Author(s) 2020.",International Journal of Robotics Research,10.1177/0278364920913938,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084791048&doi=10.1177%2f0278364920913938&partnerID=40&md5=151453a08a6f4b4016e6d42e4ad11d00,2020,2021-07-20 15:49:22,2021-07-20 15:49:22
DT25P4XK,journalArticle,2020,"Bendre, N.; Ebadi, N.; Prevost, J.J.; Najafirad, P.",Human action performance using deep neuro-fuzzy recurrent attention model,"A great number of computer vision publications have focused on distinguishing between human action recognition and classification rather than the intensity of actions performed. Indexing the intensity which determines the performance of human actions is a challenging task due to the uncertainty and information deficiency that exists in the video inputs. To remedy this uncertainty, in this paper we coupled fuzzy logic rules with the neural-based action recognition model to rate the intensity of a human action as intense or mild. In our approach, we used a Spatio-Temporal LSTM to generate the weights of the fuzzy-logic model, and then demonstrate through experiments that indexing of the action intensity is possible. We analyzed the integrated model by applying it to videos of human actions with different action intensities and were able to achieve an accuracy of 89.16% on our intensity indexing generated dataset. The integrated model demonstrates the ability of a neuro-fuzzy inference module to effectively estimate the intensity index of human actions. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2982364,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082830009&doi=10.1109%2fACCESS.2020.2982364&partnerID=40&md5=eecb15d18200fc297ca5b8488689d789,2020,2021-07-20 15:49:22,2021-07-20 15:49:22
VUVBSWDH,journalArticle,2019,"Michelioudakis, E.; Artikis, A.; Paliouras, G.",Semi-supervised online structure learning for composite event recognition,"Online structure learning approaches, such as those stemming from statistical relational learning, enable the discovery of complex relations in noisy data streams. However, these methods assume the existence of fully-labelled training data, which is unrealistic for most real-world applications. We present a novel approach for completing the supervision of a semi-supervised structure learning task. We incorporate graph-cut minimisation, a technique that derives labels for unlabelled data, based on their distance to their labelled counterparts. In order to adapt graph-cut minimisation to first order logic, we employ a suitable structural distance for measuring the distance between sets of logical atoms. The labelling process is achieved online (single-pass) by means of a caching mechanism and the Hoeffding bound, a statistical tool to approximate globally-optimal decisions from locally-optimal ones. We evaluate our approach on the task of composite event recognition by using a benchmark dataset for human activity recognition, as well as a real dataset for maritime monitoring. The evaluation suggests that our approach can effectively complete the missing labels and eventually, improve the accuracy of the underlying structure learning system. © 2019, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",Machine Learning,10.1007/s10994-019-05794-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064244432&doi=10.1007%2fs10994-019-05794-2&partnerID=40&md5=d905c36b8ce4861b936be3251a6900a1,2019,2021-07-20 15:49:22,2021-07-20 15:49:22
I8SFIEZV,journalArticle,2017,"Kersting, K.; Mladenov, M.; Tokmakov, P.",Relational linear programming,"We propose relational linear programming, a simple framework for combining linear programs (LPs) and logic programs. A relational linear program (RLP) is a declarative LP template defining the objective and the constraints through the logical concepts of objects, relations, and quantified variables. This allows one to express the LP objective and constraints relationally for a varying number of individuals and relations among them without enumerating them. Together with a logical knowledge base, effectively a logic program consisting of logical facts and rules, it induces a ground LP. This ground LP is solved using lifted linear programming. That is, symmetries within the ground LP are employed to reduce its dimensionality, if possible, and the reduced program is solved using any off-the-shelf LP solver. In contrast to mainstream LP template languages such as AMPL, which features a mixture of declarative and imperative programming styles, RLP's relational nature allows a more intuitive representation of optimization problems, in particular over relational domains. We illustrate this empirically by experiments on approximate inference in Markov logic networks using LP relaxations, on solving Markov decision processes, and on collective inference using LP support vector machines. © 2015 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2015.06.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011324180&doi=10.1016%2fj.artint.2015.06.009&partnerID=40&md5=da80f5a116c916c346b7b598030615a6,2017,2021-07-20 15:49:22,2021-07-20 15:49:22
ASJK25AL,journalArticle,2020,"Toğaçar, M.; Ergen, B.; Cömert, Z.",COVID-19 detection using deep learning models to exploit Social Mimic Optimization and structured chest X-ray images using fuzzy color and stacking approaches,"Coronavirus causes a wide variety of respiratory infections and it is an RNA-type virus that can infect both humans and animal species. It often causes pneumonia in humans. Artificial intelligence models have been helpful for successful analyses in the biomedical field. In this study, Coronavirus was detected using a deep learning model, which is a sub-branch of artificial intelligence. Our dataset consists of three classes namely: coronavirus, pneumonia, and normal X-ray imagery. In this study, the data classes were restructured using the Fuzzy Color technique as a preprocessing step and the images that were structured with the original images were stacked. In the next step, the stacked dataset was trained with deep learning models (MobileNetV2, SqueezeNet) and the feature sets obtained by the models were processed using the Social Mimic optimization method. Thereafter, efficient features were combined and classified using Support Vector Machines (SVM). The overall classification rate obtained with the proposed approach was 99.27%. With the proposed approach in this study, it is evident that the model can efficiently contribute to the detection of COVID-19 disease. © 2020 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2020.103805,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084330921&doi=10.1016%2fj.compbiomed.2020.103805&partnerID=40&md5=b87c1bb8bf1cc6f780aa06afd2b6d00e,2020,2021-07-20 15:49:22,2021-07-20 15:49:22
TN98UCVS,journalArticle,2021,"Barmann, L.; Peller-Konrad, F.; Constantin, S.; Asfour, T.; Waibel, A.",Deep Episodic Memory for Verbalization of Robot Experience,"The ability to verbalize robot experience in natural language is key for a symbiotic human-robot interaction. While first works approached this problem using template-based verbalization on symbolic episode data only, we explore a novel way in which deep learning methods are used for the creation of an episodic memory from experiences as well as the verbalization of such experience in natural language. To this end, we first collected a complex dataset consisting of more than a thousand multimodal robot episode recordings both from simulation as well as real robot executions, together with representative natural language questions and answers about the robot&#x0027;s past experience. Second, we propose and evaluate an episodic memory verbalization model consisting of a speech encoder and decoder based on the Transformer architecture, combined with an LSTM-based episodic memory auto-encoder, and evaluate the model on simulated and real data from robot execution examples. Our experimental results provide a proof-of-concept for episodic-memory-based verbalization of robot experience. IEEE",IEEE Robotics and Automation Letters,10.1109/LRA.2021.3085166,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107341053&doi=10.1109%2fLRA.2021.3085166&partnerID=40&md5=5819970574301e18706cd21b952cd72d,2021,2021-07-20 15:49:22,2021-07-20 15:49:22
T3GR24YN,journalArticle,2019,"Zhang, X.; Feng, C.; Li, R.; Lei, J.; Tang, C.",NeuralTaint: A Key Segment Marking Tool Based on Neural Network,"Dynamic taint analysis techniques are a popular dynamic software analysis method. Marking a key segment of program function by dynamic taint analysis is an important part of software vulnerability research. Key segment marking usually related to the control flow taint analysis, however, several specific program structure may cause failure in key segment marking due to the control flow dependence, and overtainting and undertainting problem. In this paper, we proposed a novel method to mark a key segment accurately and efficiently with deep learning technology. Firstly, we fit the program function execution into a continuous function by the convoluntional network, and then mark the key segment roughly through derivative information of fitted nerual network. Finally, we mark the key segment of specific program function completely and accurately by filtering and diffusion algorithm. We developed the key segment marking tool NeuralTaint on this principle. We design an experiment to select the specific neural network structure of NeuralTaint. Our extensive evaluations demonstrate that NeuralTaint significantly outperforms the two state-of-the-art traditional dynamic taint analysis tool on seven popular real-world programs. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2915681,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067200508&doi=10.1109%2fACCESS.2019.2915681&partnerID=40&md5=6aee852676607c64c444857fdea8be12,2019,2021-07-20 15:49:22,2021-07-20 15:49:22
2SYLQCKV,journalArticle,2021,"Ridhawi, I.A.; Otoum, S.; Aloqaily, M.; Boukerche, A.",Generalizing AI: Challenges and Opportunities for Plug and Play AI Solutions,"Artificial Intelligence (AI) has revolutionized today's Internet of Things (IoT) applications and services by introducing significant technological enhancements across a multitude of domains. With the deployment of the fifth generation (5G) mobile communication network, smart city visions of fast, on-demand, intelligent user-specific services are now becoming a reality. The concept of connected IoT is evolving into connected intelligent things. The advancements of both AI techniques, coupled with the sophistication of edge devices, is now leading to a new era of connected intelligence. Moving the intelligence toward end devices must account for latency demands and simplicity of selecting the type of AI technique to be used. Moreover, since most AI techniques require learning from big data sets and reasoning using a multitude of classification patterns, new simplified and collaborative solutions are now necessary more than ever. As such, the concept of introducing decentralized and distributed 'Plug and Play' (PnP) AI tools is now becoming more attractive given the vast numbers in edge devices, data volume and AI techniques. To this end, this article envisions a novel general AI solution that can be adapted to autonomously select the type of machine learning (ML) algorithm, the data set to be used, and provide reasoning in regards to data selection for optimal features extraction. Moreover, the solution performs the necessary training and all the necessary parameter fine-tunings to achieve the highest level of generality and simplicity for AI at the edge. We explore several aspects related to PnP-AI and its impact in the smart city ecosystem. © 1986-2012 IEEE.",IEEE Network,10.1109/MNET.011.2000371,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091900925&doi=10.1109%2fMNET.011.2000371&partnerID=40&md5=a5c0f2f7720de2fd96f393ec95f320ba,2021,2021-07-20 15:49:22,2021-07-20 15:49:22
UYI9PBJ8,journalArticle,2021,"Ezhilarasu, C.M.; Skaf, Z.; Jennions, I.K.",A Generalised Methodology for the Diagnosis of Aircraft Systems,"An aircraft is made up of a number of complicated systems which work in harmony to ensure safe and trouble-free flight. In order to maintain such a platform, many diagnostic and prognostic techniques have been suggested, mostly aimed at components but some at the system level. Together these form a patchwork approach to the overall problem of efficiently informing aircraft maintenance to the Original Equipment Manufacturers, the operators /airlines, and the Maintenance, Repair, and Overhaul organisations. It involves these organisations having to support several different approaches to aircraft health management, and is therefore inefficient and costly. In the current work, a streamlined methodology is put forward. This is based on OSA-CBM (Open System Architecture for Condition Based Maintenance) and can be applied to any aircraft system. Integral with this is the use of mRMR (minimum redundancy maximum relevance) for feature selection, the resulting symptom vector being used for fault diagnosis. This approach is demonstrated on three test cases: the engine, the environmental control system, and the fuel system. In each case, the digital twin setup, simulation conditions for healthy and faulty scenarios, a methodology based on OSA-CBM up to diagnostics are detailed. Diagnostics is carried out for each system in turn, using four machine learning supervised algorithms. The best performing algorithm for each system will then subsequently be used in a vehicle level reasoner called FAVER (A Framework for Aerospace Vehicle Reasoning), which requires these system diagnoses as a starting point for vehicle reasoning and fault ambiguity resolution. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3050877,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099542639&doi=10.1109%2fACCESS.2021.3050877&partnerID=40&md5=36d11c33b27c11029bca47ebdc65dfea,2021,2021-07-20 15:49:22,2021-07-20 15:49:22
RJVIQ7K2,journalArticle,2020,"Soliman, H.",Random Forest Based Searching Approach for RDF,"The blend of digital and physical worlds changed the Internet significantly. Accordingly, trends to collect, access, and deliver information have changed over the Web. Such changes raised the problems of information retrieval. Search engines retrieve requested information based on the provided keywords which is not an efficient way for rich information retrieval. Consequently, the fetching of the required information is difficult without understanding the syntax and semantics of the content. The multiple existing approaches to resolve this problem by exploiting linked data and semantic Web techniques. Such approaches serialize the content leveraging the Resource Description Framework (RDF) and process the queries using SPARQL to resolve the problem. However, an exact match between RDF content and query structure is required. Although it improves the keyword-based search, it does not provide probabilistic reasoning to find the relationship accuracy between the query and results. In this perspective, this paper proposes a machine learning (random forest) based approach to predict the fetching status of RDF by treating RDFs' requests as a classification problem. First, we preprocess the RDF to convert them into N-Triples format. Then, a feature vector is constructed for each RDF using the preprocessed RDF. After that, a random forest classifier is trained for the prediction of the fetching status of RDFs. The proposed approach is evaluated on an open-source DBpedia dataset. The 10-fold cross-validation results indicate that the performance of the proposed approach is accurate and surpasses the state-of-the-art. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2980155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082393419&doi=10.1109%2fACCESS.2020.2980155&partnerID=40&md5=8a52d29fbf9c7ab1aed679d607204d85,2020,2021-07-20 15:49:22,2021-07-20 15:49:22
TZXZ98MK,journalArticle,2013,"Zhang, Y.; Zhang, Y.; Swears, E.; Larios, N.; Wang, Z.; Ji, Q.",Modeling temporal interactions with interval temporal bayesian networks for complex activity recognition,"Complex activities typically consist of multiple primitive events happening in parallel or sequentially over a period of time. Understanding such activities requires recognizing not only each individual event but, more importantly, capturing their spatiotemporal dependencies over different time intervals. Most of the current graphical model-based approaches have several limitations. First, time - sliced graphical models such as hidden Markov models (HMMs) and dynamic Bayesian networks are typically based on points of time and they hence can only capture three temporal relations: precedes, follows, and equals. Second, HMMs are probabilistic finite-state machines that grow exponentially as the number of parallel events increases. Third, other approaches such as syntactic and description-based methods, while rich in modeling temporal relationships, do not have the expressive power to capture uncertainties. To address these issues, we introduce the interval temporal Bayesian network (ITBN), a novel graphical model that combines the Bayesian Network with the interval algebra to explicitly model the temporal dependencies over time intervals. Advanced machine learning methods are introduced to learn the ITBN model structure and parameters. Experimental results show that by reasoning with spatiotemporal dependencies, the proposed model leads to a significantly improved performance when modeling and recognizing complex activities involving both parallel and sequential events. © 1979-2012 IEEE.",IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2013.33,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883137316&doi=10.1109%2fTPAMI.2013.33&partnerID=40&md5=8607683d7058bda6ecafd596a95ecf1a,2013,2021-07-20 15:49:22,2021-07-20 15:49:22
QWW2USBQ,journalArticle,2017,"Tran, A.C.; Dietrich, J.; Guesgen, H.W.; Marsland, S.",Parallel symmetric class expression learning,"In machine learning, one often encounters data sets where a general pattern is violated by a relatively small number of exceptions (for example, a rule that says that all birds can fly is violated by examples such as penguins). This complicates the concept learning process and may lead to the rejection of some simple and expressive rules that cover many cases. In this paper we present an approach to this problem in description logic learning by computing partial descriptions (which are not necessarily entirely complete) of both positive and negative examples and combining them. Our Symmetric Parallel Class Expression Learning approach enables the generation of general rules with exception patterns included. We demonstrate that this algorithm provides significantly better results (in terms of metrics such as accuracy, search space covered, and learning time) than standard approaches on some typical data sets. Further, the approach has the added benefit that it can be paral-lelised relatively simply, leading to much faster exploration of the search tree on modern computers. ©2017 A.C. Tran, J. Dietrich, H.W. Guesgen and S. Marsland.",Journal of Machine Learning Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030184353&partnerID=40&md5=12c0fa88a4f156f2008cb37905e81641,2017,2021-07-20 15:49:22,2021-07-20 15:49:22
KPRGCT38,journalArticle,2015,"Kaburlasos, V.G.; Papakostas, G.A.",Learning Distributions of Image Features by Interactive Fuzzy Lattice Reasoning in Pattern Recognition Applications,"This paper describes the recognition of image patterns based on novel representation learning techniques by considering higher-level (meta-)representations of numerical data in a mathematical lattice. In particular, the interest here focuses on lattices of (Type-1) Intervals' Numbers (INs), where an IN represents a distribution of image features including orthogonal moments. A neural classifier, namely fuzzy lattice reasoning (flr) fuzzy-ARTMAP (FAM), or flrFAM for short, is described for learning distributions of INs; hence, Type-2 INs emerge. Four benchmark image pattern recognition applications are demonstrated. The results obtained by the proposed techniques compare well with the results obtained by alternative methods from the literature. Furthermore, due to the isomorphism between the lattice of INs and the lattice of fuzzy numbers, the proposed techniques are straightforward applicable to Type-1 and/or Type-2 fuzzy systems. The far-reaching potential for deep learning in big data applications is also discussed. © 2005-2012 IEEE.",IEEE Computational Intelligence Magazine,10.1109/MCI.2015.2437318,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937795135&doi=10.1109%2fMCI.2015.2437318&partnerID=40&md5=0a7133b1b0e670fabb44762a0c52492b,2015,2021-07-20 15:49:22,2021-07-20 15:49:22
5Y564I2L,journalArticle,2019,"Picado, J.; Termehchy, A.; Fern, A.; Ataei, P.",Logical scalability and efficiency of relational learning algorithms,"Relational learning algorithms learn the definition of a new relation in terms of existing relations in the database. The same database may be represented under different schemas for various reasons, such as efficiency, data quality, and usability. Unfortunately, the output of current relational learning algorithms tends to vary quite substantially over the choice of schema, both in terms of learning accuracy and efficiency. We introduce the property of schema independence of relational learning algorithms, and study both the theoretical and empirical dependence of existing algorithms on the common class of (de) composition schema transformations. We show theoretically and empirically that current relational learning algorithms are generally not schema independent. We propose Castor, a relational learning algorithm that achieves schema independence. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",VLDB Journal,10.1007/s00778-018-0523-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056117138&doi=10.1007%2fs00778-018-0523-8&partnerID=40&md5=853bbbf0ad19d8f34f16ebcd948866b0,2019,2021-07-20 15:49:23,2021-07-20 15:49:23
IQXK33QM,journalArticle,2019,"Maleš, L.; Marčetić, D.; Ribarić, S.",A multi-agent dynamic system for robust multi-face tracking,"The paper presents a new architecture framework in the field of expert and intelligent systems which is based on four paradigms: a novel multi-agent dynamic system architecture (MADS), an extended Belief Desire Intention (EBDI) agent community, autonomy-oriented entities (AOEs), and deep learning concepts. The main impact of the proposed framework is a new approach, or even a new way of thinking, which enables integration of the concepts of deep learning, a conventional approach to solving the domain problem, cognitive agents with mental attitudes, and concepts of nature-inspired computing. All these allow the effective use of the framework in the field of intelligent and expert systems. The significance of the framework lies in its flexibility and adaptability based on the formal logical description of EBDI agents, the definition of the behaviour of AOEs, the use of classical modules for domain problem solving, and modules based on deep-learning concepts. We believe that the example of the adaptation of the proposed architecture framework to robust multi-face tracking illustrates the significance of the proposed framework. In this paper, MADS is adapted to the first two stages of a face de-identification pipeline: robust face detection and multi-face tracking. The proposed architecture of MADS has a two-level hierarchical organization. At the first level there is a manager designed as an Extended Belief Desire Intention (mEBDI) agent. The extension of a manager BDI agent consists of a convolutional neural network-based face detector, a set of autonomous-oriented entities for the elimination of false positive face detections, and a trajectory memory. At the second level, there are many tracking agents (trEBDIs) which consist of a basic BDI agent extended with a face tracker based on position and scale correlation filters, a visual appearance memory, and a trajectory memory. The mEBDI and trEBDI agents are defined by the modal logic and are described at the implementation level. The proposed architecture for a robust multi-face tracking system was tested on a subset of YouTube music videos. The qualitative results, as well as the preliminary quantitative results expressed by the standard testing metrics, demonstrate the effective adaptation of the proposed multi-agent dynamic architecture to a robust multi-face tracking system. © 2019 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2019.02.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062232427&doi=10.1016%2fj.eswa.2019.02.008&partnerID=40&md5=f6f01326233b9d73d8cb8fbeaf5e3762,2019,2021-07-20 15:49:23,2021-07-20 15:49:23
UPCCG6XE,journalArticle,2019,"Venkatanareshbabu, K.; Nisheel, S.; Sakthivel, R.; Muralitharan, K.",Novel elegant fuzzy genetic algorithms in classification problems,"In this paper, we propose three novel algorithms such as Novel genetic algorithm complex-valued backpropagation neural network (GA-CVBNN), Novel elegant fuzzy genetic algorithm (EFGA) and elegant fuzzy genetic algorithm-based complex-valued backpropagation neural network (EFGA-CVBNN) for classification of accuracy in datasets. In GA-CVBNN, classical Genetic Algorithm has been used for selecting appropriate initial weights for CVBNN. The EFGA is developed to resolve the drawback of classical GA by employing fuzzy logic to control parameters and selective pressure of GA. The EFGA uses a Min-Heap data structure and Pareto principle to improve the classical genetic algorithm. The EFGA-CVBNN resolves the drawbacks of classical CVBNN by employing EFGA at the time of initial weight selection. From the simulation result, the GA-CVBNN performs better than existing CVBNN and it is not efficient. To enhance the performance of GA-CVBNN, we have developed EFGA-CVBNN. Experimental results on various synthetic datasets and benchmark datasets taken from UCI machine learning repository shows that EFGA-CVBNN outperforms PSO-CVBNN in terms of classification accuracy and time. Statistical t test has been used to validate the obtained results. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-018-3216-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046406347&doi=10.1007%2fs00500-018-3216-8&partnerID=40&md5=f3d9dff03f8f8045cf5ffde2a0d9dab3,2019,2021-07-20 15:49:23,2021-07-20 15:49:23
FA66MGY5,journalArticle,2011,"Li, H.; Sun, J.","On performance of case-based reasoning in Chinese business failure prediction from sensitivity, specificity, positive and negative values","Case-based reasoning (CBR) is a machine learning technique of high performance in classification problems, and it is also a chief method in predicting business failure. Recently, several techniques have been introduced into the life-cycle of CBR for business failure prediction (BFP). The drawback of former researches on CBR-based BFP is that they only use total predictive accuracy when assessing CBR. In this research, we provide evidence on performance of CBR in Chinese BFP from various views of sensitivity, specificity, positive and negative values. Data are collected from Shanghai Stock Exchange and Shenzhen Stock Exchange in China. And we present how data are preprocessed from the view of data mining. The classical CBR model on the base of Euclidean metric, the grey CBR model on the base of grey coefficient metric, and the pseudo CBR model on the base of pseudo outranking relations are employed to make a comparative study on CBR's predictive performance in BFP. Meanwhile, support vector machine (SVM) is employed to be a baseline model for comparison. The results indicate that pseudo CBR produces better performance in Chinese BFP than classical CBR and grey CBR significantly on the whole, and it outperforms SVM marginally by total predictive accuracy and sensitivity, while it is not significantly worse than SVM by specificity. © 2010 Elsevier B.V. All rights reserved.",Applied Soft Computing Journal,10.1016/j.asoc.2009.12.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957919738&doi=10.1016%2fj.asoc.2009.12.005&partnerID=40&md5=fbe3f764dcbf68ccce16a12d91422f9d,2011,2021-07-20 15:49:23,2021-07-20 15:49:23
7W6TPBNR,journalArticle,2019,"del Campo, I.; Martínez, V.; Echanobe, J.; Asua, E.; Finker, R.; Basterretxea, K.",A versatile hardware/software platform for personalized driver assistance based on online sequential extreme learning machines,"In the present scenario of technological breakthroughs in the automotive industry, machine learning is greatly contributing to the development of safer and more comfortable vehicles. In particular, personalization of the driving experience using machine learning is an innovative trend that comprises the development of both customized driver assistance systems and in-cabin comfort features. In this work, a versatile hardware/software platform for personalized driver assistance, using online sequential extreme learning machines (OS-ELM), is presented. The system, based on a programmable system-on-chip (SoC), is able to recognize the driver and personalize the behavior of the car. The platform provides high speed, small size, efficient power consumption, and true capability for real-time adaptation (i.e., on-chip self-learning). In addition, due to the plasticity and scalability of the OS-ELM algorithm and the programmable nature of the SoC, this solution is flexible enough to cope with the incremental changes that the new generation of vehicles are demanding. The implementation details of a system, suitable for current levels of driving automation, are provided. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-019-04386-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074736768&doi=10.1007%2fs00521-019-04386-4&partnerID=40&md5=6ddee170e93ece445e023c7b6ea42f36,2019,2021-07-20 15:49:23,2021-07-20 15:49:23
MGSU8Z5M,journalArticle,2015,"Wang, Z.; Lee, K.H.; Verma, N.",Overcoming Computational Errors in Sensing Platforms Through Embedded Machine-Learning Kernels,"We present an approach for overcoming computational errors at run time that originate from static hardware faults in digital processors. The approach is based on embedded machine-learning stages that learn and model the statistics of the computational outputs in the presence of errors, resulting in an error-aware model for embedded analysis. We demonstrate, in hardware, two systems for analyzing sensor data: 1) an EEG-based seizure detector and 2) an ECG-based cardiac arrhythmia detector. The systems use a small kernel of fault-free hardware (constituting <7.0% and <31% of the total areas respectively) to construct and apply the error-aware model. The systems construct their own error-aware models with minimal overhead through the use of an embedded active-learning framework. Via an field-programmable gate array implementation for hardware experiments, stuck-at faults are injected at controllable rates within synthesized gate-level netlists to permit characterization. The seizure detector demonstrates restored performance despite faults on 0.018% of the circuit nodes [causing bit error rates (BERs) up to 45%], and the arrhythmia detector demonstrates restored performance despite faults on 2.7% of the circuit nodes (causing BERs up to 50%). © 2015 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2014.2342153,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028209896&doi=10.1109%2fTVLSI.2014.2342153&partnerID=40&md5=16e17c859aad9f1b042636e7778c082d,2015,2021-07-20 15:49:23,2021-07-20 15:49:23
CMMGFVSX,journalArticle,2013,"Beloufa, F.; Chikh, M.A.",Design of fuzzy classifier for diabetes disease using Modified Artificial Bee Colony algorithm,"In this study, diagnosis of diabetes disease, which is one of the most important diseases, is conducted with artificial intelligence techniques. We have proposed a novel Artificial Bee Colony (ABC) algorithm in which a mutation operator is added to an Artificial Bee Colony for improving its performance. When the current best solution cannot be updated, a blended crossover operator (BLX-α) of genetic algorithm is applied, in order to enhance the diversity of ABC, without compromising with the solution quality. This modified version of ABC is used as a new tool to create and optimize automatically the membership functions and rules base directly from data. We take the diabetes dataset used in our work from the UCI machine learning repository. The performances of the proposed method are evaluated through classification rate, sensitivity and specificity values using 10-fold cross-validation method. The obtained classification rate of our method is 84.21% and it is very promising when compared with the previous research in the literature for the same problem. © 2013 Elsevier Ireland Ltd.",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2013.07.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883746011&doi=10.1016%2fj.cmpb.2013.07.009&partnerID=40&md5=0dd8a875bfe7251b92989e4d6ae012a4,2013,2021-07-20 15:49:23,2021-07-20 15:49:23
SE4YPEW2,journalArticle,2018,"Falomir, Z.; Kluth, T.",Qualitative spatial logic descriptors from 3D indoor scenes to generate explanations in natural language,"The challenge of describing 3D real scenes is tackled in this paper using qualitative spatial descriptors. A key point to study is which qualitative descriptors to use and how these qualitative descriptors must be organized to produce a suitable cognitive explanation. In order to find answers, a survey test was carried out with human participants which openly described a scene containing some pieces of furniture. The data obtained in this survey are analysed, and taking this into account, the QSn3D computational approach was developed which uses a XBox 360 Kinect to obtain 3D data from a real indoor scene. Object features are computed on these 3D data to identify objects in indoor scenes. The object orientation is computed, and qualitative spatial relations between the objects are extracted. These qualitative spatial relations are the input to a grammar which applies saliency rules obtained from the survey study and generates cognitive natural language descriptions of scenes. Moreover, these qualitative descriptors can be expressed as first-order logical facts in Prolog for further reasoning. Finally, a validation study is carried out to test whether the descriptions provided by QSn3D approach are human readable. The obtained results show that their acceptability is higher than 82%. © 2017, Marta Olivetti Belardinelli and Springer-Verlag GmbH Germany.",Cognitive Processing,10.1007/s10339-017-0824-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010042623&doi=10.1007%2fs10339-017-0824-7&partnerID=40&md5=a3936961db57317d76a8ed5e14b2eff0,2018,2021-07-20 15:49:23,2021-07-20 15:49:23
E6E8YEBC,journalArticle,2020,"Pham, T.-C.; Doucet, A.; Luong, C.-M.; Tran, C.-T.; Hoang, V.-D.",Improving Skin-Disease Classification Based on Customized Loss Function Combined with Balanced Mini-Batch Logic and Real-Time Image Augmentation,"Skin cancer is one of the most common cancers in the world. However, the disease is curable if detected in the beginning stage. Early detection of malignant lesions through accurate techniques and innovative technologies has a significant impact on reducing skin cancer mortality rates. Recently, artificial intelligence has come to the forefront to facilitate skin cancer diagnosis based on medical images. Many deep learning models have been studied and developed, but the imbalance of performance among classes in the multi-class classification is still a challenging problem. This study proposes a hybrid method for handling class imbalance of skin-disease classification. This method combines the data level method of balanced mini-batch logic followed by real-time image augmentation with the algorithm level method of designing new loss function. The training dataset includes 24,530 dermoscopic images of seven skin disease categories, which is by far the largest dataset of skin cancer. The performance metrics of six proposed methods are evaluated on a test dataset of 2,453 images. Our proposed EfficientNetB4-CLF model achieves the highest accuracy of 89.97% and also the highest mean recall of 86.13% with the smallest recalls' standard deviations of 7.60%. Compared to the original methods, our proposed solution not only surpasses 4.65% (86.13% vs 81.48%) of mean recalls but also reduces 4.24% of the recalls' standard deviations (from ±11.84% to ±7.60%). This result indicates that our hybrid method is highly effective in training the Deep CNN network on the skin-disease imbalanced dataset. It addresses the problem of slow learning of the minority classes in the networks by combining the data level method of balanced mini-batch logic followed by the real-time image augmentation with the algorithm level method of the newly designed loss function. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3016653,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090294072&doi=10.1109%2fACCESS.2020.3016653&partnerID=40&md5=1c5adb5da67cbb75a7682c6fbb2e77ac,2020,2021-07-20 15:49:23,2021-07-20 15:49:23
JK3R9PCT,journalArticle,2018,"Banaee, H.; Schaffernicht, E.; Loutfi, A.",Data-driven conceptual spaces: Creating semantic representations for linguistic descriptions of numerical data,"There is an increasing need to derive semantics from real-world observations to facilitate natural information sharing between machine and human. Conceptual spaces theory is a possible approach and has been proposed as mid-level representation between symbolic and sub-symbolic representations, whereby concepts are represented in a geometrical space that is characterised by a number of quality dimensions. Currently, much of the work has demonstrated how conceptual spaces are created in a knowledge-driven manner, relying on prior knowledge to form concepts and identify quality dimensions. This paper presents a method to create semantic representations using data-driven conceptual spaces which are then used to derive linguistic descriptions of numerical data. Our contribution is a principled approach to automatically construct a conceptual space from a set of known observations wherein the quality dimensions and domains are not known a priori. This novelty of the approach is the ability to select and group semantic features to discriminate between concepts in a data-driven manner while preserving the semantic interpretation that is needed to infer linguistic descriptions for interaction with humans. Two data sets representing leaf images and time series signals are used to evaluate the method. An empirical evaluation for each case study assesses how well linguistic descriptions generated from the conceptual spaces identify unknown observations. Furthermore, comparisons are made with descriptions derived on alternative approaches for generating semantic models. © 2018 AI Access Foundation. All rights reserved.",Journal of Artificial Intelligence Research,10.1613/jair.1.11258,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057746407&doi=10.1613%2fjair.1.11258&partnerID=40&md5=21520c19c926f04ecf8121541b8aa108,2018,2021-07-20 15:49:23,2021-07-20 15:49:23
ZCT9887M,journalArticle,2020,"Ruthotto, L.; Haber, E.",Deep Neural Networks Motivated by Partial Differential Equations,"Partial differential equations (PDEs) are indispensable for modeling many physical phenomena and also commonly used for solving image processing tasks. In the latter area, PDE-based approaches interpret image data as discretizations of multivariate functions and the output of image processing algorithms as solutions to certain PDEs. Posing image processing problems in the infinite-dimensional setting provides powerful tools for their analysis and solution. For the last few decades, the reinterpretation of classical image processing problems through the PDE lens has been creating multiple celebrated approaches that benefit a vast area of tasks including image segmentation, denoising, registration, and reconstruction. In this paper, we establish a new PDE interpretation of a class of deep convolutional neural networks (CNN) that are commonly used to learn from speech, image, and video data. Our interpretation includes convolution residual neural networks (ResNet), which are among the most promising approaches for tasks such as image classification having improved the state-of-the-art performance in prestigious benchmark challenges. Despite their recent successes, deep ResNets still face some critical challenges associated with their design, immense computational costs and memory requirements, and lack of understanding of their reasoning. Guided by well-established PDE theory, we derive three new ResNet architectures that fall into two new classes: parabolic and hyperbolic CNNs. We demonstrate how PDE theory can provide new insights and algorithms for deep learning and demonstrate the competitiveness of three new CNN architectures using numerical experiments. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Mathematical Imaging and Vision,10.1007/s10851-019-00903-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073820798&doi=10.1007%2fs10851-019-00903-1&partnerID=40&md5=83003f45e1816d23d5686159e2b0920d,2020,2021-07-20 15:49:24,2021-07-20 15:49:24
RB2WY3P6,journalArticle,2018,"Fan, J.; Cheng, J.",Matrix completion by deep matrix factorization,"Conventional methods of matrix completion are linear methods that are not effective in handling data of nonlinear structures. Recently a few researchers attempted to incorporate nonlinear techniques into matrix completion but there still exists considerable limitations. In this paper, a novel method called deep matrix factorization (DMF) is proposed for nonlinear matrix completion. Different from conventional matrix completion methods that are based on linear latent variable models, DMF is on the basis of a nonlinear latent variable model. DMF is formulated as a deep-structure neural network, in which the inputs are the low-dimensional unknown latent variables and the outputs are the partially observed variables. In DMF, the inputs and the parameters of the multilayer neural network are simultaneously optimized to minimize the reconstruction errors for the observed entries. Then the missing entries can be readily recovered by propagating the latent variables to the output layer. DMF is compared with state-of-the-art methods of linear and nonlinear matrix completion in the tasks of toy matrix completion, image inpainting and collaborative filtering. The experimental results verify that DMF is able to provide higher matrix completion accuracy than existing methods do and DMF is applicable to large matrices. © 2017 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2017.10.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034051911&doi=10.1016%2fj.neunet.2017.10.007&partnerID=40&md5=f599a166d6ed2b6bc8ab84e546af7a60,2018,2021-07-20 15:49:24,2021-07-20 15:49:24
NVCJVL86,journalArticle,2018,"Zhang, Z.; Ernst, G.; Sedwards, S.; Arcaini, P.; Hasuo, I.",Two-layered falsification of hybrid systems guided by Monte Carlo tree search,"Few real-world hybrid systems are amenable to formal verification, due to their complexity and black box components. Optimization-based falsification- A methodology of search-based testing that employs stochastic optimization-is thus attracting attention as an alternative quality assurance method. Inspired by the recent work that advocates coverage and exploration in falsification, we introduce a two-layered optimization framework that uses Monte Carlo tree search (MCTS), a popular machine learning technique with solid mathematical and empirical foundations (e.g., in computer Go). MCTS is used in the upper layer of our framework; it guides the lower layer of local hill-climbing optimization, thus balancing exploration and exploitation in a disciplined manner. We demonstrate the proposed framework through experiments with benchmarks from the automotive domain. © 2018 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2858463,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050594949&doi=10.1109%2fTCAD.2018.2858463&partnerID=40&md5=52885876d428ed0074876ddd608a9800,2018,2021-07-20 15:49:24,2021-07-20 15:49:24
3T65GGCX,journalArticle,2018,"Edwards, L.; Veale, M.","Enslaving the Algorithm: From a ""right to an Explanation"" to a ""right to Better Decisions""?","As concerns about unfairness and discrimination in 'black box' machine learning systems rise, a legal 'right to an explanation' has emerged as a compellingly attractive approach for challenge and redress. We outline recent debates on the limited provisions in European data protection law, and introduce and analyze newer explanation rights in French administrative law and the draft modernized Council of Europe Convention 108. While individual rights can be useful, in privacy law they have historically unreasonably burdened the average data subject. 'Meaningful information' about algorithmic logics is more technically possible than commonly thought, but this exacerbates a new 'transparency fallacy' - an illusion of remedy rather than anything substantively helpful. While rights-based approaches deserve a firm place in the toolbox, other forms of governance, such as impact assessments, 'soft law,' judicial review, and model repositories deserve more attention, alongside catalyzing agencies acting for users to control algorithmic system design. © 2018 IEEE.",IEEE Security and Privacy,10.1109/MSP.2018.2701152,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049158825&doi=10.1109%2fMSP.2018.2701152&partnerID=40&md5=e8108bc6699cbb3cb2faed8ec1ed7b0b,2018,2021-07-20 15:49:24,2021-07-20 15:49:24
4VTBIPIN,journalArticle,2020,"Cifka, O.; Simsekli, U.; Richard, G.",Groove2Groove: One-Shot Music Style Transfer with Supervision from Synthetic Data,"Style transfer is the process of changing the style of an image, video, audio clip or musical piece so as to match the style of a given example. Even though the task has interesting practical applications within the music industry, it has so far received little attention from the audio and music processing community. In this article, we present Groove2Groove, a one-shot style transfer method for symbolic music, focusing on the case of accompaniment styles in popular music and jazz. We propose an encoder-decoder neural network for the task, along with a synthetic data generation scheme to supply it with parallel training examples. This synthetic parallel data allows us to tackle the style transfer problem using end-To-end supervised learning, employing powerful techniques used in natural language processing. We experimentally demonstrate the performance of the model on style transfer using existing and newly proposed metrics, and also explore the possibility of style interpolation. © 2014 IEEE.",IEEE/ACM Transactions on Audio Speech and Language Processing,10.1109/TASLP.2020.3019642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090234144&doi=10.1109%2fTASLP.2020.3019642&partnerID=40&md5=de5529fe5cfb59f4e8ee970cf281c301,2020,2021-07-20 15:49:24,2021-07-20 15:49:24
WZ4VAF9Z,journalArticle,2015,"Droniou, A.; Ivaldi, S.; Sigaud, O.","Deep unsupervised network for multimodal perception, representation and classification","In this paper, we tackle the problem of multimodal learning for autonomous robots. Autonomous robots interacting with humans in an evolving environment need the ability to acquire knowledge from their multiple perceptual channels in an unsupervised way. Most of the approaches in the literature exploit engineered methods to process each perceptual modality. In contrast, robots should be able to acquire their own features from the raw sensors, leveraging the information elicited by interaction with their environment: learning from their sensorimotor experience would result in a more efficient strategy in a life-long perspective. To this end, we propose an architecture based on deep networks, which is used by the humanoid robot iCub to learn a task from multiple perceptual modalities (proprioception, vision, audition). By structuring high-dimensional, multimodal information into a set of distinct sub-manifolds in a fully unsupervised way, it performs a substantial dimensionality reduction by providing both a symbolic representation of data and a fine discrimination between two similar stimuli. Moreover, the proposed network is able to exploit multimodal correlations to improve the representation of each modality alone. © 2014 Elsevier B.V.",Robotics and Autonomous Systems,10.1016/j.robot.2014.11.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983535203&doi=10.1016%2fj.robot.2014.11.005&partnerID=40&md5=858d78a899d4acc56aed01d966929456,2015,2021-07-20 15:49:24,2021-07-20 15:49:24
AB9NL3XQ,journalArticle,2021,"Rafi, S.H.; Nahid-Al-Masood; Deeba, S.R.; Hossain, E.",A Short-Term Load Forecasting Method Using Integrated CNN and LSTM Network,"In this study, a new technique is proposed to forecast short-term electrical load. Load forecasting is an integral part of power system planning and operation. Precise forecasting of load is essential for unit commitment, capacity planning, network augmentation and demand side management. Load forecasting can be generally categorized into three classes such as short-term, midterm and long-term. Short-term forecasting is usually done to predict load for next few hours to few weeks. In the literature, various methodologies such as regression analysis, machine learning approaches, deep learning methods and artificial intelligence systems have been used for short-term load forecasting. However, existing techniques may not always provide higher accuracy in short-term load forecasting. To overcome this challenge, a new approach is proposed in this paper for short-term load forecasting. The developed method is based on the integration of convolutional neural network (CNN) and long short-term memory (LSTM) network. The method is applied to Bangladesh power system to provide short-term forecasting of electrical load. Also, the effectiveness of the proposed technique is validated by comparing the forecasting errors with that of some existing approaches such as long short-term memory network, radial basis function network and extreme gradient boosting algorithm. It is found that the proposed strategy results in higher precision and accuracy in short-term load forecasting. CCBY",IEEE Access,10.1109/ACCESS.2021.3060654,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101748533&doi=10.1109%2fACCESS.2021.3060654&partnerID=40&md5=9eba04819bf53bbc45a8d5c8420cf047,2021,2021-07-20 15:49:24,2021-07-20 15:49:24
4LMNRVFT,journalArticle,2021,"Gurung, S.; Naetiladdanon, S.; Sangswang, A.",A Surrogate Based Computationally Efficient Method to Coordinate Damping Controllers for Enhancement of Probabilistic Small-Signal Stability,"This paper proposes a computationally efficient method based on deep neural network and a meta-heuristic optimization algorithm known as bat algorithm to coordinate power oscillation damping controllers incorporated in renewable energy stations to enhance system small signal stability considering uncertainties. The proposed method consists of three main stages: database generation, supervised learning, and optimization using the created model. A database is first created of Probabilistic small-signal stability margin calculated using the combined cumulant and Gram-Charlier expansion and the parameters of damping controllers, which is then given to different supervised machine learning algorithms such as linear regression, support vector machine, random forest, and chiefly deep neural network to create a surrogate model. The surrogate model provides an approximate relationship between the probabilistic small-signal instability margin and damping controller parameters. An optimization problem is then formulated to minimize the surrogate probabilistic instability margin with damping controller parameters acting as constraints. Finally, this optimization problem is solved using the bat algorithm to obtain the optimized parameters for power oscillation damping controllers. Our study results tested on a large IEEE 16 machines, 68 bus system show that the power oscillation damping controllers optimized using the proposed method can largely improve the system low frequency oscillatory stability margin in a very low computational time (around 19 times faster than the conventional method). This study can be used by the power system operators to tune the parameters of damping controller in a fast manner. CCBY",IEEE Access,10.1109/ACCESS.2021.3060502,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101737105&doi=10.1109%2fACCESS.2021.3060502&partnerID=40&md5=7592f0becb83e7d1bbc117b729908ee8,2021,2021-07-20 15:49:24,2021-07-20 15:49:24
DMGEINNX,journalArticle,2019,"Liu, N.; Shen, B.; Zhang, Z.; Zhang, Z.; Mi, K.",Attention-based Sentiment Reasoner for aspect-based sentiment analysis,"Aspect-based sentiment analysis (ABSA) is a powerful way of predicting the sentiment polarity of text in natural language processing. However, understanding human emotions and reasoning from text like a human continues to be a challenge. In this paper, we propose a model, named Attention-based Sentiment Reasoner (AS-Reasoner), to alleviate the problem of how to capture precise sentiment expressions in ABSA for reasoning. AS-Reasoner assigns importance degrees to different words in a sentence to capture key sentiment expressions towards a specific aspect, and transfers them into a sentiment sentence representation for reasoning in the next layer. To obtain appropriate importance degree values for different words in a sentence, two attention mechanisms we designed: intra attention and global attention. Specifically, intra attention captures the sentiment similarity between any two words in a sentence to compute weights and global attention computes weights by a global perspective. Experiments on all four English and four Chinese datasets show that the proposed model achieves state-of-the-art accuracy and macro-F1 results for aspect term level sentiment analysis and obtains the best accuracy for aspect category level sentiment analysis. The experimental results also indicate that AS-Reasoner is language-independent. © 2019, The Author(s).",Human-centric Computing and Information Sciences,10.1186/s13673-019-0196-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073199829&doi=10.1186%2fs13673-019-0196-3&partnerID=40&md5=b1a041a676af7b467d0659845d3274cb,2019,2021-07-20 15:49:24,2021-07-20 15:49:24
PWVH5Y88,journalArticle,2019,"Pruthi, J.; Arora, S.; Khanna, K.",Modified Bird swarm algorithm for edge detection in noisy images using fuzzy reasoning,"A new bio-inspired edge detection approach is proposed to deal with the noisy images using a combination of bird swarm algorithm (BSA) and fuzzy reasoning. BSA is based on the behaviour of the birds. The birds fly through each pixel while they forage for the food and detect the edge pixels and noisy pixels that fall in their path. The direction in which the birds fly is found using fuzzy rule-based system. The pixels are classified as edge and non-edge pixels using the concept of thresholding. The noisy pixels are removed by the birds using fuzzy impulse noise detection and reduction method. The technique has been evaluated on two standard image datasets for quantitative and qualitative analysis. The results clearly indicate significant improvement over the other bio-inspired approaches, namely, Genetic Algorithm (GA), Particle Swarm Optimisation (PSO), Ant Colony Optimisation (ACO), Bacterial Foraging Algorithm (BFO), Deep Learning approach, fuzzy with BFO for noisy images, Neuro-fuzzy approach and PSO particularly for images having impulse noise in terms of Entropy, Kappa Value, Pratt’s Figure of Merit (FoM) and Structural Similarity Index Measure (SSIM). The proposed approach works well to detect the continuous, thin and smooth edges in the presence of 5–40% noise density. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",Computer Methods in Biomechanics and Biomedical Engineering: Imaging and Visualization,10.1080/21681163.2018.1523751,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054634869&doi=10.1080%2f21681163.2018.1523751&partnerID=40&md5=e66f054762a3d5321f9c640f6e2213f5,2019,2021-07-20 15:49:24,2021-07-20 15:49:24
99BCNGYV,journalArticle,2019,"Alsirhani, A.; Sampalli, S.; Bodorik, P.",DDoS Detection System: Using a Set of Classification Algorithms Controlled by Fuzzy Logic System in Apache Spark,"Distributed Denial of Service (DDoS) attacks are a major security threat against the availability of conventional or cloud computing resources. Numerous DDoS attacks, which have been launched against various organizations in the last decade, have had a direct impact on both vendors and users. Many researchers have attempted to tackle the security threat of DDoS attacks by combining classification algorithms with distributed computing. However, their solutions are static in terms of the classification algorithms used. In fact, current DDoS attacks have become so dynamic and sophisticated that they are able to pass the detection system thereby making it difficult for static solutions to detect. In this paper, we propose a dynamic DDoS attack detection system based on three main components: classification algorithms, a distributed system, and a fuzzy logic system. Our framework uses fuzzy logic to dynamically select an algorithm from a set of prepared classification algorithms that detect different DDoS patterns. Out of the many candidate classification algorithms, we use Naive Bayes, Decision Tree (Entropy), Decision Tree (Gini), and Random Forest as candidate algorithms. We have evaluated the performance of classification algorithms and their delays and validated the fuzzy logic system. We have also evaluated the effectiveness of the distributed system and its impact on the classification algorithms delay. The results show that there is a trade-off between the utilized classification algorithms accuracies and their delays. We observe that the fuzzy logic system can effectively select the right classification algorithm based on the traffic status. IEEE",IEEE Transactions on Network and Service Management,10.1109/TNSM.2019.2929425,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069914913&doi=10.1109%2fTNSM.2019.2929425&partnerID=40&md5=c3890fc596139eaf619d677ff6cd3c3b,2019,2021-07-20 15:49:24,2021-07-20 15:49:24
ZD7VLT7Z,journalArticle,2017,"Bach, S.H.; Broecheler, M.; Huang, B.; Getoor, L.",Hinge-Loss Markov random fields and probabilistic soft logic,"A fundamental challenge in developing high-impact machine learning technologies is balancing the need to model rich, structured domains with the ability to scale to big data. Many important problem areas are both richly structured and large scale, from social and biological networks, to knowledge graphs and the Web, to images, video, and natural language. In this paper, we introduce two new formalisms for modeling structured data, and show that they can both capture rich structure and scale to big data. The first, hingeloss Markov random fields (HL-MRFs), is a new kind of probabilistic graphical model that generalizes different approaches to convex inference. We unite three approaches from the randomized algorithms, probabilistic graphical models, and fuzzy logic communities, showing that all three lead to the same inference objective. We then define HL-MRFs by generalizing this unified objective. The second new formalism, probabilistic soft logic (PSL), is a probabilistic programming language that makes HL-MRFs easy to define using a syntax based on first-order logic. We introduce an algorithm for inferring most-probable variable assignments (MAP inference) that is much more scalable than general-purpose convex optimization methods, because it uses message passing to take advantage of sparse dependency structures. We then show how to learn the parameters of HL-MRFs. The learned HL-MRFs are as accurate as analogous discrete models, but much more scalable. Together, these algorithms enable HL-MRFs and PSL to model rich, structured data at scales not previously possible. © 2017 Stephen H. Bach, Matthias Broecheler, Bert Huang, and Lise Getoor.",Journal of Machine Learning Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030465241&partnerID=40&md5=67d66dd080a7762a7424c6c7fa893df0,2017,2021-07-20 15:49:24,2021-07-20 15:49:24
RC6HX6CA,journalArticle,2020,"Jain, S.; Kaur, P.",Reconceptualizing examination debar criteria using fuzzy logic,"Strict requirements regarding student attendance have always been a debated topic in academic institutions. Numerous studies carried out to relate class attendance with a student's overall performance have reported positive as well as negative results; thereby not resulting in a clear overall conclusion. Therefore, this paper presents a fuzzy logic based attendance evaluation system for higher educational institutions. The proposed fuzzy system considers four attributes: student attendance in the current course, overall performance, performance in the current course and faculty's assessment for deciding if the student should be debarred from examination, allowed taking the examination or be given reconsideration. Since the considered attributes are relevant to any course, it results in a generalized model which may be adapted according to the specific requirements of courses at different universities. The proposed model is implemented using the fuzzy logic toolkit of OCTAVE. The application of the system to actual students' data has yielded an accuracy of 95.25%. Further, for performance analysis, three classification algorithms, namely Naïve Bayes, Support Vector Machine and Neural Networks are also applied on the same dataset. © 2020 - IOS Press and the authors. All rights reserved.",Intelligent Decision Technologies,10.3233/IDT-190012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089022740&doi=10.3233%2fIDT-190012&partnerID=40&md5=0d5d304913cfd428d007f7547773c450,2020,2021-07-20 15:49:24,2021-07-20 15:49:24
64IWKYVY,journalArticle,2017,"Jiang, J.; Li, X.; Zhao, C.; Guan, Y.; Yu, Q.",Learning and inference in knowledge-based probabilistic model for medical diagnosis,"Based on a weighted knowledge graph to represent first-order knowledge and combining it with a probabilistic model, we propose a methodology for creating a medical knowledge network (MKN) in medical diagnosis. When a set of evidence is activated for a specific patient, we can generate a ground medical knowledge network that is composed of evidence nodes and potential disease nodes. By incorporating a Boltzmann machine into the potential function of a Markov network, we investigated the joint probability distribution of the MKN. To consider numerical evidence, a multivariate inference model is presented that uses conditional probability. In addition, the weights for the knowledge graph are efficiently learned from manually annotated Chinese Electronic Medical Records (CEMRs) and Blood Examination Records (BERs). In our experiments, we found numerically that an improved expression of evidence variables is necessary for medical diagnosis. Our experimental results comparing a Markov logic network and six kinds of classic machine learning algorithms on the actual CEMR database and BER database indicate that our method holds promise and that MKN can facilitate studies of intelligent diagnosis. © 2017 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2017.09.030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033491118&doi=10.1016%2fj.knosys.2017.09.030&partnerID=40&md5=2779a352ea62c5ce9f70baee6dfd57c4,2017,2021-07-20 15:49:24,2021-07-20 15:49:24
SI7W6UV5,journalArticle,2015,"Gao, Y.; Wang, H.; Liu, Y.-J.",Adaptive fuzzy control with minimal leaning parameters for electric induction motors,"In this paper, an effective adaptive tracking control approach is constructed for the sixth-order induction motor model which is in discrete-time form. The fuzzy logic systems (FLSs) are applied to estimate a control law and used to a block strict feedback form (BSFF) which is in existence of bounded disturbances. Then, a desired optimal control can be achieved. Depend on the Lyapunov analysis approach, for the considered system, the stability is guaranteed, the tracking error can be ensured to a small neighborhood around zero, and the weight estimates can be insured to semi-global uniformly ultimately bounded (SGUUB). An advantage for the controlled system is that the proposed algorithm needs only less parameters which is different from the existing results, therefore, it can reduce the calculation amount. The practicality of the scheme is verified via a simulation. © 2015 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2014.12.071,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923537368&doi=10.1016%2fj.neucom.2014.12.071&partnerID=40&md5=3de68a579551a122781d2645e8bacae9,2015,2021-07-20 15:49:24,2021-07-20 15:49:24
LI7J3D78,journalArticle,2020,"Wei, Y.; Wang, L.; Cao, H.; Shao, M.; Wu, C.",Multi-Attention Generative Adversarial Network for image captioning,"Recently, it has been shown that generative-adversarial-nets (GANs) can be directly utilized as an extension of traditional reinforcement-learning in image captioning tasks. However, the GANs-based methods generate captions as a function of only local points in the feature map without capturing non-local information. In this paper, a Multi-Attention mechanism is first proposed by utilizing both of the local and non-local evidence for more effective feature representation and reasoning in image captioning. Based on the mechanism, a Multi-Attention Generative Adversarial Image Captioning Network (MAGAN) is also proposed which contains a Multi-Attention generator and a Multi-Attention discriminator. The proposed generator is designed to generate more accurate sentences, while the proposed discriminator is employed to determine whether generated sentences are human described or machine generated. Extensive experiments are conducted to validate the proposed framework on MSCOCO benchmark dataset, and it achieves very competitive results evaluated by the evaluation server of MS COCO captioning challenge. © 2019",Neurocomputing,10.1016/j.neucom.2019.12.073,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077983352&doi=10.1016%2fj.neucom.2019.12.073&partnerID=40&md5=63e80b6f5e50c6e884cfebda587efbb2,2020,2021-07-20 15:49:24,2021-07-20 15:49:24
UYSRDVE5,journalArticle,2016,"Sikder, I.U.",A variable precision rough set approach to knowledge discovery in land cover classification,"This paper presents a granular computing approach to spatial classification and prediction of land cover classes using rough set variable precision methods. In particular, it presents an approach to characterizing large spatially clustered data sets to discover knowledge in multi-source supervised classification. The evidential structure of spatial classification is founded on the notions of equivalence relations of rough set theory. It allows expressing spatial concepts in terms of approximation space wherein a decision class can be approximated through the partition of boundary regions. The paper also identifies how approximate reasoning can be introduced by using variable precision rough sets in the context of land cover characterization. The rough set theory is applied to demonstrate an empirical application and the predictive performance is compared with popular baseline machine learning algorithms. A comparison shows that the predictive performance of the rough set rule induction is slightly higher than the decision tree and significantly outperforms the baseline models such as neural network, naïve Bayesian and support vector machine methods. © 2016 Informa UK Limited, trading as Taylor & Francis Group.",International Journal of Digital Earth,10.1080/17538947.2016.1194489,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976526025&doi=10.1080%2f17538947.2016.1194489&partnerID=40&md5=98c9a15a1e71a53c00b2c29e615844e0,2016,2021-07-20 15:49:25,2021-07-20 15:49:25
89WTFT63,journalArticle,2021,"Segura-Muros, J.Á.; Pérez, R.; Fernández-Olivares, J.",Discovering relational and numerical expressions from plan traces for learning action models,"In this paper, we propose a domain learning process build on a machine learning-based process that, starting from plan traces with (partially known) intermediate states, returns a planning domain with numeric predicates, and expressive logical/arithmetic relations between domain predicates written in the planning domain definition language (PDDL). The novelty of our approach is that it can discover relations with little information about the ontology of the target domain to be learned. This is achieved by applying a selection of preprocessing, regression, and classification techniques to infer information from the input plan traces. These techniques are used to prepare the planning data, discover relational/numeric expressions, or extract the preconditions and effects of the domain’s actions. Our solution was evaluated using several metrics from the literature, taking as experimental data plan traces obtained from several domains from the International Planning Competition. The experiments demonstrate that our proposal—even with high levels of incompleteness—correctly learns a wide variety of domains discovering relational/arithmetic expressions, showing F-Score values above 0.85 and obtaining valid domains in most of the experiments. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Applied Intelligence,10.1007/s10489-021-02232-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103206913&doi=10.1007%2fs10489-021-02232-6&partnerID=40&md5=d6cff0ee380fc6329180b53caf7e9908,2021,2021-07-20 15:49:25,2021-07-20 15:49:25
WSRD2G3T,journalArticle,2020,"Miceli, M.; Schuessler, M.; Yang, T.",Between Subjectivity and Imposition: Power Dynamics in Data Annotation for Computer Vision,"The interpretation of data is fundamental to machine learning. This paper investigates practices of image data annotation as performed in industrial contexts. We define data annotation as a sense-making practice, where annotators assign meaning to data through the use of labels. Previous human-centered investigations have largely focused on annotators? subjectivity as a major cause of biased labels. We propose a wider view on this issue: guided by constructivist grounded theory, we conducted several weeks of fieldwork at two annotation companies. We analyzed which structures, power relations, and naturalized impositions shape the interpretation of data. Our results show that the work of annotators is profoundly informed by the interests, values, and priorities of other actors above their station. Arbitrary classifications are vertically imposed on annotators, and through them, on data. This imposition is largely naturalized. Assigning meaning to data is often presented as a technical matter. This paper shows it is, in fact, an exercise of power with multiple implications for individuals and society. © 2020 Owner/Author.",Proceedings of the ACM on Human-Computer Interaction,10.1145/3415186,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094218800&doi=10.1145%2f3415186&partnerID=40&md5=0ca4bb57447f7c84289506be9a8c13b1,2020,2021-07-20 15:49:25,2021-07-20 15:49:25
F5LMTWDI,journalArticle,2019,"D’Angelo, G.; Pilla, R.; Tascini, C.; Rampone, S.",A proposal for distinguishing between bacterial and viral meningitis using genetic programming and decision trees,"Meningitis is an inflammation of the protective membranes covering the brain and the spinal cord. Meningitis can have different causes, and discriminating between meningitis etiologies is still considered a hard task, especially when some specific clinical parameters, mostly derived from blood and cerebrospinal fluid analysis, are not completely available. Although less frequent than its viral version, bacterial meningitis can be fatal, especially when diagnosis is delayed. In addition, often unnecessary antibiotic and/or antiviral treatments are used as a solution, which is not cost or health effective. In this work, we address this issue through the use of machine learning-based methodologies. We consider two distinct cases. In one case, we take into account both blood and cerebrospinal parameters; in the other, we rely exclusively on the blood data. As a result, we have rules and formulas applicable in clinical settings. Both results highlight that a combination of the clinical parameters is required to properly distinguish between the two meningitis etiologies. The results on standard and clinical datasets show high performance. The formulas achieve 100% of sensitivity in detecting a bacterial meningitis. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-018-03729-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059543858&doi=10.1007%2fs00500-018-03729-y&partnerID=40&md5=cc7de17c95fcf4cb99eef5a086b1b349,2019,2021-07-20 15:49:25,2021-07-20 15:49:25
IE2D9F3V,journalArticle,2020,"Subramanya, T.; Harutyunyan, D.; Riggio, R.",Machine learning-driven service function chain placement and scaling in MEC-enabled 5G networks,"5G mobile network technology promises to deliver unprecedented ultra-low latency and high data rates, paving the way for many novel applications and services. Network Function Virtualization (NFV) and Multi-access Edge Computing (MEC) are two of the technologies that are expected to play a pivotal role in 5G to achieve ambitious Quality of Service requirements of such applications. While NFV provides flexibility by enabling network functions to be dynamically deployed and inter-connected to realize Service Function Chains (SFC), MEC brings the computing capability to the edges of the mobile network thus reducing latency and alleviating the transport network load. However, adequate mechanisms are needed to meet the dynamically changing network service demands, to optimally utilize the network resources while, at the same time, making sure that the end-to-end latency requirement of services is always satisfied. In this work, we first propose machine learning models, in particular neural-networks, that can perform auto-scaling by predicting the required number of virtual network function instances based on the traffic demand, using the traffic traces collected over a real-operator commercial network. We then employ Integer Linear Programming (ILP) techniques to formulate and solve a joint user association and SFC placement problem, where each SFC represents a service requested by a user with end-to-end latency and data rate requirements. Finally, we propose a heuristic to address the scalability concern of the ILP model. © 2019 Elsevier B.V.",Computer Networks,10.1016/j.comnet.2019.106980,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074699825&doi=10.1016%2fj.comnet.2019.106980&partnerID=40&md5=cb331008270a62ddeb101dfceb628879,2020,2021-07-20 15:49:25,2021-07-20 15:49:25
IY45H6Y7,journalArticle,2020,"Souri, A.; Mohammed, A.S.; Yousif Potrus, M.; Malik, M.H.; Safara, F.; Hosseinzadeh, M.",Formal verification of a hybrid machine learning-based fault prediction model in internet of things applications,"By increasing the complexity of the Internet of Things (IoT) applications, fault prediction become an important challenge in interactions between human, and smart devices. Fault prediction is one of the key factors to achieve better arranging the IoT applications. Most of the current research studies evaluated the fault prediction methods using simulation environments. However, formal verification of the correctness of a fault prediction method has not been reported yet. This paper presents a behavioral modeling and formal verification of a hybrid machine learning-based fault prediction model with Multi-Layer Perceptron (MLP) and Particle Swarm Optimization (PSO) algorithms. In particular, the PSO is used for feature selection. Then, the fault prediction is considered as a behavior to be verified formally. The fault prediction behavior is divided into two types of behaviors: dimension reduction behavior and prediction behavior. For each of the behaviors, one formal model is designed. The behavioral models designed are mapped into the Labeled Transition System (LTS). The Process Analysis Toolkit (PAT) model checker is employed to evaluate the behavioral models. The accuracy of the fault prediction method is done by some existing specifications such as deadlock-free and reachability properties in terms of linear temporal logic formulas. Also, the verification of the fault prediction behaviors is used to detect the defect metrics of information-centric IoT applications. Experimental results showed that our proposed verification method has minimum verification time and memory usage for evaluating critical specification rules than other research studies. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2967629,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081083514&doi=10.1109%2fACCESS.2020.2967629&partnerID=40&md5=d421f9ac8524a3e6c708869fdcc8a02b,2020,2021-07-20 15:49:25,2021-07-20 15:49:25
APUGCNGA,journalArticle,2018,"Lohrmann, C.; Luukka, P.; Jablonska-Sabuka, M.; Kauranne, T.",A combination of fuzzy similarity measures and fuzzy entropy measures for supervised feature selection,"Large amounts of information and various features are in many machine learning applications available, or easily obtainable. However, their quality is potentially low and greater volumes of information are not always beneficial for machine learning, for instance, when not all available features in a data set are relevant for the classification task and for understanding the studied phenomenon. Feature selection aims at determining a subset of features that represents the data well, gives accurate classification results and reduces the impact of noise on the classification performance. In this paper, we propose a filter feature ranking method for feature selection based on fuzzy similarity and entropy measures (FSAE), which is an adaptation of the idea used for the wrapper function by Luukka (2011) and has an additional scaling factor. The scaling factor to the feature and class-specific entropy values that is implemented, accounts for the distance between the ideal vectors for each class. Moreover, a wrapper version of the FSAE with a similarity classifier is presented as well. The feature selection method is tested on five medical data sets: dermatology, chronic kidney disease, breast cancer, diabetic retinopathy and horse colic. The wrapper version of FSAE is compared to the wrapper introduced by Luukka (2011) and shows at least as accurate results with often considerably fewer features. In the comparison with ReliefF, Laplacian score, Fisher score and the filter version of Luukka (2011), the FSAE filter in general achieves competitive mean accuracies and results for one medical data set, the breast cancer Wisconsin data set, together with the Laplacian score in the best results over all possible feature removals. © 2018 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2018.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048549420&doi=10.1016%2fj.eswa.2018.06.002&partnerID=40&md5=add5a469b13c3b6a5376dedf9fc24f38,2018,2021-07-20 15:49:25,2021-07-20 15:49:25
K8Y3EN29,journalArticle,2018,"Espejo-Garcia, B.; Martinez-Guanter, J.; Pérez-Ruiz, M.; Lopez-Pellicer, F.J.; Javier Zarazaga-Soria, F.",Machine learning for automatic rule classification of agricultural regulations: A case study in Spain,"Currently, pest management practices require modern equipment and the use of complex information, such as regulations and guidelines. The complexity of regulations is the root cause of the emergence of automated solutions for compliance assessment by translating regulations into sets of machine-processable rules that can be run by specialized modules of farm management information systems (FMIS). However, the manual translation of rules is prohibitively costly, and therefore, this translation should be carried out with the support of artificial intelligence techniques. In this paper, we use the official Spanish phytosanitary products registry to empirically evaluate the performance of four popular machine learning algorithms in the task of correctly classifying pesticide regulations as prohibitions or obligations. Moreover, we also evaluate how to improve the performance of the algorithms in the preprocessing of the texts with natural language processing techniques. Finally, due to the specific characteristics of the texts found in pesticide regulations, resampling techniques are also evaluated. Experiments show that the combination of the machine learning algorithm Logic regression, the natural language technique part-of-speech tagging and the resampling technique Tomek links is the best performing approach, with an F1 score of 68.8%, a precision of 84.46% and a recall of 60%. The experimental results are promising, and they show that this approach can be applied to develop a computer-aided tool for transforming textual pesticide regulations into machine-processable rules. To the best of our knowledge, this is the first study that evaluates the use of artificial intelligence methods for the automatic translation of agricultural regulations into machine-processable representations. © 2018 Elsevier B.V.",Computers and Electronics in Agriculture,10.1016/j.compag.2018.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046784112&doi=10.1016%2fj.compag.2018.05.007&partnerID=40&md5=13597188cd10a7b3f3fb1c1ef84b6560,2018,2021-07-20 15:49:25,2021-07-20 15:49:25
YMP2X9RF,journalArticle,2016,"Melo, H.; Watada, J.",Gaussian-PSO with fuzzy reasoning based on structural learning for training a Neural Network,"This paper proposes Gaussian-PSO-based structural learning and fuzzy reasoning to optimize the weights and the structure of the Feed Forward Neural Network. The Neural Network is widely used for various applications; though it still has disadvantages such as learning capability and slow convergence. Back Propagation, the most used learning algorithm, has several difficulties such as the necessity for a priori specification of the network structure and sensibility to parameter settings. Recently, research studies have introduced evolutionary algorithms into the learning to improve its performance. The PSO is a population-based algorithm that has the advantage of faster convergence. However, the total number of the weights in the Neural Network determines the size of each particle, therefore the size of the network structure is computationally time consuming. The proposed method improves the learning and removes the stress by eliminating the necessity of determining a detailed network. © 2015 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2015.03.104,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947195900&doi=10.1016%2fj.neucom.2015.03.104&partnerID=40&md5=b3ddd10564886a80d91f7ae502893244,2016,2021-07-20 15:49:25,2021-07-20 15:49:25
F9P7AAGU,journalArticle,2013,"Gweon, G.; Jain, M.; McDonough, J.; Raj, B.; Rosé, C.P.",Measuring prevalence of other-oriented transactive contributions using an automated measure of speech style accommodation,"This paper contributes to a theory-grounded methodological foundation for automatic collaborative learning process analysis. It does this by illustrating how insights from the social psychology and sociolinguistics of speech style provide a theoretical framework to inform the design of a computational model. The purpose of that model is to detect prevalence of an important group knowledge integration process in raw speech data. Specifically, this paper focuses on assessment of transactivity in dyadic discussions, where a transactive contribution is operationalized as one where reasoning is made explicit, and where that reasoning builds on a prior reasoning statement within the discussion. Transactive contributions can be either self-oriented, where the contribution builds on the speaker's own prior contribution, or other-oriented, where the contribution builds on a prior contribution of a conversational partner. Other-oriented transacts are particularly central to group knowledge integration processes. An unsupervised Dynamic Bayesian Network model motivated by concepts from Speech Accommodation Theory is presented and then evaluated on the task of estimating prevalence of other-oriented transacts in dyadic discussions. The evaluation demonstrates a significant positive correlation between an automatic measure of speech style accommodation and prevalence of other-oriented transacts (R=.36,p<.05). © 2013 International Society of the Learning Sciences, Inc. and Springer Science+Business Media New York.",International Journal of Computer-Supported Collaborative Learning,10.1007/s11412-013-9172-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878111202&doi=10.1007%2fs11412-013-9172-5&partnerID=40&md5=d1eeff681cee235d62290ec32eaae2c6,2013,2021-07-20 15:49:25,2021-07-20 15:49:25
6Z6N32CC,journalArticle,2015,"Gong, H.; Klinger, J.; Damazyn, K.; Li, X.; Huang, S.",A novel procedure for statistical inference and verification of gene regulatory subnetwork,"Background: The reconstruction of gene regulatory network from time course microarray data can help us comprehensively understand the biological system and discover the pathogenesis of cancer and other diseases. But how to correctly and efficiently decifer the gene regulatory network from high-throughput gene expression data is a big challenge due to the relatively small amount of observations and curse of dimensionality. Computational biologists have developed many statistical inference and machine learning algorithms to analyze the microarray data. In the previous studies, the correctness of an inferred regulatory network is manually checked through comparing with public database or an existing model. Results: In this work, we present a novel procedure to automatically infer and verify gene regulatory networks from time series expression data. The dynamic Bayesian network, a statistical inference algorithm, is at first implemented to infer an optimal network from time series microarray data of S. cerevisiae, then, a weighted symbolic model checker is applied to automatically verify or falsify the inferred network through checking some desired temporal logic formulas abstracted from experiments or public database. Conclusions: Our studies show that the marriage of statistical inference algorithm with model checking technique provides a more efficient way to automatically infer and verify the gene regulatory network from time series expression data than previous studies. © 2015 Gong et al.; licensee BioMed Central Ltd.",BMC Bioinformatics,10.1186/1471-2105-16-S7-S7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977607158&doi=10.1186%2f1471-2105-16-S7-S7&partnerID=40&md5=80c31d89e3f79cf5fbec8d1821f5b458,2015,2021-07-20 15:49:25,2021-07-20 15:49:25
3NK9NFCA,journalArticle,2021,"Paredes, J.N.; Simari, G.I.; Martinez, M.V.; Falappa, M.A.",Detecting malicious behavior in social platforms via hybrid knowledge- and data-driven systems,"Among the wide variety of malicious behavior commonly observed in modern social platforms, one of the most notorious is the diffusion of fake news, given its potential to influence the opinions of millions of people who can be voters, consumers, or simply citizens going about their daily lives. In this paper, we implement and carry out an empirical evaluation of a version of the recently-proposed NETDER architecture for hybrid AI decision-support systems with the capability of leveraging the availability of machine learning modules, logical reasoning about unknown objects, and forecasts based on diffusion processes. NETDER is a general architecture for reasoning about different kinds of malicious behavior such as dissemination of fake news, hate speech, and malware, detection of botnet operations, prevention of cyber attacks including those targeting software products or blockchain transactions, among others. Here, we focus on the case of fake news dissemination on social platforms by three different kinds of users: non-malicious, malicious, and botnet members. In particular, we focus on three tasks: (i) determining who is responsible for posting a fake news article, (ii) detecting malicious users, and (iii) detecting which users belong to a botnet designed to disseminate fake news. Given the difficulty of obtaining adequate data with ground truth, we also develop a testbed that combines real-world fake news datasets with synthetically generated networks of users and fully-detailed traces of their behavior throughout a series of time points. We designed our testbed to be customizable for different problem sizes and settings, and make its code publicly available to be used in similar evaluation efforts. Finally, we report on the results of a thorough experimental evaluation of three variants of our model and six environmental settings over the three tasks. Our results clearly show the effects that the quality of knowledge engineering tasks, the quality of the underlying machine learning classifier used to detect fake news, and the specific environmental conditions have on smart policing efforts in social platforms. © 2021",Future Generation Computer Systems,10.1016/j.future.2021.06.033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109435960&doi=10.1016%2fj.future.2021.06.033&partnerID=40&md5=a34f603a9fd4e7b466e9a757c0a86e14,2021,2021-07-20 15:49:26,2021-07-20 15:49:26
HPZMIKY6,journalArticle,2019,"Pal, G.; Hong, X.; Wang, Z.; Wu, H.; Li, G.; Atkinson, K.",Lifelong Machine Learning and root cause analysis for large-scale cancer patient data,"Introduction: This paper presents a lifelong learning framework which constantly adapts with changing data patterns over time through incremental learning approach. In many big data systems, iterative re-training high dimensional data from scratch is computationally infeasible since constant data stream ingestion on top of a historical data pool increases the training time exponentially. Therefore, the need arises on how to retain past learning and fast update the model incrementally based on the new data. Also, the current machine learning approaches do the model prediction without providing a comprehensive root cause analysis. To resolve these limitations, our framework lays foundations on an ensemble process between stream data with historical batch data for an incremental lifelong learning (LML) model. Case description: A cancer patient’s pathological tests like blood, DNA, urine or tissue analysis provide a unique signature based on the DNA combinations. Our analysis allows personalized and targeted medications and achieves a therapeutic response. Model is evaluated through data from The National Cancer Institute’s Genomic Data Commons unified data repository. The aim is to prescribe personalized medicine based on the thousands of genotype and phenotype parameters for each patient. Discussion and evaluation: The model uses a dimension reduction method to reduce training time at an online sliding window setting. We identify the Gleason score as a determining factor for cancer possibility and substantiate our claim through Lilliefors and Kolmogorov–Smirnov test. We present clustering and Random Decision Forest results. The model’s prediction accuracy is compared with standard machine learning algorithms for numeric and categorical fields. Conclusion: We propose an ensemble framework of stream and batch data for incremental lifelong learning. The framework successively applies first streaming clustering technique and then Random Decision Forest Regressor/Classifier to isolate anomalous patient data and provides reasoning through root cause analysis by feature correlations with an aim to improve the overall survival rate. While the stream clustering technique creates groups of patient profiles, RDF further drills down into each group for comparison and reasoning for useful actionable insights. The proposed MALA architecture retains the past learned knowledge and transfer to future learning and iteratively becomes more knowledgeable over time. © 2019, The Author(s).",Journal of Big Data,10.1186/s40537-019-0261-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076427816&doi=10.1186%2fs40537-019-0261-9&partnerID=40&md5=c515e719a7bcdbfcc871fdcb80de6432,2019,2021-07-20 15:49:26,2021-07-20 15:49:26
JPS9KUYT,journalArticle,2021,"Huang, K.; Zhang, Y.; Cheng, H.D.; Xing, P.; Zhang, B.",Semantic segmentation of breast ultrasound image with fuzzy deep learning network and breast anatomy constraints,"Breast cancer is one of the most serious disease affecting women's health. Due to low cost, portable, no radiation, and high efficiency, breast ultrasound (BUS) imaging is the most popular approach for diagnosing early breast cancer. However, ultrasound images are low resolution and poor quality. Thus, developing accurate detection system is a challenging task. In this paper, we propose a fully automatic segmentation algorithm consisting of two parts: fuzzy fully convolutional network and accurately fine-tuning post-processing based on breast anatomy constraints. In the first part, the image is pre-processed by contrast enhancement, and wavelet features are employed for image augmentation. A fuzzy membership function transforms the augmented BUS images into the fuzzy domain. The features from convolutional layers are processed using fuzzy logic as well. The conditional random fields (CRFs) post-process the segmentation result. The location relation among the breast anatomy layers is utilized to improve the performance. The proposed method is applied to the dataset with 325 BUS images, and achieves state-of-the-art performance compared with that of existing methods with true positive rate 90.33%, false positive rate 9.00%, and intersection over union (IoU) 81.29% on tumor category, and overall intersection over union (mIoU) 80.47% over five categories: fat layer, mammary layer, muscle layer, background, and tumor. © 2021 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2021.04.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105028239&doi=10.1016%2fj.neucom.2021.04.012&partnerID=40&md5=a27123c8548f487012fd283f8ce16c6d,2021,2021-07-20 15:49:26,2021-07-20 15:49:26
J5ZRQ2PE,journalArticle,2020,"Sutradhar, P.R.; Connolly, M.; Bavikadi, S.; Dinakarrao, S.M.P.; Indovina, M.A.; Ganguly, A.",PPIM: A Programmable Processor-in-Memory Architecture with Precision-Scaling for Deep Learning,"Memory access latencies and low data transfer bandwidth limit the processing speed of many data intensive applications such as Convolutional Neural Networks (CNNs) in conventional Von Neumann architectures. Processing in Memory (PIM) is envisioned as a potential hardware solution for such applications as the data access bottlenecks can be avoided in PIM by performing computations within the memory die. However, PIM realizations with logic-based complex processing units within the memory present complicated fabrication challenges. In this letter, we propose to leverage the existing memory infrastructure to implement a programmable PIM (pPIM), a novel Look-Up-Table (LUT)-based PIM where all the processing units are implemented solely with LUTs, as opposed to prior LUT-based PIM implementations that combine LUT with logic circuitry for computations. This enables pPIM to perform ultra-low power low-latency operations with minimal fabrication complications. Moreover, the complete LUT-based design offers simple 'memory write' based programmability in pPIM. Enabling precision scaling further improves the performance and the power consumption for CNN applications. The programmability feature potentially makes it easier for online training implementations. Our preliminary simulations demonstrate that our proposed pPIM can achieve 2000x, 657.5x and 1.46x improvement in inference throughput per unit power consumption compared to state-of-the-art conventional processor architecture, Graphics Processing Unit (GPUs) and a prior hybrid LUT-logic based PIM respectively. Furthermore, precision scaling improves the energy efficiency of the pPIM approximately by 1.35x over its full-precision operation. © 2002-2011 IEEE.",IEEE Computer Architecture Letters,10.1109/LCA.2020.3011643,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089699598&doi=10.1109%2fLCA.2020.3011643&partnerID=40&md5=06f9e4178fed4e3bd2c4b7471080f716,2020,2021-07-20 15:49:26,2021-07-20 15:49:26
RC3B4V57,journalArticle,2021,"Chen, P.; Li, L.; Wu, J.; Zhang, Y.; Lin, W.",Temporal Reasoning Guided QoE Evaluation for Mobile Live Video Broadcasting,"Quality of experience (QoE) that serves as a direct evaluation of viewing experience from the end users is of vital importance for network optimization, and should be constantly monitored. Unlike existing video-on-demand streaming services, real-time interactivity is critical to the mobile live broadcasting experience for both broadcasters and their audiences. While existing QoE metrics that are validated on limited video contents and synthetic stall patterns have shown effectiveness in their trained QoE benchmarks, a common caveat is that they often encounter challenges in practical live broadcasting scenarios, where one needs to accurately understand the activity in the video with fluctuating QoE and figure out what is going to happen to support the real-time feedback to the broadcaster. In this paper, we propose a temporal relational reasoning guided QoE evaluation approach for mobile live video broadcasting, namely TRR-QoE, which explicitly attends to the temporal relationships between consecutive frames to achieve a more comprehensive understanding of the distortion-aware variation. In our design, video frames are first processed by deep neural network (DNN) to extract quality-indicative features. Afterwards, besides explicitly integrating features of individual frames to account for the spatial distortion information, multi-scale temporal relational information corresponding to diverse temporal resolutions are made full use of to capture temporal-distortion-aware variation. As a result, the overall QoE prediction could be derived by combining both aspects. The results of experiments conducted on a number of benchmark databases demonstrate the superiority of TRR-QoE over the representative state-of-the-art metrics. © 1992-2012 IEEE.",IEEE Transactions on Image Processing,10.1109/TIP.2021.3060255,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101749870&doi=10.1109%2fTIP.2021.3060255&partnerID=40&md5=ef32e8dad7496b242a1d1a46903d91c1,2021,2021-07-20 15:49:26,2021-07-20 15:49:26
J4R7VGME,journalArticle,2020,"Piltan, F.; Prosvirin, A.E.; Kim, J.-M.",Robot manipulator active fault-tolerant control using a machine learning-based automated robust hybrid observer,"Robotic manipulators represent a class of nonlinear and multiple-degrees-of-freedom robots that have pronounced coupling effects and can be used in various applications. The challenge of understanding complexity in a system's dynamic behavior, coupling effects, and sources of uncertainty presents substantial challenges regarding fault estimation, detection, identification, and tolerant-control (FEDIT) in a robot manipulator. Thus, a proposed active fault-tolerant control algorithm, based on an adaptive modern sliding mode observer, is represented. Due to the effect of the system's complexities and uncertainties for fault estimation, detection, and identification (FEDI), a sliding mode observer (SMO) is proposed. To address the sliding mode observer drawbacks for FEDI such as high-frequency oscillation (chattering) and fault estimation accuracy, the modern (T-S fuzzy higher order) technique is represented. In addition, the adaptive technique is applied to the modern sliding mode observer (MSMO) to self-tune the coefficients of the fault estimation observer to increase the reliability and robustness of decision-making for diagnosis of the fault. Next, the residual delivered by the adaptive MSMO (AMSMO) is split into windows, and each window is characterized by a numerical parameter. Finally, the machine learning technique known as a decision tree adaptively derives the threshold values that are used for problems of fault detection and fault identification in this work. Due to control of the effective fault, a surface automated new sliding mode controller (SANSMC) is presented in this work. To address the challenge of chattering and unlimited uncertainties (faults), the AMSMO is applied to the sliding mode controller (SMC). In addition, the surface-automated technique is used to fine-tune the surface coefficient to reduce the chattering and faults in the robot manipulator. The results show that the machine learning-based automated robust hybrid observer significantly improves the robustness, reliability, and accuracy of FEDIT in unknown conditions. © 2020 - IOS Press and the authors. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-189109,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096997048&doi=10.3233%2fJIFS-189109&partnerID=40&md5=0fc7f8e8f105cf7996fe791dc728f766,2020,2021-07-20 15:49:26,2021-07-20 15:49:26
7Q99D4U2,journalArticle,2019,"Picklum, M.; Beetz, M.",MATCALO: Knowledge-enabled machine learning in materials science,"Industrial and technological innovations constantly call for the development of materials that meet specific and novel requirement profiles. However, the engineering of materials with the desired properties is a time-, cost-, and labor-intensive process as changing the composition and the parameterization of processing steps yields an unimaginably large search space. It is therefore crucial for the future engineering of materials to be conducted in a more goal-directed and more efficient way to generate informative hypotheses revealing particularly promising experiments and thus guiding the exploration process in a more informed fashion. In this paper, we present MATCALO, an intelligent, cognitive assistant system that supports materials scientists in developing novel materials. MATCALO combines modern machine learning techniques with machine-interpretable semantic knowledge in order to model representations of relationships between materials, processes and properties and allow reasoning about them. We showcase a prototypical approach for generating such hypotheses by reverse-querying regression models learned from experimental data. However, a workable set of experiments alone generally does not suffice as it will never cover an adequately large area of the search space, which led us to the conclusion that additional background knowledge is required to build more reliable models. The readers can assure themselves of the feasibility of this approach by testing the web-based interface using the MATCALO system which will be made available to the broad community. Given a requirement profile, the system generates hypotheses on how these criteria can be achieved. © 2019 The Authors",Computational Materials Science,10.1016/j.commatsci.2019.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062905511&doi=10.1016%2fj.commatsci.2019.03.005&partnerID=40&md5=35df217ce0c6c0e944250806d6fcc06f,2019,2021-07-20 15:49:26,2021-07-20 15:49:26
AKF9GTCC,journalArticle,2021,"Mittal, V.; Gangodkar, D.; Pant, B.",Deep Graph-Long Short-Term Memory: A Deep Learning Based Approach for Text Classification,"Multi-label text classification is a challenging task in many real applications. Mostly, in all the traditional techniques, word2vec is used to show the sequential information among text. However, use of word2vec ignores logic and context relationship among text, and we treat each label as an individual unit. Therefore, the existing techniques failed to reflect the real scenarios and to gain the semantic information regarding the relationship among texts. In this paper, we propose a model Deep Graph-Long Short-Term Memory (DG-LSTM) for multi-label text classification. In the proposed model, we store the documents using the graph database. Initially, the documents are pre-processed using standard dictionaries, and afterwards it generates the classified dictionaries. These classified dictionaries are used to generate the subgraphs. The model maintains a lookup table to reduce the search space for the new documents. For classification, the model uses the deep learning technique DG-LSTM. DG-LSTM is using Deep Graph_Rectified Linear Unit activation function to avoid blow-up and dying neuron problem of Rectified Linear Unit activation function. We verify the proposed model on the legal case of Indian judiciary. The results show that the proposed model has achieved 99% accuracy to classify the fresh case into its corresponding category. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC, part of Springer Nature.",Wireless Personal Communications,10.1007/s11277-021-08331-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102699252&doi=10.1007%2fs11277-021-08331-4&partnerID=40&md5=a13427f15564e7ab393da239346bc278,2021,2021-07-20 15:49:26,2021-07-20 15:49:26
ED2CMIRT,journalArticle,2021,"Ding, W.; Abdel-Basset, M.; Hawash, H.; Pedrycz, W.",Multimodal Infant Brain Segmentation by Fuzzy-informed Deep Learning,"Magnetic resonance imaging (MRI) is a prevailing method of modal infant brain tissue analysis that precisely segments brain tissue and is vitally important for diagnosis, remediation, and analysis of early brain development. To achieve such segmentation is challenging, particularly for the brain of a six-month-old, owing to several factors: 1) poor image quality; 2) isointense contrast between white and gray matter and the simple incomplete volume consequence of a tiny brain size; and 3) discrepancies in brain tissues, illumination settings, and the vagarious region. This paper addresses these challenges with a fuzzy-informed deep learning segmentation (FI-DL-Seg) network that takes T1- and T2-weighted MRIs as inputs. First, a fuzzy logic layer encodes input to the fuzzy domain. Second, A volumetric fuzzy pooling (VFP) layer models the local fuzziness of the volumetric convolutional maps by applying fuzzification, accumulation, and defuzzification on the adjacency feature map neighborhoods. Third, the VFP layer is employed to design the fuzzy-enabled multiscale feature learning (F-MFL) module to enable the extraction of brain features in different receptive fields. Finally, we redesign the Project &amp; Excite module using the VPF layer to enable modeling uncertainty during feature recalibration, and a comprehensive training paradigm is used to learn the ideal parameters of every building block. Extensive experimental comparative studies substantiate the efficiency and accuracy of the proposed model in terms of different evaluation metrics to solve multimodal infant brain segmentation problems on the iSeg-2017 dataset. IEEE",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2021.3052461,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099732176&doi=10.1109%2fTFUZZ.2021.3052461&partnerID=40&md5=ce23f69c1c840c24f121843bae97a42f,2021,2021-07-20 15:49:26,2021-07-20 15:49:26
4VRJLV4Q,journalArticle,2019,"Dijkstra, K.; van de Loosdrecht, J.; Schomaker, L.R.B.; Wiering, M.A.",Hyperspectral demosaicking and crosstalk correction using deep learning,"Precision agriculture using unmanned aerial vehicles (UAVs) is gaining popularity. These UAVs provide a unique aerial perspective suitable for inspecting agricultural fields. With the use of hyperspectral cameras, complex inspection tasks are being automated. Payload constraints of UAVs require low weight and small hyperspectral cameras; however, such cameras with a multispectral color filter array suffer from crosstalk and a low spatial resolution. The research described in this paper aims to reduce crosstalk and to increase spatial resolution using convolutional neural networks. We propose a similarity maximization framework which is trained to perform end-to-end demosaicking and crosstalk-correction of a 4 × 4 raw mosaic image. The proposed method produces a hyperspectral image cube with 16 times the spatial resolution of the original cube while retaining a median structural similarity (SSIM) index of 0.85 (compared to an SSIM of 0.55 when using bilinear interpolation). Furthermore, this paper provides insight into the beneficial effects of crosstalk for hyperspectral demosaicking and gives best practices for several architectural and hyperparameter variations as well as a theoretical reasoning behind certain observations. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Machine Vision and Applications,10.1007/s00138-018-0965-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050754381&doi=10.1007%2fs00138-018-0965-4&partnerID=40&md5=147ee3852f3df6a87892f09f29cef44f,2019,2021-07-20 15:49:26,2021-07-20 15:49:26
7N9CPQ7B,journalArticle,2019,"Zhang, W.; Liu, G.; Tian, G.",A Coarse to Fine Indoor Visual Localization Method Using Environmental Semantic Information,"In this paper, we focus on the camera localization problem using visual semantic information. In contrast to the state of the artworks which often use visual features to do localization, we here propose a coarse to a fine mechanism to localize the camera position. First, a semantic database including object information around the target environment is constructed using a deep learning method. Second, for the coarse step of the visual localization, we match class attributes of objects in the current frame to the object database and find candidate frames that have similar objects. Third, the most similar candidate frame to the current frame is selected by CNN features. For the fine step of localization, the final pose of the camera can be estimated using feature matching with semantic information. Compared to the state of the art visual localization methods, the proposed localization method based on semantic information has higher localization accuracy. Furthermore, the proposed framework is not only useful for visual localization, but also useful for other advanced tasks of robot, e.g., loop closing detection, object searching, and task reasoning. © 2019 IEEE.",IEEE Access,10.1109/ACCESS.2019.2899049,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062919420&doi=10.1109%2fACCESS.2019.2899049&partnerID=40&md5=8718eb05fc11d3ba1a5387cbd2b03abb,2019,2021-07-20 15:49:26,2021-07-20 15:49:26
TIRWL4WH,journalArticle,2020,"Antony Rosewelt, L.; Arokia Renjit, J.",A content recommendation system for effective e-learning using embedded feature selection and fuzzy DT based CNN,"This paper proposes a new content recommendation system which combines the newly proposed embedded feature selection method and the new Fuzzy Temporal Logic based Decision Tree incorporated Convolutional Neural Network classifier. The newly proposed embedded feature selection called Fuzzy Decision Tree and Weighted Gini-Index based Feature Selection Algorithm (FDTWGI-FSA) that contains the existing incorporated the Fuzzy Decision Tree (FDT) and the Weighted Gini-index based Feature Selection Algorithm (WGIFSA) for getting optimized feature subset. Moreover, an enhanced CNN and Fuzzy Temporal Decision Tree for performing the deep learning process which is able to identify the exact e-content from the huge volume of data with the help of the recommended features by the proposed embedded feature selection method. The exact e-content can be identified after performing the five-layer network structure for extracting the relevant features and it also can be classified by applying the Fuzzy Temporal Decision Tree for the e-learners. Finally, the proposed content recommendation system provides exact content to the e-learners according to their level of understanding and it also satisfies them by providing the exact high level contents. The experiments have been conducted for evaluating the proposed content recommendation system and compared with the existing classifier including the standard CNN. © 2020-IOS Press and the authors. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-191721,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088878886&doi=10.3233%2fJIFS-191721&partnerID=40&md5=d6e95b23bc57c38aafbc7ecf3cc736ec,2020,2021-07-20 15:49:26,2021-07-20 15:49:26
SUEAA78M,journalArticle,2017,"Yousef, M.; Nigatu, D.; Levy, D.; Allmer, J.; Henkel, W.","Categorization of species based on their microRNAs employing sequence motifs, information-theoretic sequence feature extraction, and k-mers","Background: Diseases like cancer can manifest themselves through changes in protein abundance, and microRNAs (miRNAs) play a key role in the modulation of protein quantity. MicroRNAs are used throughout all kingdoms and have been shown to be exploited by viruses to modulate their host environment. Since the experimental detection of miRNAs is difficult, computational methods have been developed. Many such tools employ machine learning for pre-miRNA detection, and many features for miRNA parameterization have been proposed. To train machine learning models, negative data is of importance yet hard to come by; therefore, we recently started to employ pre-miRNAs from one species as positive data versus another species’ pre-miRNAs as negative examples based on sequence motifs and k-mers. Here, we introduce the additional usage of information-theoretic (IT) features. Results: Pre-miRNAs from one species were used as positive and another species’ pre-miRNAs as negative training data for machine learning. The categorization capability of IT and k-mer features was investigated. Both feature sets and their combinations yielded a very high accuracy, which is as good as the previously suggested sequence motif and k-mer based method. However, for obtaining a high performance, a sufficiently large phylogenetic distance between the species and sufficiently high number of pre-miRNAs in the training set is required. To examine the contribution of the IT and k-mer features, an information gain-based feature ranking was performed. Although the top 3 are IT features, 80% of the top 100 features are k-mers. The comparison of all three individual approaches (motifs, IT, and k-mers) shows that the distinction of species based on their pre-miRNAs k-mers are sufficient. Conclusions: IT sequence feature extraction enables the distinction among species and is less computationally expensive than motif calculations. However, since IT features need larger amounts of data to have enough statistics for producing highly accurate results, future categorization into species can be effectively done using k-mers only. The biological reasoning for this is the existence of a codon bias between species which can, at least, be observed in exonic miRNAs. Future work in this direction will be the ab initio detection of pre-miRNA. In addition, prediction of pre-miRNA from RNA-seq can be done. © 2017, The Author(s).",Eurasip Journal on Advances in Signal Processing,10.1186/s13634-017-0506-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032857843&doi=10.1186%2fs13634-017-0506-8&partnerID=40&md5=74d7234f9d5c4f882085168926e0fc07,2017,2021-07-20 15:49:26,2021-07-20 15:49:26
JCDQ6NSI,journalArticle,2021,"Confalonieri, R.; Weyde, T.; Besold, T.R.; Moscoso del Prado Martín, F.",Using ontologies to enhance human understandability of global post-hoc explanations of black-box models,"The interest in explainable artificial intelligence has grown strongly in recent years because of the need to convey safety and trust in the ‘how’ and ‘why’ of automated decision-making to users. While a plethora of approaches has been developed, only a few focus on how to use domain knowledge and how this influences the understanding of explanations by users. In this paper, we show that by using ontologies we can improve the human understandability of global post-hoc explanations, presented in the form of decision trees. In particular, we introduce TREPAN Reloaded, which builds on TREPAN, an algorithm that extracts surrogate decision trees from black-box models. TREPAN Reloaded includes ontologies, that model domain knowledge, in the process of extracting explanations to improve their understandability. We tested the understandability of the extracted explanations by humans in a user study with four different tasks. We evaluate the results in terms of response times and correctness, subjective ease of understanding and confidence, and similarity of free text responses. The results show that decision trees generated with TREPAN Reloaded, taking into account domain knowledge, are significantly more understandable throughout than those generated by standard TREPAN. The enhanced understandability of post-hoc explanations is achieved with little compromise on the accuracy with which the surrogate decision trees replicate the behaviour of the original neural network models. © 2021 The Author(s)",Artificial Intelligence,10.1016/j.artint.2021.103471,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101138224&doi=10.1016%2fj.artint.2021.103471&partnerID=40&md5=bbcfd5ff1a22f14273d51db4dfb39fdf,2021,2021-07-20 15:49:26,2021-07-20 15:49:26
7I4CIKII,journalArticle,2017,"Carmantini, G.S.; beim Graben, P.; Desroches, M.; Rodrigues, S.",A modular architecture for transparent computation in recurrent neural networks,"Computation is classically studied in terms of automata, formal languages and algorithms; yet, the relation between neural dynamics and symbolic representations and operations is still unclear in traditional eliminative connectionism. Therefore, we suggest a unique perspective on this central issue, to which we would like to refer as transparent connectionism, by proposing accounts of how symbolic computation can be implemented in neural substrates. In this study we first introduce a new model of dynamics on a symbolic space, the versatile shift, showing that it supports the real-time simulation of a range of automata. We then show that the Gödelization of versatile shifts defines nonlinear dynamical automata, dynamical systems evolving on a vectorial space. Finally, we present a mapping between nonlinear dynamical automata and recurrent artificial neural networks. The mapping defines an architecture characterized by its granular modularity, where data, symbolic operations and their control are not only distinguishable in activation space, but also spatially localizable in the network itself, while maintaining a distributed encoding of symbolic representations. The resulting networks simulate automata in real-time and are programmed directly, in the absence of network training. To discuss the unique characteristics of the architecture and their consequences, we present two examples: (i) the design of a Central Pattern Generator from a finite-state locomotive controller, and (ii) the creation of a network simulating a system of interactive automata that supports the parsing of garden-path sentences as investigated in psycholinguistics experiments. © 2016 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2016.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994319086&doi=10.1016%2fj.neunet.2016.09.001&partnerID=40&md5=748c2c3637c624ca09c0929de9c95e1d,2017,2021-07-20 15:49:27,2021-07-20 15:49:27
EFNRXNIV,journalArticle,2014,"Saccà, C.; Teso, S.; Diligenti, M.; Passerini, A.",Improved multi-level protein-protein interaction prediction with semantic-based regularization,"Background: Protein-protein interactions can be seen as a hierarchical process occurring at three related levels: proteins bind by means of specific domains, which in turn form interfaces through patches of residues. Detailed knowledge about which domains and residues are involved in a given interaction has extensive applications to biology, including better understanding of the binding process and more efficient drug/enzyme design. Alas, most current interaction prediction methods do not identify which parts of a protein actually instantiate an interaction. Furthermore, they also fail to leverage the hierarchical nature of the problem, ignoring otherwise useful information available at the lower levels; when they do, they do not generate predictions that are guaranteed to be consistent between levels. Results: Inspired by earlier ideas of Yip et al. (BMC Bioinformatics 10:241, 2009), in the present paper we view the problem as a multi-level learning task, with one task per level (proteins, domains and residues), and propose a machine learning method that collectively infers the binding state of all object pairs. Our method is based on Semantic Based Regularization (SBR), a flexible and theoretically sound machine learning framework that uses First Order Logic constraints to tie the learning tasks together. We introduce a set of biologically motivated rules that enforce consistent predictions between the hierarchy levels. Conclusions: We study the empirical performance of our method using a standard validation procedure, and compare its performance against the only other existing multi-level prediction technique. We present results showing that our method substantially outperforms the competitor in several experimental settings, indicating that exploiting the hierarchical nature of the problem can lead to better predictions. In addition, our method is also guaranteed to produce interactions that are consistent with respect to the protein-domain-residue hierarchy. © 2014 Saccàet al.; licensee BioMed Central Ltd.",BMC Bioinformatics,10.1186/1471-2105-15-103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899649223&doi=10.1186%2f1471-2105-15-103&partnerID=40&md5=1b43633e7e4c2afdb4300ef7ac671712,2014,2021-07-20 15:49:27,2021-07-20 15:49:27
M934ED4J,journalArticle,2020,"Leelavathy, S.; Nithya, M.",Public opinion mining using natural language processing technique for improvisation towards smart city,"In this digital world integrating smart city concepts, there is a tremendous scope and need for e-governance applications. Now people analyze the opinion of others before purchasing any product, hotel booking, stepping onto restaurants etc. and the respective user share their experience as a feedback towards the service. But there is no e-governance platform to obtain public opinion grievances towards covid19, government new laws, policies etc. With the growing availability and emergence of opinion rich information’s, new opportunities and challenges might arise in developing a technology for mining the huge set of public messages, opinions and alert the respective departments to take necessary actions and also nearby ambulances if its related to covid-19. To overcome this pandemic situation a natural language processing based efficient e-governance platform is demandful to detect the corona positive patients and provide transparency on the covid count and also alert the respective health ministry and nearby ambulance based on the user voice inputs. To convert the public voice messages into text, we used Hidden Markov Models (HMMs). To identify respective government department responsible for the respective user voice input, we perform pre-processing, part of speech, unigram, bigram, trigram analysis and fuzzy logic (machine learning technique). After identifying the responsible department, we perform 2 methods, (1) Automatic alert e-mail and message to the government departmental officials and nearby ambulance or covid camp if the user input is related to covis19. (2) Ticketing system for public and government officials monitoring. For experimental results, we used Java based web and mobile application to execute the proposed methodology. Integration of HMM, Fuzzy logic provides promising results. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",International Journal of Speech Technology,10.1007/s10772-020-09766-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096022565&doi=10.1007%2fs10772-020-09766-z&partnerID=40&md5=451d9cb8aae0a99d5e178d6f623b035f,2020,2021-07-20 15:49:27,2021-07-20 15:49:27
FAFM4J32,journalArticle,2015,"Aydın, İ.; Karaköse, M.; Akın, E.",Combined intelligent methods based on wireless sensor networks for condition monitoring and fault diagnosis,"This study presents new combined methods based on multiple wireless sensor system for real-time condition monitoring of electric machines. The established experimental setup measures multiple signals such as current and vibration on a common wireless node. The proposed methods are low-cost, intelligent, and non-intrusive. The proposed wireless network based framework is useful for analyzing and monitoring of signals from multiple induction motors. Motor current and vibration signals are simultaneously read from multiple motors through wireless nodes and the faults are estimated using two combined methods. Phase space analysis of vibration data and amplitudes of three phase current signals are used as features in combined intelligent classifiers. Stator related faults are diagnosed by analyzing the magnitudes of read current signals with fuzz logic. The vibration signal taken from the two-axis acceleration meter is normalized and phase space of this signal is constructed. The change in phase spaces are analyzed with machine learning techniques based on Gaussian Mixture Models and Bayesian classification to detect bearing faults. The phase space of vibration signals is constructed by using non-linear time series analysis and Gaussian mixtures are obtained for healthy and each faulty conditions. The constructed mixture models are classified according to their distribution on phase space by using Bayesian classification method. Four motor operating conditions- stator open phase fault, one and two bearing imbalance faults, and healthy condition are considered and related signals are obtained to evaluate the proposed system. The accuracy of the proposed system is confirmed by experimental data. © 2013, Springer Science+Business Media New York.",Journal of Intelligent Manufacturing,10.1007/s10845-013-0829-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937978544&doi=10.1007%2fs10845-013-0829-8&partnerID=40&md5=32ef5fc0653418cfd152e20e3470e4a3,2015,2021-07-20 15:49:27,2021-07-20 15:49:27
PE6WNP93,journalArticle,2020,"Muñoz, L.; Trujillo, L.; Silva, S.",Transfer learning in constructive induction with Genetic Programming,"Transfer learning (TL) is the process by which some aspects of a machine learning model generated on a source task is transferred to a target task, to simplify the learning required to solve the target. TL in Genetic Programming (GP) has not received much attention, since it is normally assumed that an evolved symbolic expression is specifically tailored to a problem’s data and thus cannot be used in other problems. The goal of this work is to present a broad and diverse study of TL in GP, considering a varied set of source and target tasks, and dealing with questions that have received little, or no attention, in previous GP literature. In particular, this work studies the performance of transferred solutions when the source and target tasks are from different domains, and when they do not share a similar input feature space. Additionally, the relationship between the success and failure of transferred solutions is studied, considering different source and target tasks. Finally, the predictability of TL performance is analyzed for the first time in GP literature. GP-based constructive induction of features is used to carry out the study, a wrapper-based approach where GP is used to construct feature transformations and an additional learning algorithm is used to fit the final model. The experimental work presents several notable results and contributions. First, TL is capable of generating solutions that outperform, in many cases, baseline methods in classification and regression tasks. Second, it is shown that some problems are good source problems while others are good targets in a TL system. Third, the transferability of solutions is not necessarily symmetric between two problems. Finally, results show that it is possible to predict the success of TL in some cases, particularly in classification tasks. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Genetic Programming and Evolvable Machines,10.1007/s10710-019-09368-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074947704&doi=10.1007%2fs10710-019-09368-y&partnerID=40&md5=cd8c2b91b8438e2f1a557eed6f25c8b5,2020,2021-07-20 15:49:27,2021-07-20 15:49:27
HNJ8U3YA,journalArticle,2015,"Li, N.-J.; Wang, W.-J.; James Hsu, C.-C.",Hybrid particle swarm optimization incorporating fuzzy reasoning and weighted particle,"In conventional particle swarm optimization (PSO), the search behavior has two principal forces of the moving direction to guide the particles toward their personal best (pbest) and the global best (gbest) positions. However, if the particle lies too close to either pbest or gbest, the optimization of the swarm is likely to be trapped into a local optimum. To overcome the local optimum problem, this paper proposes a hybrid particle swarm optimization incorporating fuzzy reasoning and a weighted particle (HPSOFW) to establish a novel search behavior model to improve the searching capability of the conventional PSO algorithm. In the proposed search behavior model, a weighted particle is incorporated into the algorithm to modify the searching direction and fuzzy reasoning is used to adjust an attraction factor and inertia weight such that the particle has a better opportunity to find the optimal solution. Based on adjustment of the attraction factor and inertia weight, the proposed search behavior model takes into consideration of both search strategies of exploitation (local search) and exploration (global search) during the optimization. Simulation results show that the proposed HPSOFW has much better performance than that of the existing optimization algorithms for ten benchmark functions. To demonstrate its feasibility, the proposed HPSOFW is also applied to the learning of neural network for nonlinear system modeling before applying it to model an energy consumption system with satisfactory performance. © 2015 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2015.04.045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945300804&doi=10.1016%2fj.neucom.2015.04.045&partnerID=40&md5=a1f56cc3938f63630cad6f368e552026,2015,2021-07-20 15:49:27,2021-07-20 15:49:27
CUV8L836,conferencePaper,2013,"Fridman, N.; Kaminka, G.A.",Using qualitative reasoning for social simulation of crowds,"The ability to model and reason about the potential violence level of a demonstration is important to the police decision making process. Unfortunately, existing knowledge regarding demonstrations is composed of partial qualitative descriptions without complete and precise numerical information. In this article we describe a first attempt to use qualitative reasoning techniques to model demonstrations. To our knowledge, such techniques have never been applied to modeling and reasoning regarding crowd behaviors, nor in particular demonstrations. We develop qualitative models consistent with the partial, qualitative social science literature, allowing us to model the interactions between different factors that influence violence in demonstrations. We then utilize qualitative simulation to predict the potential eruption of violence, at various levels, based on a description of the demographics, environmental settings, and police responses. We incrementally present and compare three such qualitative models. The results show that while two of these models fail to predict the outcomes of real-world events reported and analyzed in the literature, one model provides good results.We also examine whether a popular machine learning algorithm (decision tree learning) can be used. While the results show that the decision trees provide improved predictions, we show that the QR models can be more sensitive to changes, and can account for what-if scenarios, in contrast to decision trees. Moreover, we introduce a novel analysis algorithm that analyzes the QR simulations, to automatically determine the factors that are most important in influencing the outcome in specific real-world demonstrations. We show that the algorithm identifies factors that correspond to experts' analysis of these events. © 2013 ACM.",Computer Communication Review,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880426079&partnerID=40&md5=58e76dd9f496c15d173cefa981129889,2013,2021-07-20 15:49:27,2021-07-20 15:49:27
QHC2XSLA,journalArticle,2013,"Fridman, N.; Kaminka, G.A.",Using qualitative reasoning for social simulation of crowds,"The ability to model and reason about the potential violence level of a demonstration is important to the police decision making process. Unfortunately, existing knowledge regarding demonstrations is composed of partial qualitative descriptions without complete and precise numerical information. In this article we describe a first attempt to use qualitative reasoning techniques to model demonstrations. To our knowledge, such techniques have never been applied to modeling and reasoning regarding crowd behaviors, nor in particular demonstrations. We develop qualitative models consistent with the partial, qualitative social science literature, allowing us to model the interactions between different factors that influence violence in demonstrations. We then utilize qualitative simulation to predict the potential eruption of violence, at various levels, based on a description of the demographics, environmental settings, and police responses. We incrementally present and compare three such qualitative models. The results show that while two of these models fail to predict the outcomes of real-world events reported and analyzed in the literature, one model provides good results.We also examine whether a popular machine learning algorithm (decision tree learning) can be used. While the results show that the decision trees provide improved predictions, we show that the QR models can be more sensitive to changes, and can account for what-if scenarios, in contrast to decision trees. Moreover, we introduce a novel analysis algorithm that analyzes the QR simulations, to automatically determine the factors that are most important in influencing the outcome in specific real-world demonstrations. We show that the algorithm identifies factors that correspond to experts' analysis of these events. ©2013 ACM.",ACM Transactions on Intelligent Systems and Technology,10.1145/2483669.2483687,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880213970&doi=10.1145%2f2483669.2483687&partnerID=40&md5=7554ba8cb83243aff4357f8717f5e8b9,2013,2021-07-20 15:49:27,2021-07-20 15:49:27
Y8TSGFDR,journalArticle,2018,"Shah, N.; Chaudhari, P.; Varghese, K.",Runtime Programmable and Memory Bandwidth Optimized FPGA-Based Coprocessor for Deep Convolutional Neural Network,"The deep convolutional neural network (DCNN) is a class of machine learning algorithms based on feed-forward artificial neural network and is widely used for image processing applications. Implementation of DCNN in real-world problems needs high computational power and high memory bandwidth, in a power-constrained environment. A general purpose CPU cannot exploit different parallelisms offered by these algorithms and hence is slow and energy inefficient for practical use. We propose a field-programmable gate array (FPGA)-based runtime programmable coprocessor to accelerate feed-forward computation of DCNNs. The coprocessor can be programmed for a new network architecture at runtime without resynthesizing the FPGA hardware. Hence, it acts as a plug-and-use peripheral for the host computer. Caching is implemented for input features and filter weights using on-chip memory to reduce the external memory bandwidth requirement. Data are prefetched at several stages to avoid stalling of computational units and different optimization techniques are used to efficiently reuse the fetched data. Dataflow is dynamically adjusted in runtime for each DCNN layer to achieve consistent computational throughput across a wide range of input feature sizes and filter sizes. The coprocessor is prototyped using Xilinx Virtex-7 XC7VX485T FPGA-based VC707 board and operates at 150 MHz. Experimental results show that our implementation is 15× energy efficient than highly optimized CPU implementation and achieves consistent computational throughput of more than 140 G operations/s for a wide range of input feature sizes and filter sizes. Off-chip memory transactions decrease by 111× due to the use of the on-chip cache. © 2018 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2018.2815085,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045209972&doi=10.1109%2fTNNLS.2018.2815085&partnerID=40&md5=cc86fc363ebc7b25f91eb0e73f723c24,2018,2021-07-20 15:49:27,2021-07-20 15:49:27
PBSXQ4T7,journalArticle,2020,"Chen, G.; Li, S.",Research on location fusion of spatial geological disaster based on fuzzy SVM,"In order to effectively improve geological disaster response capacity, disaster tolerance capacity, reduce human and financial losses and other aspects of spatial data fusion plays a key role. Whether the location information can be effectively fused is able to monitor the occurrence of geological hazards and effectively identify the existing risks. In machine learning, the similarity and complementarity between support vector machine and fuzzy inference is the basis of their fusion. Support vector machine can achieve knowledge acquisition and learning, while fuzzy inference has the ability to infer knowledge rules. Aiming at the different attributes and dimensions of information spatial location data, a fuzzy fusion method based on support vector machine is proposed to describe Support vector machine related theories and models. Comparing the efficiency of support vector machine-fusion algorithm on GPLUS, OKLAHOMA and UNC with the other three algorithms, there is a great advantage in RMSE and time. The algorithm in this paper also has good performance in the three data sets on F1, which shows that the algorithm has a good effect. © 2020",Computer Communications,10.1016/j.comcom.2020.02.033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079376480&doi=10.1016%2fj.comcom.2020.02.033&partnerID=40&md5=b4bead7b32eb88fb0655ff184a5d207e,2020,2021-07-20 15:49:27,2021-07-20 15:49:27
ZJAZ8NWL,journalArticle,2020,"Yan, J.; Li, X.; Sun, S.X.; Shi, Y.; Wang, H.",A BDI Modeling Approach for Decision Support in Supply Chain Quality Inspection,"Quality inspection, a widely adopted practice in supply chains, measures whether delivered products conform to prespecified quality requirements. Due to potential economic benefits, suppliers may deliberately manipulate products to falsify test results. However, unqualified products could cause severe problems in supply chains or even tragedies, such as China's tainted milk scandal. In this paper, we propose a belief-desire-intention modeling approach to predict suppliers' behavior and provide inspection suggestions to buyers to overcome the problem. Assuming access to the production process, our approach can represent suppliers' knowledge of production and deception to mimic their reasoning processes and predict their deception intentions. This flexible approach can also adapt to environmental changes and deliver effective results. In this paper, we build a prototype system for supply chain quality inspections based on the proposed method. We conduct laboratory experiments to collect data for computational assessments of the performance of the prototype. It is shown that our proposed approach is more accurate than classic machine learning methods in detecting suppliers' deceptions. © 2017 IEEE.","IEEE Transactions on Systems, Man, and Cybernetics: Systems",10.1109/TSMC.2017.2756105,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038865765&doi=10.1109%2fTSMC.2017.2756105&partnerID=40&md5=3424a2b1938b451d59427870f7845b10,2020,2021-07-20 15:49:27,2021-07-20 15:49:27
S6GMEEU8,journalArticle,2020,"De La Rosa, E.; Yu, W.",Data-Driven Fuzzy Modeling Using Restricted Boltzmann Machines and Probability Theory,"Fuzzy modeling has many advantages over nonfuzzy methods, such as robustness with respect to uncertainties and less sensitivity to the varying dynamics of nonlinear systems. Data-driven fuzzy modeling needs to extract fuzzy rules from input and output data, and to train the fuzzy parameters of the fuzzy model. This paper takes advantages from deep learning, probability theory, fuzzy modeling, and extreme learning machines (ELMs). Restricted Boltzmann machine (RBM) and probability theory are used to overcome some common problems in data-driven modeling methods. The RBM is modified such that it can be trained with continuous values. A probability-based clustering method is proposed to partition the hidden features from the RBM. The obtained fuzzy rules have probability measurement. ELM and an optimization method are applied to train the fuzzy model. The proposed method is validated with two benchmark problems. © 2013 IEEE.","IEEE Transactions on Systems, Man, and Cybernetics: Systems",10.1109/TSMC.2018.2812156,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044086703&doi=10.1109%2fTSMC.2018.2812156&partnerID=40&md5=452149b4df56a80f77aff218f0cc8ac9,2020,2021-07-20 15:49:27,2021-07-20 15:49:27
TXXVBA8R,journalArticle,2019,"D'Agostino, D.; Quarati, A.; Clematis, A.; Morganti, L.; Corni, E.; Giansanti, V.; Cesini, D.; Merelli, I.",SoC-based computing infrastructures for scientific applications and commercial services: Performance and economic evaluations,"Energy consumption represents one of the most relevant issues by now in operating computing infrastructures, from traditional High Performance Computing Centers to Cloud Data Centers. Low power System-on-Chip (SoC) architectures, originally developed in the context of mobile and embedded technologies, are becoming attractive also for scientific and industrial applications given their increasing computing performances, coupled with relatively low costs and power demands. In this paper, we investigate the performance of the most representative SoCs for a computational intensive N-body benchmark, a simple deep learning based application and a real-life application taken from the field of molecular biology. The goal is to assess the trade-off among time-to-solution, energy-to-solution and economical aspects for both scientific and commercial purposes they are able to achieve in comparison to traditional server-grade architectures adopted in present infrastructures. © 2019 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2019.01.024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061100184&doi=10.1016%2fj.future.2019.01.024&partnerID=40&md5=b8e6035ae0ef952cf85c355f0e9e5f59,2019,2021-07-20 15:49:27,2021-07-20 15:49:27
ZT2VTTLA,journalArticle,2021,"Saini, J.K.; Bansal, D.",Detecting online recruitment of terrorists: towards smarter solutions to counter terrorism,"Increased usage of social media led to formation of online communities of terrorist groups for discussing many violent plans. These online communities present big data to researchers to identify hidden patterns and behaviors to generate actionable intelligence which can be useful for security agencies. Recruiting new people over online social media is on emerging trend which presents how internet is exploited by terrorist groups. In this paper, we present automatic model to detect online cyber recruitment over social media. Online discussions regarding recruitment of terrorists or building new connections for recruitment of violent extremist have been labelled. Two experts labelled 730 messages from five dark web discussion forums named ‘Ansar1’, ‘Gawaher’, ‘Islamic Awakening’, ‘Islamic Network’ and ‘Mywic’ as YES (recruitment) and NO (non-recruitment). Statistical analysis has been done on two independent labelled messages to find mutual agreement between two judges. Kohen’s Kappa coefficient computed is 0.87 at p = 0.01 which signifies higher mutual agreement. Five machine learning classifiers namely support vector machine, logic boosting, random forest, generalized linear model and maximum entropy based model are developed to further classify recruitment labels. To the best of our knowledge no such work has been done by collaborating data from multiple dark web discussion forums to automatic identify online recruitment of terrorists. Our proposed model presents smart solution with usage of computational techniques to quantify terrorist behavior analysis and detect online recruitment of violent extremists over online social media and dark web forums. © 2021, Bharati Vidyapeeth's Institute of Computer Applications and Management.",International Journal of Information Technology (Singapore),10.1007/s41870-021-00620-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101771797&doi=10.1007%2fs41870-021-00620-2&partnerID=40&md5=2b03476aad0f0f5a71ceb9bec99a24b6,2021,2021-07-20 15:49:28,2021-07-20 15:49:28
SZ3PYU6U,journalArticle,2020,"Yu, H.; Lei, X.; Song, Z.; Liu, C.; Wang, J.",Supervised Network-Based Fuzzy Learning of EEG Signals for Alzheimer's Disease Identification,"Accurate identification of Alzheimer's disease (AD) with electroencephalograph (EEG) is crucial in the clinical diagnosis of neurological disorders. However, the effectiveness and accuracy of manually labeling EEG signals are barely satisfactory, due to lacking effective biomarkers. In this paper, we propose a novel machine learning method network-based Takagi-Sugeno-Kang (N-TSK) for AD identification which employs the complex network theory and TSK fuzzy system. With the construction of functional network of AD subjects, the topological features of weighted and unweighted networks are extracted. Taken the network parameters as independent inputs, a fuzzy-system-based TSK model is established and further trained to identify AD EEG signals. Experimental results demonstrate the effectiveness of the proposed scheme in AD identification and ability of N-TSK fuzzy classifiers. The highest accuracy can achieve 97.3%for patients with closed eyes and 94.78%with open eyes. In addition, the performance of weighted N-TSK largely exceeds unweighted N-TSK. By further optimizing the network features utilized in the N-TSK fuzzy classifiers, it is found that local efficiency and clustering coefficient are the most effective factors in AD identification. This work provides a potential tool for identifying neurological disorders from the perspective of functional networks with EEG signal, especially contributing to the diagnosis and identification of AD. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2019.2903753,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077800858&doi=10.1109%2fTFUZZ.2019.2903753&partnerID=40&md5=50e856c91970b6137ca5e5ba63e5ce5f,2020,2021-07-20 15:49:28,2021-07-20 15:49:28
2XL6T2RN,journalArticle,2019,"Delvaux, J.","Machine-Learning Attacks on PolyPUFs, OB-PUFs, RPUFs, LHS-PUFs, and PUF-FSMs","A physically unclonable function (PUF) is a circuit of which the input-output behavior is designed to be sensitive to the random variations of its manufacturing process. This building block hence facilitates the authentication of any given device in a population of identically laid-out silicon chips, similar to the biometric authentication of a human. The focus and novelty of this paper is the development of efficient impersonation attacks on the following five Arbiter PUF-based authentication protocols: 1) the so-called Poly PUF protocol of Konigsmark et al. as published in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems in 2016; 2) the so-called OB-PUF protocol of Gao et al. as presented at the IEEE Conference PerCom 2016; 3) the so-called RPUF protocol of Ye et al. as presented at the IEEE Conference AsianHOST 2016; 4) the so-called LHS-PUF protocol of Idriss and Bayoumi as presented at the IEEE Conference RFID-TA 2017; and 5) the so-called PUF-FSM protocol of Gao et al. as published in the IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems in 2018. The common flaw of all five designs is that the use of lightweight obfuscation logic provides insufficient protection against machine-learning attacks. © 2005-2012 IEEE.",IEEE Transactions on Information Forensics and Security,10.1109/TIFS.2019.2891223,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065619494&doi=10.1109%2fTIFS.2019.2891223&partnerID=40&md5=bae26d78fa7e9eea42a020e38b9c930e,2019,2021-07-20 15:49:28,2021-07-20 15:49:28
AAX8B7EJ,journalArticle,2019,"Shen, K.-Y.; Sakai, H.; Tzeng, G.-H.",Comparing Two Novel Hybrid MRDM Approaches to Consumer Credit Scoring Under Uncertainty and Fuzzy Judgments,"In the recent years, various statistical and computational intelligence or machine learning techniques have contributed to the progress of automation or semiautomation for measuring consumer credit scoring in the banking sector. However, most of the Taiwanese commercial banks still rely on seasoned staffs’ judgments on making the final approvals or rejections. To enhance the understanding and transparency of a decision support system (or model) that can assist bank staffs on making their consumer credit loan decisions—while uncertainty exist—is of high business value. One of the promising approaches is multiple rule-based decision-making (MRDM), a subfield of the hybrid multiple criteria decision-making that leverages the advantages of machine learning, soft computing, and decision methods (or techniques). The MRDM approach reveals comprehensible logics (rules or patterns) that can be justified and compared with the existing knowledge of veterans to reinforce the confidence of their judgments. Therefore, in the present study, we propose and compare two MRDM approaches in assisting decision makers on the consumer credit loan evaluations. A set of historical data from a commercial bank in Taiwan is analyzed for illustrating the plausible pros and cons of the two approaches with discussions. © 2018, Taiwan Fuzzy Systems Association and Springer-Verlag GmbH Germany, part of Springer Nature.",International Journal of Fuzzy Systems,10.1007/s40815-018-0525-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063694766&doi=10.1007%2fs40815-018-0525-0&partnerID=40&md5=b8b5b9f2f83ff61949edf319e14ef60f,2019,2021-07-20 15:49:28,2021-07-20 15:49:28
U42IUQEN,journalArticle,2014,"Goumopoulos, C.; O'Flynn, B.; Kameas, A.",Automated zone-specific irrigation with wireless sensor/actuator network and adaptable decision support,"Precision irrigation based on the ""speaking plant"" approach can save water and maximize crop yield, but implementing irrigation control can be challenging in system integration and decision making. In this paper we describe the design of an adaptable decision support system and its integration with a wireless sensor/actuator network (WSAN) to implement autonomous closed-loop zone-specific irrigation. Using an ontology for defining the application logic emphasizes system flexibility and adaptability and supports the application of automatic inferential and validation mechanisms. Furthermore, a machine learning process has been applied for inducing new rules by analyzing logged datasets for extracting new knowledge and extending the system ontology in order to cope, for example, with a sensor type failure or to improve the accuracy of a plant state diagnosis. A deployment of the system is presented for zone specific irrigation control in a greenhouse setting. Evaluation of the developed system was performed in terms of derivation of new rules by the machine learning process, WSN performance and mote lifetime. The effectiveness of the developed system was validated by comparing its agronomic performance to traditional agricultural practices. © 2014 Elsevier B.V.",Computers and Electronics in Agriculture,10.1016/j.compag.2014.03.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899801180&doi=10.1016%2fj.compag.2014.03.012&partnerID=40&md5=34ef9b95893ca95cc87ee07abc85d156,2014,2021-07-20 15:49:28,2021-07-20 15:49:28
2PIESAC8,journalArticle,2019,"Riboni, D.; Murtas, M.",Sensor-based activity recognition: One picture is worth a thousand words,"In several domains, including healthcare and home automation, it is important to unobtrusively monitor the activities of daily living (ADLs) carried out by people at home. A popular approach consists in the use of sensors attached to everyday objects to capture user interaction, and ADL models to recognize the current activity based on the temporal sequence of used objects. Often, ADL models are automatically extracted from labeled datasets of activities and sensor events, using supervised learning techniques. Unfortunately, acquiring such datasets in smart homes is expensive and violates users’ privacy. Hence, an alternative solution consists in manually defining ADL models based on common sense, exploiting logic languages such as description logics. However, manual specification of ADL ontologies is cumbersome, and rigid ontological definitions fail to capture the variability of activity execution. In this paper, we introduce a radically new approach enabled by the recent proliferation of tagged visual contents available on the Web. Indeed, thanks to the popularity of social network applications, people increasingly share pictures and videos taken during the execution of every kind of activity. Often, shared contents are tagged with metadata, manually specified by their owners, that concisely describe the depicted activity. Those metadata represent an implicit activity label of the picture or video. Moreover, today's computer vision tools support accurate extraction of tags describing the situation and the objects that appear in the visual content. By reasoning with those tags and their corresponding activity labels, we can reconstruct accurate models of a comprehensive set of human activities executed in the most disparate situations. This approach overcomes the main shortcomings of existing techniques. Compared to supervised learning methods, it does not require the acquisition of training sets of sensor events and activities. Compared to knowledge-based methods, it does not involve any manual modeling effort, and it captures a comprehensive array of execution modalities. Through extensive experiments with large datasets of real-world ADLs, we show that this approach is practical and effective. © 2019 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2019.07.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069616209&doi=10.1016%2fj.future.2019.07.020&partnerID=40&md5=ea748f7257dce0f12621c91cb9933723,2019,2021-07-20 15:49:28,2021-07-20 15:49:28
HSBR4X9E,journalArticle,2021,"Šourek, G.; Železný, F.; Kuželka, O.",Beyond graph neural networks with lifted relational neural networks,"We introduce a declarative differentiable programming framework, based on the language of Lifted Relational Neural Networks, where small parameterized logic programs are used to encode deep relational learning scenarios through the underlying symmetries. When presented with relational data, such as various forms of graphs, the logic program interpreter dynamically unfolds differentiable computation graphs to be used for the program parameter optimization by standard means. Following from the declarative, relational logic-based encoding, this results into a unified representation of a wide range of neural models in the form of compact and elegant learning programs, in contrast to the existing procedural approaches operating directly on the computational graph level. We illustrate how this idea can be used for a concise encoding of existing advanced neural architectures, with the main focus on Graph Neural Networks (GNNs). Importantly, using the framework, we also show how the contemporary GNN models can be easily extended towards higher expressiveness in various ways. In the experiments, we demonstrate correctness and computation efficiency through comparison against specialized GNN frameworks, while shedding some light on the learning performance of the existing GNN models. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",Machine Learning,10.1007/s10994-021-06017-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107933318&doi=10.1007%2fs10994-021-06017-3&partnerID=40&md5=b2968eb7449989b12c78a1b0ef9fbb03,2021,2021-07-20 15:49:28,2021-07-20 15:49:28
WV6S8ZTM,journalArticle,2019,"Safaei, A.; Wu, Q.M.J.; Akilan, T.; Yang, Y.",System-on-a-Chip (SoC)-Based Hardware Acceleration for an Online Sequential Extreme Learning Machine (OS-ELM),"Machine learning algorithms such as those for object classification in images, video content analysis, and human action recognition are used to extract meaningful information from data recorded by image sensors and cameras. Among the existing machine learning algorithms for such purposes, extreme learning machines (ELMs) and online sequential ELMs (OS-ELMs) are well known for their computational efficiency and performance when processing large datasets. The latter approach was derived from the ELM approach and optimized for real-time application. However, OS-ELM classifiers are computationally demanding, and the existing state-of-the-art computing platforms are not efficient enough for embedded systems, especially for applications with strict requirements in terms of low power consumption, high throughput, and low latency. This paper presents the implementation of an ELM/OS-ELM in a customized system-on-a-chip field-programmable gate array-based architecture to ensure efficient hardware acceleration. The acceleration process comprises parallel extraction, deep pipelining, and efficient shared memory communication. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2878162,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055676851&doi=10.1109%2fTCAD.2018.2878162&partnerID=40&md5=9305aba0bef13f437517cb3710697696,2019,2021-07-20 15:49:28,2021-07-20 15:49:28
PJVC5HUM,journalArticle,2019,"Chen, D.; Zhang, X.; Wang, L.L.; Han, Z.",Prediction of Cloud Resources Demand Based on Hierarchical Pythagorean Fuzzy Deep Neural Network,"Having stepped into the era of information explosion, storing, processing and analyzing the vast data sometimes are quite intractable problems. However, it is impossible for personal computer or devices to tackle with such heavy workloads. Then, companies that provides cloud computational services come into business. Aiming at minimizing the expenditures, the most important part is how many cloud services the customers should reserve in advance because different amounts they consume will yield different expense. The emerging machine learning method provides a powerful tool to address such a prediction problem. In this paper, we propose a hierarchical Pythagorean fuzzy deep neural network to forecast the quantity of requisite cloud services. Besides the employment of fuzzy logic, the neural representation is also utilized as a complementary method. The knowledge acquired from fuzzy and neural perspectives are coalesced as the final transformed data to be put into the learning systems. Based on the anticipation of the deep neural network, the consumers are able to decide the amount of cloud services to purchase. Numerical results based on the real data set from Carnegie Mellon University demonstrate that the proposed model yields the economical predictions and outperforms the prediction by the traditional neural network. IEEE",IEEE Transactions on Services Computing,10.1109/TSC.2019.2906901,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070698945&doi=10.1109%2fTSC.2019.2906901&partnerID=40&md5=d9cda189571b6f6dbafccf343b34489a,2019,2021-07-20 15:49:28,2021-07-20 15:49:28
84WHP2H7,journalArticle,2017,"Ni, L.; Huang, H.; Liu, Z.; Joshi, R.V.; Yu, H.",Distributed in-memory computing on binary RRAM crossbar,"The recently emerging resistive random-access memory (RRAM) can provide nonvolatile memory storage but also intrinsic computing for matrix-vector multiplication, which is ideal for the low-power and high-throughput data analytics accelerator performed in memory. However, the existing RRAM crossbar-based computing is mainly assumed as a multilevel analog computing, whose result is sensitive to process nonuniformity as well as additional overhead from AD-conversion and I/O. In this article, we explore the matrix-vector multiplication accelerator on a binary RRAM crossbar with adaptive 1-bit-comparator-based parallel conversion. Moreover, a distributed in-memory computing architecture is also developed with the according control protocol. Both memory array and logic accelerator are implemented on the binary RRAM crossbar, where the logic-memory pair can be distributed with the control bus protocol. Experimental results have shown that compared to the analog RRAM crossbar, the proposed binary RRAM crossbar can achieve significant area savings with better calculation accuracy. Moreover, significant speedup can be achieved for matrix-vector multiplication in neural network-based machine learning such that the overall training and testing time can be both reduced. In addition, large energy savings can be also achieved when compared to the traditional CMOS-based out-of-memory computing architecture. © 2017 ACM.",ACM Journal on Emerging Technologies in Computing Systems,10.1145/2996192,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017184214&doi=10.1145%2f2996192&partnerID=40&md5=e69a29b22626bec932c094beb1196f52,2017,2021-07-20 15:49:28,2021-07-20 15:49:28
XHDRS8R3,journalArticle,2020,"Dashtipour, K.; Gogate, M.; Li, J.; Jiang, F.; Kong, B.; Hussain, A.",A hybrid Persian sentiment analysis framework: Integrating dependency grammar based rules and deep neural networks,"Social media hold valuable, vast and unstructured information on public opinion that can be utilized to improve products and services. The automatic analysis of such data, however, requires a deep understanding of natural language. Current sentiment analysis approaches are mainly based on word co-occurrence frequencies, which are inadequate in most practical cases. In this work, we propose a novel hybrid framework for concept-level sentiment analysis in Persian language, that integrates linguistic rules and deep learning to optimize polarity detection. When a pattern is triggered, the framework allows sentiments to flow from words to concepts based on symbolic dependency relations. When no pattern is triggered, the framework switches to its subsymbolic counterpart and leverages deep neural networks (DNN) to perform the classification. The proposed framework outperforms state-of-the-art approaches (including support vector machine, and logistic regression) and DNN classifiers (long short-term memory, and Convolutional Neural Networks) with a margin of 10–15% and 3–4% respectively, using benchmark Persian product and hotel reviews corpora. © 2019",Neurocomputing,10.1016/j.neucom.2019.10.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075436840&doi=10.1016%2fj.neucom.2019.10.009&partnerID=40&md5=ed6a3b3b822b73e53f969d9f34d7a645,2020,2021-07-20 15:49:28,2021-07-20 15:49:28
4NHJJHQJ,journalArticle,2020,"Ren, P.; Dai, H.; Chen, W.",Distributed cooperative learning over time-varying random networks using a gossip-based communication protocol,"Motivated by applications of distributed estimation and distributed decision making in wireless sensor networks (WSNs) and unmanned aerial vehicle (UAV) networks, we study a distributed learning problem over time-varying undirected random networks. Using a gossip-based communication protocol, a novel distributed cooperative learning (DCL) algorithm, termed the gossip-based DCL (GBDCL) algorithm, is presented to solve the problem by training the raw data distributed and blocked throughout different nodes. Exploiting the robustness of the gossip-based protocol, each node is guaranteed to build the same learning model in theory against random disconnections and communication route variations in the network topology. It is proved that the GBDCL algorithm converges to the optimal consensus asymptotically. The correctness and effectiveness of the presented GBDCL algorithm are verified in the theoretical analysis and simulations. © 2019 Elsevier B.V.",Fuzzy Sets and Systems,10.1016/j.fss.2019.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066083269&doi=10.1016%2fj.fss.2019.05.009&partnerID=40&md5=8047c42dd8d0f9e78371f5975d74da9d,2020,2021-07-20 15:49:28,2021-07-20 15:49:28
JAQHXN3H,journalArticle,2015,"Araghi, S.; Khosravi, A.; Creighton, D.",Intelligent cuckoo search optimized traffic signal controllers for multi-intersection network,"Traffic congestion in urban roads is one of the biggest challenges of 21 century. Despite a myriad of research work in the last two decades, optimization of traffic signals in network level is still an open research problem. This paper for the first time employs advanced cuckoo search optimization algorithm for optimally tuning parameters of intelligent controllers. Neural Network (NN) and Adaptive Neuro-Fuzzy Inference System (ANFIS) are two intelligent controllers implemented in this study. For the sake of comparison, we also implement Q-learning and fixed-time controllers as benchmarks. Comprehensive simulation scenarios are designed and executed for a traffic network composed of nine four-way intersections. Obtained results for a few scenarios demonstrate the optimality of trained intelligent controllers using the cuckoo search method. The average performance of NN, ANFIS, and Q-learning controllers against the fixed-time controller are 44%, 39%, and 35%, respectively. © 2015 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2015.01.063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923233737&doi=10.1016%2fj.eswa.2015.01.063&partnerID=40&md5=f69bea40d61d7639511d62cc18c9bf00,2015,2021-07-20 15:49:28,2021-07-20 15:49:28
3C5R8JC6,journalArticle,2014,"Farid, R.; Sammut, C.",Plane-based object categorisation using relational learning,"We use Inductive Logic Programming (ILP) to learn classifiers for generic object recognition from point clouds, as generated by 3D cameras, such as the Kinect. Each point cloud is segmented into planar surfaces. Each subset of planes that represents an object is labelled and predicates describing those planes and their relationships are used for learning. Our claim is that a relational description for classes of 3D objects can be built for robust object categorisation in real robotic application. To test the hypothesis, labelled sets of planes from 3D point clouds gathered during the RoboCup Rescue Robot competition are used as positive and negative examples for an ILP system. The robustness of the results is evaluated by 10-fold cross validation. In addition, common household objects that have curved surfaces are used for evaluation and comparison against a well-known non-relational classifier. The results show that ILP can be successfully applied to recognise objects encountered by a robot especially in an urban search and rescue environment. © 2013 The Author(s).",Machine Learning,10.1007/s10994-013-5352-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891371919&doi=10.1007%2fs10994-013-5352-9&partnerID=40&md5=65ac00500e4df29c2181ebefbfd6f112,2014,2021-07-20 15:49:28,2021-07-20 15:49:28
DMUJWDV8,journalArticle,2017,"Ghosh, S.; Li, J.; Cao, L.; Ramamohanarao, K.",Septic shock prediction for ICU patients via coupled HMM walking on sequential contrast patterns,"Background and objective Critical care patient events like sepsis or septic shock in intensive care units (ICUs) are dangerous complications which can cause multiple organ failures and eventual death. Preventive prediction of such events will allow clinicians to stage effective interventions for averting these critical complications. Methods It is widely understood that physiological conditions of patients on variables such as blood pressure and heart rate are suggestive to gradual changes over a certain period of time, prior to the occurrence of a septic shock. This work investigates the performance of a novel machine learning approach for the early prediction of septic shock. The approach combines highly informative sequential patterns extracted from multiple physiological variables and captures the interactions among these patterns via coupled hidden Markov models (CHMM). In particular, the patterns are extracted from three non-invasive waveform measurements: the mean arterial pressure levels, the heart rates and respiratory rates of septic shock patients from a large clinical ICU dataset called MIMIC-II. Evaluation and results For baseline estimations, SVM and HMM models on the continuous time series data for the given patients, using MAP (mean arterial pressure), HR (heart rate), and RR (respiratory rate) are employed. Single channel patterns based HMM (SCP-HMM) and multi-channel patterns based coupled HMM (MCP-HMM) are compared against baseline models using 5-fold cross validation accuracies over multiple rounds. Particularly, the results of MCP-HMM are statistically significant having a p-value of 0.0014, in comparison to baseline models. Our experiments demonstrate a strong competitive accuracy in the prediction of septic shock, especially when the interactions between the multiple variables are coupled by the learning model. Conclusions It can be concluded that the novelty of the approach, stems from the integration of sequence-based physiological pattern markers with the sequential CHMM model to learn dynamic physiological behavior, as well as from the coupling of such patterns to build powerful risk stratification models for septic shock patients. © 2016",Journal of Biomedical Informatics,10.1016/j.jbi.2016.12.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007438910&doi=10.1016%2fj.jbi.2016.12.010&partnerID=40&md5=834cde6801bd76c1d7d247c52a598577,2017,2021-07-20 15:49:28,2021-07-20 15:49:28
IV96DJWL,journalArticle,2020,"Akbari, Y.; Tabatabaei, S.",A New Method to Find a High Reliable Route in IoT by Using Reinforcement Learning and Fuzzy Logic,"Recently, Internet is moving quickly toward the interaction of objects, computing devices, sensors, and which are usually indicated as the Internet of things (IoT). The main monitoring infrastructure of IoT systems main monitoring infrastructure of IoT systems is wireless sensor networks. A wireless sensor network is composed of a large number of sensor nodes. Each sensor node has sensing, computing, and wireless communication capability. The sensor nodes send the data to a sink or a base station by using wireless transmission techniques However, sensor network systems require suitable routing structure to optimizing the lifetime. For providing reasonable energy consumption and optimizing the lifetime of WSNs, novel, efficient and economical schemes should be developed. In this paper, for enhancing network lifetime, a novel energy-efficient mechanism is proposed based on fuzzy logic and reinforcement learning. The fuzzy logic system and reinforcement learning is based on the remained energies of the nodes on the routes, the available bandwidth and the distance to the sink. This study also compares the performance of the proposed method with the fuzzy logic method and IEEE 802.15.4 protocol. The simulations of the proposed method which were carried out by OPNET (Optimum Network performance) indicated that the proposed method performed better than other protocols such as fuzzy logic and IEEE802.15.4 in terms of power consumption and network lifetime. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Wireless Personal Communications,10.1007/s11277-020-07086-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078041126&doi=10.1007%2fs11277-020-07086-8&partnerID=40&md5=cfa26f1b08fc3fcf2ef46d0886a1a268,2020,2021-07-20 15:49:28,2021-07-20 15:49:28
FIW3E95G,journalArticle,2019,"Wang, H.; Li, J.; He, K.",Hierarchical ensemble reduction and learning for resource-constrained computing,"Generic tree ensembles (such as Random Forest, RF) rely on a substantial amount of individual models to attain desirable performance. The cost of maintaining a large ensemble could become prohibitive in applications where computing resources are stringent. In this work, a hierarchical ensemble reduction and learning framework is proposed. Experiments show our method consistently outperforms RF in terms of both accuracy and retained ensemble size. In other words, ensemble reduction is achieved with enhancement in accuracy rather than degradation. The method can be executed efficiently, up to >590× time reduction than a recent ensemble reduction work. We also developed Boolean logic encoding techniques to directly tackle multiclass problems. Moreover, our framework bridges the gap between software-based ensemble methods and hardware computing in the IoT era. We developed a novel conversion paradigm that supports the automatic deployment of >500 trees on a chip. Our proposed method reduces power consumption and overall area utilization by >21.5% and >62%, respectively, comparing with RF. The hierarchical approach provides rich opportunities to balance between the computation (training and response time), the hardware resource (memory and energy), and accuracy. © 2019 Association for Computing Machinery.",ACM Transactions on Design Automation of Electronic Systems,10.1145/3365224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077752069&doi=10.1145%2f3365224&partnerID=40&md5=6841c8de49f138e35f22aa2a0af2e701,2019,2021-07-20 15:49:29,2021-07-20 15:49:29
5VLEPYHF,journalArticle,2021,"Nicolau, M.; Agapitos, A.",Choosing function sets with better generalisation performance for symbolic regression models,"Supervised learning by means of Genetic Programming (GP) aims at the evolutionary synthesis of a model that achieves a balance between approximating the target function on the training data and generalising on new data. The model space searched by the Evolutionary Algorithm is populated by compositions of primitive functions defined in a function set. Since the target function is unknown, the choice of function set’s constituent elements is primarily guided by the makeup of function sets traditionally used in the GP literature. Our work builds upon previous research of the effects of protected arithmetic operators (i.e. division, logarithm, power) on the output value of an evolved model for input data points not encountered during training. The scope is to benchmark the approximation/generalisation of models evolved using different function set choices across a range of 43 symbolic regression problems. The salient outcomes are as follows. Firstly, Koza’s protected operators of division and exponentiation have a detrimental effect on generalisation, and should therefore be avoided. This result is invariant of the use of moderately sized validation sets for model selection. Secondly, the performance of the recently introduced analytic quotient operator is comparable to that of the sinusoidal operator on average, with their combination being advantageous to both approximation and generalisation. These findings are consistent across two different system implementations, those of standard expression-tree GP and linear Grammatical Evolution. We highlight that this study employed very large test sets, which create confidence when benchmarking the effect of different combinations of primitive functions on model generalisation. Our aim is to encourage GP researchers and practitioners to use similar stringent means of assessing generalisation of evolved models where possible, and also to avoid certain primitive functions that are known to be inappropriate. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Genetic Programming and Evolvable Machines,10.1007/s10710-020-09391-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084697671&doi=10.1007%2fs10710-020-09391-4&partnerID=40&md5=1572b286d9709a8269c1d9b1f98216ad,2021,2021-07-20 15:49:29,2021-07-20 15:49:29
PKIKQ79Z,journalArticle,2020,"Simpkin, C.; Taylor, I.; Harborne, D.; Bent, G.; Preece, A.; Ganti, R.K.",Efficient orchestration of Node-RED IoT workflows using a Vector Symbolic Architecture,"Numerous workflow systems span multiple scientific domains and environments, and for the Internet of Things (IoT), Node-RED offers an attractive Web based user interface to execute IoT service-based workflows. However, like most workflow systems, it coordinates the workflow centrally, and cannot run within more transient environments where nodes are mobile. To address this gap, we show how Node-RED workflows can be migrated into a decentralized execution environment for operation on mobile ad-hoc networks, and we demonstrate this by converting a Node-RED based traffic congestion detection workflow to operate in a decentralized environment. The approach uses a Vector Symbolic Architecture (VSA) to dynamically convert Node-Red applications into a compact semantic vector representation that encodes the service interfaces and the workflow in which they are embedded. By extending existing services interfaces, with a simple cognitive layer that can interpret and exchange the vectors, we show how the required services can be dynamically discovered and interconnected into the required workflow in a completely decentralized manner. The resulting system provides a convenient environment where the Node-RED front-end graphical composition tool can be used to orchestrate decentralized workflows. In this paper, we further extend this work by introducing a new dynamic VSA vector compression scheme that compresses vectors for on-the-wire communication, thereby reducing communication bandwidth while maintaining the semantic information content. This algorithm utilizes the holographic properties of the symbolic vectors to perform compression taking into consideration the number of combined vectors along with similarity bounds that determine conflict with other encoded vectors used in the same context. The resulting savings make this approach extremely efficient for discovery in service based decentralized workflows. © 2020",Future Generation Computer Systems,10.1016/j.future.2020.04.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083883859&doi=10.1016%2fj.future.2020.04.005&partnerID=40&md5=833a0870a91f25f7ba46a09739cec42e,2020,2021-07-20 15:49:29,2021-07-20 15:49:29
VX4W2S95,journalArticle,2020,"Gobinath, S.; Madheswaran, M.",Deep perceptron neural network with fuzzy PID controller for speed control and stability analysis of BLDC motor,"Speed regulation is one of the significant characteristics to be adopted in the field of brushless DC motor drive for effective and accurate speed and position control operations. In this paper, stability analysis and performance characteristics of brushless direct current motor are studied and implemented with a new deep learning neural network—fuzzy-tuned proportional integral derivative (PID) speed controller. Deep learning architecture is designed for the multi-layer perceptron network, and the output from the neural module fires the rules of the fuzzy inference system mechanism. The parameters of deep perceptron neural network (DPNN) are tuned for near optimal solutions using the unified multi-swarm particle swarm optimization, and in turn the optimized DPNN selects the parameters of the fuzzy inference system. Deep learning neural network with the fuzzy inference system tunes the gain values of the PID controller and performs an effective speed regulation. The performance characteristics of the designed speed controller are tested for a step change in input speed and also for impulsive load disturbances. Further, the stability analysis of the new proposed controller is investigated with Lyapunov stability criterion by deriving the positive definite functions. The weight parameters of DPNN model and the number of rules of fuzzy system are tuned for their near optimal solutions using multi-swarm particle swarm optimization. From the results, it is well proven that the proposed controller is more stable and guarantees consistent performance than other considered controllers in all aspects. Simulation-based comparisons illustrate that the design methodologies outperform other controller designs from the literature. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-019-04532-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075351767&doi=10.1007%2fs00500-019-04532-z&partnerID=40&md5=d1833e8a067e73000fafacb757574c9e,2020,2021-07-20 15:49:29,2021-07-20 15:49:29
GILHZ559,journalArticle,2019,"Jha, K.K.; Dutta, H.S.",Mutual Information based hybrid model and deep learning for Acute Lymphocytic Leukemia detection in single cell blood smear images,"Background and objective: Due to the development in digital microscopic imaging, image processing and classification has become an interesting area for diagnostic research. Various techniques are available in the literature for the detection of Acute Lymphocytic Leukemia from the single cell blood smear images. The purpose of this work is to develop an effective method for leukemia detection. Methods: This work has developed deep learning based leukemia detection module from the blood smear images. Here, the detection scheme carries out pre-processing, segmentation, feature extraction and classification. The segmentation is done by the proposed Mutual Information (MI) based hybrid model, which combines the segmentation results of the active contour model and fuzzy C means algorithm. Then, from the segmented images, the statistical and the Local Directional Pattern (LDP) features are extracted and provided to the proposed Chronological Sine Cosine Algorithm (SCA) based Deep CNN classifier for the classification. Results: For the experimentation, the blood smear images are considered from the AA-IDB2 database and evaluated based on metrics, such as True Positive Rate (TPR), True Negative Rate (TNR), and accuracy. Simulation results reveal that the proposed Chronological SCA based Deep CNN classifier has the accuracy of 98.7%. Conclusions: The performance of the proposed Chronological SCA-based Deep CNN classifier is compared with the state-of-the-art methods. The analysis shows that the proposed classifier has comparatively improved performance and determines the leukemia from the blood smear images. © 2019",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2019.104987,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069693272&doi=10.1016%2fj.cmpb.2019.104987&partnerID=40&md5=daec6edbf9fea63e3702547451023185,2019,2021-07-20 15:49:29,2021-07-20 15:49:29
PC5WMS5N,journalArticle,2021,"Burke, M.; Subr, K.; Ramamoorthy, S.",Action Sequencing Using Visual Permutations,"Humans can easily reason about the sequence of high level actions needed to complete tasks, but it is particularly difficult to instill this ability in robots trained from relatively few examples. This work considers the task of neural action sequencing conditioned on a single reference visual state. This task is extremely challenging as it is not only subject to the significant combinatorial complexity that arises from large action sets, but also requires a model that can perform some form of symbol grounding, mapping high dimensional input data to actions, while reasoning about action relationships. This letter takes a permutation perspective and argues that action sequencing benefits from the ability to reason about both permutations and ordering concepts. Empirical analysis shows that neural models trained with latent permutations outperform standard neural architectures in constrained action sequencing tasks. Results also show that action sequencing using visual permutations is an effective mechanism to initialise and speed up traditional planning techniques and successfully scales to far greater action set sizes than models considered previously. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2021.3059630,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101233680&doi=10.1109%2fLRA.2021.3059630&partnerID=40&md5=3bd6a834df5b040d7949defac2b5f551,2021,2021-07-20 15:49:29,2021-07-20 15:49:29
93DYJFNT,journalArticle,2018,"Dutta, S.; Wasilewski, P.",Dialogue in hierarchical learning of concept using prototypes and counterexamples,"This paper is an attempt to introduce a notion of dialogue among different agents, e.g. a user or situation descriptor and an information provider or machine, to better understand a user's need of information. When a user describes a concept through some keywords first the system needs to choose the relevant attributes for initiating the search. Regarding this, a series of dialogues may help the system to better understand the user's language of expresssion. After fixing the relevant attributes, the system needs to look at its repertoire of positive and negative cases of different concepts characterized with respect to this set of attributes. Then through a step-by-step process of matching and comparing similarity of user's described situation with those positive and negative cases of a concept, incorporating several layers of dialogue, the system may provide information closer to the user's need. The target of this paper is to develop a theoretical model, incorporating the above features, for characterizing one's concern of concept based on dialogue and similarity based reasoning. © 2018 IOS Press. All rights reserved.",Fundamenta Informaticae,10.3233/FI-2018-1711,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051957012&doi=10.3233%2fFI-2018-1711&partnerID=40&md5=5a221cea81d13473646b981ca9cd8bea,2018,2021-07-20 15:49:29,2021-07-20 15:49:29
8G8V84NS,journalArticle,2016,"Bolchini, C.; Cassano, L.",A Novel Approach to Incremental Functional Diagnosis for Complex Electronic Boards,"Incremental functional diagnosis aims at minimising the number of tests to be executed to perform the diagnosis, to limit efforts and costs. Iteratively the test to be executed is selected and based on the collected outcome, either the faulty component is identified or a new test is performed. This paper proposes a novel approach based on the syndromes occurrence probability, that defines how i) to process syndromes compatible with the partial syndrome being incrementally collected and ii) to select the next test. The proposal is evaluated and compared against a number of existing techniques based on machine-learning strategies, outperforming them. © 1968-2012 IEEE.",IEEE Transactions on Computers,10.1109/TC.2015.2417537,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961728578&doi=10.1109%2fTC.2015.2417537&partnerID=40&md5=38b721d7cbc56bdd1c47a3580861ede3,2016,2021-07-20 15:49:29,2021-07-20 15:49:29
34ZM8Z9U,journalArticle,2020,"Mudiyanselage, T.K.B.; Xiao, X.; Zhang, Y.; Pan, Y.",Deep Fuzzy Neural Networks for Biomarker Selection for Accurate Cancer Detection,"Different biomedical computing methods for cancer-specific gene recognition have been developed in recent years. Currently, building an open-box machine learning system to discover explainable knowledge from gene expression data is a difficult research problem due to a large number of genes, a small number of samples, and noise. Fuzzy systems can be used to deal with data ambiguity and noise issues and extract meaningful knowledge from gene data. In this article, we create a new deep fuzzy neural network to handle the uncertainty in gene data to generate useful knowledge for specific disease diagnosis. A new hybrid algorithm is designed to preprocess data and select informative genes for accurate cancer detection. Various experiments using six different cancer datasets indicate that the new method has better and more reliable performance than the other conventional classification methods with different gene selection methods. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2019.2958295,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097354589&doi=10.1109%2fTFUZZ.2019.2958295&partnerID=40&md5=a7141837a1ef50bfec2a72de706ecd7e,2020,2021-07-20 15:49:29,2021-07-20 15:49:29
S2AGJUN6,journalArticle,2020,"de Campos Souza, P.V.; Soares, E.A.; Guimarães, A.J.; Araujo, V.S.; Araujo, V.J.S.; Rezende, T.S.",Autonomous Data Density pruning fuzzy neural network for Optical Interconnection Network,"Traditionally, fuzzy neural networks have parametric clustering methods based on equally spaced membership functions to fuzzify inputs of the model. In this sense, it produces an excessive number calculations for the parameters’ definition of the network architecture, which may be a problem especially for real-time large-scale tasks. Therefore, this paper proposes a new model that uses a non-parametric technique for the fuzzification process. The proposed model uses an autonomous data density approach in a pruned fuzzy neural network, wich favours the compactness of the model. The performance of the proposed approach is evaluated through the usage of databases related to the Optical Interconnection Network. Finally, binary patterns classification tests for the identification of temporal distribution (asynchronous or client–server) were performed and compared with state-of-the-art fuzzy neural-based and traditional machine learning approaches. Results demonstrated that the proposed model is an efficient tool for these challenging classification tasks. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Evolving Systems,10.1007/s12530-020-09336-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084204599&doi=10.1007%2fs12530-020-09336-3&partnerID=40&md5=9c904554a63338f4cecda6e154487bc5,2020,2021-07-20 15:49:29,2021-07-20 15:49:29
X7RU9AYX,journalArticle,2019,"Moss, D.J.M.; Boland, D.; Leong, P.H.W.","A Two-Speed, Radix-4, Serial-Parallel Multiplier","In this paper, we present a two-speed, radix-4, serial-parallel multiplier for accelerating applications such as digital filters, artificial neural networks, and other machine learning algorithms. Our multiplier is a variant of the serial-parallel (SP) modified radix-4 Booth multiplier that adds only the nonzero Booth encodings and skips over the zero operations, making the latency dependent on the multiplier value. Two subcircuits with different critical paths are utilized so that throughput and latency are improved for a subset of multiplier values. The multiplier is evaluated on an Intel Cyclone V field-programmable gate array against standard parallel-parallel and SP multipliers across four different process-voltage-temperature corners. We show that for bit widths of 32 and 64, our optimizations can result in a 1.42× - 3.36× improvement over the standard parallel Booth multiplier in terms of area-time depending on the input set. © 1993-2012 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2018.2883645,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058884496&doi=10.1109%2fTVLSI.2018.2883645&partnerID=40&md5=449615da6b55a7bd95a8e063ae22081a,2019,2021-07-20 15:49:29,2021-07-20 15:49:29
D7ZF73VV,journalArticle,2018,"Lu, X.; Ming, L.; Liu, W.; Li, H.-X.",Probabilistic regularized extreme learning machine for robust modeling of noise data,"The extreme learning machine (ELM) has been extensively studied in the machine learning field and has been widely implemented due to its simplified algorithm and reduced computational costs. However, it is less effective for modeling data with non-Gaussian noise or data containing outliers. Here, a probabilistic regularized ELM is proposed to improve modeling performance with data containing non-Gaussian noise and/or outliers. While traditional ELM minimizes modeling error by using a worst-case scenario principle, the proposed method constructs a new objective function to minimize both mean and variance of this modeling error. Thus, the proposed method considers the modeling error distribution. A solution method is then developed for this new objective function and the proposed method is further proved to be more robust when compared with traditional ELM, even when subject to noise or outliers. Several experimental cases demonstrate that the proposed method has better modeling performance for problems with non-Gaussian noise or outliers. © 2013 IEEE.",IEEE Transactions on Cybernetics,10.1109/TCYB.2017.2738060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028460503&doi=10.1109%2fTCYB.2017.2738060&partnerID=40&md5=da75ada09d12548091c3b6caab1ad5a6,2018,2021-07-20 15:49:30,2021-07-20 15:49:30
W247MPIM,journalArticle,2018,"Xue, Y.; Li, X.; Blanton, R.D.",Improving Diagnostic Resolution of Failing ICs Through Learning,"Diagnosis is the first analysis step for uncovering the root cause of failure for a defective integrated logic circuit. The conventional objective of identifying failure locations has been augmented with various physically-aware diagnosis techniques that are intended to improve both resolution and accuracy. Despite these advances, it is often the case, however, that resolution, i.e., the number of locations or candidates reported by diagnosis, exceeds the number of actual failing locations. To address this major challenge, a novel, machine-learning-based resolution improvement methodology named physically-aware diagnostic resolution enhancement (PADRE) is described. PADRE uses easily-available tester and simulation data to extract features that uniquely characterize each candidate. PADRE applies machine learning to the features to identify candidates that correspond to the actual failure locations. Through various experiments, PADRE is shown to significantly improve resolution with virtually no negative impact on accuracy. Additional experiments demonstrate that PADRE is robust against data set variation and feature-data availability. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2016.2611499,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047477070&doi=10.1109%2fTCAD.2016.2611499&partnerID=40&md5=b0257281efec8e7137ba13b191f7f177,2018,2021-07-20 15:49:30,2021-07-20 15:49:30
9SJE87YL,journalArticle,2017,"Telford, R.D.; Galloway, S.; Stephen, B.; Elders, I.",Diagnosis of series DC Arc faults - A machine learning approach,"Increasing prevalence of dc sources and loads has resulted in dc distribution being reconsidered at a microgrid level. However, in comparison to ac systems, the lack of a natural zero crossing has traditionally meant that protecting dc systems is inherently more difficult - this protection issue is compounded when attempting to diagnose and isolate fault conditions. One such condition is the series arc fault, which poses significant protection issues as their presence negates the logic of overcurrent protection philosophies. This paper proposes the IntelArc system to accurately diagnose series arc faults in dc systems. IntelArc combines time-frequency and time-domain extracted features with hidden Markov models (HMMs) to discriminate between nominal transient behavior and arc fault behavior across a variety of operating conditions. Preliminary testing of the system is outlined with results showing that the system has the potential for accurate, generalized diagnosis of series arc faults in dc systems. © 2005-2012 IEEE.",IEEE Transactions on Industrial Informatics,10.1109/TII.2016.2633335,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029685215&doi=10.1109%2fTII.2016.2633335&partnerID=40&md5=18b8b0e4ebbb5227d55c075576c64431,2017,2021-07-20 15:49:30,2021-07-20 15:49:30
YKD5KM6J,journalArticle,2015,"Maratea, M.; Pulina, L.; Ricca, F.",Multi-engine ASP solving with policy adaptation,"The recent application of Machine Learning techniques to the Answer Set Programming (ASP) field proved to be effective. In particular, the multi-engine ASP solver me-asp is efficient: it is able to solve more instances than any other ASP system that participated to the 3rd ASP Competition on the 'System Track' benchmarks. In the me-asp approach, classification methods inductively learn offline algorithm selection policies starting from both a set of features of instances in a training set, and the solvers performance on such instances. In this article we present an improvement to the multi-engine framework of me-asp, in which we add the capability of updating the learned policies when the original approach fails to give good predictions. An experimental analysis, conducted on training and test sets of ground instances obtained from the ones submitted to the 'System ZTrack' of the 3rd ASP Competition, shows that the policy adaptation improves the performance of me-asp when applied to test sets containing domains of instances that were not considered for training. © The Author, 2013. Published by Oxford University Press. All rights reserved.",Journal of Logic and Computation,10.1093/logcom/ext068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958767515&doi=10.1093%2flogcom%2fext068&partnerID=40&md5=5ff12320f01539bb446c6f7d506ab018,2015,2021-07-20 15:49:30,2021-07-20 15:49:30
PI23DMWS,journalArticle,2012,"Aviad, B.; Roy, G.","A decision support method, based on bounded rationality concepts, to reveal feature saliency in clustering problems","In many real-life data mining problems, there is no a-priori classification (no target attribute that is known in advance). The lack of a target attribute (target column/class label) makes the division process into a set of groups very difficult to define and construct. The end user needs to exert considerable effort to interpret the results of diverse algorithms because there is no pre-defined reliable benchmark. To overcome this drawback the current paper proposes a methodology based on bounded-rationality theory. It implements an S-shaped function as a saliency measure to represent the end user's logic to determine the features that characterize each potential group. The methodology is demonstrated on three well-known datasets from the UCI machine-learning repository. The grouping uses cluster analysis algorithms, since clustering techniques do not need a target attribute. © 2012 Elsevier B.V. All rights reserved.",Decision Support Systems,10.1016/j.dss.2012.05.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868648706&doi=10.1016%2fj.dss.2012.05.037&partnerID=40&md5=71c8988f4aa8b121b4a362e84df8bac5,2012,2021-07-20 15:49:30,2021-07-20 15:49:30
HSNBQPZ3,journalArticle,2021,"Xia, M.; Shao, H.; Ma, X.; De silva, C.W.",A Stacked GRU-RNN-based Approach for Predicting Renewable Energy and Electricity Load for Smart Grid Operation,"Predictions of renewable energy (RE) generation and electricity load are critical to smart grid operation. However, the prediction task remains challenging due to the intermittent and chaotic character of RE sources, and the diverse user behavior and power consumers. This paper presents a novel method for the prediction of RE generation and electricity load using improved stacked gated recurrent unit-recurrent neural network (GRU-RNN) for both uni-variate and multi-variate scenarios. First, multiple sensitive monitoring parameters or historical electricity consumption data are selected according to the correlation analysis to form the input data. Second, a stacked GRU-RNN using a simplified GRU is constructed with improved training algorithm based on AdaGrad and adjustable momentum. The modified GRU-RNN structure and improved training method enhance training efficiency and robustness. Third, the stacked GRU-RNN is used to establish an accurate mapping between the selected variables and RE generation or electricity load due to its self-feedback connections and improved training mechanism. The proposed method is verified by using two experiments: prediction of wind power generation using multiple weather parameters and prediction of electricity load with historical energy consumption data. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods of machine learning or deep learning in achieving an accurate energy prediction for effective smart grid operation. IEEE",IEEE Transactions on Industrial Informatics,10.1109/TII.2021.3056867,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100853431&doi=10.1109%2fTII.2021.3056867&partnerID=40&md5=c86820148ee698d41da90bb396217219,2021,2021-07-20 15:49:30,2021-07-20 15:49:30
3CP8TMIC,journalArticle,2017,"Fang, Y.; Liu, Z.-H.; Min, F.",A PSO algorithm for multi-objective cost-sensitive attribute reduction on numeric data with error ranges,"Multi-objective cost-sensitive attribute reduction is an attractive problem in supervised machine learning. Most research has focused on single-objective minimal test cost reduction or dealt with symbolic data. In this paper, we propose a particle swarm optimization algorithm for the attribute reduction problem on numeric data with multiple costs and error ranges and use three metrics with which to evaluate the performance of the algorithm. The proposed algorithm benefits from a fitness function based on the positive region, the selected n types of the test cost, a set of constant weight values wik, and a designated non-positive exponent λ. We design a learning strategy by setting dominance principles, which ensures the preservation of Pareto-optimal solutions and the rejection of redundant solutions. With different parameter settings, our PSO algorithm searches for a sub-optimal reduct set. Finally, we test our algorithm on seven UCI (University of California, Irvine) datasets. Comparisons with alternative approaches including the λ-weighted method and exhaustive calculation method of reduction are analyzed. Experimental results indicate that our heuristic algorithm outperforms existing algorithms. © 2016, Springer-Verlag Berlin Heidelberg.",Soft Computing,10.1007/s00500-016-2260-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978173215&doi=10.1007%2fs00500-016-2260-5&partnerID=40&md5=ab1758405d86a43433f87ea35dfca991,2017,2021-07-20 15:49:30,2021-07-20 15:49:30
XEE25WKH,journalArticle,2014,"Román, P.E.; Velásquez, J.D.",A neurology-inspired model of web usage,"The problem of predicting human behavior has been a great challenge for several disciplines including computer science. In particular, web user browsing behavior has been studied from the machine learning point of view, a field that has been coined web usage mining (WUM). However, current WUM techniques can be negatively impacted by changes in web site structure and content (e.g. Web 2.0). The key reason behind this issue may be that machine learning algorithms learn the observed behavior according to a particular training set, but do not model the user behavior under different conditions. We propose a simulation model that mimics human interaction with the web by recovering observed navigational steps. This web usage model is inspired by a neurophysiology's stochastic description of decision making and by the information utility of web page content. The proposed model corresponds to a high-dimensional stochastic process based on the leaky competing accumulator (LCA) neural model. We solve high-dimensional issues by considering a mesh-less symbolic interpolation. As a proof-of-concept we test the web user simulation system on an academic web site by recovering most of the observed behavior (73%). Therefore, our approach operationally describes web users that seem to react as observed users confronted by changes in the web site interface. © 2013 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2013.10.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894088597&doi=10.1016%2fj.neucom.2013.10.012&partnerID=40&md5=0f53d764131cd641e0fcf9f9ef069acd,2014,2021-07-20 15:49:30,2021-07-20 15:49:30
6C824LWP,journalArticle,2020,"Rahman, S.; Khan, S.; Barnes, N.",Deep0Tag: Deep Multiple Instance Learning for Zero-Shot Image Tagging,"Zero-shot learning aims to perform visual reasoning about unseen objects. In-line with the success of deep learning on object recognition problems, several end-to-end deep models for zero-shot recognition have been proposed in the literature. These models are successful in predicting a single unseen label given an input image but do not scale to cases where multiple unseen objects are present. Here, we focus on the challenging problem of zero-shot image tagging, where multiple labels are assigned to an image, that may relate to objects, attributes, actions, events, and scene type. Discovery of these scene concepts requires the ability to process multi-scale information. To encompass global as well as local image details, we propose an automatic approach to locate relevant image patches and model image tagging within the Multiple Instance Learning (MIL) framework. To the best of our knowledge, we propose the first end-to-end trainable deep MIL framework for the multi-label zero-shot tagging problem. We explore several alternatives for instance-level evidence aggregation and perform an extensive ablation study to identify the optimal pooling strategy. Due to its novel design, the proposed framework has several interesting features: 1) unlike previous deep MIL models, it does not use any off-line procedure (e.g., Selective Search or EdgeBoxes) for bag generation. 2) During test time, it can process any number of unseen labels given their semantic embedding vectors. 3) Using only image-level seen labels as weak annotation, it can produce a localized bounding box for each predicted label. We experiment with the large-scale NUS-WIDE and MS-COCO datasets and achieve superior performance across conventional, zero-shot, and generalized zero-shot tagging tasks. © 1999-2012 IEEE.",IEEE Transactions on Multimedia,10.1109/TMM.2019.2924511,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077792443&doi=10.1109%2fTMM.2019.2924511&partnerID=40&md5=2565c6f791e02727961e37daa6bf0676,2020,2021-07-20 15:49:30,2021-07-20 15:49:30
VLL3G339,journalArticle,2021,"Masdari, M.; Khezri, H.",Towards fuzzy anomaly detection-based security: a comprehensive review,"In the data security context, anomaly detection is a branch of intrusion detection that can detect emerging intrusions and security attacks. A number of anomaly detection systems (ADSs) have been proposed in the literature that using various algorithms and techniques try to detect the intrusions and anomalies. This paper focuses on the ADS schemes which have applied fuzzy logic in combination with other machine learning and data mining techniques to deal with the inherent uncertainty in the intrusion detection process. For this purpose, it first presents the key knowledge about intrusion detection systems and then classifies the fuzzy ADS approaches regarding their utilized fuzzy algorithm. Afterward, it summarizes their major contributions and illuminates their advantages and limitations. Finally, concluding issues and directions for future researches in the fuzzy ADS context are highlighted. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Fuzzy Optimization and Decision Making,10.1007/s10700-020-09332-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088531637&doi=10.1007%2fs10700-020-09332-x&partnerID=40&md5=2b903c0b79ad854430b2eb52a2cee7a2,2021,2021-07-20 15:49:30,2021-07-20 15:49:30
K5RQ4DII,journalArticle,2019,"Huitzil, I.; Dranca, L.; Bernad, J.; Bobillo, F.",Gait recognition using fuzzy ontologies and Kinect sensor data,"Gait recognition involves the automatic classification of human people from sequences of data about their movement patterns. It is an interesting problem with several applications, such as security or medicine. Even low cost sensors can be used to capture pose sequences with enough quality to make a successful classification possible. In this paper, we describe the use of fuzzy ontologies to represent sequences of Microsoft Kinect gait data and some biometric features relevant for the gait recognition computed after them, enabling more reusable and interpretable datasets. We also propose a novel recognition algorithm based on fuzzy logic that outperforms state-of-the-art methods for straight line walks. We also face the problem of the identification of unknown individuals that are not present in the system knowledge base. © 2019 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2019.07.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070635553&doi=10.1016%2fj.ijar.2019.07.012&partnerID=40&md5=1559eda84664f5c83550b366e9086a16,2019,2021-07-20 15:49:30,2021-07-20 15:49:30
67L5K36W,journalArticle,2018,"Zhu, Q.; Wu, Y.; Li, Y.; Han, J.; Zhou, X.",Text mining based theme logic structure identification: application in library journals,"Purpose: Library intelligence institutions, which are a kind of traditional knowledge management organization, are at the frontline of the big data revolution, in which the use of unstructured data has become a modern knowledge management resource. The paper aims to discuss this issue. Design/methodology/approach: This research combined theme logic structure (TLS), artificial neural network (ANN), and ensemble empirical mode decomposition (EEMD) to transform unstructured data into a signal-wave to examine the research characteristics. Findings: Research characteristics have a vital effect on knowledge management activities and management behavior through concentration and relaxation, and ultimately form a quasi-periodic evolution. Knowledge management should actively control the evolution of the research characteristics because the natural development of six to nine years was found to be difficult to plot. Originality/value: Periodic evaluation using TLS-ANN-EEMD gives insights into journal evolution and allows journal managers and contributors to follow the intrinsic mode functions and predict the journal research characteristics tendencies. © 2018, Qing Zhu, Yiqiong Wu, Yuze Li, Jing Han and Xiaoyang Zhou.",Library Hi Tech,10.1108/LHT-10-2017-0211,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044566896&doi=10.1108%2fLHT-10-2017-0211&partnerID=40&md5=9da2bc405318a871b9837f75914e87f4,2018,2021-07-20 15:49:30,2021-07-20 15:49:30
9TE3B22I,journalArticle,2014,"Lin, Y.-L.; Hsieh, J.-G.; Jeng, J.-H.",Robust decomposition with guaranteed robustness for cellular neural networks implementing an arbitrary Boolean function,"Given a general Boolean function, an algorithm is proposed in this paper to find a sequence of robust uncoupled cellular neural networks, with logic operators as the conjunctions, implementing the given Boolean function. Each resulting robust uncoupled cellular neural network in the decomposition has a margin greater than or equal to a pre-specified value. For reasonable robustness levels, the proposed algorithm usually requires lesser number of searching templates and provides faster execution than those by using other similar methods. Furthermore, a mechanism for practical tradeoff between the guaranteed robustness and the complexity in terms of the number of terms in the decomposition is provided in our proposed algorithm. © 2014 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2014.05.058,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904806760&doi=10.1016%2fj.neucom.2014.05.058&partnerID=40&md5=65b868031e3efc9388032682f3b39741,2014,2021-07-20 15:49:30,2021-07-20 15:49:30
74T2P27N,journalArticle,2021,"Novaes, M.P.; Carvalho, L.F.; Lloret, J.; Proença, Jr., M.L.",Adversarial Deep Learning approach detection and defense against DDoS attacks in SDN environments,"Over the last few years, Software Defined Networking (SDN) paradigm has become an emerging architecture to design future networks and to meet new application demands. SDN provides resources for improving network control and management by separating control and data plane, and the logical control is centralized in a controller. However, the centralized control logic can be an ideal target for malicious attacks, mainly Distributed Denial of Service (DDoS) attacks. Recently, Deep Learning has become a powerful technique applied in cybersecurity, and many Network Intrusion Detection (NIDS) have been proposed in recent researches. Some studies have indicated that deep neural networks are sensitive in detecting adversarial attacks. Adversarial attacks are instances with certain perturbations that cause deep neural networks to misclassify. In this paper, we proposed a detection and defense system based on Adversarial training in SDN, which uses Generative Adversarial Network (GAN) framework for detecting DDoS attacks and applies adversarial training to make the system less sensitive to adversarial attacks. The proposed system includes well-defined modules that enable continuous traffic monitoring using IP flow analysis, enabling the anomaly detection system to act in near-real-time. We conducted the experiments on two distinct scenarios, with emulated data and the public dataset CICDDoS 2019. Experimental results demonstrated that the system efficiently detected up-to-date common types of DDoS attacks compared to other approaches. © 2021 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2021.06.047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108874877&doi=10.1016%2fj.future.2021.06.047&partnerID=40&md5=ef01686a18a78e97bc2d8b95bee4de83,2021,2021-07-20 15:49:30,2021-07-20 15:49:30
6QWL97RQ,journalArticle,2021,"Nogales, A.; Delgado-Martos, E.; Melchor, Á.; García-Tejedor, Á.J.",ARQGAN: An evaluation of generative adversarial network approaches for automatic virtual inpainting restoration of Greek temples,"Image reconstruction has received much attention and has advanced in recent years with the rise of deep learning. Deep neural models have been able to perform image-to-image translation by transferring pictorial styles, coloring old photographs or filling in missing parts. This last technique is known as image inpainting and enables restoration of damaged or missing parts of an image or photograph to obtain the complete picture. However, it is not always possible to properly define which parts are missing or to identify where they are missing, as in the case of superimposing new information on an already complete image. In this paper, we propose the use of generative adversarial networks (GANs), a well-known deep learning model, for virtual inpainting restoration of artificial landscape images containing archaeological remains of Greek temples. The network identifies key features determined by the internal logic of the architectural style denoted by the ruins and adds the missing architectural elements to obtain an image of the restored building. Unlike other studies, it does not receive any information on which elements should be added or where. Virtual inpainting restoration is capable of representing a building's envelope but also integrates particular aspects of the building related to the architectural language used for its design. The restoration of the fundamental parts of the classical Greek order was consistent, and the results were evaluated with objective metrics and through a subjective survey between academics and architects. They showed that adding segmented images to the training dataset gives better results. © 2021 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2021.115092,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105581024&doi=10.1016%2fj.eswa.2021.115092&partnerID=40&md5=7595ac466cfb9f3d08ec9ef5f2719a55,2021,2021-07-20 15:49:30,2021-07-20 15:49:30
ITELQWEH,journalArticle,2021,"Min, Z.",Public welfare organization management system based on FPGA and deep learning,"Public welfare organization system Promoting social support is one of the government's influence relieving benevolent associations. To qualify as exempt organizations, community or government support, the association exercises should simply replace the network edge, welfare organization is conducive to select another person, and every person should be registering for details. The social ground considered as network partner to the exercise of social, government support element in any case, despite all the affiliation, if the network is advantageous. Costs associated social modernization typically does not include support groups. Long-term support will be used to understand the impact of decisions and related efforts in the political mission exercises. So, the government's social support groups altruistic relationship. A Field-Programmable Gate Array (FPGA), and a Graphics Processing Unit (GPU) to improve the throughput of the cellular neural network. Rather, Field-Programmable Gate Array (FPGA) accelerating the depth learning network is not just one reason, but also because of its ability in energy efficiency, the maximum parallelism. In this article, we review recent prior art deeply accelerated learning networks in the FPGA. We stress the importance of using a variety of techniques to improve the acceleration performance of important features. We also offer suggestions for improving the use of FPGA accelerated cellular neural networks. In this paper, research methods represent FPGA-based accelerator's recent trends in depth learning networks. Therefore, this review is expected to be useful in depth study researchers' direct and efficient hardware accelerator future development. © 2020",Microprocessors and Microsystems,10.1016/j.micpro.2020.103333,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093701173&doi=10.1016%2fj.micpro.2020.103333&partnerID=40&md5=7cfe2b67c828d0b6d113481592952a1e,2021,2021-07-20 15:49:31,2021-07-20 15:49:31
4ZLMDYNN,journalArticle,2021,"Shalyminov, I.; Sordoni, A.; Atkinson, A.; Schulz, H.",GRTr: Generative-Retrieval Transformers for Data-Efficient Dialogue Domain Adaptation,"Domain adaptation has recently become a key problem in dialogue systems research. Deep learning, while being the preferred technique for modeling such systems, works best given massive training data. However, in real-world scenarios, such resources are rarely available for new domains, and the ability to train with a few dialogue examples can be considered essential. Pre-training on large data sources and adapting to the target data has become the standard method for few-shot problems within the deep learning framework. In this paper, we present GRTr, a hybrid generative-retrieval model based on the large-scale general-purpose language model GPT-2 fine-tuned to the multi-domain MetaLWOz dataset. In addition to robust and diverse response generation provided by the GPT-2, our model is able to estimate generation confidence, and is equipped with retrieval logic as a fallback for the cases when the estimate is low. GRTr is the winning entry at the fast domain adaptation task of DSTC-8 in human evaluation (&gt;4% improvement over the 2nd place system). It also attains superior performance to a series of baselines on automated metrics on MetaLWOz and MultiWoz, a multi-domain dataset of goal-oriented dialogues. In this paper, we also conduct a study of GRTr's performance in the setup of limited adaptation data, evaluating the model's overall response prediction performance on MetaLWOz and goal-oriented performance on MultiWoz. IEEE",IEEE/ACM Transactions on Audio Speech and Language Processing,10.1109/TASLP.2021.3074779,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104642179&doi=10.1109%2fTASLP.2021.3074779&partnerID=40&md5=9f53f5bcd7645da6dae9dce68c2de6be,2021,2021-07-20 15:49:31,2021-07-20 15:49:31
SVM3N84Z,journalArticle,2020,"Gu, D.; Su, K.; Zhao, H.",A case-based ensemble learning system for explainable breast cancer recurrence prediction,"Significant progress has been achieved in recent years in the application of artificial intelligence (AI) for medical decision support. However, many AI-based systems often only provide a final prediction to the doctor without an explanation of its underlying decision-making process. In scenarios concerning deadly diseases, such as breast cancer, a doctor adopting an auxiliary prediction is taking big risks, as a bad decision can have very harmful consequences for the patient. We propose an auxiliary decision support system that combines ensemble learning with case-based reasoning to help doctors improve the accuracy of breast cancer recurrence prediction. The system provides a case-based interpretation of its prediction, which is easier for doctors to understand, helping them assess the reliability of the system's prediction and make their decisions accordingly. Our application and evaluation in a case study focusing on breast cancer recurrence prediction shows that the proposed system not only provides reasonably accurate predictions but is also well-received by oncologists. © 2020 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2020.101858,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085971774&doi=10.1016%2fj.artmed.2020.101858&partnerID=40&md5=bc2bb14d5ea5746dcceb3771356e9c2b,2020,2021-07-20 15:49:31,2021-07-20 15:49:31
UTZCNSMZ,journalArticle,2015,"Jayashree, L.S.; Palakkal, N.; Papageorgiou, E.I.; Papageorgiou, K.",Application of fuzzy cognitive maps in precision agriculture: a case study on coconut yield management of southern India’s Malabar region,"Coconut is one of the major perennial food crops that has a long development phase of 44 months. The climatic and seasonal variations affect all stages of coconut’s long development cycle. Besides, the soil composition also plays a vital role in deciding the coconut yield behavior. The present study is focused on categorizing the coconut production level for the given set of agro-climatic conditions using the methodology of fuzzy cognitive map (FCM) enhanced by its learning capabilities. Additionally, an attempt is made to study the impact of climatic variations and weather parameters on the coconut yield behavior using the reasoning capabilities of FCM. Real coconut field data of different seasons for the period from 2009 to 2013 of Kerala state’s Malabar region were used for training and evaluation of the FCM. The present work demonstrates the classification and prediction capabilities of FCM for the described precision agriculture application, with the two most known and efficient FCM learning approaches, viz., nonlinear Hebbian (NHL) and data-driven nonlinear Hebbian (DDNHL). The DDNHL-FCM offers an overall classification accuracy of 96 %. The various case studies furnished in the paper demonstrate the power of NHL-FCM in effectively reasoning new knowledge pertaining to the presented precision agriculture application. © 2015, The Natural Computing Applications Forum.",Neural Computing and Applications,10.1007/s00521-015-1864-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941995381&doi=10.1007%2fs00521-015-1864-5&partnerID=40&md5=57d62f5b9a1453114d994a01b68e4f9c,2015,2021-07-20 15:49:31,2021-07-20 15:49:31
PHLLBDJ6,journalArticle,2015,"Cambria, E.; Gastaldo, P.; Bisio, F.; Zunino, R.",An ELM-based model for affective analogical reasoning,"Between the dawn of the Internet through year 2003, there were just a few dozens exabytes of information on the Web. Today, that much information is created weekly. The opportunity to capture the opinions of the general public about social events, political movements, company strategies, marketing campaigns, and product preferences has raised increasing interest both in the scientific community, for the exciting open challenges, and in the business world, for the remarkable fallouts in marketing and financial prediction. Keeping up with the ever-growing amount of unstructured information on the Web, however, is a formidable task and requires fast and efficient models for opinion mining. In this paper, we explore how the high generalization performance, low computational complexity, and fast learning speed of extreme learning machines can be exploited to perform analogical reasoning in a vector space model of affective common-sense knowledge. In particular, by enabling a fast reconfiguration of such a vector space, extreme learning machines allow the polarity associated with natural language concepts to be calculated in a more dynamic and accurate way and, hence, perform better concept-level sentiment analysis. © 2014 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2014.01.064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969228568&doi=10.1016%2fj.neucom.2014.01.064&partnerID=40&md5=98817fd95ea15415776848a46a1d30b3,2015,2021-07-20 15:49:31,2021-07-20 15:49:31
Y2QDE6DW,journalArticle,2014,"Tang, D.; Wong, T.C.; Chin, K.S.; Kwong, C.K.",Evaluation of user satisfaction using evidential reasoning-based methodology,"For the sake of gaining competitive advantages, it is important to evaluate the satisfaction level of a product or service from the users' perspective. This can be done by investigating the relationship among customer attributes (customer requirements) and design attributes (product configurations). However, such relationship would be highly non-linear in nature. In this regard, many approaches have been proposed over traditional linear methods. Particularly, the Adaptive Neuro-Fuzzy Inference System (ANFIS) method has been prevalently utilized in modeling such vague and complex relationship among these attributes and evaluating user satisfaction towards certain products or services. Despite the fact that the ANFIS method can explicitly model the non-linear relation among these attributes, it may be restricted if uncertain information can be observed due to subjectivity and incompleteness. To overcome these limitations, a belief rule base (BRB) approach with evidential reasoning (ER) is applied in this paper. For justification purpose, both the ANFIS and BRB methods are applied to the same case. Comparison results indicate that the BRB is capable of minimizing the human biases in evaluating user satisfaction and rectifying the inappropriateness associated with the ANFIS method. Also, the BRB method can generate more rational and informative evaluation results. © 2014 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2014.01.055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904250146&doi=10.1016%2fj.neucom.2014.01.055&partnerID=40&md5=80361973488416788017070869a311df,2014,2021-07-20 15:49:31,2021-07-20 15:49:31
ARPT2JTW,journalArticle,2020,"Nayyeri, M.; Alam, M.M.; Lehmann, J.; Vahdati, S.",3D learning and reasoning in link prediction over knowledge graphs,"Knowledge Graph Embeddings (KGE) are used for representation learning in Knowledge Graphs (KGs) by measuring the likelihood of a relation between nodes. Rotation-based approaches, specially axis-angle representations, were shown to improve the performance of many Machine Learning (ML)-based models in different tasks including link prediction. There is a perceived disconnect between the topics ofKGE models and axis-angle rotation-based approaches. This is particuarly visible when considering the ability of KGEs to learn relational patterns such as symmetry, inversion, implication, equivalence, composition, and refiexivity considering axis-angle rotation-based approaches. In this article, we propose RodE, a new KGE model which employs an axis-angle representation for rotations based on Rodrigues' formula. RodE inherits the main advantages of 3-dimensional rotation from angle, orientation and distance preservation in the embedding space. Thus, the model efficiently captures the similarity between the nodes in a graph in the vector space. Our experiments show that RodE outperforms state-of-the-art models on standard datasets. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",IEEE Access,10.1109/ACCESS.2020.3034183,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102804082&doi=10.1109%2fACCESS.2020.3034183&partnerID=40&md5=afaf2cbadab96bbb84a54b028c9d54bc,2020,2021-07-20 15:49:31,2021-07-20 15:49:31
JIH6WN46,journalArticle,2016,"Moreira-Matias, L.; Cats, O.; Gama, J.; Mendes-Moreira, J.; De Sousa, J.F.",An online learning approach to eliminate Bus Bunching in real-time,"Recent advances in telecommunications created new opportunities for monitoring public transport operations in real-time. This paper presents an automatic control framework to mitigate the Bus Bunching phenomenon in real-time. The framework depicts a powerful combination of distinct Machine Learning principles and methods to extract valuable information from raw location-based data. State-of-the-art tools and methodologies such as Regression Analysis, Probabilistic Reasoning and Perceptron's learning with Stochastic Gradient Descent constitute building blocks of this predictive methodology. The prediction's output is then used to select and deploy a corrective action to automatically prevent Bus Bunching. The performance of the proposed method is evaluated using data collected from 18 bus routes in Porto, Portugal over a period of one year. Simulation results demonstrate that the proposed method can potentially reduce bunching by 68% and decrease average passenger waiting times by 4.5%, without prolonging in-vehicle times. The proposed system could be embedded in a decision support system to improve control room operations. © 2016 Elsevier B.V. All rights reserved.",Applied Soft Computing Journal,10.1016/j.asoc.2016.06.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976407092&doi=10.1016%2fj.asoc.2016.06.031&partnerID=40&md5=c3f7e7b8e7aa982947e51b7df6a1cc58,2016,2021-07-20 15:49:31,2021-07-20 15:49:31
KZ7ICVQC,journalArticle,2013,"Berrar, D.; Lozano, J.A.",Significance tests or confidence intervals: Which are preferable for the comparison of classifiers?,"Null hypothesis significance tests and their p-values currently dominate the statistical evaluation of classifiers in machine learning. Here, we discuss fundamental problems of this research practice. We focus on the problem of comparing multiple fully specified classifiers on a small-sample test set. On the basis of the method by Quesenberry and Hurst, we derive confidence intervals for the effect size, i.e. the difference in true classification performance. These confidence intervals disentangle the effect size from its uncertainty and thereby provide information beyond the p-value. This additional information can drastically change the way in which classification results are currently interpreted, published and acted upon. We illustrate how our reasoning can change, depending on whether we focus on p-values or confidence intervals. We argue that the conclusions from comparative classification studies should be based primarily on effect size estimation with confidence intervals, and not on significance tests and p-values. © 2013 Taylor and Francis Group, LLC.",Journal of Experimental and Theoretical Artificial Intelligence,10.1080/0952813X.2012.680252,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877652134&doi=10.1080%2f0952813X.2012.680252&partnerID=40&md5=ceb5634dfeff01992e09890ecb159bbf,2013,2021-07-20 15:49:32,2021-07-20 15:49:32
P4PFGJPT,journalArticle,2020,"Freixas, J.",On the enumeration of Boolean functions with distinguished variables,"Boolean functions have a fundamental role in neural networks and machine learning. Enumerating these functions and significant subclasses is a highly complex problem. Therefore, it is of interest to study subclasses that escape this limitation and can be enumerated by means of sequences depending on the number of variables. In this article, we obtain seven new formulas corresponding to enumerations of some subclasses of Boolean functions. The versatility of these functions does the problem interesting to several different fields as game theory, hypergraphs, reliability, cryptography or logic gates. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-020-05422-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096326218&doi=10.1007%2fs00500-020-05422-5&partnerID=40&md5=3314e9e71f43d0bb4f84b75b3848f4f2,2020,2021-07-20 15:49:32,2021-07-20 15:49:32
SFV8DBSQ,journalArticle,2021,"Sutradhar, P.R.; Bavikadi, S.; Connolly, M.; Prajapati, S.K.; Indovina, M.A.; Pudukotaidinakarrao, S.M.; Ganguly, A.",Look-up-Table based Processing-in-MemoryArchitecture with Programmable Precision-Scalingfor Deep Learning Applications,"Processing in memory (PIM) architecture, with its ability to perform ultra-low-latency parallel processing, is regarded as a more suitable alternative to von Neumann computing architectures for implementing data-intensive applications such as DNNs and CNNs. In this paper, we present a Look-up-Table (LUT) based PIM architecture aimed at CNN/DNN acceleration that replaces logic-based processing with pre-calculated results stored inside the LUTs to perform complex computations on the DRAM memory platform. Our LUT-based DRAM-PIM architecture offers superior performance with higher energy-efficiency compared to the conventional bit-wise parallel PIM architectures, while at the same time avoids fabrication challenges associated with the in-memory implementation of logic circuits. Also, the processing elements can be programmed and re-programmed to perform virtually any operation, including operations of Convolutional, Fully Connected, Pooling, and Activating Layers of CNN/DNN. Furthermore, it is capable of operating on several combinations of bit-widths of the operand data and thereby offers a wider range of flexibility across performance, precision and efficiency. Transmission Gate (TG) realization of the circuitry ensures minimal footprint from the PIM architecture. Our simulations demonstrate that the proposed architecture can perform AlexNet inference at a nearly 13x faster rate and 125x more efficiency compared to state-of-the-art GPU and also provides 1.35x higher throughput at 2.5x higher energy-efficiency compared to other DRAM-implemented LUT-PIM architectures. IEEE",IEEE Transactions on Parallel and Distributed Systems,10.1109/TPDS.2021.3066909,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103183339&doi=10.1109%2fTPDS.2021.3066909&partnerID=40&md5=d92ed64f6354c4477c7d948e0f81eaae,2021,2021-07-20 15:49:32,2021-07-20 15:49:32
RKPLTR2A,journalArticle,2020,"Xiao, Z.; Li, F.; Wu, R.; Jiang, H.; Hu, Y.; Ren, J.; Cai, C.; Iyengar, A.",TrajData: On Vehicle Trajectory Collection with Commodity Plug-and-Play OBU Devices,"For years, vehicle trajectory data have increasingly been important for a wide range of applications, from driver behavior investigation/classification, travel time/distance estimation, and routing in vehicular networks, to vehicle energy/emission evaluation. This article presents TrajData, the first systematic solution to reliable vehicle trajectory data collection, with only reliance on commercial-off-the-shelf (COTS) onboard unit (OBU) devices that utilize lightweight GPS modules and low-cost onboard diagnostics (OBD) readers. In the practical use of trajectory collection, GPS outages inevitably occur in urban environments thereby leading to large trajectory errors as well as missing vehicle location data. To resolve this, we propose a novel data-fusion-enabled deep learning approach with the purpose of achieving reliable vehicle trajectory collection in various urban road conditions. Specifically, we leverage motion information retrieved from OBD readers in TrajData to help reconstruct the trajectory data during GPS outages. By investigating the changes of direction angle from the OBD readings, we can identify different types of road sections. Furthermore, we integrate the neural arithmetic logic units (NALUs) into our trajectory reconstruction model to tame the challenges when GPS outages take place in various road sections. Experimental results from realistic data have demonstrated the effectiveness and reliability of the proposed method. In the road test, TrajData achieves an average position error below 15-m around a 60-s GPS outage, even in complex road sections, i.e., continuous turns and driving with accelerations/decelerations resulting in frequent changes of direction and speed. © 2014 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2020.3001566,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092169042&doi=10.1109%2fJIOT.2020.3001566&partnerID=40&md5=e4baf19c5c44fab88de34253231b0c77,2020,2021-07-20 15:49:32,2021-07-20 15:49:32
WK7KKF4E,journalArticle,2019,"Cao, P.; Gan, Y.; Dai, X.",Model-based sensorless robot collision detection under model uncertainties with a fast dynamics identification,"This article presents a novel model-based sensorless collision detection scheme for human–robot interaction. In order to recognize external impacts exerted on the manipulator with sensitivity and robustness without additional exteroceptive sensors, the method based on torque residual, which is the difference between nominal and actual torque, is adopted using only motor-side information. In contrast to classic dynamics identification procedure which requires complicated symbolic derivation, a sequential dynamics identification was proposed by decomposing robot dynamics into gravity and friction item, which is simple in symbolic expression and easy to identify with least squares method, and the remaining structure-complex torque effect. Subsequently, the remaining torque effect was reformulated to overcome the structural complexity of original expression and experimentally recovered using a machine learning approach named Lasso while keeping the involving candidates number reduced to a certain degree. Moreover, a state-dependent dynamic threshold was developed to handle the abnormal peaks in residual due to model uncertainties. The effectiveness of the proposed method was experimentally validated on a conventional industrial manipulator, which illustrates the feasibility and simplicity of the collision detection method. © The Author(s) 2019.",International Journal of Advanced Robotic Systems,10.1177/1729881419853713,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067361578&doi=10.1177%2f1729881419853713&partnerID=40&md5=fc0a472414d3cf73229057f89aee9fb3,2019,2021-07-20 15:49:32,2021-07-20 15:49:32
SDNHD4FX,journalArticle,2011,"Armengol, E.",Classification of melanomas in situ using knowledge discovery with explained case-based reasoning,"Objective: Early diagnosis of melanoma is based on the ABCD rule which considers asymmetry, border irregularity, color variegation, and a diameter larger than 5. mm as the characteristic features of melanomas. When a skin lesion presents these features it is excised as prevention. Using a non-invasive technique called dermoscopy, dermatologists can give a more accurate evaluation of skin lesions, and can therefore avoid the excision of lesions that are benign. However, dermatologists need to achieve a good dermatoscopic classification of lesions prior to extraction. In this paper we propose a procedure called LazyCL to support dermatologists in assessing the classification of skin lesions. Our goal is to use LazyCL for generating a domain theory to classify melanomas in situ. Methods: To generate a domain theory, the LazyCL procedure uses a combination of two artificial intelligence techniques: case-based reasoning and clustering. First LazyCL randomly creates clusters and then uses a lazy learning method called lazy induction of descriptions (LID) with leave-one-out on them. By means of LID, LazyCL collects explanations of why the cases in the database should belong to a class. Then the analysis of relationships among explanations produces an understandable clustering of the dataset. After a process of elimination of redundancies and merging of clusters, the set of explanations is reduced to a subset of it describing classes that are "" almost"" discriminant. The remaining explanations form a preliminary domain theory that is the basis on which experts can perform knowledge discovery. Results: We performed two kinds of experiments. First ones consisted on using LazyCL on a database containing the description of 76 melanomas. The domain theory obtained from these experiments was compared on previous experiments performed using a different clustering method called self-organizing maps (SOM).Results of both methods, LazyCL and SOM, were similar. The second kind of experiments consisted on using LazyCL on well known domains coming from the machine learning repository of the Irvine University. Thus, since these domains have known solution classes, we can prove that the clusters build by LazyCL are correct. Conclusions: We can conclude that LazyCL that uses explained case-based reasoning for knowledge discovery is feasible for constructing a domain theory. On one hand, experiments on the melanoma database show that the domain theory build by LazyCL is easy to understand. Explanations provided by LID are easily understood by domain experts since these descriptions involve the same attributes than they used to represent domain objects. On the other hand, experiments on standard machine learning data sets show that LazyCL is a good method of clustering since all clusters produced are correct. © 2010 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2010.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952509940&doi=10.1016%2fj.artmed.2010.09.001&partnerID=40&md5=cfde6260d531c99c975a0747270f3a24,2011,2021-07-20 15:49:32,2021-07-20 15:49:32
XNRMQE6L,journalArticle,2020,"Sikka, P.; Asati, A.R.; Shekhar, C.",High-speed and area-efficient Sobel edge detector on field-programmable gate array for artificial intelligence and machine learning applications,"Sobel edge detector is an algorithm commonly used in image processing and computer vision to extract edges from input images using derivative of image pixels in x and y directions against surrounding pixels. Most artificial intelligence and machine learning applications require image processing algorithms running in real time on hardware systems like field-programmable gate array (FPGAs). They typically require high throughput to match real-time speeds and since they run alongside other processing algorithms, they are required to be area efficient as well. This article proposes a high-speed and low-area implementation of the Sobel edge detection algorithm. We created the design using a novel high-level synthesis (HLS) design method based on application specific bit widths for intermediate data nodes. Register transfer level code was generated using MATLAB hardware description language (HDL) coder for HLS. The generated HDL code was implemented on Xilinx Kintex 7 field programmable gate array (FPGA) using Xilinx Vivado software. Our implementation results are superior to those obtained for similar implementations using the vendor library block sets as well as those obtained by other researchers using similar implementations in the recent past in terms of area and speed. We tested our algorithm on Kintex 7 using real-time input video with a frame resolution of 1920 × 1080. We also verified the functional simulation results with a golden MATLAB implementation using FPGA in the loop feature of HDL Verifier. In addition, we propose a generic area, speed, and power improvement methodology for different HLS tools and application designs. © 2020 Wiley Periodicals LLC.",Computational Intelligence,10.1111/coin.12334,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084438379&doi=10.1111%2fcoin.12334&partnerID=40&md5=472212efe50e30d23f122af1796c7991,2020,2021-07-20 15:49:32,2021-07-20 15:49:32
2M9NNJ37,journalArticle,2019,"Wahab, O.A.; Kara, N.; Edstrom, C.; Lemieux, Y.",MAPLE: A Machine Learning Approach for Efficient Placement and Adjustment of Virtual Network Functions,"As one of the many advantages of cloud computing, Network Function Virtualization (NFV) has revolutionized the network and telecommunication industry through enabling the migration of network functions from expensive dedicated hardware to software-defined components that run in the form of Virtual Network Functions (VNFs). However, with NFV comes numerous challenges related mainly to the complexity of deploying and adjusting VNFs in the physical networks, owing to the huge number of nodes and links in today's datacenters, and the inter-dependency among VNFs forming a certain network service. Several contributions have been made in an attempt to answer these challenges, where most of the existing solutions focus on the static placement of VNFs and overlook the dynamic aspect of the problem, which arises mainly due to the ever-changing resource availability in the cloud datacenters and the continuous mobility of the users. Few attempts have been lately made to incorporate the dynamic aspect to the VNF deployment solutions. The main problem of these approaches lies in their reactive readjustment scheme which determines the placement/migration strategy upon the receipt of a new request or the happening of a certain event, thus resulting in high setup latencies. In this paper, we take advantage of machine learning to reduce the complexity of the placement and readjustment processes through designing a cluster-based proactive solution. The solution consists of (1) an Integer Linear Programming (ILP) model that considers a tradeoff between the minimization of the latency, Service-Level Objective (SLO) violation cost, hardware utilization, and VNF readjustment cost, (2) an optimized k-medoids clustering approach which proactively partitions the substrate network into a set of disjoint on-demand clusters and (3) data-driven cluster-based placement and readjustment algorithms that capitalize on machine learning to intelligently eliminate some cost functions from the optimization problem to boost its feasibility in large-scale networks. Simulation results show that the proposed solution considerably reduces the readjustment time and decrease the hardware utilization compared to the K-means, original k-medoids and migration without clustering approaches. © 2019 Elsevier Ltd",Journal of Network and Computer Applications,10.1016/j.jnca.2019.06.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067443855&doi=10.1016%2fj.jnca.2019.06.003&partnerID=40&md5=a265fafa6da3450fa5aa1bc1fa2ea336,2019,2021-07-20 15:49:32,2021-07-20 15:49:32
NNZXIKAQ,journalArticle,2021,"Alexandre, F.",A global framework for a systemic view of brain modeling,"The brain is a complex system, due to the heterogeneity of its structure, the diversity of the functions in which it participates and to its reciprocal relationships with the body and the environment. A systemic description of the brain is presented here, as a contribution to developing a brain theory and as a general framework where specific models in computational neuroscience can be integrated and associated with global information flows and cognitive functions. In an enactive view, this framework integrates the fundamental organization of the brain in sensorimotor loops with the internal and the external worlds, answering four fundamental questions (what, why, where and how). Our survival-oriented definition of behavior gives a prominent role to pavlovian and instrumental conditioning, augmented during phylogeny by the specific contribution of other kinds of learning, related to semantic memory in the posterior cortex, episodic memory in the hippocampus and working memory in the frontal cortex. This framework highlights that responses can be prepared in different ways, from pavlovian reflexes and habitual behavior to deliberations for goal-directed planning and reasoning, and explains that these different kinds of responses coexist, collaborate and compete for the control of behavior. It also lays emphasis on the fact that cognition can be described as a dynamical system of interacting memories, some acting to provide information to others, to replace them when they are not efficient enough, or to help for their improvement. Describing the brain as an architecture of learning systems has also strong implications in Machine Learning. Our biologically informed view of pavlovian and instrumental conditioning can be very precious to revisit classical Reinforcement Learning and provide a basis to ensure really autonomous learning. © 2021, The Author(s).",Brain Informatics,10.1186/s40708-021-00126-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100920929&doi=10.1186%2fs40708-021-00126-4&partnerID=40&md5=ab623622f5ab9dec00c2ab4621918ec2,2021,2021-07-20 15:49:32,2021-07-20 15:49:32
4T8XRICS,journalArticle,2016,"Zhang, Y.; Wu, H.-Y.; Du, J.; Xu, J.; Wang, J.; Tao, C.; Li, L.; Xu, H.",Extracting drug-enzyme relation from literature as evidence for drug drug interaction,"Background: Information about drug-drug interactions (DDIs) is crucial for computational applications such as pharmacovigilance and drug repurposing. However, existing sources of DDIs have the problems of low coverage, low accuracy and low agreement. One common type of DDIs is related to the mechanism of drug metabolism: a DDI relation may be caused by different interactions (e.g., substrate, inhibit) between drugs and enzymes in the drug metabolism process. Thus, information from drug enzyme interactions (DEIs) serves as important supportive evidence for DDIs. Further, potential DDIs present implicitly could be detected by inference and reasoning based on DEIs. Methods: In this article, we propose a hybrid approach to combining machine learning algorithm with trigger words and syntactic patterns, for DEI relation extraction from biomedical literature. The extracted DEI relations are used for reasoning to infer potential DDI relations, based on a defined drug-enzyme ontology incorporating biological knowledge. Results: Evaluation results demonstrate that the performance of DEI relation extraction is promising, with an F-measure of 84.97 % on the in vivo dataset and 65.58 % on the in vitro dataset. Further, the inferred DDIs achieved a precision of 83.19 % on the in vivo dataset and 70.94 % on the in vitro dataset, respectively. A further examination showed that the overlaps between our inferred DDIs and those present in DrugBank were 42.02 % on the in vivo dataset and 19.23 % on the in vitro dataset, respectively. Conclusions: This paper proposed an effective approach to extract DEI relations from biomedical literature. Potential DDIs not present in existing knowledge bases were then inferred based on the extracted DEIs, demonstrating the capability of the proposed approach to detect DDIs with scientific evidence for pharmacovigilance and drug repurposing applications. © 2016 Zhang et al.",Journal of Biomedical Semantics,10.1186/s13326-016-0052-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963585171&doi=10.1186%2fs13326-016-0052-6&partnerID=40&md5=c6915a579fdb03a332aa212e85e34802,2016,2021-07-20 15:49:32,2021-07-20 15:49:32
SZE8ZFFL,journalArticle,2011,"Corazza, A.; Di Martino, S.; Ferrucci, F.; Gravino, C.; Mendes, E.",Investigating the use of Support Vector Regression for web effort estimation,"Support Vector Regression (SVR) is a new generation of Machine Learning algorithms, suitable for predictive data modeling problems. The objective of this paper is twofold: first, to investigate the effectiveness of SVR for Web effort estimation using a cross-company dataset; second, to compare different SVR configurations looking at the one that presents the best performance. In particular, we took into account three variables' preprocessing strategies (no-preprocessing, normalization, and logarithmic), in combination with two different dependent variables (effort and inverse effort). As a result, SVR was applied using six different data configurations. Moreover, to understand the suitability of kernel functions to handle non-linear problems, SVR was applied without a kernel, and in combination with the Radial Basis Function (RBF) and the Polynomial kernels, thus obtaining 18 different SVR configurations. To identify, for each configuration, which were the best values for each of the parameters we defined a procedure based on a leave-one-out cross-validation approach. The dataset employed was the Tukutuku database, which has been adopted in many previous Web effort estimation studies. Three different training and test set splits were used, including respectively 130 and 65 projects. The SVR-based predictions were also benchmarked against predictions obtained using Manual StepWise Regression and Case-Based Reasoning. Our results showed that the configuration corresponding to the logarithmic features' preprocessing, in combination with the RBF kernel provided the best results for all three data splits. In addition, SVR provided significantly superior prediction accuracy than all the considered benchmarking techniques. © 2010 Springer Science+Business Media, LLC.",Empirical Software Engineering,10.1007/s10664-010-9138-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953191774&doi=10.1007%2fs10664-010-9138-4&partnerID=40&md5=fe37bb19d27cf5f63a0b53391ce5b065,2011,2021-07-20 15:49:32,2021-07-20 15:49:32
BRAEVA3G,journalArticle,2020,"Xi, Y.; Zhang, Y.; Ding, S.; Wan, S.",Visual question answering model based on visual relationship detection,"visual question answering (VQA) is a learning task involving two major fields of computer vision and natural language processing. The development of deep learning technology has contributed to the advancement of this research area. Although the research on the question answering model has made great progress, the low accuracy of the VQA model is mainly because the current question answering model structure is relatively simple, the attention mechanism of model is deviated from human attention and lacks a higher level of logical reasoning ability. In response to the above problems, we propose a VQA model based on multi-objective visual relationship detection. Firstly, the appearance feature is used to replace the image features from the original object, and the appearance model is extended by the principle of word vector similarity. The appearance features and relationship predicates are then fed into the word vector space and represented by a fixed length vector. Finally, through the concatenation of elements between the image feature and the question vector are fed into the classifier to generate an output answer. Our method is benchmarked on the DQAUAR data set, and evaluated by the Acc WUPS@0.0 and WUPS@0.9. © 2019",Signal Processing: Image Communication,10.1016/j.image.2019.115648,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072767554&doi=10.1016%2fj.image.2019.115648&partnerID=40&md5=7b0be23ccadeacb2e8fdd5c876a0842f,2020,2021-07-20 15:49:32,2021-07-20 15:49:32
KLTAZDPV,journalArticle,2018,"Jiang, J.; Xie, J.; Zhao, C.; Su, J.; Guan, Y.; Yu, Q.",Max-margin weight learning for medical knowledge network,"Background and objective: The application of medical knowledge strongly affects the performance of intelligent diagnosis, and method of learning the weights of medical knowledge plays a substantial role in probabilistic graphical models (PGMs). The purpose of this study is to investigate a discriminative weight-learning method based on a medical knowledge network (MKN). Methods: We propose a training model called the maximum margin medical knowledge network (M3KN), which is strictly derived for calculating the weight of medical knowledge. Using the definition of a reasonable margin, the weight learning can be transformed into a margin optimization problem. To solve the optimization problem, we adopt a sequential minimal optimization (SMO) algorithm and the clique property of a Markov network. Ultimately, M3KN not only incorporates the inference ability of PGMs but also deals with high-dimensional logic knowledge. Results: The experimental results indicate that M3KN obtains a higher F-measure score than the maximum likelihood learning algorithm of MKN for both Chinese Electronic Medical Records (CEMRs) and Blood Examination Records (BERs). Furthermore, the proposed approach is obviously superior to some classical machine learning algorithms for medical diagnosis. To adequately manifest the importance of domain knowledge, we numerically verify that the diagnostic accuracy of M3KN is gradually improved as the number of learned CEMRs increase, which contain important medical knowledge. Conclusions: Our experimental results show that the proposed method performs reliably for learning the weights of medical knowledge. M3KN outperforms other existing methods by achieving an F-measure of 0.731 for CEMRs and 0.4538 for BERs. This further illustrates that M3KN can facilitate the investigations of intelligent healthcare. © 2018 Elsevier B.V.",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2018.01.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041550100&doi=10.1016%2fj.cmpb.2018.01.005&partnerID=40&md5=b51dda43144e3194deeef2a8e1241012,2018,2021-07-20 15:49:33,2021-07-20 15:49:33
3M6EZ6PX,journalArticle,2019,"Hashemi, M.; Hall, M.",Detecting and classifying online dark visual propaganda,"The staggering increase in the amount of information on the World Wide Web (referred to as Web) has made Web page classification essential to retrieve useful information while filtering out unwanted, futile, or harmful contents. This massive information-sharing platform is occasionally abused for propagating extreme and radical ideologies and posing threats to national security and citizens. Detecting the so called dark material has gained more impetus following the recent outbreak of extremist groups and radical ideologies across the Web. The goal of this project, being the first of its own, is to surveil online social networks (OSN) and Web for real-time detection of visual propaganda by violent extremist organizations (VEOs). This is valuable not only for flagging and removing such content from OSN and Web, but also to provide military insight and narrative context inside VEOs. Visual propaganda by VEOs are not only detected, but also further classified based on the type of VEO and focus or intent of the image into hard propaganda, soft propaganda, symbolic propaganda, landscape, and organizational communications. Over 1.2 million images were automatically collected from suspicious OSN accounts and Web pages over a course of four years. Out of which, 120,000 images were manually classified to provide the training data for a convolutional neural network. An overall generalization accuracy of 97.02% and F1 of 97.89% were achieved for a binary classification or mere detection of visual VEO propaganda and an overall generalization accuracy of 86.08% and F1¯ of 85.76% for an eight-way classification based on the intent of the image. © 2019 Elsevier B.V.",Image and Vision Computing,10.1016/j.imavis.2019.06.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069840826&doi=10.1016%2fj.imavis.2019.06.001&partnerID=40&md5=f85c41b08ccb64de01f130c43bba7dae,2019,2021-07-20 15:49:33,2021-07-20 15:49:33
M4I5ELLW,journalArticle,2021,"Huang, H.; Yang, J.; Rong, H.-J.; Du, S.",A generic FPGA-based hardware architecture for recursive least mean p-power extreme learning machine,"Recursive least mean p-power extreme learning machine (RLMP-ELM) is a newly proposed online machine learning algorithm and is able to provide a robust online prediction of the datasets with noises of different statistics. To further explore the proposed RLMP-ELM to be used in real-world embedded systems, a generic serial FPGA-based hardware architecture of RLMP-ELM is presented in this paper. The entire hardware architecture of RLMP-ELM includes three serial processing modules, which are implemented parameterizably and can be adapted for different application requirements. The hardware framework is in a serial fashion, but parallelization efforts are focused on the processes with high computing complexity by analysis of potential inter-task dependency. To overcome the limitation of memory bandwidth, the block RAM and ping-pong on-chip buffer are applied to improve the computational throughput. The validation experiments are performed through five datasets with different p values. Accuracy results show that our implementation on FPGA could achieve similar accuracy compared to 64-bit floating-point software implementation. We also report and compare hardware performance of our proposed architecture with other existing implementations. The results show that our hardware architecture offers the excellent balance among accuracy, logic occupation and hardware performance. © 2021",Neurocomputing,10.1016/j.neucom.2021.05.069,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109149389&doi=10.1016%2fj.neucom.2021.05.069&partnerID=40&md5=33eba638b88b57c4fa46cac7c26c7233,2021,2021-07-20 15:49:33,2021-07-20 15:49:33
KUC9MHM3,journalArticle,2021,"Sahani, M.; Swain, B.K.; Dash, P.K.",FPGA-based favourite skin colour restoration using improved histogram equalization with variable enhancement degree and ensemble extreme learning machine,"This paper presents skin color enhancement based on favorite skin color to agree with user-defined favorite skin color using improved histogram equalization with variable enhancement degree (IHEwVED) and machine learning methods. The skin color to be adjusted in the input image is shifted to favorite skin color by using novel control parameters of the proposed IHEwVED method. Three different novel display device-dependent color image processing methods are introduced based on hsv and yiq color space to obtain the desired enhanced output images. A reduced convolutional neural network and the novel ensemble extreme learning machine (EELM) architectures are developed and implemented in a field-programmable gate array to test, synthesize, and validate the recognition capability of the user-defined favorite skin color. The less computational complex proposed IHEwVED-EELM method recognizes 45 to 50 favorite skin color per second of test images by consuming 0.035 second training time with training root mean square error (RMSE) of 0.0048 and testing RMSE of 0.01208. Finally, a stand-alone favorite skin color restoration system is developed using the high-speed video processor NI-PXI-1031 based on the IHEwVED-EELM method in the Python-OpenCV environment. The laboratory experimental performances ascertain the real-time ability of the proposed favorite skin color restoration method. © 2020 The Authors. IET Image Processing published by John Wiley & Sons Ltd on behalf of The Institution of Engineering and Technology",IET Image Processing,10.1049/ipr2.12101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104258877&doi=10.1049%2fipr2.12101&partnerID=40&md5=ed320aaa2d60914ccc001abb28880b30,2021,2021-07-20 15:49:33,2021-07-20 15:49:33
QUT4VYBZ,journalArticle,2021,"Singh, V.K.; Govindarasu, M.",A Cyber-Physical Anomaly Detection for Wide-Area Protection using Machine Learning,"Wide-area protection scheme (WAPS) provides system-wide protection by detecting and mitigating small and large-scale disturbances that are difficult to resolve using local protection schemes. As this protection scheme is evolving from a substation-based distributed remedial action scheme (DRAS) to the control center-based centralized RAS (CRAS), it presents severe challenges to their cybersecurity because of its heavy reliance on an insecure grid communication, and its compromise would lead to system failure. This paper presents an architecture and methodology for developing a cyber-physical anomaly detection system (CPADS) that utilizes synchrophasor measurements and properties of network packets to detect data integrity and communication failure attacks on measurement and control signals in CRAS. The proposed machine leaning-based methodology applies a rules-based approach to select relevant input features, utilizes variational mode decomposition (VMD) and decision tree (DT) algorithms to develop multiple classification models, and performs final event identification using a rules-based decision logic. We have evaluated the proposed methodology of CPADS using the IEEE 39 bus system for several performance measures (accuracy, recall, precision, and F-measure) in a cyber-physical testbed environment. Our experimental results reveal that the proposed algorithm (VMD-DT) of CPADS outperforms the existing machine learning classifiers during noisy and noise-free measurements while incurring an acceptable processing overhead. IEEE",IEEE Transactions on Smart Grid,10.1109/TSG.2021.3066316,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103200711&doi=10.1109%2fTSG.2021.3066316&partnerID=40&md5=37bfc477a7597c181f153d53c5575760,2021,2021-07-20 15:49:33,2021-07-20 15:49:33
GQEUII25,journalArticle,2020,"Gonzalez-Guerrero, P.; Tracy Ii, T.; Guo, X.; Sreekumar, R.; Lenjani, M.; Skadron, K.; Stan, M.R.",Towards on-node Machine Learning for Ultra-low-power Sensors Using Asynchronous ΣΔ Streams,"We propose a novel architecture to enable low-power, complex on-node data processing, for the next generation of sensors for the internet of things (IoT), smartdust, or edge intelligence. Our architecture combines near-analog-memory-computing (NAM) and asynchronous-computing-with-streams (ACS), eliminating the need for ADCs. ACS enables ultra-low power, massive computational resources required to execute on-node complex Machine Learning (ML) algorithms; while NAM addresses the memory-wall that represents a common bottleneck for ML and other complex functions. In ACS an analog value is mapped to an asynchronous stream that can take one of two logic levels (vh, vl). This stream-based data representation enables area/power-efficient computing units such as a multiplier implemented as an AND gate yielding savings in power of g1/490% compared to digital approaches. The generation of streams for NAM and ACS in a brute force manner, using analog-to-digital-converters (ADCs) and digital-to-streams-converters, would sky-rocket the power-latency-energy cost making the approach impractical. Our NAM-ACS architecture eliminates expensive conversions, enabling an end-to-end processing on asynchronous streams data-path. We tailor the NAM-ACS architecture for random forest (RaF), an ML algorithm, chosen for its ability to classify using a reduced number of features. Simulations show that our NAM-ACS architecture enables 75% of savings in power compared with a single ADC, obtaining a classification accuracy of 85% using an RaF-inspired algorithm. © 2020 ACM.",ACM Journal on Emerging Technologies in Computing Systems,10.1145/3404975,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093658754&doi=10.1145%2f3404975&partnerID=40&md5=81eaa846f2df5be66d607ed5cb6d143a,2020,2021-07-20 15:49:33,2021-07-20 15:49:33
ZBMRXZUT,journalArticle,2019,"Tan, K.; Wang, R.; Li, M.; Gong, Z.",Discriminating soybean seed varieties using hyperspectral imaging and machine learning,"Purifying soybean seed is greatly beneficial for improving the quality of soybean planting and products. Since the seeds are usually evaluated in many fields for sowing and oilseed processing, they must be identified quickly and accurately for selecting a correct variety. In this study, an acceptable method combining spectral information operating in the spectral wavelengths from 400 nm to 1000 nm, with machine learning was proposed to classify 10 soybean varieties. After pre-processing original data, principal component analysis (PCA) was performed to reduce the spectra dimensionality. The texture feature parameters (energy, entropy, inertia moment and correlation) were extracted from three feature images selected by PCA. In this way, each sample was represented by 9 features including PC scores, mean value and standard deviations of inertia moment under three feature images. In the experimental study, soft independent modelling of class analogy (SIMCA), partial squares discriminant analysis (PLS-DA), genetic algorithm optimized BP neural network (GA-BP) and Takagi-Sugeno fuzzy neural network (T-S fuzzy neural network) were built to distinguish different soybean varieties. The performance of the classification models was analysed when they were applied to the feature matrix. The average accuracy of the training set was higher than 96% and the average accuracy of the testing set was higher than 84%. Although GA-BP network model got the highest predictive accuracy with 92%, T-S fuzzy neural network model was the best choice considering the accepted identification accuracy, better stability and less computation cost. © 2019 - IOS Press and the authors. All rights reserved.",Journal of Computational Methods in Sciences and Engineering,10.3233/JCM-193562,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075740981&doi=10.3233%2fJCM-193562&partnerID=40&md5=d04c4e8c1704e3fe6326d5c69d6a864a,2019,2021-07-20 15:49:33,2021-07-20 15:49:33
6IQNGNRB,journalArticle,2019,"Nguyen, T.G.; Phan, T.V.; Nguyen, B.T.; So-In, C.; Baig, Z.A.; Sanguanpong, S.",SeArch: A Collaborative and Intelligent NIDS Architecture for SDN-Based Cloud IoT Networks,"The explosive rise of intelligent devices with ubiquitous connectivity have dramatically increased Internet of Things (IoT) traffic in the cloud environment and created potential attack surfaces for cyber-attacks. Traditional security approaches are insufficient and inefficient to address security threats in cloud-based IoT networks. In this vein, software defined networking (SDN), network function virtualization (NFV), and machine learning techniques introduce numerous advantages that can effectively resolve cybersecurity matters for cloud-based IoT systems. In this paper, we propose a collaborative and intelligent network-based intrusion detection system (NIDS) architecture, namely SeArch for SDN-based cloud IoT networks. It composes a hierarchical layer of intelligent IDS nodes working in collaboration to detect anomalies and formulate policy into the SDN-based IoT gateway devices to stop malicious traffic as fast as possible. We first describe a new NIDS architecture with a comprehensive analysis in terms of the system resource and path selection optimizations. Next, the system process logic is extensively investigated through main consecutive procedures, including initialization, runtime operation, and database update. Afterward, we conduct a detailed implementation of the proposed solution in an SDN-based environment and perform a variety of experiments. Finally, evaluation results of the $SeArch$ architecture yield outstanding performance in anomaly detection and mitigation as well as bottleneck problem handling in the SDN-based cloud IoT networks in comparison with existing solutions. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2932438,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071166519&doi=10.1109%2fACCESS.2019.2932438&partnerID=40&md5=e227852b9e22db114bbd28f4e1d676ab,2019,2021-07-20 15:49:33,2021-07-20 15:49:33
5G6VGPHI,journalArticle,2017,"Orsolic, I.; Pevec, D.; Suznjevic, M.; Skorin-Kapov, L.",A machine learning approach to classifying YouTube QoE based on encrypted network traffic,"Due to the widespread use of encryption in Over-The-Top video streaming traffic, network operators generally lack insight into application-level quality indicators (e.g., video quality levels, buffer underruns, stalling duration). They are thus faced with the challenge of finding solutions for monitoring service performance and estimating customer Quality of Experience (QoE) degradations based solely on passive monitoring solutions deployed within their network. We address this challenge by considering the concrete case of YouTube, whereby we present a methodology for the classification of end users’ QoE when watching YouTube videos, based only on statistical properties of encrypted network traffic. We have developed a system called YouQ which includes tools for monitoring and analysis of application-level quality indicators and corresponding traffic traces. Collected data is then used for the development of machine learning models for QoE classification based on computed traffic features per video session. To test the YouQ system and methodology, we collected a dataset corresponding to 1060 different YouTube videos streamed across 39 different bandwidth scenarios, and tested various classification models. Classification accuracy was found to be up to 84% when using three QoE classes (“low”, “medium” or “high”) and up to 91% when using binary classification (classes “low” and “high”). To improve the models in the future, we discuss why and when prediction errors occur. Moreover, we have analysed YouTube’s adaptation algorithm, thus providing valuable insight into the logic behind the quality level selection strategy, which may also be of interest in improving future QoE estimation algorithms. © 2017, Springer Science+Business Media New York.",Multimedia Tools and Applications,10.1007/s11042-017-4728-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018750008&doi=10.1007%2fs11042-017-4728-4&partnerID=40&md5=f57be2bb9a298e2adbb24b0301792901,2017,2021-07-20 15:49:33,2021-07-20 15:49:33
PQ6IPNZM,journalArticle,2017,"Verwer, S.; Zhang, Y.; Ye, Q.C.",Auction optimization using regression trees and linear models as integer programs,"In a sequential auction with multiple bidding agents, the problem of determining the ordering of the items to sell in order to maximize the expected revenue is highly challenging. The challenge is largely due to the fact that the autonomy and private information of the agents heavily influence the outcome of the auction. The main contribution of this paper is two-fold. First, we demonstrate how to apply machine learning techniques to solve the optimal ordering problem in sequential auctions. We learn regression models from historical auctions, which are subsequently used to predict the expected value of orderings for new auctions. Given the learned models, we propose two types of optimization methods: a black-box best-first search approach, and a novel white-box approach that maps learned regression models to integer linear programs (ILP), which can then be solved by any ILP-solver. Although the studied auction design problem is hard, our proposed optimization methods obtain good orderings with high revenues. Our second main contribution is the insight that the internal structure of regression models can be efficiently evaluated inside an ILP solver for optimization purposes. To this end, we provide efficient encodings of regression trees and linear regression models as ILP constraints. This new way of using learned models for optimization is promising. As the experimental results show, it significantly outperforms the black-box best-first search in nearly all settings. © 2015 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2015.05.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930456355&doi=10.1016%2fj.artint.2015.05.004&partnerID=40&md5=f9c71ba434efb9563b7ddf74084bbed9,2017,2021-07-20 15:49:33,2021-07-20 15:49:33
9QJYZ5BJ,journalArticle,2013,"Ruhrmair, U.; Solter, J.; Sehnke, F.; Xu, X.; Mahmoud, A.; Stoyanova, V.; Dror, G.; Schmidhuber, J.; Burleson, W.; Devadas, S.",PUF modeling attacks on simulated and silicon data,"We discuss numerical modeling attacks on several proposed strong physical unclonable functions (PUFs). Given a set of challenge-response pairs (CRPs) of a Strong PUF, the goal of our attacks is to construct a computer algorithm which behaves indistinguishably from the original PUF on almost all CRPs. If successful, this algorithm can subsequently impersonate the Strong PUF, and can be cloned and distributed arbitrarily. It breaks the security of any applications that rest on the Strong PUF's unpredictability and physical unclonability. Our method is less relevant for other PUF types such as Weak PUFs. The Strong PUFs that we could attack successfully include standard Arbiter PUFs of essentially arbitrary sizes, and XOR Arbiter PUFs, Lightweight Secure PUFs, and Feed-Forward Arbiter PUFs up to certain sizes and complexities. We also investigate the hardness of certain Ring Oscillator PUF architectures in typical Strong PUF applications. Our attacks are based upon various machine learning techniques, including a specially tailored variant of logistic regression and evolution strategies. Our results are mostly obtained on CRPs from numerical simulations that use established digital models of the respective PUFs. For a subset of the considered PUFs-namely standard Arbiter PUFs and XOR Arbiter PUFs-we also lead proofs of concept on silicon data from both FPGAs and ASICs. Over four million silicon CRPs are used in this process. The performance on silicon CRPs is very close to simulated CRPs, confirming a conjecture from earlier versions of this work. Our findings lead to new design requirements for secure electrical Strong PUFs, and will be useful to PUF designers and attackers alike. © 2005-2012 IEEE.",IEEE Transactions on Information Forensics and Security,10.1109/TIFS.2013.2279798,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887313066&doi=10.1109%2fTIFS.2013.2279798&partnerID=40&md5=8f55e3afdb7c8731e1afdb884869cafc,2013,2021-07-20 15:49:33,2021-07-20 15:49:33
9B76QITB,journalArticle,2020,"Zhang, C.; Yang, Z.; He, X.; Deng, L.","Multimodal Intelligence: Representation Learning, Information Fusion, and Applications","Deep learning methods haverevolutionized speech recognition, image recognition, and natural language processing since 2010. Each of these tasks involves a single modality in their input signals. However, many applications in the artificial intelligence field involve multiple modalities. Therefore, it is of broad interest to study the more difficult and complex problem of modeling and learning across multiple modalities. In this paper, we provide a technical review of available models and learning methods for multimodal intelligence. The main focus of this review is the combination of vision and natural language modalities, which has become an important topic in both the computer vision and natural language processing research communities. This review provides a comprehensive analysis of recent works on multimodal deep learning from three perspectives: learning multimodal representations, fusing multimodal signals at various levels, and multimodal applications. Regarding multimodal representation learning, we review the key concepts of embedding, which unify multimodal signals into a single vector space and thereby enable cross-modality signal processing. We also review the properties of many types of embeddings that are constructed and learned for general downstream tasks. Regarding multimodal fusion, this review focuses on special architectures for the integration of representations of unimodal signals for a particular task. Regarding applications, selected areas of a broad interest in the current literature are covered, including image-to-text caption generation, text-to-image generation, and visual question answering. We believe that this review will facilitate future studies in the emerging field of multimodal intelligence for related communities. © 2007-2012 IEEE.",IEEE Journal on Selected Topics in Signal Processing,10.1109/JSTSP.2020.2987728,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083734422&doi=10.1109%2fJSTSP.2020.2987728&partnerID=40&md5=9d3cb0d27b74f1cfef6cff9aa167b68f,2020,2021-07-20 15:49:33,2021-07-20 15:49:33
T4AVNNAX,journalArticle,2018,"Li, W.; Meng, W.; Su, C.; Kwok, L.F.",Towards false alarm reduction using fuzzy if-Then rules for medical cyber physical systems,"Cyber-Physical Systems (CPS) are integrations of computation, networking, and physical processes. Its process control is often referred to as embedded systems. Generally, CPS and Internet of Things have the same basic architecture, whereas the former shows a higher combination and coordination between physical and computational elements, i.e., wireless sensor networks can be a vital part of CPS applications. With the rapid development, CPS has been applied to healthcare industry, where a wide range of medical sensors are used within a healthcare organization. However, these sensors may generate a large number of false alarms in practice, which could significantly reduce the system effectiveness. Targeting on this issue, in this work, we attempt to design a Medical Fuzzy Alarm Filter (named MFAFilter) for healthcare environments by means of fuzzy logic, especially fuzzy if-Then rules, which could handle the vague and imprecise among data. In the evaluation, we conducted two major experiments to explore the performance of our approach in a simulated and a real network environment, respectively. Experimental results demonstrate that the use of fuzzy if-Then rules could achieve a better accuracy as compared to the traditional supervised algorithms, and that our designed filter is effective in the practical environment. © 2018 IEEE.",IEEE Access,10.1109/ACCESS.2018.2794685,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040924131&doi=10.1109%2fACCESS.2018.2794685&partnerID=40&md5=5b8ce9f76d28fff37fcda64d6f57f991,2018,2021-07-20 15:49:34,2021-07-20 15:49:34
THIZLKJE,journalArticle,2015,"Nguyen, T.; Khosravi, A.; Creighton, D.; Nahavandi, S.",Classification of healthcare data using genetic fuzzy logic system and wavelets,"Healthcare plays an important role in promoting the general health and well-being of people around the world. The difficulty in healthcare data classification arises from the uncertainty and the high-dimensional nature of the medical data collected. This paper proposes an integration of fuzzy standard additive model (SAM) with genetic algorithm (GA), called GSAM, to deal with uncertainty and computational challenges. GSAM learning process comprises three continual steps: rule initialization by unsupervised learning using the adaptive vector quantization clustering, evolutionary rule optimization by GA and parameter tuning by the gradient descent supervised learning. Wavelet transformation is employed to extract discriminative features for high-dimensional datasets. GSAM becomes highly capable when deployed with small number of wavelet features as its computational burden is remarkably reduced. The proposed method is evaluated using two frequently-used medical datasets: the Wisconsin breast cancer and Cleveland heart disease from the UCI Repository for machine learning. Experiments are organized with a five-fold cross validation and performance of classification techniques are measured by a number of important metrics: accuracy, F-measure, mutual information and area under the receiver operating characteristic curve. Results demonstrate the superiority of the GSAM compared to other machine learning methods including probabilistic neural network, support vector machine, fuzzy ARTMAP, and adaptive neuro-fuzzy inference system. The proposed approach is thus helpful as a decision support system for medical practitioners in the healthcare practice. © 2014 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2014.10.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910637574&doi=10.1016%2fj.eswa.2014.10.027&partnerID=40&md5=01d6475687aa273e77768e7878720d33,2015,2021-07-20 15:49:34,2021-07-20 15:49:34
B9IDA3AQ,journalArticle,2020,"Pazienza, A.; Grossi, D.; Grasso, F.; Palmieri, R.; Zito, M.; Ferilli, S.",An abstract argumentation approach for the prediction of analysts' recommendations following earnings conference calls,"Financial analysts constitute an important element of financial decision-making in stock exchanges throughout the world. By leveraging on argumentative reasoning, we develop a method to predict financial analysts' recommendations in earnings conference calls (ECCs), an important type of financial communication. We elaborate an analysis to select those reliable arguments in the Questions Answers (QA) part of ECCs that analysts evaluate to estimate their recommendation. The observation date of stock recommendation update may variate during the next quarter: it can be either the day after the ECC or it can take weeks. Our objective is to anticipate analysts' recommendations by predicting their judgment with the help of abstract argumentation. In this paper, we devise our approach to the analysis of ECCs, by designing a general processing framework which combines natural language processing along with abstract argumentation evaluation techniques to produce a final scoring function, representing the analysts' prediction about the company's trend. Then, we evaluate the performance of our approach by specifying a strategy to predict analysts recommendations starting from the evaluation of the argumentation graph properly instantiated from an ECC transcript. We also provide the experimental setting in which we perform the predictions of recommendations as a machine learning classification task. The method is shown to outperform approaches based only on sentiment analysis. © 2019-IOS Press and the authors. All rights reserved.",Intelligenza Artificiale,10.3233/IA-190026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078915179&doi=10.3233%2fIA-190026&partnerID=40&md5=490d26156f4e3c618e5ac0fbe3b3f566,2020,2021-07-20 15:49:34,2021-07-20 15:49:34
S95G589G,journalArticle,2021,"Kang, C.; Yu, X.; Wang, S.-H.; Guttery, D.S.; Pandey, H.M.; Tian, Y.; Zhang, Y.-D.",A Heuristic Neural Network Structure Relying on Fuzzy Logic for Images Scoring,"Traditional deep learning methods are suboptimal in classifying ambiguity features, which often arise in noisy and hard to predict categories, especially, to distinguish semantic scoring. Semantic scoring, depending on semantic logic to implement evaluation, inevitably contains fuzzy description and misses some concepts, for example, the ambiguous relationship between normal and probably normal always presents unclear boundaries (normal - more likely normal - probably normal). Thus, human error is common when annotating images. Differing from existing methods that focus on modifying kernel structure of neural networks, this article proposes a dominant fuzzy fully connected layer (FFCL) for breast imaging reporting and data system (BI-RADS) scoring and validates the universality of this proposed structure. This proposed model aims to develop complementary properties of scoring for semantic paradigms, while constructing fuzzy rules based on analyzing human thought patterns, and to particularly reduce the influence of semantic conglutination. Specifically, this semantic-sensitive defuzzifier layer projects features occupied by relative categories into semantic space, and a fuzzy decoder modifies probabilities of the last output layer referring to the global trend. Moreover, the ambiguous semantic space between two relative categories shrinks during the learning phases, as the positive and negative growth trends of one category appearing among its relatives were considered. We first used the Euclidean distance to zoom in the distance between the real scores and the predicted scores, and then employed two sample t test method to evidence the advantage of the FFCL architecture. Extensive experimental results performed on the curated breast imaging subset of digital database of screening mammography dataset show that our FFCL structure can achieve superior performances for both triple and multiclass classification in BI-RADS scoring, outperforming the state-of-the-art methods. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2020.2966163,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098872004&doi=10.1109%2fTFUZZ.2020.2966163&partnerID=40&md5=181189cb6622f255cce4df0b60847815,2021,2021-07-20 15:49:34,2021-07-20 15:49:34
TNCSQ7X2,journalArticle,2021,"Baskin, C.; Liss, N.; Schwartz, E.; Zheltonozhskii, E.; Giryes, R.; Bronstein, A.M.; Mendelson, A.",UNIQ: Uniform Noise Injection for Non-Uniform Qantization of Neural Networks,"We present a novel method for neural network quantization. Our method, named UNIQ, emulates a non-uniform k-quantile quantizer and adapts the model to perform well with quantized weights by injecting noise to the weights at training time. As a by-product of injecting noise to weights, we find that activations can also be quantized to as low as 8-bit with only a minor accuracy degradation. Our non-uniform quantization approach provides a novel alternative to the existing uniform quantization techniques for neural networks. We further propose a novel complexity metric of number of bit operations performed (BOPs), and we show that this metric has a linear relation with logic utilization and power. We suggest evaluating the trade-off of accuracy vs. complexity (BOPs). The proposed method, when evaluated on ResNet18/34/50 and MobileNet on ImageNet, outperforms the prior state of the art both in the low-complexity regime and the high accuracy regime. We demonstrate the practical applicability of this approach, by implementing our non-uniformly quantized CNN on FPGA. © 2021 ACM.",ACM Transactions on Computer Systems,10.1145/3444943,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109098163&doi=10.1145%2f3444943&partnerID=40&md5=601fe8e18cb41ec1d8ed418346e79a57,2021,2021-07-20 15:49:34,2021-07-20 15:49:34
SGJKER23,journalArticle,2021,"Siddique, N.; Paheding, S.; Elkin, C.P.; Devabhaktuni, V.",U-net and its variants for medical image segmentation: A review of theory and applications,"U-net is an image segmentation technique developed primarily for image segmentation tasks. These traits provide U-net with a high utility within the medical imaging community and have resulted in extensive adoption of U-net as the primary tool for segmentation tasks in medical imaging. The success of U-net is evident in its widespread use in nearly all major image modalities, from CT scans and MRI to Xrays and microscopy. Furthermore, while U-net is largely a segmentation tool, there have been instances of the use of U-net in other applications. Given that U-net&#x2019;s potential is still increasing, this narrative literature review examines the numerous developments and breakthroughs in the U-net architecture and provides observations on recent trends. We also discuss the many innovations that have advanced in deep learning and discuss how these tools facilitate U-net. In addition, we review the different image modalities and application areas that have been enhanced by U-net. CCBY",IEEE Access,10.1109/ACCESS.2021.3086020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107339751&doi=10.1109%2fACCESS.2021.3086020&partnerID=40&md5=76ca8ca8d415ccda45507145fa7ee254,2021,2021-07-20 15:49:34,2021-07-20 15:49:34
FH2AIUFJ,journalArticle,2020,"Ensign, D.; Neville, S.; Paul, A.; Venkatasubramanian, S.",The complexity of explaining neural networks through (group) invariants,"Ever since the work of Minsky and Papert, it has been thought that neural networks derive their effectiveness by finding representations of the data that are invariant with respect to the task. In other words, the representations eliminate components of the data that vary in a way that is irrelevant. These invariants are naturally expressed with respect to group operations, and thus an understanding of these groups is key to explaining the effectiveness of the neural network. Moreover, a line of work in deep learning has shown that explicit knowledge of group invariants can lead to more effective training results. In this paper, we investigate the difficulty of discovering anything about these implicit invariants. Unfortunately, our main results are negative: we show that a variety of questions around investigating invariant representations are NP-hard, even in approximate settings. Moreover, these results do not depend on the kind of architecture used: in fact, our results follow as soon as the network architecture is powerful enough to be universal. The key idea behind our results is that if we can find the symmetries of a problem then we can solve it. © 2019 Elsevier B.V.",Theoretical Computer Science,10.1016/j.tcs.2019.11.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075445213&doi=10.1016%2fj.tcs.2019.11.012&partnerID=40&md5=a8de91367d48f8b41eed0506dc9c898f,2020,2021-07-20 15:49:34,2021-07-20 15:49:34
ARB68DCC,journalArticle,2020,"Du, G.L.; Wang, Z.; Li, C.; Liu, P.X.",A TSK-type Convolutional Recurrent Fuzzy Network for Predicting Driving Fatigue,"Driver fatigue monitoring is very important for driving safety, and many intricate factors in driving make fatigue monitoring harder. To effectively predict driving fatigue, this paper proposes a new deep learning framework called TSK-type Convolution Recurrent Fuzzy Network (TCRFN) based on the spatial and temporal characteristics of EEG signals. In TCRFN, the convolution block is first introduced to extract spatial dependencies from EEG signals. Furthermore, since EEG noise has a strong spatial dependence, this Convolutional Neural Networks (CNN) is used to reduce the impact of noise. Additionally, a new local feedback method in Fuzzy Neural Network (FNN) is proposed to process the EEG signals, which can better capture the temporal dependencies from EEG signals. Finally, a logarithmic spatial activation layer function is used in the proposed TCRFN. The activation performance of this function is smoother, which allows more feature numbers and provides better prediction. The experimental results show that the proposed TCRFN model has better anti-noise performance and prediction accuracy compared with other widely-used and state-of-the-art models. IEEE",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2020.2992856,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085745330&doi=10.1109%2fTFUZZ.2020.2992856&partnerID=40&md5=e35dcf67238e44c0195dd923c8c1fcd3,2020,2021-07-20 15:49:34,2021-07-20 15:49:34
PVUE3DDF,journalArticle,2020,"Khooban, M.H.; Gheisarnejad, M.",A Novel Deep Reinforcement Learning Controller Based Type-II Fuzzy System: Frequency Regulation in Microgrids,"The high-penetration of distributed generation technologies in the form of MicroGrids (MGs), in recent years, has increased the risk of frequency instability since their energy is supplied by renewable energy resources (RESs) with uncertain nature. Under such circumstances, providing an MG model with an efficient load frequency control (LFC) has a fundamental role in restoring the stability of the unstructured power system. In this study, a hybrid power system with the application of the Tidal Power Unit (TPU) and Vehicle-to-Grid (V2G) is effectively planed as an isolated MG. A new fractional gradient descent (FGD) based on a single-input interval type-2 fuzzy logic controller (SIT2-FLC) is suggested as the main LFC controller, where the footprint of uncertainty (FOU) coefficient of the SIT2-FLC is specifically adjusted to enhance the LFC performance. Additionally, a deep deterministic policy gradient (DDPG) with the actor-critic framework is considered to generate the supplementary control action, which is useful for the frequency stabilization by adapting to the randomness of load disturbances and RESs. Lastly, a model-in-the-loop (MiL) simulation is conducted to appraise the feasibility and applicability of the suggested design method from a systemic perspective. IEEE",IEEE Transactions on Emerging Topics in Computational Intelligence,10.1109/TETCI.2020.2964886,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082837743&doi=10.1109%2fTETCI.2020.2964886&partnerID=40&md5=05390d0862aeca857a25f6b11e67ec41,2020,2021-07-20 15:49:34,2021-07-20 15:49:34
LTCVMWCC,journalArticle,2020,"Pattanaik, M.L.; Choudhary, R.; Kumar, B.",Prediction of frictional characteristics of bituminous mixes using group method of data handling and multigene symbolic genetic programming,"A safe road transport requires adequate friction between vehicle tires and pavement surface for safe travels. Adequate friction is essential for a vehicle to safely maneuver. Inadequate friction is directly correlated to the accident hazard, particularly in wet weather conditions. Quality of pavement materials has a prodigious effect on skid resistance and high-quality materials allow adequate pavement frictional resistance for an extended period. Science evaluation of frictional characteristics depends on physical, chemical, and mineralogical properties of the aggregate, type of mix, binder/bitumen content, water film thickness, etc., and it necessities a costly and time-consuming test protocol. In the present research, a model is developed for the evaluation of skid resistance in terms of the British pendulum number (BPN), using experimental observations, with the aid of machine learning tools. In the present work, group method of data handling (GMDH) and multigene symbolic genetic programming (MSGP) have been used to model the BPN. Developed model is capable to simplify extremely nonlinear deviations in data as well as forecast the frictional performance from experimental data. It is also found that the performance of the MSGP (R2 = 0.99) is more encouraging and better than that of the GMDH model (R2 = 0.98) for the prediction of BPN. The analytical expression obtained through MSGP in the present study has been also subjected to sensitivity analysis to assess the effect of individual parameters in prediction of BPN. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Engineering with Computers,10.1007/s00366-019-00802-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068183964&doi=10.1007%2fs00366-019-00802-4&partnerID=40&md5=0d056b7f8c9e25a57c746b731234cba1,2020,2021-07-20 15:49:34,2021-07-20 15:49:34
T87KI7TJ,journalArticle,2018,"Gou, Z.; Han, L.; Sun, L.; Zhu, J.; Yan, H.",Constructing Dynamic Topic Models Based on Variational Autoencoder and Factor Graph,"Topic models are widely used in various fields of machine learning and statistics. Among them, the dynamic topic model (DTM) is the most popular time-series topic model for the dynamic representations of text corpora. A major challenge is that the posterior distribution of DTM requires a complex reasoning process with the high cost of computing time in modeling, and even a tiny change of model requires restructuring. For these reasons, the variability and generality of DTM is so poor that DTM is difficult to be carried out. In this paper, we introduce a new method for constructing DTM based on variational autoencoder and factor graphs. This model uses re-parameterization of the variational lower bound to generate a lower bound estimator which is optimized by standard stochastic gradient descent method directly. At the same time, the optimization process is simplified by integrating the dynamic factor graph in the state space to achieve a better model. The experimental dataset uses a journal paper corpus that mainly focuses on natural language processing and spans twenty-five years (1984-2009) from DBLP. Experiment results indicate that the proposed method is effective and feasible by comparing several state-of-the-art baselines. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2869838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053353852&doi=10.1109%2fACCESS.2018.2869838&partnerID=40&md5=59dd2a048ecf2f36f43422ec72fbab6d,2018,2021-07-20 15:49:34,2021-07-20 15:49:34
8K4KGVVG,journalArticle,2019,"Casacuberta, D.; Guersenzvaig, A.",Using Dreyfus’ legacy to understand justice in algorithm-based processes,"As AI is linked to more and more aspects of our lives, the need for algorithms that can take decisions that are not only accurate but also fair becomes apparent. It can be seen both in discussions of future trends such as autonomous vehicles or the issue of superintelligence, as well as actual implementations of machine learning used to decide whether a person should be admitted in certain university or will be able to return a credit. In this paper, we will use Dreyfus’ account on ethical expertise to show that, to give an AI some ability to make ethical judgements, a pure symbolic, conceptual approach is not enough. We also need the ability to make sense of the surroundings to reframe and define situations in a dynamic way, using multiple perspectives in a pre-reflective way. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",AI and Society,10.1007/s00146-018-0803-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040862791&doi=10.1007%2fs00146-018-0803-2&partnerID=40&md5=c4a1a1edd9b9a4cf12b7f4ad9003884d,2019,2021-07-20 15:49:34,2021-07-20 15:49:34
H344I8AG,journalArticle,2018,"Sanyal, A.; Kumar, P.; Kar, P.; Chawla, S.; Sebastiani, F.",Optimizing non-decomposable measures with deep networks,"We present a class of algorithms capable of directly training deep neural networks with respect to popular families of task-specific performance measures for binary classification such as the F-measure, QMean and the Kullback–Leibler divergence that are structured and non-decomposable. Our goal is to address tasks such as label-imbalanced learning and quantification. Our techniques present a departure from standard deep learning techniques that typically use squared or cross-entropy loss functions (that are decomposable) to train neural networks. We demonstrate that directly training with task-specific loss functions yields faster and more stable convergence across problems and datasets. Our proposed algorithms and implementations offer several advantages including (i) the use of fewer training samples to achieve a desired level of convergence, (ii) a substantial reduction in training time, (iii) a seamless integration of our implementation into existing symbolic gradient frameworks, and (iv) assurance of convergence to first order stationary points. It is noteworthy that the algorithms achieve this, especially point (iv), despite being asked to optimize complex objective functions. We implement our techniques on a variety of deep architectures including multi-layer perceptrons and recurrent neural networks and show that on a variety of benchmark and real data sets, our algorithms outperform traditional approaches to training deep networks, as well as popular techniques used to handle label imbalance. © 2018, The Author(s).",Machine Learning,10.1007/s10994-018-5736-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049596317&doi=10.1007%2fs10994-018-5736-y&partnerID=40&md5=c740abb25655902029931b9932d3caa8,2018,2021-07-20 15:49:34,2021-07-20 15:49:34
HG6XFNAU,journalArticle,2020,"Krishnamurthy, P.; Karri, R.; Khorrami, F.",Anomaly Detection in Real-Time Multi-Threaded Processes Using Hardware Performance Counters,"We propose a novel methodology for real-time monitoring of software running on embedded processors in cyber-physical systems (CPS). The approach uses real-time monitoring of hardware performance counters (HPC) and applies to multi-threaded and interrupt-driven processes typical in programmable logic controller (PLC) implementation of real-time controllers. The methodology uses a black-box approach to profile the target process using HPCs. The time series of HPC measurements over a time window under known-good operating conditions is used to train a machine learning classifier. At run-time, this trained classifier classifies the time series of HPC measurements as baseline (i.e., probabilistically corresponding to a model learned from the training data) or anomalous. The baseline versus anomalous labels over successive time windows offer robustness against the stochastic variability of code execution on the embedded processor and detect code modifications. We demonstrate effectiveness of the approach on an embedded PLC in a hardware-in-the-loop (HITL) testbed emulating a benchmark industrial process. In addition, to illustrate the scalability of the approach, we also apply the methodology to a second PLC platform running a representative embedded control process. © 2005-2012 IEEE.",IEEE Transactions on Information Forensics and Security,10.1109/TIFS.2019.2923577,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072757553&doi=10.1109%2fTIFS.2019.2923577&partnerID=40&md5=3bfd93807beb9772bbfcc751e7910147,2020,2021-07-20 15:49:34,2021-07-20 15:49:34
F44QEXN9,journalArticle,2017,"Min, H.; Mobahi, H.; Irvin, K.; Avramovic, S.; Wojtusiak, J.",Predicting activities of daily living for cancer patients using an ontology-guided machine learning methodology,"Background: Bio-ontologies are becoming increasingly important in knowledge representation and in the machine learning (ML) fields. This paper presents a ML approach that incorporates bio-ontologies and its application to the SEER-MHOS dataset to discover patterns of patient characteristics that impact the ability to perform activities of daily living (ADLs). Bio-ontologies are used to provide computable knowledge for ML methods to ""understand"" biomedical data. Results: This retrospective study included 723 cancer patients from the SEER-MHOS dataset. Two ML methods were applied to create predictive models for ADL disabilities for the first year after a patient's cancer diagnosis. The first method is a standard rule learning algorithm; the second is that same algorithm additionally equipped with methods for reasoning with ontologies. The models showed that a patient's race, ethnicity, smoking preference, treatment plan and tumor characteristics including histology, staging, cancer site, and morphology were predictors for ADL performance levels one year after cancer diagnosis. The ontology-guided ML method was more accurate at predicting ADL performance levels (P < 0.1) than methods without ontologies. Conclusions: This study demonstrated that bio-ontologies can be harnessed to provide medical knowledge for ML algorithms. The presented method demonstrates that encoding specific types of hierarchical relationships to guide rule learning is possible, and can be extended to other types of semantic relationships present in biomedical ontologies. The ontology-guided ML method achieved better performance than the method without ontologies. The presented method can also be used to promote the effectiveness and efficiency of ML in healthcare, in which use of background knowledge and consistency with existing clinical expertise is critical. © 2017 The Author(s).",Journal of Biomedical Semantics,10.1186/s13326-017-0149-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029495342&doi=10.1186%2fs13326-017-0149-6&partnerID=40&md5=06b0a8f424e0a7560094d2ff3d7f50c2,2017,2021-07-20 15:49:35,2021-07-20 15:49:35
D4IWIEZN,journalArticle,2018,"Miao, J.; Li, M.; Roy, S.; Ma, Y.; Yu, B.",SD-PUF: Spliced Digital Physical Unclonable Function,"Digital circuit physical unclonable function (PUF) has been attracting attentions for the merits of resilience to the environmental and operational variations that analog PUFs suffer from. Existing state-of-the-art digital circuit PUFs, however, are either hybrid of analog-digital circuits which are still under the shadow of vulnerability, or impractical for real-world applications. In this paper, we propose a novel highly nonlinear and secure digital PUF (D-PUF) and the spliced version SD-PUF. The fingerprints are extracted from intentionally induced very large-scale integration interconnect randomness during lithography process, as well as a post-silicon shuffling process. Strongly skewed CMOS latches are used to ensure the immunity against environmental and operational variations. Crucially, a highly nonlinear logic network is proposed to effectively spread and augment any subtle interconnect randomness, which also enables strong resilience against machine learning attacks. On top of it, the expandable architecture of the proposed logic network empowers a novel post-silicon shuffle-splice mechanism, where multiple randomly selected D-PUFs are spliced to be one SD-PUF, pushing the statistical security to a much higher level, while significantly reducing the mask cost per PUF device. It also decouples the trustworthy demands enforced to the foundries or other third party manufacturers. Our proposed PUFs demonstrate close to ideal performance in terms of statistical metrics, including 0 intra-Hamming distance. Various state-of-the-art machine learning models show prediction accuracies almost no better than random guesses when attacking to the proposed PUFs. We also mathematically prove the probability of existence of identical SD-PUF pair is significantly lower than that of D-PUF pair, e.g., such probability of an SD-PUF spliced by 30 D-PUFs is 2.3\times 10^-22 , which is 19 order magnitude lower than that of D-PUF. Benefited from the proposed shuffle-splice mechanism, the mask cost per SD-PUF is also reduced by 300\times than that of D-PUF. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2017.2740296,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028472615&doi=10.1109%2fTCAD.2017.2740296&partnerID=40&md5=ba1d06af4fc43cd7d00801a1444a06e2,2018,2021-07-20 15:49:35,2021-07-20 15:49:35
RG5R22EJ,journalArticle,2020,"Jaradat, M.A.; Sawaqed, L.S.; Alzgool, M.M.",Optimization of PIDD2-FLC for blood glucose level using particle swarm optimization with linearly decreasing weight,"In this paper, a Proportional-Integral-Differential plus second order derivative Fuzzy Logic Controller (PIDD2-FLC) is implemented aiming to maintain blood glucose level (BGL) normal in type I diabetic subjects. In type I diabetes, insulin-secreting cells are destroyed and, hence, patient depends on external insulin to maintain BGL. The suggested controller is responsible for driving a micro-pump that injects the diabetic patient with proper insulin dose, and a continuous BGL sensor is used for feedback. The nonlinear patient model used is the two-delay differential model, and the reference model for BGLs considered as in the treatment model planned using the two-delay differential model with oscillatory behavior. Particle Swarm Optimization with Linearly Decreasing Weight (LDW-PSO) algorithm is used to optimize the proposed controllers to match the reference model performance. Finally, a comparison is held between the optimized controller with other FLC controllers structures taking into consideration the normal and abnormal conditions. PIDD2-FLC with seven linguistic fuzzy membership functions (MFs) was found to have the best performance overall under different examining conditions. © 2020 Elsevier Ltd",Biomedical Signal Processing and Control,10.1016/j.bspc.2020.101922,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081009258&doi=10.1016%2fj.bspc.2020.101922&partnerID=40&md5=1b47074d347b5a6e1fd09e268ed3f6a0,2020,2021-07-20 15:49:35,2021-07-20 15:49:35
EELPDYMD,journalArticle,2018,"Abdallah, S.",PRISM revisited: Declarative implementation of a probabilistic programming language using multi-prompt delimited control,"PRISM is a probabilistic programming language based on Prolog, augmented with primitives to represent probabilistic choice. It is implemented using a combination of low level support from a modified version of B-Prolog, source level program transformation, and libraries for inference and learning implemented in C. More recently, developers working with functional programming languages have taken the approach of embedding probabilistic primitives into an existing language, with little or no modification to the host language, often by using delimited continuations. Captured continuations represent pieces of the probabilistic program which can be manipulated to achieve a great variety of computational effects useful for inference. In this paper, I will describe an approach based on delimited control operators recently introduced into SWI Prolog. These are used to create a system of nested effect handlers which together implement a core functionality of PRISM—the building of explanation graphs—entirely in Prolog and using an order of magnitude less code. Other declarative programming tools, such as constraint logic programming, are used to implement tools for inference, such as the inside-outside and EM algorithms, lazy best-first explanation search, and MCMC samplers. By embedding the functionality of PRISM into SWI Prolog, users gain access to its rich libraries and development environment. By expressing the functionality of PRISM in a small amount of pure, high-level Prolog, this implementation facilitates further experimentation with the mechanisms of probabilistic logic programming, including new probabilistic modelling features and inference algorithms, such as variational inference in models with real-valued variables. © 2018 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2018.10.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055171458&doi=10.1016%2fj.ijar.2018.10.012&partnerID=40&md5=08d119834364ebdef32521516608124a,2018,2021-07-20 15:49:35,2021-07-20 15:49:35
7FUU345F,journalArticle,2016,"Heckman, D.; Frank, A.; Arnold, M.; Gietz, P.; Roth, C.",Citation segmentation from sparse & noisy data: A joint inference approach with Markov logic networks,"This article presents an approach to citation segmentation that addresses special challenges as typically found in Digital Humanities applications. We perform citation segmentation from Optical Character Recognition (OCR) input obtained from volumes of a printed bibliography, the Turkology Annual. This showcase application features serious difficulties for state-of-the-art techniques in citation segmentation: multilingual citation entries, lack of data redundancy, inconsistencies, and noise from OCR input. Our approach is based on Markov logic networks (MLN) (Richardson and Domingos, Markov logic networks. Machine Learning, 62(1): 107-36, 2006), a framework of statistical relational learning that combines first-order logic with probabilistic modeling. Formalization in first-order logic offers high expressivity and flexibility, and makes it possible to tailor segmentation to specific conventions of a given bibliography. We show that in face of the specific difficulties found with segmenting references from a digitized bibliography, our MLN formalizations outperform state-of-the-art statistical methods. We obtain 88% F1-score for exact field match, a 24.8% increase over a conditional random fields-based system baseline. In contrast to prior work, we address a data set featuring sparse and noisy data. Our method extends Poon and Domingos (Joint Inference in information extraction. In Proceedings of the Twenty-Second National Conference on Artificial Intelligence. Vancouver, Canada: AAAI Press, 2007)'s approach by applying joint inference at the field level. By this move, we are able to cope with the lack of citation redundancy and noise in the data. Our approach can be characterized as knowledge-based and hence does not rely on annotated training data. The rule sets we designed can be adapted to other bibliographies, or further types of digitized sources, such as historical dictionaries or encyclopedias. © The Author 2015. Published by Oxford University.",Digital Scholarship in the Humanities,10.1093/llc/fqu061,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84974674552&doi=10.1093%2fllc%2ffqu061&partnerID=40&md5=c868ff8104f6649b3845d8b7c3985226,2016,2021-07-20 15:49:35,2021-07-20 15:49:35
M984FH2B,journalArticle,2015,"Bayoudh, M.; Roux, E.; Richard, G.; Nock, R.",Structural knowledge learning from maps for supervised land cover/use classification: Application to the monitoring of land cover/use maps in French Guiana,"The number of satellites and sensors devoted to Earth observation has become increasingly elevated, delivering extensive data, especially images. At the same time, the access to such data and the tools needed to process them has considerably improved. In the presence of such data flow, we need automatic image interpretation methods, especially when it comes to the monitoring and prediction of environmental and societal changes in highly dynamic socio-environmental contexts. This could be accomplished via artificial intelligence.The concept described here relies on the induction of classification rules that explicitly take into account structural knowledge, using Aleph, an Inductive Logic Programming (ILP) system, combined with a multi-class classification procedure. This methodology was used to monitor changes in land cover/use of the French Guiana coastline. One hundred and fifty-eight classification rules were induced from 3 diachronic land cover/use maps including 38 classes. These rules were expressed in first order logic language, which makes them easily understandable by non-experts. A 10-fold cross-validation gave significant average values of 84.62%, 99.57% and 77.22% for classification accuracy, specificity and sensitivity, respectively. Our methodology could be beneficial to automatically classify new objects and to facilitate object-based classification procedures. Highlights: © 2014 Elsevier Ltd.",Computers and Geosciences,10.1016/j.cageo.2014.08.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84918522033&doi=10.1016%2fj.cageo.2014.08.013&partnerID=40&md5=20de798f9fdf4adc8e6ed2b5a134228a,2015,2021-07-20 15:49:35,2021-07-20 15:49:35
EHLC6GVU,journalArticle,2020,"Li, Y.; Ma, L.; Tan, W.; Sun, C.; Cao, D.; Li, J.",GRNet: Geometric relation network for 3D object detection from point clouds,"Rapid detection of 3D objects in indoor environments is essential for indoor mapping and modeling, robotic perception and localization, and building reconstruction. 3D point clouds acquired by a low-cost RGB-D camera have become one of the most commonly used data sources for 3D indoor mapping. However, due to the sparse surface, empty object center, and various scales of point cloud objects, 3D bounding boxes are challenging to be estimated and located accurately. To address this, geometric shape, topological structure, and object relation are commonly employed to extract box reasoning information. In this paper, we describe the geometric feature among object points as an intra-object feature and the relation feature between different objects as an inter-object feature. Based on these two features, we propose an end-to-end point cloud geometric relation network focusing on 3D object detection, which is termed as geometric relation network (GRNet). GRNet first extracts intra-object and inter-object features for each representative point using our proposed backbone network. Then, a centralization module with a scalable loss function is proposed to centralize each representative object point to its center. Next, proposal points are sampled from these shifted points, following a proposal feature pooling operation. Finally, an object-relation learning module is applied to predict bounding box parameters. Such parameters are the additive sum of prediction results from the relation-based inter-object feature and the aggregated intra-object feature. Our model achieves state-of-the-art 3D detection results with 59.1% mAP@0.25 and 39.1% mAP@0.5 on ScanNetV2 dataset, 58.4% mAP@0.25 and 34.9% mAP@0.5 on SUN RGB-D dataset. © 2020",ISPRS Journal of Photogrammetry and Remote Sensing,10.1016/j.isprsjprs.2020.05.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085217744&doi=10.1016%2fj.isprsjprs.2020.05.008&partnerID=40&md5=cbfa85069ee2beaa1de323270b6b1b68,2020,2021-07-20 15:49:35,2021-07-20 15:49:35
28JWIDTQ,journalArticle,2016,"Wei, S.; Hagras, H.; Alghazzawi, D.",A cloud computing based Big-Bang Big-Crunch fuzzy logic multi classifier system for Soccer video scenes classification,"Soccer video summarization and classification is becoming a very important topic due to the world wide importance and popularity of soccer games which drives the need to automatically classify video scenes thus enabling better sport analysis, refereeing, training, advertisement, etc. Machine learning has been applied to the task of sports video classification. However, for some specific image and video problems (like sports video scenes classification), the learning task becomes convoluted and difficult due to the dynamic nature of the video sequence and the associated uncertainties relating to changes in light conditions, background, camera angle, occlusions and indistinguishable scene features, etc. The majority of previous techniques (such as SVM, neural network, decision tree, etc.) applied to sports video classifications did not provide a consummate solution, and such models could not be easily understood by human users; meanwhile, they increased the complexity and time of computation and the associated costs of the involved standalone machines. Hence, there is a need to develop a system which is able to address these drawbacks and handle the high levels of uncertainty in video scenes classification and undertake the heavy video processing securely and efficiently on a cloud computing based instance. Hence, in this paper we present a cloud computing based multi classifier systems which aggregates three classifiers based on neural networks and two fuzzy logic classifiers based on type-1 fuzzy logic and type-2 fuzzy logic classification systems which were optimized by a Big-Bang Big crunch optimization to maximize the system performance. We will present several real world experiments which shows the proposed classification system operating in real-time to produce high classification accuracies for soccer videos which outperforms the standalone classification systems based on neural networks, type-1 and type-2 fuzzy logic systems. © 2016, Springer-Verlag Berlin Heidelberg.",Memetic Computing,10.1007/s12293-016-0207-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84982969324&doi=10.1007%2fs12293-016-0207-0&partnerID=40&md5=bb74390af817be0b08ca69b64c4b27d3,2016,2021-07-20 15:49:35,2021-07-20 15:49:35
S4P2RRPI,journalArticle,2016,"Masson, M.-H.; Destercke, S.; Denoeux, T.",Modelling and predicting partial orders from pairwise belief functions,"In this paper, we introduce a generic way to represent and manipulate pairwise information about partial orders (representing rankings, preferences,..) with belief functions. We provide generic and practical tools to make inferences from this pairwise information and illustrate their use on the machine learning problems that are label ranking and multi-label prediction. Our approach differs from most other quantitative approaches handling complete or partial orders, in the sense that partial orders are here considered as primary objects and not as incomplete specifications of ideal but unknown complete orders. © 2014, Springer-Verlag Berlin Heidelberg.",Soft Computing,10.1007/s00500-014-1553-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958120394&doi=10.1007%2fs00500-014-1553-9&partnerID=40&md5=94d57abc578fc1630703956a67fba83e,2016,2021-07-20 15:49:35,2021-07-20 15:49:35
YUM5G349,journalArticle,2013,"Wu, C.-H.; Su, W.-H.",Lattice-based clustering and genetic programming for coordinate transformation in GPS applications,"Coordinate transformation is essential in many georeferencing applications. Level-wised transformation can be considered as a regression problem and done by machine-learning approaches. However, inaccurate and biased results are usually derived when training data do not uniformly distribute. In this paper, the performance of regression-based coordinate transformation for GPS applications is discussed. A lattice-based clustering method is developed and integrated with genetic programming for building better regression models of coordinate transformation. The GPS application area is first partitioned into lattices with lattice sizes being determined by the geographic locations and distribution of the GPS reference points. Clustering is then performed on lattices, not on data points. Each cluster of lattices serves as a training data set for a genetic regression model of coordinate transformation. In this manner, the data points contained in the different lattices can be considered to be of the same importance. Biased regression results caused by the imbalanced distribution of data can also be eliminated. The experimental results show that the proposed method can further improve the positioning accuracy than previous methods. © 2012 Elsevier Ltd.",Computers and Geosciences,10.1016/j.cageo.2012.09.022,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870221898&doi=10.1016%2fj.cageo.2012.09.022&partnerID=40&md5=62a855ecf1428b5ba1848263c2024de6,2013,2021-07-20 15:49:35,2021-07-20 15:49:35
ME8C7BNI,journalArticle,2021,"Kluska, J.; Madera, M.",Extremely simple classifier based on fuzzy logic and gene expression programming,"In this paper, we propose a new design of a very simple data-driven binary classifier and conduct an empirical study of its performance. The data contain continuous and categorical variables. The classification system consists of highly interpretable fuzzy metarules. A new theorem is developed that guarantees that these metarules are equivalent to algebraic expressions. The algebraic expressions are obtained using the gene expression programming technique. The number of features in the modeled dataset does not affect the complexity of the metarules. The performance of the resulting metarules is comparable to that of the rules created by most of the popular machine learning methods. The newly introduced classifier (GPR) appears to be the simplest among the fuzzy rule-based classifiers. Its effectiveness was tested on 16 datasets and compared with 22 other classification algorithms. GPR turned out to be surprisingly good; i.e., it belongs to the group of the best classifiers when the quality criterion is the area under the ROC curve and the classification accuracy. The Scott-Knott analysis indicates that, in terms of performance, GPR is commensurate with the leading group of 3 algorithms, and the Wilcoxon test confirmed the statistical reliability of the obtained results. High interpretability is proved with examples of classification models. © 2021",Information Sciences,10.1016/j.ins.2021.05.041,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107647952&doi=10.1016%2fj.ins.2021.05.041&partnerID=40&md5=a75db9e61b3dd16f0f699d90f1e20266,2021,2021-07-20 15:49:35,2021-07-20 15:49:35
HCTNV8GA,journalArticle,2021,"Zhang, X.; Jiao, L.; Granmo, O.; Goodwin, M.",On the Convergence of Tsetlin Machines for the IDENTITY- and NOT Operators,"The Tsetlin Machine (TM) is a recent machine learning algorithm with several distinct properties, such as interpretability, simplicity, and hardware-friendliness. Although numerous empirical evaluations report on its performance, the mathematical analysis of its convergence is still open. In this article, we analyze the convergence of the TM with only one clause involved for classification. More specifically, we examine two basic logical operators, namely, the ?IDENTITY?- and ?NOT? operators. Our analysis reveals that the TM, with just one clause, can converge correctly to the intended logical operator, learning from training data over an infinite time horizon. Besides, it can capture arbitrarily rare patterns and select the most accurate one when two candidate patterns are incompatible, by configuring a granularity parameter. The analysis of the convergence of the two basic operators lays the foundation for analyzing other logical operators. These analyses altogether, from a mathematical perspective, provide new insights on why TMs have obtained state-of-the-art performance on several pattern recognition problems. IEEE",IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2021.3085591,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107334753&doi=10.1109%2fTPAMI.2021.3085591&partnerID=40&md5=e46fc54c821139a2cdb506f26fce4dec,2021,2021-07-20 15:49:35,2021-07-20 15:49:35
MPAUDCD8,journalArticle,2020,"Liu, Y.; Lu, H.; Li, X.; Zhao, D.; Wu, W.; Lu, G.",A Novel Approach for Service Function Chain Dynamic Orchestration in Edge Clouds,"Network function virtualization (NFV) and mobile edge computing (MEC) are two of the promising technologies that are expected to play a critical role in mobile edge cloud networks, thus satisfying ambitious quality of experience (QoE) requirements of the Internet of things (IoT) applications. In MEC-NFV system, a service function chain (SFC) consists of an ordered set of virtual network functions (VNFs) that are connected based on the business logic of service providers. However, the inefficiency of the SFC orchestration process is one major problem due to the dynamic nature of mobile edge cloud networks and abundance of IoT terminals. In this letter, a quantum machine learning (QML)-based scheme is proposed as solution that can handle complex and dynamic SFC orchestration in mobile edge cloud networks. Simulation results show that our proposal significantly provides more than 8-fold reduction of run time compared to the Viterbi algorithm, and the end-to-end delay is only about 1.1 times of the exact solution. © 1997-2012 IEEE.",IEEE Communications Letters,10.1109/LCOMM.2020.3000588,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092710795&doi=10.1109%2fLCOMM.2020.3000588&partnerID=40&md5=4f5b707c16a185b585ad756c5e77c572,2020,2021-07-20 15:49:35,2021-07-20 15:49:35
BPZQ5UEC,journalArticle,2020,"Zakarija, I.; Škopljanac-Mačina, F.; Blašković, B.",Automated simulation and verification of process models discovered by process mining,"This paper presents a novel approach for automated analysis of process models discovered using process mining techniques. Process mining explores underlying processes hidden in the event data generated by various devices. Our proposed Inductive machine learning method was used to build business process models based on actual event log data obtained from a hotel's Property Management System (PMS). The PMS can be considered as a Multi Agent System (MAS) because it is integrated with a variety of external systems and IoT devices. Collected event log combines data on guests stay recorded by hotel staff, as well as data streams captured from telephone exchange and other external IoT devices. Next, we performed automated analysis of the discovered process models using formal methods. Spin model checker was used to simulate process model executions and automatically verify the process model. We proposed an algorithm for the automatic transformation of the discovered process model into a verification model. Additionally, we developed a generator of positive and negative examples. In the verification stage, we have also used Linear temporal logic (LTL) to define requested system specifications. We find that the analysis results will be well suited for process model repair. © 2020, © 2020 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.",Automatika,10.1080/00051144.2020.1734716,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081893976&doi=10.1080%2f00051144.2020.1734716&partnerID=40&md5=c97004002fc1b52f274cbf311929a006,2020,2021-07-20 15:49:36,2021-07-20 15:49:36
T39UWX6H,journalArticle,2020,"Sankhwar, S.; Gupta, D.; Ramya, K.C.; Sheeba Rani, S.; Shankar, K.; Lakshmanaprabu, S.K.",Improved grey wolf optimization-based feature subset selection with fuzzy neural classifier for financial crisis prediction,"In present days, prediction of financial crisis of a company is a hot research area. The use of data mining and machine learning algorithms assists to resolve the financial crisis prediction (FCP) problem. Since financial data contain more demographical and unwanted information, it might decrease the classification performance significantly. So, feature selection (FS) process is applied to choose useful data and remove the irrelevant repetitive data. This paper introduces a novel predictive framework for FCP model by the incorporation of improved grey wolf optimization (IGWO) and fuzzy neural classifier (FNC). An IGWO algorithm is derived by the integration of GWO algorithm and tumbling effect. The presented IGWO-based FS method is employed to discover the optimal features from the financial data. For classification purposes, FNC is employed. The proposed method is experimented on two benchmark data sets, namely Australian Credit and German data set under several of performance metrics. The experimental values verified the superior nature of the proposed FCP model over the compared methods. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-019-04323-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073809146&doi=10.1007%2fs00500-019-04323-6&partnerID=40&md5=c36bd838d22dc231e84ab88b875323fd,2020,2021-07-20 15:49:36,2021-07-20 15:49:36
XJ8VG2F3,journalArticle,2019,"Labib, S.M.",Investigation of the likelihood of green infrastructure (GI)enhancement along linear waterways or on derelict sites (DS)using machine learning,"Studies evaluating the potential for green infrastructure (GI)development using traditional Boolean logic-based multi-criteria analysis methods are not capable of predicting future GI development in dynamic urbanscapes. This study evaluated both artificial neural network (ANN)and adaptive, network-based fuzzy inference system (ANFIS)algorithms in conjunction with statistical modelling to predict green or grey transformation likelihoods for derelict sites (DS)and vacant sites along waterway corridors (WWC)in Manchester based on ecological, environmental, and social criteria. The soft-computing algorithms had better predictive capacity at 72% accuracy versus the 65% of logistic models. Site sizes, population coverage, and air pollution were identified as the main influencers in the potential for site transformation. In Manchester, the likelihood of GI transformation was higher for WWC than derelict sites at 80% versus 60% likelihood, respectively. Furthermore, DS were more likely to transform into grey development based on current trends and urban planning practice. © 2019",Environmental Modelling and Software,10.1016/j.envsoft.2019.05.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065834365&doi=10.1016%2fj.envsoft.2019.05.006&partnerID=40&md5=813712389e7a41ec405f09a6f81f9345,2019,2021-07-20 15:49:36,2021-07-20 15:49:36
KPSTYA8X,journalArticle,2019,"Rebouças Filho, P.P.; Gomes, S.L.; e Nascimento, N.M.M.; Medeiros, C.M.S.; Outay, F.; de Albuquerque, V.H.C.",Energy production predication via Internet of Thing based machine learning system,"Wind energy is an interesting source of alternative energy to complement the Brazilian energy matrix. However, one of the great challenges lies in managing this resource, due to its uncertainty behavior. This study addresses the estimation of the electric power generation of a wind turbine, so that this energy can be used efficiently and sustainable. Real wind and power data generated in set of wind turbines installed in a wind farm in Ceará State, Brazil, were used to obtain the power curve from a wind turbine using logistic regression, integrated with Nonlinear Autoregressive neural networks to forecast wind speeds. In our system the average error in power generation estimate is of 29 W for 5 days ahead forecast. We decreased the error in the manufacturer's power curve in 63%, with a logics regression approach, providing a 2.7 times more accurate estimate. The results have a large potential impact for the wind farm managers since it could drive not only the operation and maintenance but management level of energy sells. © 2019 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2019.01.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062439817&doi=10.1016%2fj.future.2019.01.020&partnerID=40&md5=efd871778fd4cd231fb38eb8362e407b,2019,2021-07-20 15:49:36,2021-07-20 15:49:36
SRPNH4AW,journalArticle,2017,"Zhang, Z.; Liu, Y.; Zhang, Z.",Field-aware matrix factorization for recommender systems,"Predicting user response is one of the core machine learning tasks in recommender systems (RS). The matrix factorization (MF)-based model has been proved to be a useful tool to improve the performance of recommendation. Many existing matrix factorization-based models mainly rely on adding some side information into basic MF to enable the model to fully express the data. However, most of the side information is measured based on the statistics or empirical formula. Also, the latent features of side information cannot be deeply mined. In this paper, we focus on mining the influence of field information (useful side information) to improve the performance of prediction. Based on the MF framework, we propose a field-aware matrix factorization (FMF) model. In FMF, the interactions between user/item and field can be captured and learned in the latent vector spaces. We propose efficient implementations to train FMF. Then, we comprehensively analyze FMF and compare this model with the state-of-the-art models. The analysis of experiments on two large data sets demonstrates that our method is very useful in RS. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2017.2787741,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040046655&doi=10.1109%2fACCESS.2017.2787741&partnerID=40&md5=4e3ebecaf6ae57ed57ddb1d1cb701847,2017,2021-07-20 15:49:36,2021-07-20 15:49:36
XMD8F38A,journalArticle,2016,"Rubin, S.H.; Bouabana-Tebibel, T.; Hoadjli, Y.",On the empirical justification of theoretical heuristic transference and learning,"The solution of intractable problems implies the use of heuristics. Quantum computers may find use for optimization problems, but have yet to solve any NP-hard problems. This paper demonstrates results in game theory for domain transference and the reuse of problem-solving knowledge through the application of learned heuristics. It goes on to explore the possibilities for the acquisition of heuristics for the solution of the NP-hard TSP problem. Here, it is found that simple heuristics (e.g., pairwise exchange) often work best in the context of more or less sophisticated experimental designs. Often, these problems are not amenable to exclusive logic solutions; but rather, require the application of hybrid approaches predicated on search. In general, such approaches are based on randomization and supported by parallel processing. This means that heuristic solutions emerge from attempts to randomize the search space. The paper goes on to present a constructive proof of the unbounded density of knowledge in support of the Semantic Randomization Theorem (SRT). It highlights this result and its potential impact upon the community of machine learning researchers. © 2016, Springer Science+Business Media New York (outside the USA).",Information Systems Frontiers,10.1007/s10796-016-9661-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975298241&doi=10.1007%2fs10796-016-9661-y&partnerID=40&md5=0a5dbfc65036b92443d8463a13fef279,2016,2021-07-20 15:49:36,2021-07-20 15:49:36
266WS4LM,journalArticle,2016,"Zhang, C.; Su, X.; Hu, Y.; Zhang, Z.; Deng, Y.",An Evidential Spam-Filtering Framework,"Spam, also known as unsolicited bulk e-mail (UBE), has recently become a serious threat that negatively impacts the usability of legitimate mails. In this article, an evidential spam-filtering framework is proposed. As a useful tool to handle uncertainty, the Dempster–Shafer theory of evidence (D–S theory) is integrated into the proposed approach. Five representative features from an e-mail header are analyzed. With a machine-learning algorithm, e-mail headers with known classifications are used to train the framework. When using the framework for a given e-mail header, its representative features are quantified. Although in classical probability theory, possibilities are forcedly assigned even when information is not adequate, in our approach, for every word in an e-mail subject, basic probability assignments (BPA) are assigned in a more flexible way, thus providing a more reasonable result. Finally, BPAs are combined and transformed into pignistic probabilities for decision-making. Empirical trials on real-world datasets show the efficiency of the proposed framework. © 2016, Copyright © Taylor & Francis Group, LLC.",Cybernetics and Systems,10.1080/01969722.2016.1182351,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976443410&doi=10.1080%2f01969722.2016.1182351&partnerID=40&md5=8a6fd2ed574432e391c5c2f8cd1a0a21,2016,2021-07-20 15:49:36,2021-07-20 15:49:36
MLJ9UZEI,journalArticle,2020,"Huang, L.; Li, G.; Li, Y.; Lin, L.",Lightweight adversarial network for salient object detection,"Recent advance on salient object detection benefits mostly from the revival of Convolutional Neural Networks (CNNs). However, with these CNN based models, the predicted saliency map is usually incomplete, that is, spatially inconsistent with the corresponding ground truth, because of the inherent complexity of the object and the inaccuracy of object boundary detection resulted from regular convolution and pooling operations. Besides, the breakthrough on saliency detection accuracy of current state-of-the-art deep models comes at the expense of high computational cost, which contradicts its role as a pretreatment procedure for other computer vision tasks. To alleviate these issues, we propose a lightweight adversarial network for salient object detection, which simultaneously improves the accuracy and efficiency by enforcing higher-order spatial consistency via adversarial training and lowering the computational cost through lightweight bottleneck blocks, respectively. Moreover, multi-scale contrast module is utilized to sufficiently capture contrast prior for visual saliency reasoning. Comprehensive experiments demonstrate that our method is superior to the state-of-the-art works on salient object detection in both accuracy and efficiency. © 2019 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2019.09.100,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076550634&doi=10.1016%2fj.neucom.2019.09.100&partnerID=40&md5=22772a74e864dd5ed8c5c2bc21898f99,2020,2021-07-20 15:49:36,2021-07-20 15:49:36
C4VRB5SU,journalArticle,2020,"Wu, C.; Luo, G.; Guo, C.; Ren, Y.; Zheng, A.; Yang, C.",An attention-based multi-task model for named entity recognition and intent analysis of Chinese online medical questions,"In this paper, we propose an attention-based multi-task neural network model for text classification and sequence tagging and then apply it to the named entity recognition and the intent analysis of Chinese online medical questions. We found that the use of both attention and multi-task learning improved the performance of these tasks. Our method achieved superior performance in named entity recognition and intent analysis compared with other baseline methods; the method is a light-weight solution that is suitable for deployment on small servers. Furthermore, we took advantage of the model's capabilities for these two tasks and built a simple question-answering system for cardiovascular issues. Users and service providers can monitor the logic of the answers generated by this system. © 2020 Elsevier Inc.",Journal of Biomedical Informatics,10.1016/j.jbi.2020.103511,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088043934&doi=10.1016%2fj.jbi.2020.103511&partnerID=40&md5=ab1f5689a18e0402a213b6353e3cdd21,2020,2021-07-20 15:49:36,2021-07-20 15:49:36
TFKKNJ57,journalArticle,2021,"Davis, G.P.; Katz, G.E.; Gentili, R.J.; Reggia, J.A.",Compositional memory in attractor neural networks with one-step learning,"Compositionality refers to the ability of an intelligent system to construct models out of reusable parts. This is critical for the productivity and generalization of human reasoning, and is considered a necessary ingredient for human-level artificial intelligence. While traditional symbolic methods have proven effective for modeling compositionality, artificial neural networks struggle to learn systematic rules for encoding generalizable structured models. We suggest that this is due in part to short-term memory that is based on persistent maintenance of activity patterns without fast weight changes. We present a recurrent neural network that encodes structured representations as systems of contextually-gated dynamical attractors called attractor graphs. This network implements a functionally compositional working memory that is manipulated using top-down gating and fast local learning. We evaluate this approach with empirical experiments on storage and retrieval of graph-based data structures, as well as an automated hierarchical planning task. Our results demonstrate that compositional structures can be stored in and retrieved from neural working memory without persistent maintenance of multiple activity patterns. Further, memory capacity is improved by the use of a fast store-erase learning rule that permits controlled erasure and mutation of previously learned associations. We conclude that the combination of top-down gating and fast associative learning provides recurrent neural networks with a robust functional mechanism for compositional working memory. © 2021 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2021.01.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101137555&doi=10.1016%2fj.neunet.2021.01.031&partnerID=40&md5=a0df50a5452361f7d5d07cc1de096c46,2021,2021-07-20 15:49:36,2021-07-20 15:49:36
3SI4RLKT,journalArticle,2021,"Zia, T.; Windridge, D.",A generative adversarial network for single and multi-hop distributional knowledge base completion,"Knowledge bases (KBs) inherently lack reasoning ability, limiting their effectiveness for tasks such as question–answering and query expansion. Machine-learning is hence commonly employed for representation learning in order to learn semantic features useful for generalization. Most existing methods utilize discriminative models that require both positive and negative samples to learn a decision boundary. KBs, by contrast, contain only positive samples, necessitating that negative samples are generated by replacing the head/tail of predicates with randomly-chosen entities. They are thus frequently easily discriminable from positive samples, which can prevent learning of sufficiently robust classifiers. Generative models, however, do not require negative samples to learn the distribution of positive samples; stimulated by recent developments in Generative Adversarial Networks (GANs), we propose a novel framework, Knowledge Completion GANs (KCGANs), for competitively training generative link prediction models against discriminative belief prediction models. KCGAN thus invokes a game between generator-network G and discriminator-network D in which G aims to understand underlying KB structure by learning to perform link prediction while D tries to gain knowledge about the KB by learning predicate/triplet classification. Two key challenges are addressed: 1) Classical GAN architectures’ inability to easily generate samples over discrete entities; 2) the inefficiency of softmax for learning distributions over large sets of entities. As a step toward full first-order logical reasoning we further extend KCGAN to learn multi-hop logical entailment relations between entities by enabling G to compose a multi-hop relational path between entities and D to discriminate between real and fake paths. KCGAN is tested on benchmarks WordNet and FreeBase datasets and evaluated on link prediction and belief prediction tasks using MRR and HIT@ 10, achieving best-in-class performance. © 2021 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2021.04.128,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108940773&doi=10.1016%2fj.neucom.2021.04.128&partnerID=40&md5=858f493b94639ee1fe9db18ed8b75e2f,2021,2021-07-20 15:49:36,2021-07-20 15:49:36
SEQFGB27,journalArticle,2015,"Herrero, P.; Pesl, P.; Reddy, M.; Oliver, N.; Georgiou, P.; Toumazou, C.",Advanced insulin bolus advisor based on run-to-run control and case-based reasoning,"This paper presents an advanced insulin bolus advisor for people with diabetes on multiple daily injections or insulin pump therapy. The proposed system, which runs on a smartphone, keeps the simplicity of a standard bolus calculator while enhancing its performance by providing more adaptability and flexibility. This is achieved by means of applying a retrospective optimization of the insulin bolus therapy using a novel combination of run-to-run (R2R) that uses intermittent continuous glucose monitoring data, and case-based reasoning (CBR). The validity of the proposed approach has been proven by in-silico studies using the FDA-accepted UVa-Padova type 1 diabetes simulator. Tests under more realistic in-silico scenarios are achieved by updating the simulator to emulate intrasubject insulin sensitivity variations and uncertainty in the capillarity measurements and carbohydrate intake. The CBR(R2R) algorithm performed well in simulations by significantly reducing the mean blood glucose, increasing the time in euglycemia and completely eliminating hypoglycaemia. Finally, compared to an R2R stand-alone version of the algorithm, the CBR(R2R) algorithm performed better in both adults and adolescent populations, proving the benefit of the utilization of CBR. In particular, the mean blood glucose improved from 166 ± 39 to 150 ± 16 in the adult populations (p = 0.03) and from 167 ± 25 to 162 ± 23 in the adolescent population (p = 0.06). In addition, CBR(R2R) was able to completely eliminate hypoglycaemia, while the R2R alone was not able to do it in the adolescent population. 2168-2194 © 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2014.2331896,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929353053&doi=10.1109%2fJBHI.2014.2331896&partnerID=40&md5=9017c93720ae4e8607f9f5aa56bab86d,2015,2021-07-20 15:49:36,2021-07-20 15:49:36
6WYVAT6L,journalArticle,2019,"García, J.; Fernández, F.",Probabilistic policy reuse for safe reinforcement learning,"This work introduces Policy Reuse for Safe Reinforcement Learning, an algorithm that combines Probabilistic Policy Reuse and teacher advice for safe exploration in dangerous and continuous state and action reinforcement learning problems in which the dynamic behavior is reasonably smooth and the space is Euclidean. The algorithm uses a continuously increasing monotonic risk function that allows for the identification of the probability to end up in failure from a given state. Such a risk function is defined in terms of how far such a state is from the state space known by the learning agent. Probabilistic Policy Reuse is used to safely balance the exploitation of actual learned knowledge, the exploration of newactions, and the request of teacher advice in parts of the state space considered dangerous. Specifically, the π-reuse exploration strategy is used. Using experiments in the helicopter hover task and a business management problem, we show that the p-reuse exploration strategy can be used to completely avoid the visit to undesirable situations while maintaining the performance (in terms of the classical long-term accumulated reward) of the final policy achieved. © 2019 Association for Computing Machinery.",ACM Transactions on Autonomous and Adaptive Systems,10.1145/3310090,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063225311&doi=10.1145%2f3310090&partnerID=40&md5=94a4bd6a6d9d50b1b6f063046d54a054,2019,2021-07-20 15:49:36,2021-07-20 15:49:36
ZLR49V3N,journalArticle,2016,"Zhang, H.; Yao, D.; Ramakrishnan, N.; Zhang, Z.",Causality reasoning about network events for detecting stealthy malware activities,"Malicious software activities have become more and more clandestine, making them challenging to detect. Existing security solutions rely heavily on the recognition of known code or behavior signatures, which are incapable of detecting new malware patterns. We propose to discover the triggering relations on network requests and leverage the structural information to identify stealthy malware activities that cannot be attributed to a legitimate cause. The triggering relation is defined as the temporal and causal relationship between two events. We design and compare rule- and learning-based methods to infer the triggering relations on network data. We further introduce a user-intention based security policy for pinpointing stealthy malware activities based on a triggering relation graph. We extensively evaluate our solution on a DARPA dataset and 7 GB real-world network traffic. Results indicate that our dependence analysis successfully detects various malware activities including spyware, data exfiltrating malware, and DNS bots on hosts. With good scalability for large datasets, the learning-based method achieves better classification accuracy than the rule-based one. The significance of our traffic reasoning approach is its ability to detect new and stealthy malware activities. © 2016 The Authors.",Computers and Security,10.1016/j.cose.2016.01.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957038910&doi=10.1016%2fj.cose.2016.01.002&partnerID=40&md5=7c9f2b7da443d8976eacce7b86d7f805,2016,2021-07-20 15:49:36,2021-07-20 15:49:36
IY8TVGLD,journalArticle,2013,"Trinh, V.C.; Gonzalez, A.J.",Discovering contexts from observed human performance,"This paper describes an investigation to determine the technical feasibility of discovering and identifying the various contexts experienced by a human performer (called an actor) solely from a trace of time-stamped values of variables. More specifically, the goal of this research was to discover the contexts that a human actor experienced, while performing a tactical task in a simulated environment, the sequence of these contexts and their temporal duration.We refer to this process as the contextualization of the performance trace. In the process of doing this, we devised a context discovery algorithm called context partitioning and clustering (COPAC). The relevant variables that were observed in the trace were selected a priori by a human. The output of the COPAC algorithm was qualitatively compared with manual (human) contextualization of the same traces. One possible use of such automated context discovery is to help build autonomous tactical agents capable of performing the same tasks as the human actor. As such, we also quantitatively compared the results of using the COPAC-derived contexts with those obtained with human-derived contextualization in building autonomous tactical agents. Test results are described and discussed. © 2013 IEEE.",IEEE Transactions on Human-Machine Systems,10.1109/TSMC.2013.2262272,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890115811&doi=10.1109%2fTSMC.2013.2262272&partnerID=40&md5=1f255da2b1bcb0c728e2d3731a6d4664,2013,2021-07-20 15:49:36,2021-07-20 15:49:36
93KCINI8,journalArticle,2013,"Jiang, L.; Li, C.",An augmented value difference measure,"How to learn distances from categorical variables (nominal attributes) is a key problem in instance-based learning and other paradigms of machine learning. Recent work in distance learning has shown that a surprisingly simple Value Difference Metric (VDM), with strong assumptions of independence among attributes, is competitive with state-of-the-art distance functions such as Short and Fukunaga Metric (SFM) and Minimum Risk Metric (MRM). This fact raises the question of whether a distance function with less restrictive assumptions can perform even better. In order to answer this question, we proposed an augmented memory-based reasoning (MBR) transform. Based on our augmented MBR transform, we then developed an Augmented Value Difference Measure (AVDM) for learning distances from categorical variables. We experimentally tested our AVDM using 36 natural domains and three artificial Monk's domains, taken from the University of California at Irvine repository, and compared it to its competitor such as VDM, SFM, MRM, ODVDM, and MSFM. The compared results show that our AVDM can generally improve accuracy in domains that involve correlated attributes without reducing accuracy in ones that do not. © 2013 Elsevier Ltd. All rights reserved.",Pattern Recognition Letters,10.1016/j.patrec.2013.03.030,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876780565&doi=10.1016%2fj.patrec.2013.03.030&partnerID=40&md5=ad5fac3c1161e626d81ea2b43883cf58,2013,2021-07-20 15:49:36,2021-07-20 15:49:36
ECQUHWNM,journalArticle,2021,"Saini, S.K.; Gupta, R.",Artificial intelligence methods for analysis of electrocardiogram signals for cardiac abnormalities: state-of-the-art and future challenges,"Cardiovascular diseases (CVDs) in India and globally are the major cause of mortality, as revealed by the World Health Organization (WHO). The irregularities in the pace of heartbeats, called cardiac arrhythmias or heart arrhythmias, are one of the commonly diagnosed CVDs caused by ischemic heart disease, hypertension, alcohol intake, and stressful lifestyle. Other than the listed CVDs, the abnormality in the cardiac rhythm caused by the long term mental stress (stimulated by Autonomic Nervous System (ANS)) is a challenging issue for researchers. Early detection of cardiac arrhythmias through automatic electronic techniques is an important research field since the invention of electrocardiogram (ECG or EKG) and advanced machine learning algorithms. ECG (EKG) provides the record of variations in electrical activity associated with the cardiac cycle, used by cardiologists and researchers as a gold standard to study the heart function. The present work is aimed to provide an extensive survey of work done by researchers in the area of automated ECG analysis and classification of regular & irregular classes of heartbeats by conventional and modern artificial intelligence (AI) methods. The artificial intelligence (AI) based methods have emerged popularly during the last decade for the automatic and early diagnosis of clinical symptoms of arrhythmias. In this work, the literature is explored for the last two decades to review the performance of AI and other computer-based techniques to analyze the ECG signals for the prediction of cardiac (heart rhythm) disorders. The existing ECG feature extraction techniques and machine learning (ML) methods used for ECG signal analysis and classification are compared using the performance metrics like specificity, sensitivity, accuracy, positive predictivity value, etc. Some popular AI methods, which include, artificial neural networks (ANN), Fuzzy logic systems, and other machine learning algorithms (support vector machines (SVM), k-nearest neighbor (KNN), etc.) are considered in this review work for the applications of cardiac arrhythmia classification. The popular ECG databases available publicly to evaluate the classification accuracy of the classifier are also mentioned. The aim is to provide the reader, the prerequisites, the methods used in the last two decades, and the systematic approach, all at one place to further purse a research work in the area of cardiovascular abnormalities detection using the ECG signal. As a contribution to the current work, future challenges for real-time remote ECG acquisition and analysis using the emerging technologies like wireless body sensor network (WBSN) and the internet of things (IoT) are identified. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.",Artificial Intelligence Review,10.1007/s10462-021-09999-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105218250&doi=10.1007%2fs10462-021-09999-7&partnerID=40&md5=7bd2cc6226cd159f5aca9acf55e63c98,2021,2021-07-20 15:49:37,2021-07-20 15:49:37
SG97K89D,journalArticle,2019,"Patel, D.; Shah, S.; Chhinkaniwala, H.",Fuzzy logic based multi document summarization with improved sentence scoring and redundancy removal technique,"Nowadays abundant amount of information is available on Internet which makes it difficult for the users to locate desired information. Automatic methods are needed to efficiently sieve and scavenge useful information from the Internet. Text summarization is identified and accepted as one of the solutions to find desired contents from one or more documents. The objective of proposed multi-document summarization is to gain good content coverage with information diversity. The proposed statistical feature based model utilizes the fuzzy model to deal with the imprecise and uncertainty of feature weight. Redundancy removal using cosine similarity is presented as enrichment to proposed work. The proposed approach is compared with DUC (Document Understanding Conference) participant systems and other summarization systems such as TexLexAn, ItemSum, Yago Summarizer, MSSF and PatSum using ROUGE measure on dataset DUC 2004. The experimental results show that our proposed work achieves a significant performance improvement over the other summarizers. © 2019",Expert Systems with Applications,10.1016/j.eswa.2019.05.045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066788806&doi=10.1016%2fj.eswa.2019.05.045&partnerID=40&md5=dc17439ed577c1bf1eda54ad9a45e52f,2019,2021-07-20 15:49:37,2021-07-20 15:49:37
4AHSRKPA,journalArticle,2019,"Siebers, M.; Schmid, U.",Please delete that! Why should I?: Explaining learned irrelevance classifications of digital objects,"Dare2Del is an assistive system which facilitates intentional forgetting of irrelevant digital objects. For an assistive system to be helpful, the user has to trust the system’s decisions. Explanations are a crucial component in establishing this trust. We will introduce different types of explanations which can vary along different dimensions such as level of detail and modality suitable for different application contexts. We will outline the cognitive companion system Dare2Del which is intended to support users managing digital objects in a working environment. Core of Dare2Del is an interpretable machine learning mechanism which induces decision rules to classify whether a digital objects is irrelevant. In this paper, we focus on irrelevance of files. We formalize the decision making process as logic inference. Finally, we present a method to generate verbal explanations for irrelevance decisions and point out how such explanations can be constructed on different levels of details. Furthermore, we show how verbal explanations can be related to the path context of the file. We conclude with a short discussion of the scope and restrictions of our approach. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",KI - Kunstliche Intelligenz,10.1007/s13218-018-0565-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076621680&doi=10.1007%2fs13218-018-0565-5&partnerID=40&md5=93f0e5712cbc369375697f12dac5d681,2019,2021-07-20 15:49:37,2021-07-20 15:49:37
IM4RNW4R,journalArticle,2017,"Kolb, S.; Paramonov, S.; Guns, T.; De Raedt, L.",Learning constraints in spreadsheets and tabular data,"Spreadsheets, comma separated value files and other tabular data representations are in wide use today. However, writing, maintaining and identifying good formulas for tabular data and spreadsheets can be time-consuming and error-prone. We investigate the automatic learning of constraints (formulas and relations) in raw tabular data in an unsupervised way. We represent common spreadsheet formulas and relations through predicates and expressions whose arguments must satisfy the inherent properties of the constraint. The challenge is to automatically infer the set of constraints present in the data, without labeled examples or user feedback. We propose a two-stage generate and test method where the first stage uses constraint solving techniques to efficiently reduce the number of candidates, based on the predicate signatures. Our approach takes inspiration from inductive logic programming, constraint learning and constraint satisfaction. We show that we are able to accurately discover constraints in spreadsheets from various sources. © 2017, The Author(s).",Machine Learning,10.1007/s10994-017-5640-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020207644&doi=10.1007%2fs10994-017-5640-x&partnerID=40&md5=c9bbe6830d2d265fd79962b5a25177e8,2017,2021-07-20 15:49:37,2021-07-20 15:49:37
IZAY6Y2T,journalArticle,2018,"Hu, T.; Oksanen, K.; Zhang, W.; Randell, E.; Furey, A.; Sun, G.; Zhai, G.",An evolutionary learning and network approach to identifying key metabolites for osteoarthritis,"Metabolomics studies use quantitative analyses of metabolites from body fluids or tissues in order to investigate a sequence of cellular processes and biological systems in response to genetic and environmental influences. This promises an immense potential for a better understanding of the pathogenesis of complex diseases. Most conventional metabolomics analysis methods exam one metabolite at a time and may overlook the synergistic effect of combining multiple metabolites. In this article, we proposed a new bioinformatics framework that infers the non-linear synergy among multiple metabolites using a symbolic model and subsequently, identify key metabolites using network analysis. Such a symbolic model is able to represent a complex non-linear relationship among a set of metabolites associated with osteoarthritis (OA) and is automatically learned using an evolutionary algorithm. Applied to the Newfoundland Osteoarthritis Study (NFOAS) dataset, our methodology was able to identify nine key metabolites including some known osteoarthritis-associated metabolites and some novel metabolic markers that have never been reported before. The results demonstrate the effectiveness of our methodology and more importantly, with further investigations, propose new hypotheses that can help better understand the OA disease. © 2018 Hu et al.",PLoS Computational Biology,10.1371/journal.pcbi.1005986,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044744016&doi=10.1371%2fjournal.pcbi.1005986&partnerID=40&md5=c42e699fe76e0c0cb905c3d257b2283d,2018,2021-07-20 15:49:37,2021-07-20 15:49:37
VXW8NFXN,journalArticle,2016,"Ban, J.-C.; Chang, C.-H.",The Spatial Complexity of Inhomogeneous Multi-layer Neural Networks,"Inhomogeneous multi-layer neural networks (IHMNNs) have been applied in various fields, for example, biological and ecological contexts. This work studies the learning problem of IHMNNs with an activation function f(x)=12(|x+1|-|x-1|) that derives from cellular neural networks, which can be adapted to the study of the vision systems of mammals. Applying the well-developed theory of symbolic dynamics, the explicit formulae of the topological entropy of the output and hidden spaces are given. We also demonstrate that, for any λ∈[0,log2] and > >0, parameters such that the topological entropy h of the hidden/output space of IHMNN that satisfies |h-λ|<ϵ exists. This means that the collection of topological entropies is dense in the closed interval [0,log 2], which leads to the fact that IHMNNs are universal machines in some sense and hence are more efficient in learning algorithms. This paper aims to provide a mathematical foundation for the illustration of the capability of machine learning, while the method we have adopted can be extended to the investigation of multi-layer neural networks with other activation functions. © 2014, Springer Science+Business Media New York.",Neural Processing Letters,10.1007/s11063-014-9400-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954373297&doi=10.1007%2fs11063-014-9400-7&partnerID=40&md5=3d899c07de8117595164b4833f9e9497,2016,2021-07-20 15:49:37,2021-07-20 15:49:37
CBUVV5TK,journalArticle,2021,"Amador-Domínguez, E.; Serrano, E.; Manrique, D.; Hohenecker, P.; Lukasiewicz, T.",An ontology-based deep learning approach for triple classification with out-of-knowledge-base entities,"Knowledge graphs (KGs) are one of the most common frameworks for knowledge representation. However, they suffer from a severe scalability problem that hinders their usage. KG embedding aims to provide a solution to this issue. Nonetheless, general approaches are incapable of representing and reasoning about information not previously contained in the graph. This paper proposes to leverage semantic and ontological information for a significant benefit of knowledge graph completion, focusing on triple classification. The goal of this task is to determine whether a given fact holds. Furthermore, this paper also considers the classification of facts that include entities that have not been seen during training, denoted out-of-knowledge-base or OOKB entities. An incremental method is presented, composed of six stages. Although the proposal can be applied to any KG embedding model, this work focuses on its application for semantic matching models, such as ComplEx and DistMult. Compared to other approaches, our proposal is model-agnostic, computationally inexpensive, and does not require retraining. The results show that triple classification accuracy scales up to 15% with the proposed approach, as well as accelerating the convergence of the model to its optimal solution. Furthermore, facts containing OOKB entities can be classified with a reasonable accuracy. © 2021 Elsevier Inc.",Information Sciences,10.1016/j.ins.2021.02.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102271445&doi=10.1016%2fj.ins.2021.02.018&partnerID=40&md5=45df46e472c45e6d29b8a28ce0877665,2021,2021-07-20 15:49:37,2021-07-20 15:49:37
3EYNX8S6,journalArticle,2021,"Tan, G.; Wei, P.; He, Y.; Xu, H.; Shi, X.",Solving the playing strategy of Dou Dizhu using convolutional neural network: A residual learning approach,"Poker is the typical game of incomplete information, and remains a longstanding challenge problem in artificial intelligence (AI). The game of Dou Dizhu has been viewed as a thorny topic in AI since it is featured with hidden information and large branching factors, and the cooperation and competition should also be handled. In this article, deep learning is adopted to train a supervised learning playing strategy network (PSN) for Dou Dizhu directly from expert human playing. Through experiments, it was found that the sample design with the appropriate historical playing hand sequence and more features of the playing situation, can help the PSN learn more competitive and accurate playing strategies faster. In the online game platform, the strategy network-based game agent reaches an average winning rate of 52.22% against the human players. In addition, the analysis of the gameplay data against human players shows that the playing strategy network has learned the rules of playing and the characteristics of card recognition and reasonable demolition, cooperation and reasoning. Finally, we improve the performance of the PSN in the aspect of sample design. Then, the experimental results show that with proper marking of the number of remaining hands, the performance of the PSN can be enhanced. © 2021 - IOS Press. All rights reserved.",Journal of Computational Methods in Sciences and Engineering,10.3233/JCM-204344,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103680637&doi=10.3233%2fJCM-204344&partnerID=40&md5=ba499f31271579ee7c0ee2ce9eac6c9e,2021,2021-07-20 15:49:37,2021-07-20 15:49:37
5SI58X45,journalArticle,2020,"Zhu, W.; Wang, X.; Gao, W.",Multimedia Intelligence: When Multimedia Meets Artificial Intelligence,"Owing to the rich emerging multimedia applications and services in the past decade, super large amount of multimedia data has been produced for the purpose of advanced research in multimedia. Furthermore, multimedia research has made great progress on image/video content analysis, multimedia search and recommendation, multimedia streaming, multimedia content delivery etc. At the same time, Artificial Intelligence (AI) has undergone a 'new' wave of development since being officially regarded as an academic discipline in 1950s, which should give credits to the extreme success of deep learning. Thus, one question naturally arises: What happens when multimedia meets Artificial Intelligence? To answer this question, we introduce the concept of Multimedia Intelligence through investigating the mutual-influence between multimedia and Artificial Intelligence. We explore the mutual influences between multimedia and Artificial Intelligence from two aspects: i) multimedia drives Artificial Intelligence to experience a paradigm shift towards more explainability and ii) Artificial Intelligence in turn injects new ways of thinking for multimedia research. As such, these two aspects form a loop in which multimedia and Artificial Intelligence interactively enhance each other. In this paper, we discuss what and how efforts have been done in literature and share our insights on research directions that deserve further study to produce potentially profound impact on multimedia intelligence. © 1999-2012 IEEE.",IEEE Transactions on Multimedia,10.1109/TMM.2020.2969791,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090119559&doi=10.1109%2fTMM.2020.2969791&partnerID=40&md5=116ca795a48d9f639dd033539d39261d,2020,2021-07-20 15:49:37,2021-07-20 15:49:37
XGBCUV7K,journalArticle,2020,"Meher, S.K.","Granular space, knowledge-encoded deep learning architecture and remote sensing image classification","Hand-crafted features of remotely sensed (RS) image require the involvement of expensive human experts for classification. This factor motivates for designing the classification model with representative feature learning-based deep architecture to automate the feature extraction process and improve the generalization capability of the model. With this reasoning, we propose a deep auto-encoder neural network (NN) architecture with knowledge-encoded granular space for the classification of RS images. The network works with wavelet-rough granulated spaces and its architecture is designed with the encoded domain knowledge that strategically initializes the network parameters. Mostly, the learning time and performance of deep auto-encoders are persuaded by randomly selected weights and thus, we aim here to minimize these efforts with the domain knowledge. Neighborhood rough sets (NRS) are used to encode the domain knowledge and explore the contextual information for improved decision. We perform the knowledge-encoding operation for all stages of the auto-encoder. The proposed model thus exploits the mutual merits of deep network, wavelet-rough granular space and knowledge-encoding method. Comparative experimental results with multispectral and hyperspectral RS images demonstrate the superiority of our model to the related advanced methods. © 2020 Elsevier Ltd",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2020.103647,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083331590&doi=10.1016%2fj.engappai.2020.103647&partnerID=40&md5=006dc47f33cd1ea7d4b48e7b3296810e,2020,2021-07-20 15:49:37,2021-07-20 15:49:37
JI5LLKGL,journalArticle,2020,"Mueller, C.A.; Birk, A.",Visual Object Categorization Based on Hierarchical Shape Motifs Learned From Noisy Point Cloud Decompositions,"Object shape is a key cue that contributes to the semantic understanding of objects. In this work we focus on the categorization of real-world object point clouds to particular shape types. Therein surface description and representation of object shape structure have significant influence on shape categorization accuracy, when dealing with real-world scenes featuring noisy, partial and occluded object observations. An unsupervised hierarchical learning procedure is utilized here to symbolically describe surface characteristics on multiple semantic levels. Furthermore, a constellation model is proposed that hierarchically decomposes objects. The decompositions are described as constellations of symbols (shape motifs) in a gradual order, hence reflecting shape structure from local to global, i.e., from parts over groups of parts to entire objects. The combination of this multi-level description of surfaces and the hierarchical decomposition of shapes leads to a representation which allows to conceptualize shapes. An object discrimination has been observed in experiments with seven categories featuring instances with sensor noise, occlusions as well as inter-category and intra-category similarities. Experiments include the evaluation of the proposed description and shape decomposition approach, and comparisons to Fast Point Feature Histograms, a Vocabulary Tree and a neural network-based Deep Learning method. Furthermore, experiments are conducted with alternative datasets which analyze the generalization capability of the proposed approach. © 2019, Springer Nature B.V.",Journal of Intelligent and Robotic Systems: Theory and Applications,10.1007/s10846-019-01016-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065714332&doi=10.1007%2fs10846-019-01016-y&partnerID=40&md5=19df65c12551a4eee92d51c25253713f,2020,2021-07-20 15:49:37,2021-07-20 15:49:37
A6DLQ7CW,journalArticle,2020,"Umer, M.; Imtiaz, Z.; Ullah, S.; Mehmood, A.; Choi, G.S.; On, B.-W.",Fake news stance detection using deep learning architecture (CNN-LSTM),"Society and individuals are negatively influenced both politically and socially by the widespread increase of fake news either way generated by humans or machines. In the era of social networks, the quick rotation of news makes it challenging to evaluate its reliability promptly. Therefore, automated fake news detection tools have become a crucial requirement. To address the aforementioned issue, a hybrid Neural Network architecture, that combines the capabilities of CNN and LSTM, is used with two different dimensionality reduction approaches, Principle Component Analysis (PCA) and Chi-Square. This work proposed to employ the dimensionality reduction techniques to reduce the dimensionality of the feature vectors before passing them to the classifier. To develop the reasoning, this work acquired a dataset from the Fake News Challenges (FNC) website which has four types of stances: agree, disagree, discuss, and unrelated. The nonlinear features are fed to PCA and chi-square which provides more contextual features for fake news detection. The motivation of this research is to determine the relative stance of a news article towards its headline. The proposed model improves results by 4% and 20% in terms of Accuracy and F1-score. The experimental results show that PCA outperforms than Chi-square and state-of-the-art methods with 97.8% accuracy. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3019735,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091199880&doi=10.1109%2fACCESS.2020.3019735&partnerID=40&md5=bf69e43067b115bc1437491c0e29df39,2020,2021-07-20 15:49:38,2021-07-20 15:49:38
LPZWPATD,journalArticle,2017,"Garcia-Gasulla, D.; Ayguadé, E.; Labarta, J.; Béjar, J.; Cortés, U.; Suzumura, T.; Chen, R.",A visual embedding for the unsupervised extraction of abstract semantics,"Vector-space word representations obtained from neural network models have been shown to enable semantic operations based on vector arithmetic. In this paper, we explore the existence of similar information on vector representations of images. For that purpose we define a methodology to obtain large, sparse vector representations of image classes, and generate vectors through the state-of-the-art deep learning architecture GoogLeNet for 20 K images obtained from ImageNet. We first evaluate the resultant vector-space semantics through its correlation with WordNet distances, and find vector distances to be strongly correlated with linguistic semantics. We then explore the location of images within the vector space, finding elements close in WordNet to be clustered together, regardless of significant visual variances (e.g., 118 dog types). More surprisingly, we find that the space unsupervisedly separates complex classes without prior knowledge (e.g., living things). Afterwards, we consider vector arithmetics. Although we are unable to obtain meaningful results on this regard, we discuss the various problem we encountered, and how we consider to solve them. Finally, we discuss the impact of our research for cognitive systems, focusing on the role of the architecture being used. © 2016 Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2016.11.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006959877&doi=10.1016%2fj.cogsys.2016.11.008&partnerID=40&md5=e4de379141ad80bea333c21671de1634,2017,2021-07-20 15:49:38,2021-07-20 15:49:38
VINUCYBK,journalArticle,2019,"Yao, F.",Design and simulation of integrated education information teaching system based on fuzzy logic,"At present, China has great difficulty in obtaining the reliability of teaching data sources. In order to further improve the effectiveness of data mining and reduce the difficulty of data acquisition, this paper studies the design and simulation of integrated education information teaching system based on fuzzy logic. Bayesian algorithm can perform data mining, feature recognition and classification on data in big data, so that it can effectively process massive data sources. By weighting the different network structures, the number of undirected edges in the network is reduced, and then small data sets that can be processed by multiple traditional algorithms are sampled from the big data set, and data is generated by using the Bayesian network toolkit Samiam. The modules respectively generate data sets of different sizes and construct a teaching data source generation model. The experimental results show that RSEM on Child and Alarm data can take less time and achieve an accuracy of 86.17% compared with the whole data set under the same effect. This paper proposes a Bayesian network structure integration model, which can solve the problem of data acquisition difficulties, and is also a further improvement of data mining technology. © 2019 - IOS Press and the authors. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-179303,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074488554&doi=10.3233%2fJIFS-179303&partnerID=40&md5=44d93cfbe8a2868f06e8a9a072bb2c06,2019,2021-07-20 15:49:38,2021-07-20 15:49:38
MZ8IPWI7,journalArticle,2016,"Zhang, S.; Shanbhag, N.R.",Embedded Algorithmic Noise-Tolerance for Signal Processing and Machine Learning Systems via Data Path Decomposition,"Low overhead error-resiliency techniques such as algorithmic noise-tolerance (ANT) have been shown to be particularly effective for signal processing and machine learning kernels. However, the overhead of conventional ANT can be as large as 30% due to the use of explicit estimator block. To overcome this overhead, embedded ANT (E-ANT) is proposed [S. Zhang and N. Shanbhag, 'Embedded error compensation for energy efficient DSP systems,' in Proc. GlobalSIP, Dec. 2014, pp. 30-34], where the estimator is embedded in the main computation block via data path decomposition (DPD). E-ANT reduces the logic overhead to be below 8% as compared with the 20%-30% associated with conventional reduced precision replica (RPR) ANT system while maintaining the same error compensation functionality. DPD was first proposed in our original paper [Zhang and Shanbhag, 2014] where its benefits were studied in the case of a simple multiply-accumulator (MAC) kernel. This paper builds upon [Zhang and Shanbhag, 2014] by 1) providing conditions for the existence of DPD, 2) demonstrating DPD for a variety of commonly employed kernels in signal processing and machine learning applications, and 3) evaluating the robustness improvement and energy savings of DPD at the system level for an SVM-based EEG seizure classification application. Simulation results in a commercial 45 nm CMOS process show that E-ANT can compensate for error rates up to 0.38 for errors in FE only, and 0.17 for errors in FE and CE, while maintain a true positive rate $p-\rm tp > 0.9$ and a false positive rate $p-\rm fp\leq 0.01$. This represents a greater than 3-orders-of-magnitude improvement in error tolerance over the conventional system. This error tolerance is employed to reduce energy via the use of voltage overscaling (VOS). E-ANT is able to achieve 51% energy savings when errors are in FE only, and up to 43% savings when errors are in both FE and CE. © 1991-2012 IEEE.",IEEE Transactions on Signal Processing,10.1109/TSP.2016.2546224,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976358960&doi=10.1109%2fTSP.2016.2546224&partnerID=40&md5=b4a94bc36e431b006a8128c14bd57bfc,2016,2021-07-20 15:49:38,2021-07-20 15:49:38
ESAW72TQ,journalArticle,2012,"Subasi, A.",Medical decision support system for diagnosis of neuromuscular disorders using DWT and fuzzy support vector machines,"The motor unit action potentials (MUAPs) in an electromyographic (EMG) signal provide a significant source of information for the assessment of neuromuscular disorders. In this work, different types of machine learning methods were used to classify EMG signals and compared in relation to their accuracy in classification of EMG signals. The models automatically classify the EMG signals into normal, neurogenic or myopathic. The best averaged performance over 10 runs of randomized cross-validation is also obtained by different classification models. Some conclusions concerning the impacts of features on the EMG signal classification were obtained through analysis of the classification techniques. The comparative analysis suggests that the fuzzy support vector machines (FSVM) modelling is superior to the other machine learning methods in at least three points: slightly higher recognition rate; insensitivity to overtraining; and consistent outputs demonstrating higher reliability. The combined model with discrete wavelet transform (DWT) and FSVM achieves the better performance for internal cross validation (External cross validation) with the area under the reciever operating characteristic (ROC) curve (AUC) and accuracy equal to 0.996 (0.970) and 97.67% (93.5%), respectively. These results show that the proposed model have the potential to obtain a reliable classification of EMG signals, and to assist the clinicians for making a correct diagnosis of neuromuscular disorders. © 2012 Elsevier Ltd.",Computers in Biology and Medicine,10.1016/j.compbiomed.2012.06.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84864128286&doi=10.1016%2fj.compbiomed.2012.06.004&partnerID=40&md5=302bbf9ef1dfdd2ed2991c64235250d5,2012,2021-07-20 15:49:38,2021-07-20 15:49:38
VVW2E88P,journalArticle,2017,"Cambria, E.; Poria, S.; Gelbukh, A.; Thelwall, M.",Sentiment Analysis Is a Big Suitcase,"Although most works approach it as a simple categorization problem, sentiment analysis is actually a suitcase research problem that requires tackling many natural language processing (NLP) tasks. The expression 'sentiment analysis' itself is a big suitcase (like many others related to affective computing, such as emotion recognition or opinion mining) that all of us use to encapsulate our jumbled idea about how our minds convey emotions and opinions through natural language. The authors address the composite nature of the problem via a three-layer structure inspired by the 'jumping NLP curves' paradigm. In particular, they argue that there are (at least) 15 NLP problems that need to be solved to achieve human-like performance in sentiment analysis. © 2017 IEEE.",IEEE Intelligent Systems,10.1109/MIS.2017.4531228,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038582534&doi=10.1109%2fMIS.2017.4531228&partnerID=40&md5=a1a4abcf7cd5bd1c41b4e88e7e3d64f7,2017,2021-07-20 15:49:38,2021-07-20 15:49:38
6SAG6Y34,journalArticle,2019,"Verda, D.; Parodi, S.; Ferrari, E.; Muselli, M.",Analyzing gene expression data for pediatric and adult cancer diagnosis using logic learning machine and standard supervised methods,"Background: Logic Learning Machine (LLM) is an innovative method of supervised analysis capable of constructing models based on simple and intelligible rules. In this investigation the performance of LLM in classifying patients with cancer was evaluated using a set of eight publicly available gene expression databases for cancer diagnosis. LLM accuracy was assessed by summary ROC curve (sROC) analysis and estimated by the area under an sROC curve (sAUC). Its performance was compared in cross validation with that of standard supervised methods, namely: decision tree, artificial neural network, support vector machine (SVM) and k-nearest neighbor classifier. Results: LLM showed an excellent accuracy (sAUC = 0.99, 95%CI: 0.98-1.0) and outperformed any other method except SVM. Conclusions: LLM is a new powerful tool for the analysis of gene expression data for cancer diagnosis. Simple rules generated by LLM could contribute to a better understanding of cancer biology, potentially addressing therapeutic approaches. © 2019 The Author(s).",BMC Bioinformatics,10.1186/s12859-019-2953-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075473315&doi=10.1186%2fs12859-019-2953-8&partnerID=40&md5=60c7adcb1a7b735c3e4eac4d01bef61e,2019,2021-07-20 15:49:38,2021-07-20 15:49:38
NP2UJHIS,journalArticle,2020,"Li, N.; Zhou, S.; Wu, Z.; Zhang, B.; Zhao, G.",Statistical modeling and knowledge-based segmentation of cerebral artery based on TOF-MRA and MR-T1,"Background and objective: For cerebrovascular segmentation from time-of-flight (TOF) magnetic resonance angiography (MRA), the focused issues are segmentation accuracy, vascular network coverage ratio, and cerebral artery and vein (CA/CV) separation. Therefore, cerebral artery segmentation is a challenging work, while a complete solution is lacking so far. Methods: The preprocessing of skull-stripping and Hessian-based feature extraction is first implemented to acquire an indirect prior knowledge of vascular distribution and shape. Then, a novel intensity- and shape-based Markov statistical modeling is proposed for complete cerebrovascular segmentation, where our low-level process employs a Gaussian mixture model to fit the intensity histogram of the skull-stripped TOF-MRA data, while our high-level process employs the vascular shape prior to construct the energy function. To regularize the individual data processes, Markov regularization parameter is automatically estimated by using a machine-learning algorithm. Further, cerebral artery and vein (CA/CV) separation is explored with a series of morphological logic operations, which are based on a direct priori knowledge on the relationship of arteriovenous topology and brain tissues in between TOF-MRA and MR-T1. Results: We employed 109 sets of public datasets from MIDAS for qualitative and quantitative assessment. The Dice similarity coefficient, false negative rate (FNR), and false positive rate (FPR) of 0.933, 0.158, and 0.091% on average, as well as CA/CV separation results with the agreement, FNR, and FPR of 0.976, 0.041, and 0.022 on average. For clinical visual assessment, our methods can segment various sizes of the vessel in different contrast region, especially performs better on vessels of small size in low contrast region. Conclusion: Our methods obtained satisfying results in visual and quantitative evaluation. The proposed method is capable of accurate cerebrovascular segmentation and efficient CA/CV separation. Further, it can stimulate valuable clinical applications on the computer-assisted cerebrovascular intervention according to the neurosurgeon's recommendation. © 2019",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2019.105110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075109214&doi=10.1016%2fj.cmpb.2019.105110&partnerID=40&md5=5596982abc4652117460b0bbe29f4540,2020,2021-07-20 15:49:38,2021-07-20 15:49:38
E4BPHPKC,journalArticle,2016,"Wolff, J.G.",The SP Theory of Intelligence: Distinctive Features and Advantages,"This paper aims to highlight distinctive features of the SP theory of intelligence, realized in the SP computer model, and its apparent advantages compared with some AI-related alternatives. Perhaps most importantly, the theory simplifies and integrates observations and concepts in AI-related areas, and has potential to simplify and integrate of structures and processes in computing systems. Unlike most other AI-related theories, the SP theory is itself a theory of computing, which can be the basis for new architectures for computers. Fundamental in the theory is information compression via the matching and unification of patterns and, more specifically, via a concept of multiple alignment. The theory promotes transparency in the representation and processing of knowledge, and unsupervised learning of natural structures via information compression. It provides an interpretation of aspects of mathematics and an interpretation of phenomena in human perception and cognition. concepts in the theory may be realized in terms of neurons and their inter-connections (SP-neural). These features and advantages of the SP system are discussed in relation to AI-related alternatives: the concept of minimum length encoding and related concepts, how computational and energy efficiency in computing may be achieved, deep learning in neural networks, unified theories of cognition and related research, universal search, Bayesian networks and some other models for AI, IBM's Watson, solving problems associated with big data and in the development of intelligence in autonomous robots, pattern recognition and vision, the learning and processing of natural language, exact and inexact forms of reasoning, representation and processing of diverse forms of knowledge, and software engineering. In conclusion, the SP system can provide a firm foundation for the long-term development of AI and related areas, and at the same time, it may deliver useful results on relatively short timescales. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2015.2513822,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979807614&doi=10.1109%2fACCESS.2015.2513822&partnerID=40&md5=3df22c8997675560d24ee4beda1bab43,2016,2021-07-20 15:49:38,2021-07-20 15:49:38
H4I5S662,journalArticle,2021,"Gupta, M.; Upadhyay, V.; Kumar, P.; Al-Turjman, F.",Implementation of autonomous driving using Ensemble-M in simulated environment,"Making autonomous driving a safe, feasible, and better alternative is one of the core problems the world is facing today. The horizon of the applications of AI and deep learning has changed the perspective of the human mind. Initially, what used to be thought of as the subtle impossible task is applicable today, and that too in the feasibly efficient way. Computer vision tasks powered with highly tuned CNNs are outperforming humans in many fields. Introductory implementations of the autonomous vehicle were merely achieved using raw image processing, and hard programmed rule-based logic systems along with machine/deep learning were used as secondary objective handlers. With the autonomous driving method proposed by Nvidia, the usability of CNNs is more adequate, adaptable, and applicable. In this paper, we propose the ensemble implementation of CNN-based regression models for autonomous driving. We have taken simulator generated driving view image dataset along with a mapped file of steering angle in radians. After applying image pre-processing and augmentation, we have used two CNN models along with their ensemble and compared their performance to minimize the risks of unsafe driving. We have compared Nvidia proposed CNN, MobileNet-V2 as regression model and Ensemble-M results for comparison of their respective performance, MSE scores and compute time to process. In result analysis, the MobileNet-V2 model performs better in densely featured roads and the Nvidia model performs better in sparsely featured roads, whereas Ensemble-M normalizes the performance of both models and efficiently results in the least MSE score (0.0201) with the highest computation time utilization making autonomous driving better, safer alternative. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Soft Computing,10.1007/s00500-021-05954-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108381917&doi=10.1007%2fs00500-021-05954-4&partnerID=40&md5=a1a99a1bb0ecc1a66528fa3e1ecceff0,2021,2021-07-20 15:49:38,2021-07-20 15:49:38
RPBYCC5T,journalArticle,2021,"Garain, A.; Ray, B.; Singh, P.K.; Ahmadian, A.; Senu, N.; Sarkar, R.",GRANet: A Deep Learning Model for Classification of Age and Gender from Facial Images,"The problem of gender and age identification has been addressed by many researchers, however, the attention given to it compared to the other related problems of face recognition in particular is far less. The success achieved in this domain has not seen much improvement compared to the other face recognition problems. Any language in the world has a separate set of words and grammatical rules when addressing people of different ages. The decision associated with its usage, relies on our ability to demarcate these individual characteristics like gender and age from the facial appearances at one glance. With the rapid usage of Artificial Intelligence (AI) based systems in different fields, we expect that such decision making capability of these systems match as much as to the human capability. To this end, in this work, we have designed a deep learning based model, called GRANet (Gated Residual Attention Network), for the prediction of age and gender from the facial images. This is a modified and improved version of Residual Attention Network where we have included the concept of Gate in the architecture. Gender identification is a binary classification problem whereas prediction of age is a regression problem. We have decomposed this regression problem into a combination of classification and regression problems for achieving better accuracy. Experiments have been done on five publicly available standard datasets namely FG-Net, Wikipedia, AFAD, UTKFAce and AdienceDB. Obtained results have proven its effectiveness for both age and gender classification, thus making it a proper candidate for the same against any other stateof-the-art methods. The link to working code of our architecture can be found herepypigithub. CCBY",IEEE Access,10.1109/ACCESS.2021.3085971,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107371767&doi=10.1109%2fACCESS.2021.3085971&partnerID=40&md5=8e54c5e5abc4e5d4398b4c18b5c7c40f,2021,2021-07-20 15:49:38,2021-07-20 15:49:38
BAMGPUCX,journalArticle,2021,"Es-Sabery, F.; Hair, A.; Qadir, J.; Sainz-De-Abajo, B.; Garcia-Zapirain, B.; Torre-DIez, I.",Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier,"At present, with the growing number of Web 2.0 platforms such as Instagram, Facebook, and Twitter, users honestly communicate their opinions and ideas about events, services, and products. Owing to this rise in the number of social platforms and their extensive use by people, enormous amounts of data are produced hourly. However, sentiment analysis or opinion mining is considered as a useful tool that aims to extract the emotion and attitude from the user-posted data on social media platforms by using different computational methods to linguistic terms and various Natural Language Processing (NLP). Therefore, enhancing text sentiment classification accuracy has become feasible, and an interesting research area for many community researchers. In this study, a new Fuzzy Deep Learning Classifier (FDLC) is suggested for improving the performance of data-sentiment classification. Our proposed FDLC integrates Convolutional Neural Network (CNN) to build an effective automatic process for extracting the features from collected unstructured data and Feedforward Neural Network (FFNN) to compute both positive and negative sentimental scores. Then, we used the Mamdani Fuzzy System (MFS) as a fuzzy classifier to classify the outcomes of the two used deep (CNN+FFNN) learning models in three classes, which are: Neutral, Negative, and Positive. Also, to prevent the long execution time taking by our hybrid proposed FDLC, we have implemented our proposal under the Hadoop cluster. An experimental comparative study between our FDLC and some other suggestions from the literature is performed to demonstrate our offered classifier's effectiveness. The empirical result proved that our FDLC performs better than other classifiers in terms of true positive rate, true negative rate, false positive rate, false negative rate, error rate, precision, classification rate, kappa statistic, F1-score and time consumption, complexity, convergence, and stability. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3053917,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106847388&doi=10.1109%2fACCESS.2021.3053917&partnerID=40&md5=0ac6e3c956a44104600e5c8a92fc0476,2021,2021-07-20 15:49:38,2021-07-20 15:49:38
HHX6AHUX,journalArticle,2021,"Choe, S.; Seong, H.; Kim, E.",Indoor Place Category Recognition for a Cleaning Robot by Fusing a Probabilistic Approach and Deep Learning,"Indoor place category recognition for a cleaning robot is a problem in which a cleaning robot predicts the category of the indoor place using images captured by it. This is similar to scene recognition in computer vision as well as semantic mapping in robotics. Compared with scene recognition, the indoor place category recognition considered in this article differs as follows: 1) the indoor places include typical home objects; 2) a sequence of images instead of an isolated image is provided because the images are captured successively by a cleaning robot; and 3) the camera of the cleaning robot has a different view compared with those of cameras typically used by human beings. Compared with semantic mapping, indoor place category recognition can be considered as a component in semantic SLAM. In this article, a new method based on the combination of a probabilistic approach and deep learning is proposed to address indoor place category recognition for a cleaning robot. Concerning the probabilistic approach, a new place-object fusion method is proposed based on Bayesian inference. For deep learning, the proposed place-object fusion method is trained using a convolutional neural network in an end-to-end framework. Furthermore, a new recurrent neural network, called the Bayesian filtering network (BFN), is proposed to conduct time-domain fusion. Finally, the proposed method is applied to a benchmark dataset and a new dataset developed in this article, and its validity is demonstrated experimentally. IEEE",IEEE Transactions on Cybernetics,10.1109/TCYB.2021.3052499,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101758727&doi=10.1109%2fTCYB.2021.3052499&partnerID=40&md5=5941e7d6c9b9fa3ee2767afac5df3408,2021,2021-07-20 15:49:38,2021-07-20 15:49:38
AQ3NWCYN,journalArticle,2018,"Wu, B.-F.; Chen, Y.-S.; Huang, C.-W.; Chang, P.-J.",An Uphill Safety Controller with Deep Learning-Based Ramp Detection for Intelligent Wheelchairs,"In a society with aging population, the demand for electric wheelchairs is growing with the advancement of automation. However, many accidents have occurred due to the misjudgment of the slope angle and wheelchair speed while the wheelchair is traveling on ramps. This research employs the light electronic assistance pal compact motor package to reduce the weight and size of conventional electric wheelchairs. The modular design of proposed uphill controller and ramp detection functions allows users to easily select and incorporate only the functions they need. This paper proposes a ramp detection model implemented using the deep learning algorithm with CNN-4 structure to analyze depth image data. The model's recognition time of each video frame is 11 times faster than that of the AlexNet and GoogleNet. The uphill safety controller is designed as an adaptive network-based fuzzy inference system with Q-learning. The safe speed is automatically calculated according to the angle obtained from slope classification and revised in real-time during the slope driving to prevent the user from moving towards the dangerous ramp or rolling back due to inadequate speed. The accuracy of ramp detection is further increased by 5% to 97.1% due to assistance from the voting system processing and the gyroscope output data. The 5° ramp experiment of our uphill controller with ramp classification takes 20 s to complete the slope driving which is 23% faster than the controller without ramp detection. The energy consumption is also one half less than the experiment without uphill detection. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2839729,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047604174&doi=10.1109%2fACCESS.2018.2839729&partnerID=40&md5=785f968f41640d538ec949ee2527ab7b,2018,2021-07-20 15:49:38,2021-07-20 15:49:38
8SNDYRFT,journalArticle,2018,"Azarkhish, E.; Rossi, D.; Loi, I.; Benini, L.",Neurostream: Scalable and Energy Efficient Deep Learning with Smart Memory Cubes,"High-performance computing systems are moving towards 2.5D and 3D memory hierarchies, based on High Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) to mitigate the main memory bottlenecks. This trend is also creating new opportunities to revisit near-memory computation. In this paper, we propose a flexible processor-in-memory (PIM) solution for scalable and energy-efficient execution of deep convolutional networks (ConvNets), one of the fastest-growing workloads for servers and high-end embedded systems. Our co-design approach consists of a network of Smart Memory Cubes (modular extensions to the standard HMC) each augmented with a many-core PIM platform called NeuroCluster. NeuroClusters have a modular design based on NeuroStream coprocessors (for Convolution-intensive computations) and general-purpose RISC-V cores. In addition, a DRAM-friendly tiling mechanism and a scalable computation paradigm are presented to efficiently harness this computational capability with a very low programming effort. NeuroCluster occupies only 8 percent of the total logic-base (LoB) die area in a standard HMC and achieves an average performance of 240 GFLOPS for complete execution of full-featured state-of-the-art (SoA) ConvNets within a power budget of 2.5 W. Overall 11 W is consumed in a single SMC device, with 22.5 GFLOPS/W energy-efficiency which is 3.5X better than the best GPU implementations in similar technologies. The minor increase in system-level power and the negligible area increase make our PIM system a cost-effective and energy efficient solution, easily scalable to 955 GFLOPS with a small network of just four SMCs. © 2017 IEEE.",IEEE Transactions on Parallel and Distributed Systems,10.1109/TPDS.2017.2752706,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030640525&doi=10.1109%2fTPDS.2017.2752706&partnerID=40&md5=bc8ef43380db53cf01b04d899e1758a1,2018,2021-07-20 15:49:38,2021-07-20 15:49:38
N3T9CNBV,journalArticle,2021,"Kubosawa, S.; Onishi, T.; Tsuruoka, Y.",Computing operation procedures for chemical plants using whole-plant simulation models,"Chemical plants are complex dynamical systems. Optimising plant operation for non-stationary scenarios, such as changing the output product and recovering from abrupt disturbances, is challenging because a chemical plant has many operation points and complex responses. A plant simulator can be used to compute the optimal procedures. However, because of modelling errors or contingent changes in the external conditions, such as weather and feed purity, there exist gaps between the behaviour of a simulator and that of a real plant. This poses another challenge in a simulator-based approach, which adds to the computational complexity of the problem. In this study, we propose a simulator-based approach for optimising chemical plant operations using deep reinforcement learning and knowledge-based automated reasoning. Specifically, a reinforcement learning agent is trained on a whole-plant simulator with a policy gradient algorithm, using automated reasoning to narrow down the action space of the agent. To maintain the optimality of the procedures in a real plant, a simple method for the state and parameter estimation of the system at run time is introduced. This method can improve the accuracy of the response prediction model (i.e. the plant simulator) on which the agent depends. The presented method is evaluated on a real chemical distillation plant. The experimental results indicate that the proposed approach consumed only half the time and steam (heat energy) in comparison with that in the case of human-emulated procedures. © 2021 The Author(s)",Control Engineering Practice,10.1016/j.conengprac.2021.104878,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108992830&doi=10.1016%2fj.conengprac.2021.104878&partnerID=40&md5=74e1e46ada88fdae0cf18fc15b8ec010,2021,2021-07-20 15:49:39,2021-07-20 15:49:39
ZPU5A82K,journalArticle,2020,"Robbins, S.",AI and the path to envelopment: knowledge as a first step towards the responsible regulation and use of AI-powered machines,"With Artificial Intelligence (AI) entering our lives in novel ways—both known and unknown to us—there is both the enhancement of existing ethical issues associated with AI as well as the rise of new ethical issues. There is much focus on opening up the ‘black box’ of modern machine-learning algorithms to understand the reasoning behind their decisions—especially morally salient decisions. However, some applications of AI which are no doubt beneficial to society rely upon these black boxes. Rather than requiring algorithms to be transparent we should focus on constraining AI and those machines powered by AI within microenvironments—both physical and virtual—which allow these machines to realize their function whilst preventing harm to humans. In the field of robotics this is called ‘envelopment’. However, to put an ‘envelope’ around AI-powered machines we need to know some basic things about them which we are often in the dark about. The properties we need to know are the: training data, inputs, functions, outputs, and boundaries. This knowledge is a necessary first step towards the envelopment of AI-powered machines. It is only with this knowledge that we can responsibly regulate, use, and live in a world populated by these machines. © 2019, The Author(s).",AI and Society,10.1007/s00146-019-00891-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074574493&doi=10.1007%2fs00146-019-00891-1&partnerID=40&md5=7ebe8bbbc7560ad52df02b86b3d59cd6,2020,2021-07-20 15:49:39,2021-07-20 15:49:39
PNE6VVBB,journalArticle,2020,"Althubaiti, S.; Kafkas, S.; Abdelhakim, M.; Hoehndorf, R.",Combining lexical and context features for automatic ontology extension,"Background: Ontologies are widely used across biology and biomedicine for the annotation of databases. Ontology development is often a manual, time-consuming, and expensive process. Automatic or semi-automatic identification of classes that can be added to an ontology can make ontology development more efficient. Results: We developed a method that uses machine learning and word embeddings to identify words and phrases that are used to refer to an ontology class in biomedical Europe PMC full-text articles. Once labels and synonyms of a class are known, we use machine learning to identify the super-classes of a class. For this purpose, we identify lexical term variants, use word embeddings to capture context information, and rely on automated reasoning over ontologies to generate features, and we use an artificial neural network as classifier. We demonstrate the utility of our approach in identifying terms that refer to diseases in the Human Disease Ontology and to distinguish between different types of diseases. Conclusions: Our method is capable of discovering labels that refer to a class in an ontology but are not present in an ontology, and it can identify whether a class should be a subclass of some high-level ontology classes. Our approach can therefore be used for the semi-automatic extension and quality control of ontologies. The algorithm, corpora and evaluation datasets are available at http://github.com/bio-ontology-research-group/ontology-extension. © 2020 The Author(s).",Journal of Biomedical Semantics,10.1186/s13326-019-0218-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077785975&doi=10.1186%2fs13326-019-0218-0&partnerID=40&md5=c60602a13fcd21bd8f6b13e6e54ce516,2020,2021-07-20 15:49:39,2021-07-20 15:49:39
BHL5LK25,journalArticle,2019,"Rehman, T.; Khan, S.; Hwang, G.-J.; Abbas, M.A.",Automatically solving two-variable linear algebraic word problems using text mining,"The teaching and learning of algebraic word problems is a basic component of elementary education. Recently, to facilitate its learning, a few approaches for automatically solving algebraic and arithmetic word problems have been proposed. These systems generally use either natural language processing (NLP) or a combination of NLP and machine learning. However, they have low accuracy due to their large feature sets, extracted using limited preprocessing techniques. In this research work, we propose a template-based approach that was developed by following a two-step process. In the first step, we predict an equation template from a training dataset using NLP and a classification mechanism. The next step is to instantiate the predicted template with nouns and numbers through reasoning. To validate the proposed methodology, a prototype system was implemented. We then compared the proposed system with the existing systems using their respective datasets and the proposed dataset. The experimental results show improvement in accuracy, with an average precision of 80.6% and average recall of 83.5%. © 2018 John Wiley & Sons, Ltd",Expert Systems,10.1111/exsy.12358,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056751650&doi=10.1111%2fexsy.12358&partnerID=40&md5=ac0eff02e22431a5066bae5b2e67d563,2019,2021-07-20 15:49:39,2021-07-20 15:49:39
NZSW4WNE,journalArticle,2013,"Ritter, F.E.; Kennedy, W.G.; Best, B.J.",The best papers from BRIMS 2011: Models of users and teams interacting,"This special issue is similar to our previous special issues (Kennedy et al. in Comput. Math. Organ. Theory 16(3):217-219, 2010; 17(3):225-228, 2011) in that it includes articles based on the award winning conference papers of the, here, 2011 BRiMS Annual Conference. These articles were reviewed by the editors, extended to journal article length, and then peer-reviewed and revised before being accepted. The articles include a new way to evaluate designs of interfaces for safety critical systems (Bolton in Comput. Math. Organ. Theory, 2012), an article that extends our understanding of how to model situation awareness (SA) in a cognitive architecture (Rodgers et al. in Comput. Math. Organ. Theory, 2012), an article that presents electroencephalography (EEG) data used to derive dynamic neurophysiologic models of engagement in teamwork (Stevens et al. in Comput. Math. Organ. Theory, 2012), and an article that demonstrates using machine learning to generate models and an example application of that tool (Best in Comput. Math. Organ. Theory, 2012). After presenting a brief summary of each paper we will see some recurrent themes of task analysis, team and individual models, spatial reasoning, usability issues, and particularly that they are models that interact with each other or systems. © 2012 Springer Science+Business Media New York.",Computational and Mathematical Organization Theory,10.1007/s10588-012-9140-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881034353&doi=10.1007%2fs10588-012-9140-z&partnerID=40&md5=8c3cab1c46b05fbfa16d727aa757ecb5,2013,2021-07-20 15:49:39,2021-07-20 15:49:39
885BDVI4,journalArticle,2019,"Large, J.; Bagnall, A.; Malinowski, S.; Tavenard, R.",On time series classification with dictionary-based classifiers,"A family of algorithms for time series classification (TSC) involve running a sliding window across each series, discretising the window to form a word, forming a histogram of word counts over the dictionary, then constructing a classifier on the histograms. A recent evaluation of two of this type of algorithm, Bag of Patterns (BOP) and Bag of Symbolic Fourier Approximation Symbols (BOSS) found a significant difference in accuracy between these seemingly similar algorithms. We investigate this phenomenon by deconstructing the classifiers and measuring the relative importance of the four key components between BOP and BOSS. We find that whilst ensembling is a key component for both algorithms, the effect of the other components is mixed and more complex. We conclude that BOSS represents the state of the art for dictionary-based TSC. Both BOP and BOSS can be classed as bag of words approaches. These are particularly popular in Computer Vision for tasks such as image classification. We adapt three techniques used in Computer Vision for TSC: Scale Invariant Feature Transform; Spatial Pyramids; and Histogram Intersection. We find that using Spatial Pyramids in conjunction with BOSS (SP) produces a significantly more accurate classifier. SP is significantly more accurate than standard benchmarks and the original BOSS algorithm. It is not significantly worse than the best shapelet-based or deep learning approaches, and is only outperformed by an ensemble that includes BOSS as a constituent module. © 2019 - IOS Press and the authors. All rights reserved.",Intelligent Data Analysis,10.3233/IDA-184333,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074455247&doi=10.3233%2fIDA-184333&partnerID=40&md5=d818b8079f0c08c380fc716323aa420a,2019,2021-07-20 15:49:39,2021-07-20 15:49:39
W7GVGJXA,journalArticle,2018,"Xu, C.; Shen, J.; Du, X.; Zhang, F.",An Intrusion Detection System Using a Deep Neural Network with Gated Recurrent Units,"To improve the performance of network intrusion detection systems (IDS), we applied deep learning theory to intrusion detection and developed a deep network model with automatic feature extraction. In this paper, we consider the characteristics of the time-related intrusion and propose a novel IDS that consists of a recurrent neural network with gated recurrent units (GRU), multilayer perceptron (MLP), and softmax module. Experiments on the well-known KDD 99 and NSL-KDD data sets show that the system has leading performance. The overall detection rate was 99.42% using KDD 99 and 99.31% using NSL-KDD with false positive rates as low as 0.05% and 0.84%, respectively. In particular, for detecting the denial of service attacks, the system achieved detection rates of 99.98% and 99.55%, respectively. Comparative experiments showed that the GRU is more suitable as a memory unit for IDS than LSTM, and proved that it is an effective simplification and improvement of LSTM. Moreover, the bidirectional GRU can reach the best performance compared with the recently published methods. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2867564,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052626703&doi=10.1109%2fACCESS.2018.2867564&partnerID=40&md5=100c9db281f7aae1432dbbab9dcfbe91,2018,2021-07-20 15:49:39,2021-07-20 15:49:39
V3YL3PL9,journalArticle,2021,"Liu, Z.; Mohammadzadeh, A.; Turabieh, H.; Mafarja, M.; Band, S.S.; Mosavi, A.",A new online learned interval type-3 fuzzy control system for solar energy management systems,"In this article, a novel method based on interval type-3 fuzzy logic systems (IT3-FLSs) and an online learning approach is designed for power control and battery charge planing for photovoltaic (PV)/battery hybrid systems. Unlike the other methods, the dynamics of battery, PV and boost converters are considered to be fully unknown. Also, the effects of variation of temperature, radiation, and output load are taken into account. The robustness and the asymptotic stability of the proposed method is analyzed by the Lyapunov/LaSalle's invariant set theorems, and the tuning rules are extracted for IT3-FLS. Also, the upper bound of approximation error (AE) is approximated, and then a new compensator is designed to deal with the effects of dynamic AEs. The superiority of the proposed method is examined in several conditions and is compared with some other well-known methods. It is shown that the schemed method results in high performance under difficult conditions such as variation of temperature and radiation and abruptly changing in the output load. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3049301,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099177784&doi=10.1109%2fACCESS.2021.3049301&partnerID=40&md5=a854c008d092e98e3bec3f86e9ba2ecf,2021,2021-07-20 15:49:39,2021-07-20 15:49:39
AI5GC8ND,journalArticle,2020,"Thiruvengadam, C.; Palanivelan, M.; Senthil Kumar, K.; Jayasankar, T.",Low power approximate adder based repetitive iteration cord (LP-ARICO) algorithm for high-speed applications,"In the most recent decade, the CORDIC calculation has drawn wide consideration from the industry and scholarly community for different applications, for example, DSP, SDR, bio-signal processing, Machine Learning, and communication systems, etc. This approach is an iterative calculation, and it involves effortless shift-add tasks, for logic cell utilization of fundamental, rudimentary capacities, present large calculations, and requires huge power. The key prospects for achieving overall output at a faster rate, less power, and minimal area is to modify the adder used. In this paper, the significance of low power Approximate Adder based Repetitive Iteration CORDIC (LP-ARICO) technique that obtains sine/cosine value. The algorithmic quality decreases system, which prompts a decrease in implementation difficulty through misusing the importance. This modifies be actualized in favor of the decrease in power utilization and area effective plan for the iterative procedure. The proposed LP-ARICO architecture achieves higher throughput and decreases idleness. The obtained results show that the proposed system accomplishes delay, power, and area reduction of 45.13%, 4.02%, and 31.12% individually beyond the other techniques at the expense of a 5.3% increase in throughput. © 2020",Microprocessors and Microsystems,10.1016/j.micpro.2020.103260,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091647546&doi=10.1016%2fj.micpro.2020.103260&partnerID=40&md5=21a2955411664d85dbccf940155a603d,2020,2021-07-20 15:49:39,2021-07-20 15:49:39
MP3PPLVE,journalArticle,2018,"Tenorio-González, A.C.; Morales, E.F.",Automatic discovery of concepts and actions,"A truly autonomous artificial intelligence agent should be able to drive its own learning process. That is, decide what to explore and what to learn, identifying what constitutes potential useful data as examples of concepts or what strategy to follow to solve a new task. Different efforts have been developed in machine learning towards this aim. Approaches that introduce new concepts, like predicate invention in Inductive Logic Programming (ILP) techniques, normally require the selection of examples by the user. Techniques that learn behavior policies through exploration like Reinforcement Learning (RL) with intrinsic motivation, to guide the agent into interesting areas to discover new goals, assume that all the states and actions are predefined in advance. In this paper, we describe a system, called ADC, that combines techniques from ILP with predicate invention and RL with intrinsic motivation to discover new concepts, states and actions to learn behavior policies. ADC drives its own learning process, collecting its own examples for autonomously learning concepts. These new concepts can be used to describe its environment and define new states and actions used to learn behaviors to solve tasks. We show the effectiveness of our approach in simulated robotics environments. © 2017 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2017.09.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030121361&doi=10.1016%2fj.eswa.2017.09.023&partnerID=40&md5=14049114c446f5157353c22c89bc1b12,2018,2021-07-20 15:49:39,2021-07-20 15:49:39
Z33NJGZ9,journalArticle,2012,"Li, Y.; Guo, M.",A new relational Tri-training system with adaptive data editing for inductive logic programming,"Relational Tri-training (R-Tri-training for short), as a relational semi-supervised learning system, can effectively exploit unlabeled examples to improve the generalization ability. However, the R-Tri-training may also suffer from the common problem in traditional semi-supervised learning, i.e., the performance is usually not stable for the unlabeled examples often be wrongly labeled and accumulated during the iterative learning process. In this paper, a new Relational Tri-training system named ADE-R-Tri-training (R-Tri-training with Adaptive Data Editing) is proposed. Not only does it employ a specific data editing technique to identify and correct the examples possibly mislabeled throughout the co-labeling iterations, but it also takes an adaptive strategy to decide whether to trigger the editing operation according to different cases. The adaptive strategy consists of five pre-conditional theorems, all of which ensure the iterative reduction of classification error under PAC (Probably Approximately Correct) learning theory. Experiments on well-known benchmarks show that ADE-R-Tri-training can more effectively enhance the performance of the hypothesis learned than R-Tri-training. © 2012 Elsevier B.V. All rights reserved.",Knowledge-Based Systems,10.1016/j.knosys.2012.04.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84866479316&doi=10.1016%2fj.knosys.2012.04.021&partnerID=40&md5=f920869c0a0105153a3e4e95d34bace9,2012,2021-07-20 15:49:39,2021-07-20 15:49:39
UCPTMTUK,journalArticle,2017,"Hidalgo, J.I.; Colmenar, J.M.; Kronberger, G.; Winkler, S.M.; Garnica, O.; Lanchares, J.",Data Based Prediction of Blood Glucose Concentrations Using Evolutionary Methods,"Predicting glucose values on the basis of insulin and food intakes is a difficult task that people with diabetes need to do daily. This is necessary as it is important to maintain glucose levels at appropriate values to avoid not only short-term, but also long-term complications of the illness. Artificial intelligence in general and machine learning techniques in particular have already lead to promising results in modeling and predicting glucose concentrations. In this work, several machine learning techniques are used for the modeling and prediction of glucose concentrations using as inputs the values measured by a continuous monitoring glucose system as well as also previous and estimated future carbohydrate intakes and insulin injections. In particular, we use the following four techniques: genetic programming, random forests, k-nearest neighbors, and grammatical evolution. We propose two new enhanced modeling algorithms for glucose prediction, namely (i) a variant of grammatical evolution which uses an optimized grammar, and (ii) a variant of tree-based genetic programming which uses a three-compartment model for carbohydrate and insulin dynamics. The predictors were trained and tested using data of ten patients from a public hospital in Spain. We analyze our experimental results using the Clarke error grid metric and see that 90% of the forecasts are correct (i.e., Clarke error categories A and B), but still even the best methods produce 5 to 10% of serious errors (category D) and approximately 0.5% of very serious errors (category E). We also propose an enhanced genetic programming algorithm that incorporates a three-compartment model into symbolic regression models to create smoothed time series of the original carbohydrate and insulin time series. © 2017, Springer Science+Business Media, LLC.",Journal of Medical Systems,10.1007/s10916-017-0788-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027006592&doi=10.1007%2fs10916-017-0788-2&partnerID=40&md5=87e31929912f141ebb16b38b5526b81d,2017,2021-07-20 15:49:39,2021-07-20 15:49:39
686UAB4E,journalArticle,2016,"Nasibov, E.; Kantarci Savaş, S.; Vahaplar, A.; Kinay, A.Ö.",A survey on geographic classification of virgin olive oil with using T-operators in Fuzzy Decision Tree Approach,"Olive oil is a crucial agricultural food product from past to present. The quality control of this product is too difficult. The geographic classification of it has an importance for the countries in order to provide the traceability. This paper aims to present a classification system for the geographic classification of virgin olive oil based on chemical parameters which contain uncertainty. Proposed system constructs the rules by using fuzzy decision tree algorithm. This algorithm builds rules by using ID3 algorithm with fuzzy entropy on the fuzzified data. The reasoning procedure based on rule based classification is handled with different T-operators. Fuzzy c-means algorithm is used in order to fuzzify the olive oil data set. The cluster numbers of each variable are decided according to partition coefficient validity criteria. The model is examined by using different decision tree approaches (C4.5 and standard version Fuzzy ID3 algorithm). The quality of proposed FID3 reasoning method with nine different T-operators is analyzed by using accuracy rates handled with 20 threshold values. Also, the conclusions are supported by statistical analysis. Experimental results show that fuzzy reasoning method has a crucial manner for the geographic classification. The classification system can perform better performance via different parameters for parametric T-operators. © 2016 Elsevier B.V.",Chemometrics and Intelligent Laboratory Systems,10.1016/j.chemolab.2016.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964329814&doi=10.1016%2fj.chemolab.2016.04.004&partnerID=40&md5=00a1ca7a2da872bfc8dab3044d56facd,2016,2021-07-20 15:49:39,2021-07-20 15:49:39
48YXPSLM,journalArticle,2021,"Zhang, Y.; Haghighi, P.D.; Burstein, F.; Yao, L.; Cicuttini, F.",On-Device Lumbar-Pelvic Movement Detection Using Dual-IMU: A DNN-Based Approach,"Lumbar-pelvic movements (LPMs) are generally performed in the clinical setting to identify limitations in a range of movements. Continuous monitoring of these movements can provide real-time feedback to both patients and medical experts with the potential of identifying activities that may precipitate symptoms of low back pain (LBP) as well as improving therapy by providing a personalised approach. Recent advances in mobile computing technology and wearable sensors have paved the way for developing mobile physical activity monitoring applications with more advanced and complex algorithms, such as deep neural network (DNN) based models. However, there is a lack of prior studies that focus on real-time LPMs detection with multimodal sensory data. Meanwhile, most research in the area of body movement detection do not consider the potential transition logic of the constituent low-level body movements (e.g., LPMs) within their corresponding high-level physical activity. This information could significantly increase accuracy of detection results. Further, current studies mainly perform deep learning-based movement detection in the cloud (or a backend server) that could increase network bandwidth and response time. To address these limitations, this paper proposes a novel LPMs detection approach using an enhanced and adapted hybrid DNN model, which includes a convolutional neural network followed by a long-short term memory recurrent network (CNN-LSTM), and performs detection locally on the mobile device. The results of a comparative evaluation of the proposed model and baseline models are described. We also introduce a set of domain-specific pre-defined rules, based on the transition logic information, to reconstruct the detection outputs to further improve the detection accuracy. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3074755,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104663664&doi=10.1109%2fACCESS.2021.3074755&partnerID=40&md5=92ddfb5e4f9f65160bf3b396343c947d,2021,2021-07-20 15:49:40,2021-07-20 15:49:40
45FQ4Z49,journalArticle,2021,"Wang, Z.; Gu, T.; Zhu, Y.; Li, D.; Yang, H.; Du, W.",FLDNet: Frame Level Distilling Neural Network for EEG Emotion Recognition,"Based on the current researches of EEG emotion recognition, there are some limitations, such as hand-engineered features, redundant and meaningless signal frames and the loss of frame-to-frame correlation. In this paper, a novel deep learning framework is proposed, named Frame Level Distilling Neural Network (FLDNet), to learn distilled features from correlation of different frames. A layer named frame gate is designed to integrate weighted semantic information on multiple frames for removing redundant and meaningless signal frames. Triple-net structure is introduced to distill the learned features net by net for replacing the hand-engineered features with professional knowledge. To be specific, one neural network will be normally trained for several epoches. Then, a second network of the same structure will be initialized again to learn the extracted features from the frame gate of the first neural network based on the output of the first net. Similarly, the third net improves the features based on the frame gate of the second network. To utilize the representation ability of the triple neural network, an ensemble layer is conducted to integrate the discriminative ability of proposed framework for final decision. Consequently, the proposed FLDNet provides an effective way to capture the correlation between different frames and automatically learn distilled high-level features for emotion recognition. The experiments are carried out, in a subject independent emotion recognition task, on public emotion datasets of DEAP and DREAMER benchmarks, which have demonstrated the effectiveness and robustness of the proposed FLDNet. IEEE",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2021.3049119,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099265133&doi=10.1109%2fJBHI.2021.3049119&partnerID=40&md5=2a1cd1a12c2a7b275f64394e7659bfcf,2021,2021-07-20 15:49:40,2021-07-20 15:49:40
YT9IGQ9K,journalArticle,2019,"Teso, S.; Masera, L.; Diligenti, M.; Passerini, A.",Combining learning and constraints for genome-wide protein annotation,"Background: The advent of high-throughput experimental techniques paved the way to genome-wide computational analysis and predictive annotation studies. When considering the joint annotation of a large set of related entities, like all proteins of a certain genome, many candidate annotations could be inconsistent, or very unlikely, given the existing knowledge. A sound predictive framework capable of accounting for this type of constraints in making predictions could substantially contribute to the quality of machine-generated annotations at a genomic scale. Results: We present Ocelot, a predictive pipeline which simultaneously addresses functional and interaction annotation of all proteins of a given genome. The system combines sequence-based predictors for functional and protein-protein interaction (PPI) prediction with a consistency layer enforcing (soft) constraints as fuzzy logic rules. The enforced rules represent the available prior knowledge about the classification task, including taxonomic constraints over each GO hierarchy (e.g. a protein labeled with a GO term should also be labeled with all ancestor terms) as well as rules combining interaction and function prediction. An extensive experimental evaluation on the Yeast genome shows that the integration of prior knowledge via rules substantially improves the quality of the predictions. The system largely outperforms GoFDR, the only high-ranking system at the last CAFA challenge with a readily available implementation, when GoFDR is given access to intra-genome information only (as Ocelot), and has comparable or better results (depending on the hierarchy and performance measure) when GoFDR is allowed to use information from other genomes. Our system also compares favorably to recent methods based on deep learning. © 2019 The Author(s).",BMC Bioinformatics,10.1186/s12859-019-2875-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067382080&doi=10.1186%2fs12859-019-2875-5&partnerID=40&md5=427e9ef2c29ab2a7a617de1d3f715dce,2019,2021-07-20 15:49:40,2021-07-20 15:49:40
CY7N9YEJ,journalArticle,2021,"Shahira, K.C.; Lijiya, A.",Towards Assisting the Visually Impaired: A Review on Techniques for Decoding the Visual Data from Chart Images,"The textual data of a document is supplemented by the graphical information in it. To make communication easier, they contain tables, charts and images. However, it excludes a section of our population - the visually impaired. With technological advancements, the blind can access the documents through text to speech software solutions. In this method, even images can be conveyed by reading out the figure captions. However, charts and other statistical comparisons which involve critical information are difficult to be 'read' out this way. Aim of this paper is to analyse various methods available to solve this vexatious issue. We survey the state-of-the-art works that do the exact opposite of graphing tools. In this paper, we explore the existing literature in understanding the graphs and extracting the visual encoding from them. We classify these approaches into modality-based approaches, conventional and deep-learning based methods. The survey also contains comparisons and analyses relevant study datasets. As an outcome of this survey, we observe that: (i) All existing works under each category need decoding in a variety of graphs. (ii) Among the approaches, deep learning performs remarkably well in localisation and classification. However, it needs further improvements in reasoning from chart images. (iii) Research works are still in progress to access data from vector images. Recreating data from the raster images has unresolved issues. Based on this study, the various applications of decoding the graphs, challenges and future possibilities are also discussed. This paper explores current works in the extraction of chart data, which seek to enable researchers in Human Computer Interaction to achieve human-level perception of visual data by machines. In this era of visual summarisation of data, the AI approaches can automate the underlying data extraction and hence provide the natural language descriptions to support visually disabled users. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3069205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103772656&doi=10.1109%2fACCESS.2021.3069205&partnerID=40&md5=d38e53edd7bfd0edfdcd0c30fedef0c5,2021,2021-07-20 15:49:40,2021-07-20 15:49:40
DKZRW5T8,journalArticle,2020,"Shaheen, N.; Raza, B.; Shahid, A.R.; Alquhayz, H.",A Novel Optimized Case-Based Reasoning Approach with K-Means Clustering and Genetic Algorithm for Predicting Multi-Class Workload Characterization in Autonomic Database and Data Warehouse System,"Data management systems are essential elements for any organization which is dealing with large volume of data now a days. Due to increase in data volume, and its complexities, it has become more challenging job for workload management system to maintain its performance. So, there is a need of such a system that can autonomically deal with such complexities with less or without human involvement. Performance of these systems can be improved by making the systems well-aware about the workload entering into the system. The workload of a prevalent typical database and data warehouse system can be characterized into three types that is Online Transaction Processing (OLTP), Decision Support Systems (DSS) and Mixed type of workload. Currently, autonomic characterization of workload into a binary class such as OLTP and DSS is being carried out as reported in the literature, however, characterizing the workload into three types that refers to a multi-class classification problem is relatively a more challenging task. In this study, we propose a novel optimized Case-based Reasoning (CBR) approach based on clustering for autonomically characterizing the workload into multi-class types before entering into the system. We implement four phases of CBR along with case-base generation and map it to the elements of autonomic MAPE-K model. In Retrieve phase, k-means clustering is used for enhancing retrieval efficiency and workload types predictions are made in Reuse phase. Genetic Algorithm is used in Revise and Adapt phase of CBR. Few autonomic self_∗ characteristics are incorporated to make it autonomic. We performed various experiments and results show that the proposed model outperforms in prediction as compared to existing approaches. We performed post-hoc test for the validation of results in comparison with other machine learning classifiers using the Friedman test that show that the proposed model stands out as the best classifier. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3000139,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086743059&doi=10.1109%2fACCESS.2020.3000139&partnerID=40&md5=4f5e3e858b74f059333e05898cbc8af3,2020,2021-07-20 15:49:40,2021-07-20 15:49:40
SF856I5W,journalArticle,2021,"Aranguren, I.; Valdivia, A.; Morales-Castañeda, B.; Oliva, D.; Abd Elaziz, M.; Perez-Cisneros, M.",Improving the segmentation of magnetic resonance brain images using the LSHADE optimization algorithm,"Segmentation is an essential preprocessing step in techniques for image analysis. The automatic segmentation of brain magnetic resonance imaging has been exhaustively investigated since the accurate use of this kind of methods permits the diagnosis and identification of several diseases. Thresholding is a straightforward and efficient technique for image segmentation. Nonetheless, thresholding based approaches tend to increase the computational cost based on the number of thresholds used for the segmentation. Therefore, metaheuristic algorithms are an important tool that helps to find the optimal values in multilevel thresholding. The adaptive differential evolution, based in numerous successes through history, with linear population size reduction (LSHADE) is a robust metaheuristic algorithm that efficiently solves numerical optimization problems. The main advantage of LSHADE is its capability to adapt its internal parameters according to prior knowledge acquired along the evolutionary process. Meanwhile, the continuous reduction of the population improves the exploitation process. This article presents a multilevel thresholding approach based on the LSHADE method for the segmentation of magnetic resonance brain imaging. The proposed method has been tested using three groups of reference images— the first group consists of grayscale standard benchmark images, the second group consists of magnetic resonance T2-weighted brain images, and the third group is formed by images of unhealthy brains affected by tumors. In turn, the performance of the intended approach was compared with distinct metaheuristic algorithms and machine learning methods. The statistically verified results demonstrate that the suggested approach improves consistency and segmentation quality. © 2020 Elsevier Ltd",Biomedical Signal Processing and Control,10.1016/j.bspc.2020.102259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096239201&doi=10.1016%2fj.bspc.2020.102259&partnerID=40&md5=71f2db17e1af92a0b60cab7b0ad261bc,2021,2021-07-20 15:49:40,2021-07-20 15:49:40
MPNGQFIU,journalArticle,2021,"Ferdaus, M.M.; Chakrabortty, R.K.; Ryan, M.J.",Multiobjective Automated Type-2 Parsimonious Learning Machine to Forecast Time-Varying Stock Indices Online,"Real-time forecasting of the financial time-series data is challenging for many machine learning (ML) algorithms. First, many ML models operate offline, where they need a batch of data, which may not be available during training. Besides, due to a fixed architecture of the majority of the offline-based ML models, they suffer to deal with the uncertain nature of financial time-series data. In contrast, online learning mode evolving-structured ML models could be promising for financial time-series forecasting. For real-time deployment of such models, low memory demand is a must. Besides, the model's explainability plays a crucial role in forecasting financial time-series. Considering all the requirements, a rule-based autonomous neuro-fuzzy learning algorithm called the parsimonious learning machine (PALM) is proposed here to forecast time-varying stock indices. To provide efficient automation of the proposed algorithm by maintaining the model explainability in terms of limited number linguistic IF-THEN rules, two popular multiobjective evolutionary algorithms (MEAs), such as a real-coded genetic algorithm (GA) and a self-adaptive differential evolution (DE) algorithm are utilized here. In addition, fuzzy type-2 variants of PALMs' are considered here due to better uncertainty handling capacity than their type-1 counterparts. To evaluate the proposed algorithm's performance, the closing stock price of fifteen (15) different stock market indices are predicted here. From the results, it is observed that the MEA-based PALMs are performing better than the state-of-the-art benchmark online ML models and providing a rule-based explainable model to the end-user. IEEE","IEEE Transactions on Systems, Man, and Cybernetics: Systems",10.1109/TSMC.2021.3061389,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102258706&doi=10.1109%2fTSMC.2021.3061389&partnerID=40&md5=2bb653d4f5f31b72ab61ac3239b556b8,2021,2021-07-20 15:49:40,2021-07-20 15:49:40
3HH5PU2G,journalArticle,2019,"Alam, M.M.; Tehranipoor, M.; Forte, D.",Recycled FPGA Detection Using Exhaustive LUT Path Delay Characterization and Voltage Scaling,"Field-programmable gate arrays (FPGAs) have been extensively used because of their lower nonrecurring engineering and design costs, instant availability and reduced visibility of failure, high performance, and power benefits. Reports indicate that previously used or recycled FPGAs are infiltrating the electronics' supply chain and making the security and reliability of the critical systems and networks vulnerable. Current recycled integrated circuit (IC) detection procedures include parametric, functional, and burn-in tests that require golden or reference data. Besides, they are time consuming, require expensive equipment, and do not focus on FPGAs. In this article, we propose two recycled FPGA detection methods based on supervised and unsupervised machine learning algorithms. We develop a sophisticated ring oscillator (RO) design to exploit the degradation of lookup tables (LUTs) and use them in the proposed methods. In the supervised method, a one-class classifier is trained with RO frequencies, kurtosis, and skewness data obtained from unused FPGAs, which differentiates unused and aged FPGAs. The unsupervised method uses $k$-means clustering and Silhouette value analysis to detect suspect recycled components with very little (if any) golden information. In addition, we introduce a voltage scaling-assisted RO frequency measurement technique that improves the classification. The proposed methods are examined for Spartan-3A and Spartan-6 FPGAs, and the result shows that both methods are effective in detecting recycled FPGAs, which experience accelerated aging for at least 12 h equivalent to 70 days in real-time age. © 1993-2012 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2019.2933278,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075630825&doi=10.1109%2fTVLSI.2019.2933278&partnerID=40&md5=118362adb9ccf4b299aaf021f695c188,2019,2021-07-20 15:49:40,2021-07-20 15:49:40
GTCSSYCY,journalArticle,2018,"Xue, J.; Zhou, S.; Liu, Q.; Liu, X.; Yin, J.","Financial time series prediction using ℓ2,1RF-ELM","Financial time series forecasting is a complicated task because the behavior of investors can be influenced by lots of tiny and unpredictable factors. In this paper, in order to maximize the return of capital and manage liquidity risk effectively, an ℓ2,1-norm and Random Fourier Mapping based Extreme Learning Machine(ℓ2,1 RF-ELM) is applied to the problem of financial time series prediction. The advantages of ELM in efficiency and generalization performance over traditional fuzzy neural network(FNN) algorithms have been demonstrated on a wide range of problems from different fields, thanks to the integration of ℓ2,1-norm, the ℓ2,1 RF-ELM is able to automatically prune the irrelevant and redundant hidden neurons to form a more discriminative and compact hidden layer. The performance of the ℓ2,1RF-ELM is compared with other hidden layer enforcement algorithms, two long-term time series data sets, including TianChi and BCS, are used for this comparison. The performance of the ℓ2,1RF-ELM was comparable to those of other widely used machine learning techniques like support vector machines (SVM), artificial neural networks (ANN) and other popular ELM method. The experiments demonstrate favorable prediction results of the ℓ2,1RF-ELM in terms of annualized return, prediction error and running time. In addition, we also find that the underlying rules of the correlation between cash inflow and outflow that can help us improve accuracy, which is valuable for financial institutions to predict the trend of liquidity. © 2017 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2017.04.076,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029685703&doi=10.1016%2fj.neucom.2017.04.076&partnerID=40&md5=29754ee9d7708cf29955c4dc85c7e32f,2018,2021-07-20 15:49:40,2021-07-20 15:49:40
SIA8FGPH,journalArticle,2020,"Brito da Silva, L.E.; Elnabarawy, I.; Wunsch, II, D.C.","Distributed dual vigilance fuzzy adaptive resonance theory learns online, retrieves arbitrarily-shaped clusters, and mitigates order dependence","This paper presents a novel adaptive resonance theory (ART)-based modular architecture for unsupervised learning, namely the distributed dual vigilance fuzzy ART (DDVFA). DDVFA consists of a global ART system whose nodes are local fuzzy ART modules. It is equipped with distributed higher-order activation and match functions and a dual vigilance mechanism. Together, these allow DDVFA to perform unsupervised modularization, create multi-prototype cluster representations, retrieve arbitrarily-shaped clusters, and reduce category proliferation. Another important contribution is the reduction of order-dependence, an issue that affects any agglomerative clustering method. This paper demonstrates two approaches for mitigating order-dependence: pre-processing using visual assessment of cluster tendency (VAT) or post-processing using a novel Merge ART module. The former is suitable for batch processing, whereas the latter also works for online learning. Experimental results in online mode carried out on 30 benchmark data sets show that DDVFA cascaded with Merge ART statistically outperformed the best other ART-based systems when samples were randomly presented. Conversely, they were found to be statistically equivalent in offline mode when samples were pre-processed using VAT. Remarkably, performance comparisons to non-ART-based clustering algorithms show that DDVFA (which learns incrementally) was also statistically equivalent to the non-incremental (offline) methods of density-based spatial clustering of applications with noise (DBSCAN), single linkage hierarchical agglomerative clustering (SL-HAC), and k-means, while retaining the appealing properties of ART. Links to the source code and data are provided. Considering the algorithm's simplicity, online learning capability, and performance, it is an ideal choice for many agglomerative clustering applications. © 2019 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2019.08.033,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072740654&doi=10.1016%2fj.neunet.2019.08.033&partnerID=40&md5=417757f3dc0d52bc37de089a96aa4a5d,2020,2021-07-20 15:49:40,2021-07-20 15:49:40
U8FBR47F,journalArticle,2021,"Bertsimas, D.; Stellato, B.",The voice of optimization,"We introduce the idea that using optimal classification trees (OCTs) and optimal classification trees with-hyperplanes (OCT-Hs), interpretable machine learning algorithms developed by Bertsimas and Dunn (Mach Learn 106(7):1039–1082, 2017), we are able to obtain insight on the strategy behind the optimal solution in continuous and mixed-integer convex optimization problem as a function of key parameters that affect the problem. In this way, optimization is not a black box anymore. Instead, we redefine optimization as a multiclass classification problem where the predictor gives insights on the logic behind the optimal solution. In other words, OCTs and OCT-Hs give optimization a voice. We show on several realistic examples that the accuracy behind our method is in the 90–100% range, while even when the predictions are not correct, the degree of suboptimality or infeasibility is very low. We compare optimal strategy predictions of OCTs and OCT-Hs and feedforward neural networks (NNs) and conclude that the performance of OCT-Hs and NNs is comparable. OCTs are somewhat weaker but often competitive. Therefore, our approach provides a novel insightful understanding of optimal strategies to solve a broad class of continuous and mixed-integer optimization problems. © 2020, The Author(s), under exclusive licence to Springer Science+Business Media LLC, part of Springer Nature.",Machine Learning,10.1007/s10994-020-05893-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088087673&doi=10.1007%2fs10994-020-05893-5&partnerID=40&md5=00a98c8308682b3b19e6aac17541e288,2021,2021-07-20 15:49:40,2021-07-20 15:49:40
XVUJBUP5,journalArticle,2021,"Zhang, L.; Shi, Y.; Chang, Y.-C.; Lin, C.-T.",Hierarchical Fuzzy Neural Networks with Privacy Preservation for Heterogeneous Big Data,"Heterogeneous big data poses many challenges in machine learning. Its enormous scale, high dimensionality, and inherent uncertainty make almost every aspect of machine learning difficult, from providing enough processing power to maintaining model accuracy to protecting privacy. However, perhaps the most imposing problem is that big data is often interspersed with sensitive personal data. Hence, we propose a privacy-preserving hierarchical fuzzy neural network to address these technical challenges while also alleviating privacy concerns. The network is trained with a two-stage optimization algorithm, and the parameters at low levels of the hierarchy are learned with a scheme based on the well-known alternating direction method of multipliers, which does not reveal local data to other agents. Coordination at high levels of the hierarchy is handled by the alternating optimization method, which converges very quickly. The entire training procedure is scalable, fast, and does not suffer from gradient vanishing problems like the methods based on backpropagation. Comprehensive simulations conducted on both regression and classification tasks demonstrate the effectiveness of the proposed model. Our code is available online.1 © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2020.3021713,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098843078&doi=10.1109%2fTFUZZ.2020.3021713&partnerID=40&md5=cb20c08733fe940098aa9f0c1d33089e,2021,2021-07-20 15:49:40,2021-07-20 15:49:40
992T4VZ9,journalArticle,2019,"Singh, V.; Verma, N.K.; Cui, Y.",Type-2 Fuzzy PCA Approach in Extracting Salient Features for Molecular Cancer Diagnostics and Prognostics,"Machine learning is becoming a powerful tool for cancer diagnosis and prognosis based on classification using high dimensional molecular data. However, extracting classification features from high-dimensional datasets remains a challenging problem. Principal component analysis (PCA) is a widely used method for dimensionality reduction. However, it is well-known that PCA and most PCA-based feature extraction methods are sensitive to noise, which may affect the accuracy of the subsequent classification. To address this problem, here we have proposed a robust fuzzy principal component analysis (PCA) with interval type-2 (IT-2) fuzzy membership functions for feature extraction. We have tested the performance of three widely used classifiers using the features extracted by proposed approaches and other feature extraction methods - PCA-based feature extraction methods (i.e. conventional PCA and fuzzy PCA), linear discriminant analysis (LDA), and support vector machine recursive feature elimination (SVM-RFE). The proposed feature extraction approaches showed better performance on cancer transcriptome and proteome datasets. © 2002-2011 IEEE.",IEEE Transactions on Nanobioscience,10.1109/TNB.2019.2917814,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068395390&doi=10.1109%2fTNB.2019.2917814&partnerID=40&md5=d6baf8a4728f0183f823b688d3ba49ae,2019,2021-07-20 15:49:40,2021-07-20 15:49:40
V3ABHKF4,journalArticle,2018,"Ward, L.; Dunn, A.; Faghaninia, A.; Zimmermann, N.E.R.; Bajaj, S.; Wang, Q.; Montoya, J.; Chen, J.; Bystrom, K.; Dylla, M.; Chard, K.; Asta, M.; Persson, K.A.; Snyder, G.J.; Foster, I.; Jain, A.",Matminer: An open source toolkit for materials data mining,"As materials data sets grow in size and scope, the role of data mining and statistical learning methods to analyze these materials data sets and build predictive models is becoming more important. This manuscript introduces matminer, an open-source, Python-based software platform to facilitate data-driven methods of analyzing and predicting materials properties. Matminer provides modules for retrieving large data sets from external databases such as the Materials Project, Citrination, Materials Data Facility, and Materials Platform for Data Science. It also provides implementations for an extensive library of feature extraction routines developed by the materials community, with 47 featurization classes that can generate thousands of individual descriptors and combine them into mathematical functions. Finally, matminer provides a visualization module for producing interactive, shareable plots. These functions are designed in a way that integrates closely with machine learning and data analysis packages already developed and in use by the Python data science community. We explain the structure and logic of matminer, provide a description of its various modules, and showcase several examples of how matminer can be used to collect data, reproduce data mining studies reported in the literature, and test new methodologies. © 2018 Elsevier B.V.",Computational Materials Science,10.1016/j.commatsci.2018.05.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047245440&doi=10.1016%2fj.commatsci.2018.05.018&partnerID=40&md5=aa8399f66db1b83c76651f534d11a897,2018,2021-07-20 15:49:40,2021-07-20 15:49:40
SJVHXAYN,journalArticle,2018,"Pham, C.; Nguyen, L.A.T.; Tran, N.H.; Huh, E.-N.; Hong, C.S.",Phishing-Aware: A Neuro-Fuzzy Approach for Anti-Phishing on Fog Networks,"Phishing detection is recognized as a criminal issue of Internet security. By deploying a gateway anti-phishing in the networks, these current hardware-based approaches provide an additional layer of defense against phishing attacks. However, such hardware devices are expensive and inefficient in operation due to the diversity of phishing attacks. With promising technologies of virtualization in fog networks, an anti-phishing gateway can be implemented as software at the edge of the network and embedded robust machine learning techniques for phishing detection. In this paper, we use uniform resource locator features and Web traffic features to detect phishing websites based on a designed neuro-fuzzy framework (dubbed Fi-NFN). Based on the new approach, fog computing as encouraged by Cisco, we design an anti-phishing model to transparently monitor and protect fog users from phishing attacks. The experiment results of our proposed approach, based on a large-scale dataset collected from real phishing cases, have shown that our system can effectively prevent phishing attacks and improve the security of the network. © 2004-2012 IEEE.",IEEE Transactions on Network and Service Management,10.1109/TNSM.2018.2831197,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046362949&doi=10.1109%2fTNSM.2018.2831197&partnerID=40&md5=99c03d4387f6c62139fc544997ed5184,2018,2021-07-20 15:49:40,2021-07-20 15:49:40
QMBMFN7E,journalArticle,2018,"Tang, J.; Liu, F.; Zhang, W.; Ke, R.; Zou, Y.",Lane-changes prediction based on adaptive fuzzy neural network,"Lane changing maneuver is one of the most important driving behaviors. Unreasonable lane changes can cause serious collisions and consequent traffic delays. High precision prediction of lane changing intent is helpful for improving driving safety. In this study, by fusing information from vehicle sensors, a lane changing predictor based on Adaptive Fuzzy Neural Network (AFFN) is proposed to predict steering angles. The prediction model includes two parts: fuzzy neural network based on Takagi–Sugeno fuzzy inference, in which an improved Least Squares Estimator (LSE) is adopt to optimize parameters; adaptive learning algorithm to update membership functions and rule base. Experiments are conducted in the driving simulator under scenarios with different speed levels of lead vehicle: 60 km/h, 80 km/h and 100 km/h. Prediction results show that the proposed method is able to accurately follow steering angle patterns. Furthermore, comparison of prediction performance with several machine learning methods further verifies the learning ability of the AFNN. Finally, a sensibility analysis indicates heading angles and acceleration of vehicle are also important factors for predicting lane changing behavior. © 2017 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2017.09.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029525192&doi=10.1016%2fj.eswa.2017.09.025&partnerID=40&md5=d1d6bceb4121e19ad32c5fa259cd4b0e,2018,2021-07-20 15:49:41,2021-07-20 15:49:41
V8RNE6NJ,journalArticle,2014,"Cassar, G.; Barnaghi, P.; Moessner, K.",Probabilistic matchmaking methods for automated service discovery,"Automated service discovery enables human users or software agents to form queries and to search and discover the services based on different requirements. This enables implementation of high-level functionalities such as service recommendation, composition, and provisioning. The current service search and discovery on the Web is mainly supported by text and keyword-based solutions which offer very limited semantic expressiveness to service developers and consumers. This paper presents a method using probabilistic machine-learning techniques to extract latent factors from semantically enriched service descriptions. The latent factors are used to construct a model to represent different types of service descriptions in a vector form. With this transformation, heterogeneous service descriptions can be represented, discovered, and compared on the same homogeneous plane. The proposed solution is scalable to large service datasets and provides an efficient mechanism that enables publishing and adding new services to the registry and representing them using latent factors after deployment of the system. We have evaluated our solution against logic-based and keyword-based service search and discovery solutions. The results show that the proposed method performs better than other solutions in terms of precision and normalized discounted cumulative gain values. © 2013 IEEE.",IEEE Transactions on Services Computing,10.1109/TSC.2013.28,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919715542&doi=10.1109%2fTSC.2013.28&partnerID=40&md5=9e601c452910a403319778df9d267b43,2014,2021-07-20 15:49:41,2021-07-20 15:49:41
M69LKNRW,journalArticle,2013,"Chong, L.; Abbas, M.M.; Medina Flintsch, A.; Higgs, B.",A rule-based neural network approach to model driver naturalistic behavior in traffic,"This paper proposes a rule-based neural network model to simulate driver behavior in terms of longitudinal and lateral actions in two driving situations, namely car-following situation and safety critical events. A fuzzy rule based neural network is constructed to obtain driver individual driving rules from their vehicle trajectory data. A machine learning method reinforcement learning is used to train the neural network such that the neural network can mimic driving behavior of individual drivers. Vehicle actions by neural network are compared to actions from naturalistic data. Furthermore, this paper applies the proposed method to analyze the heterogeneities of driving behavior from different drivers' data.Driving data in the two driving situations are extracted from Naturalistic Truck Driving Study and Naturalistic Car Driving Study databases provided by the Virginia Tech Transportation Institute according to pre-defined criteria. Driving actions were recorded in instrumented vehicles that have been equipped with specialized sensing, processing, and recording equipment. © 2012 Elsevier Ltd.",Transportation Research Part C: Emerging Technologies,10.1016/j.trc.2012.09.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878140389&doi=10.1016%2fj.trc.2012.09.011&partnerID=40&md5=25c073aaea1e5c34dc172da2ede452e2,2013,2021-07-20 15:49:41,2021-07-20 15:49:41
NCF2I89A,journalArticle,2020,"Lee, R.S.T.",Chaotic Type-2 Transient-Fuzzy Deep Neuro-Oscillatory Network (CT2TFDNN) for Worldwide Financial Prediction,"Over the years, financial engineering ranging from the study of financial signals to the modeling of financial prediction is one of the most exciting topics for both academia and financial community. With the flourishing AI technology in the past 20 years, various hybrid intelligent financial prediction systems with the integration of neural networks, chaos theory, fuzzy logic, and genetic algorithms have been proposed. An interval type-2 fuzzy logic system (IT2FLS) with its remarkable capability for the modeling of highly uncertain events and attributes provides a perfect tool to interpret various financial phenomena and patterns. In this paper, the author proposes a chaotic type-2 transient-fuzzy deep neuro-oscillatory network with retrograde signaling (CT2TFDNN) for worldwide financial prediction. With the extension of author's original work on Lee oscillator- A chaotic discrete-time neural oscillator with profound transient-chaotic property-CT2TFDNN provides: Effective modeling of an IT2FLS with a chaotic transient-fuzzy membership function; and effective time-series network training and prediction using a chaotic deep neuro-oscillatory network with retrograde signaling. CT2TFDNN not only provides a fast chaotic fuzzy-neuro deep learning and forecast solution, but also successfully resolves the massive data overtraining and deadlock problems, which are usually imposed by traditional recurrent neural networks using classical sigmoid-based activation functions. From the implementation perspective, CT2TFDNN is integrated with 2048 trading-day time-series financial data and top-10 major financial signals as fuzzy financial signals for the real-time prediction of 129 worldwide financial products that consists of: Nine major cryptocurrencies, 84 worldwide forex, 19 major commodities, and 17 worldwide financial indices. © 2019 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2019.2914642,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075248510&doi=10.1109%2fTFUZZ.2019.2914642&partnerID=40&md5=a36d63a882f669d7e5608ba40d853e4b,2020,2021-07-20 15:49:41,2021-07-20 15:49:41
W6RDF6L2,journalArticle,2020,"Dias, S.B.; Dolianiti, F.S.; Hadjileontiadou, S.J.; Diniz, J.A.; Hadjileontiadis, L.J.",On modeling the quality of concept mapping toward more intelligent online learning feedback: a fuzzy logic-based approach,"A new model, namely fuzzy inference system (FIS) concept mapping (FISCMAP), is proposed here that explores the fuzzy logic constructs within a computer-based concept mapping (CM) environment. FISCMAP involves modeling techniques as a vehicle to improve the intelligence of an online learning feedback environment that could promote personalization and adaptation to the student’s online educational needs, thus, fighting info-exclusion. From this perspective, the CMapTools is used as the online environment that captures the students’ actions and choices during the CM construction. Eight CMapTools measurements are considered to form inputs to a five-level FIS, equipped with 115 expert’s fuzzy rules. The CMapTools data were drawn from a blended (b)-learning course offered by a Greek Higher Education Institution, involving 20 Master’s students. Experimental results have shown that the proposed FISCMAP scheme, when used for the evaluation of users’ Quality of Concept Map (QoCM) via constructive CM variables (metrics), can provide intelligent descriptors regarding the students’ online CM. Furthermore, the FISCMAP’s dynamic analysis of QoCM and identification of the students’ transitional step behavior, during the development of the CM, provide further insight in the CM building strategies they adopt, forming constructive feedback. The latter reinforces the students’ ability to reflect on and analyze material in order to form reasoned judgments, clearly contributing to their critical thinking and deeper learning. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Universal Access in the Information Society,10.1007/s10209-019-00656-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067829539&doi=10.1007%2fs10209-019-00656-z&partnerID=40&md5=3f699ef76e470d9d3510df450a51f8f6,2020,2021-07-20 15:49:41,2021-07-20 15:49:41
ARR4ESKL,journalArticle,2018,"Pazhoumand-Dar, H.",Fuzzy association rule mining for recognising daily activities using Kinect sensors and a single power meter,"The recognition of activities of daily living (ADLs) by home monitoring systems can be helpful in order to objectively assess the health-related living behaviour and functional ability of older adults. Many ADLs involve human interactions with household electrical appliances (HEAs) such as toasters and hair dryers. Advances in sensor technology have prompted the development of intelligent algorithms to recognise ADLs via inferential information provided from the use of HEAs. The use of robust unsupervised machine learning techniques with inexpensive and retrofittable sensors is an ongoing focus in the ADL recognition research. This paper presents a novel unsupervised activity recognition method for elderly people living alone. This approach exploits a fuzzy-based association rule-mining algorithm to identify the home occupant’s interactions with HEAs using a power sensor, retrofitted at the house electricity panel, and a few Kinect sensors deployed at various locations within the home. A set of fuzzy rules is learned automatically from unlabelled sensor data to map the occupant’s locations during ADLs to the power signatures of HEAs. The fuzzy rules are then used to classify ADLs in new sensor data. Evaluations in real-world settings in this study demonstrated the potential of using Kinect sensors in conjunction with a power meter for the recognition of ADLs. This method was found to be significantly more accurate than just using power consumption data. In addition, the evaluation results confirmed that, owing to the use of fuzzy logic, the proposed method tolerates real-life variations in ADLs where the feature values in new sensor data differ slightly from those in the learning patterns. © 2017, Springer-Verlag GmbH Germany.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-017-0571-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049582530&doi=10.1007%2fs12652-017-0571-8&partnerID=40&md5=6a3b6a7d2ec7cd86129d7f2cd26c321b,2018,2021-07-20 15:49:41,2021-07-20 15:49:41
8R44NM2X,journalArticle,2016,"Burns, G.A.; Dasigi, P.; de Waard, A.; Hovy, E.H.",Automated detection of discourse segment and experimental types from the text of cancer pathway results sections,"Automated machine-reading biocuration systems typically use sentence-by-sentence information extraction to construct meaning representations for use by curators. This does not directly reflect the typical discourse structure used by scientists to construct an argument from the experimental data available within a article, and is therefore less likely to correspond to representations typically used in biomedical informatics systems (let alone to the mental models that scientists have). In this study, we develop Natural Language Processing methods to locate, extract, and classify the individual passages of text from articles' Results sections that refer to experimental data. In our domain of interest (molecular biology studies of cancer signal transduction pathways), individual articles may contain as many as 30 small-scale individual experiments describing a variety of findings, upon which authors base their overall research conclusions. Our system automatically classifies discourse segments in these texts into seven categories (fact, hypothesis, problem, goal, method, result, implication) with an F-score of 0.68. These segments describe the essential building blocks of scientific discourse to (i) provide context for each experiment, (ii) report experimental details and (iii) explain the data's meaning in context. We evaluate our system on text passages from articles that were curated in molecular biology databases (the Pathway Logic Datum repository, the Molecular Interaction MINT and INTACT databases) linking individual experiments in articles to the type of assay used (coprecipitation, phosphorylation, translocation etc.). We use supervised machine learning techniques on text passages containing unambiguous references to experiments to obtain baseline F1 scores of 0.59 for MINT, 0.71 for INTACT and 0.63 for Pathway Logic. Although preliminary, these results support the notion that targeting information extraction methods to experimental results could provide accurate, automated methods for biocuration. We also suggest the need for finer-grained curation of experimental methods used when constructing molecular biology databases. © The Author(s) 2016. Published by Oxford University Press.",Database : the journal of biological databases and curation,10.1093/database/baw122,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033390253&doi=10.1093%2fdatabase%2fbaw122&partnerID=40&md5=073c2a32ac565e2dc5b2e11e76b14116,2016,2021-07-20 15:49:41,2021-07-20 15:49:41
ZDB6PH89,journalArticle,2014,"Shamshirband, S.; Patel, A.; Anuar, N.B.; Kiah, M.L.M.; Abraham, A.",Cooperative game theoretic approach using fuzzy Q-learning for detecting and preventing intrusions in wireless sensor networks,"Owing to the distributed nature of denial-of-service attacks, it is tremendously challenging to detect such malicious behavior using traditional intrusion detection systems in Wireless Sensor Networks (WSNs). In the current paper, a game theoretic method is introduced, namely cooperative Game-based Fuzzy Q-learning (G-FQL). G-FQL adopts a combination of both the game theoretic approach and the fuzzy Q-learning algorithm in WSNs. It is a three-player strategy game consisting of sink nodes, a base station, and an attacker. The game performs at any time a victim node in the network receives a flooding packet as a DDoS attack beyond a specific alarm event threshold in WSN. The proposed model implements cooperative defense counter-attack scenarios for the sink node and the base station to operate as rational decision-maker players through a game theory strategy. In order to evaluate the performance of the proposed model, the Low Energy Adaptive Clustering Hierarchy (LEACH) was simulated using NS-2 simulator. The model is subsequently compared against other existing soft computing methods, such as fuzzy logic controller, Q-learning, and fuzzy Q-learning, in terms of detection accuracy, counter-defense, network lifetime and energy consumption, to demonstrate its efficiency and viability. The proposed models attack detection and defense accuracy yield a greater improvement than existing above-mentioned machine learning methods. In contrast to the Markovian game theoretic, the proposed model operates better in terms of successful defense rate. © 2014 Elsevier Ltd.",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2014.02.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84900419562&doi=10.1016%2fj.engappai.2014.02.001&partnerID=40&md5=94494609749bbd5915397a7a88465e07,2014,2021-07-20 15:49:41,2021-07-20 15:49:41
IG4KHQSM,journalArticle,2019,"Sun, X.; Li, J.; Wei, X.; Li, C.; Tao, J.",Emotional conversation generation based on a Bayesian deep neural network,"The field of conversation generation using neural networks has attracted increasing attention from researchers for several years. However, traditional neural language models tend to generate a generic reply with poor semantic logic and no emotion. This article proposes an emotional conversation generation model based on a Bayesian deep neural network that can generate replies with rich emotions, clear themes, and diverse sentences. The topic and emotional keywords of the replies are pregenerated by introducing commonsense knowledge in the model. The reply is divided into multiple clauses, and then a multidimensional generator based on the transformer mechanism proposed in this article is used to iteratively generate clauses from two dimensions: sentence granularity and sentence structure. Subjective and objective experiments prove that compared with existing models, the proposed model effectively improves the semantic logic and emotional accuracy of replies. This model also significantly enhances the diversity of replies, largely overcoming the shortcomings of traditional models that generate safe replies. © 2019 Association for Computing Machinery.",ACM Transactions on Information Systems,10.1145/3368960,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077738395&doi=10.1145%2f3368960&partnerID=40&md5=e84bcd5ec75b7b4322b5fd977232f89e,2019,2021-07-20 15:49:41,2021-07-20 15:49:41
7Y3T37LH,journalArticle,2018,"Cvetković, B.; Szeklicki, R.; Janko, V.; Lutomski, P.; Luštrek, M.",Real-time activity monitoring with a wristband and a smartphone,"Activity monitoring is a very important task in lifestyle and health domains where physical activity of a person plays an important role in further reasoning or for providing personalized recommendations. To make such services available to a broader population, one should use devices that most users already have, such as smartphones. Since trends show an increasing popularity of wrist-worn wearables we also consider a sensor-rich wristband as an optional device in this research. We present a real-time activity monitoring algorithm which utilizes data from the smartphone sensors, wristband sensors or their fusion for activity recognition and estimation of energy expenditure of the user. The algorithm detects which devices are present and uses an interval of walking for gravity detection and normalization of the orientation of the devices. The normalized data is afterwards used for the detection of the location of the smartphone on the body, which serves as a context for the selection of location-specific classification model for activity recognition. The recognized activity is finally used for the selection of one or multiple regression models for the estimation of the user's energy expenditure. To develop the machine-learning models, which can be deployed on the smartphone, we optimized the number and type of extracted features via automatic feature selection. We evaluated each step of the algorithm and each device configuration, and compared the human energy expenditure estimation results against the Bodymedia armband and Microsoft Band 2. We also evaluated the benefit of decision fusion where appropriate. The results show that we achieve a 87% ± 5% average accuracy for activity recognition and that we outperformed both competing devices in the estimation of human energy expenditure by achieving the mean absolute error of 0.6 ± 0.1 MET on average. © 2017 Elsevier B.V.",Information Fusion,10.1016/j.inffus.2017.05.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020177947&doi=10.1016%2fj.inffus.2017.05.004&partnerID=40&md5=e8c82df91b61fb3e5f74994284102312,2018,2021-07-20 15:49:41,2021-07-20 15:49:41
47J9ZIZM,journalArticle,2021,"Yang, M.; Li, C.; Shen, Y.; Wu, Q.; Zhao, Z.; Chen, X.",Hierarchical Human-Like Deep Neural Networks for Abstractive Text Summarization,"Developing an abstractive text summarization (ATS) system that is capable of generating concise, appropriate, and plausible summaries for the source documents is a long-term goal of artificial intelligence (AI). Recent advances in ATS are overwhelmingly contributed by deep learning techniques, which have taken the state-of-the-art of ATS to a new level. Despite the significant success of previous methods, generating high-quality and human-like abstractive summaries remains a challenge in practice. The human reading cognition, which is essential for reading comprehension and logical thinking, is still relatively new territory and underexplored in deep neural networks. In this article, we propose a novel Hierarchical Human-like deep neural network for ATS (HH-ATS), inspired by the process of how humans comprehend an article and write the corresponding summary. Specifically, HH-ATS is composed of three primary components (i.e., a knowledge-aware hierarchical attention module, a multitask learning module, and a dual discriminator generative adversarial network), which mimic the three stages of human reading cognition (i.e., rough reading, active reading, and postediting). Experimental results on two benchmark data sets (CNN/Daily Mail and Gigaword) demonstrate that HH-ATS consistently and substantially outperforms the compared methods. © 2012 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2020.3008037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100259165&doi=10.1109%2fTNNLS.2020.3008037&partnerID=40&md5=649e7637a269915317549fbafb407cca,2021,2021-07-20 15:49:41,2021-07-20 15:49:41
7U6ABE5D,journalArticle,2021,"Kenny, E.M.; Ford, C.; Quinn, M.; Keane, M.T.",Explaining black-box classifiers using post-hoc explanations-by-example: The effect of explanations and error-rates in XAI user studies,"In this paper, we describe a post-hoc explanation-by-example approach to eXplainable AI (XAI), where a black-box, deep learning system is explained by reference to a more transparent, proxy model (in this situation a case-based reasoner), based on a feature-weighting analysis of the former that is used to find explanatory cases from the latter (as one instance of the so-called Twin Systems approach). A novel method (COLE-HP) for extracting the feature-weights from black-box models is demonstrated for a convolutional neural network (CNN) applied to the MNIST dataset; in which extracted feature-weights are used to find explanatory, nearest-neighbours for test instances. Three user studies are reported examining people's judgements of right and wrong classifications made by this XAI twin-system, in the presence/absence of explanations-by-example and different error-rates (from 3-60%). The judgements gathered include item-level evaluations of both correctness and reasonableness, and system-level evaluations of trust, satisfaction, correctness, and reasonableness. Several proposals are made about the user's mental model in these tasks and how it is impacted by explanations at an item- and system-level. The wider lessons from this work for XAI and its user studies are reviewed. © 2021 The Authors",Artificial Intelligence,10.1016/j.artint.2021.103459,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100051479&doi=10.1016%2fj.artint.2021.103459&partnerID=40&md5=ccd4bea7bb8ddcfcd62ba469b508756a,2021,2021-07-20 15:49:41,2021-07-20 15:49:41
V474AMMN,journalArticle,2020,"Ouyang, X.; Karanam, S.; Wu, Z.; Chen, T.; Huo, J.; Zhou, X.S.; Wang, Q.; Cheng, J.",Learning Hierarchical Attention for Weakly-supervised Chest X-Ray Abnormality Localization and Diagnosis,"We consider the problem of abnormality localization for clinical applications. While deep learning has driven much recent progress in medical imaging, many clinical challenges are not fully addressed, limiting its broader usage. While recent methods report high diagnostic accuracies, physicians have concerns trusting these algorithm results for diagnostic decision-making purposes because of a general lack of algorithm decision reasoning and interpretability. One potential way to address this problem is to further train these models to localize abnormalities in addition to just classifying them. However, doing this accurately will require a large amount of disease localization annotations by clinical experts, a task that is prohibitively expensive to accomplish for most applications. In this work, we take a step towards addressing these issues by means of a new attention-driven weakly supervised algorithm comprising a hierarchical attention mining framework that unifies activation- and gradient-based visual attention in a holistic manner. Our key algorithmic innovations include the design of explicit ordinal attention constraints, enabling principled model training in a weakly-supervised fashion, while also facilitating the generation of visual-attention-driven model explanations by means of localization cues. On two large-scale chest X-ray datasets (NIH ChestX-ray14 and CheXpert), we demonstrate significant localization performance improvements over the current state of the art while also achieving competitive classification performance. IEEE",IEEE Transactions on Medical Imaging,10.1109/TMI.2020.3042773,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097940315&doi=10.1109%2fTMI.2020.3042773&partnerID=40&md5=d44b0c97020e693fd556dccb3b58f782,2020,2021-07-20 15:49:41,2021-07-20 15:49:41
XDMI4QR7,journalArticle,2015,"Gayathri, K.S.; Elias, S.; Ravindran, B.",Hierarchical activity recognition for dementia care using Markov Logic Network,"Statistics reveal that globally, the aging population in different stages of dementia are struggling to cope with daily activities and are progressively becoming dependent on care takers thereby making dementia care a challenging social problem. Healthcare systems in smart environments that aim to address this growing social need require the support of technology to recognize and respond in an ubiquitous manner. To incorporate an efficient activity recognition and abnormality detection system in smart environments, the routine activities of the occupant are modeled and any deviation from the activity model is recognized as abnormality. Recognition systems are generally designed using machine learning strategies and in this paper a novel hybrid, data and knowledge-driven approach is introduced. Markov Logic Network (MLN) used in our design is a suitable approach for activity recognition as it integrates common sense knowledge with a probabilistic model that augments the recognition ability of the system. The proposed activity recognition system for dementia care uses a hierarchical approach to detect abnormality in occupant behavior using MLN. The abnormality in the context of dementia care is identified in terms of factors associated with the activity such as objects, location, time and duration. The task of recognition is done in a hierarchical manner based on the priority of the factor that is associated with each layer. The motivation for designing a hierarchical approach was to enable each layer to commence its computation after inferring from the lower layers that the ongoing activity was normal with regard to the associated factors. This hierarchical feature enables quick decisions, as factors that require immediate attention are processed first at the lowest layer. Experimental results indicate that the hierarchical approach has higher accuracy in recognition and efficient response time when compared to the existing approaches. © 2014, Springer-Verlag London.",Personal and Ubiquitous Computing,10.1007/s00779-014-0827-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925492200&doi=10.1007%2fs00779-014-0827-7&partnerID=40&md5=9cf6a98a151fb8fb86a687bb1ab21c9b,2015,2021-07-20 15:49:41,2021-07-20 15:49:41
HY5N66PC,journalArticle,2019,"Natarajan, A.; Kumarasamy, S.",Efficient Segmentation of Brain Tumor Using FL-SNM with a Metaheuristic Approach to Optimization,"Nowadays, automatic tumor detection from brain images is extremely significant for many diagnostic as well as therapeutic purposes, due to the unpredictable shape and appearance of tumors. In medical image analysis, the automatic segmentation of tumors from brain using magnetic resonance imaging (MRI) data is the most critical issue. Existing research has some limitations, such as high processing time and lower accuracy, because of the time required for the training process. In this research, a new automatic segmentation process is introduced using machine learning and a swarm intelligence scheme. Here, a fuzzy logic with spiking neuron model (FL-SNM) is proposed for segmenting the brain tumor region in MR images. Initially, input images are preprocessed to remove Gaussian and Poisson noise using a modified Kuan filter (MKF). In the MKF, the optimal selection of the minimum MSE of image pixels is achieved using a random search algorithm (RSA), which improves the peak signal-to-noise ratio (PSNR). Then, the image is smoothed using an anisotropic diffusion filter (ADF) to reduce the over-filtering problem. Afterwards, to extract statistical texture features, Fisher’s linear-discriminant analysis (FLDA) is used. Finally, extracted features are transferred to the FL-SNM process and this scheme effectively segments the tumor region. In FL-SNM, the consequent parameters such as weight and bias play an important role in segmenting the region. Therefore, optimizing the weight parameter values using a chicken behavior-based swarm intelligence (CSI) algorithm, is proposed. The proposed (FL-SNM) scheme attained better performance in terms of high accuracy (94.87%), sensitivity (92.07%), specificity (99.34%), precision rate (89.36%), recall rate (88.39%), F-measure (95.06%), G-mean (95.63%), and DSC rate (91.2%), compared to existing convolutional neural networks (CNNs) and hierarchical self-organizing maps (HSOMs). © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Medical Systems,10.1007/s10916-018-1135-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059474316&doi=10.1007%2fs10916-018-1135-y&partnerID=40&md5=fa609ea7a3d6d497bb669ceae3eafbcb,2019,2021-07-20 15:49:42,2021-07-20 15:49:42
AE8R2LH9,journalArticle,2015,"Lukac, M.; Kameyama, M.",An algorithm selection based platform for image understanding using high-level symbolic feedback and machine learning,"Natural image processing and understanding encompasses hundreds of different algorithms. Each algorithm generates best results for a particular set of input features and configurations of the objects/regions in the input image (environment). To obtain the best possible result of processing in a reliable manner, we propose an algorithm selection approach that selects the best algorithm for a each input image. The proposed algorithm selection starts by first selecting an algorithm using low level features such as color intensity, histograms, spectral coefficients or so and a user given context if available. The resulting high-level image description is analyzed for logical inconsistencies (contradictions) and image regions that must be processed using a different algorithm are selected. The high-level description and the optional user-given context are used by a Bayesian Network to estimate the cause of the error in the processing. The same Bayesian Network also generates new candidate algorithm for each region containing the contradiction in an iterative manner. This iterative selection stops when the high-level inconsistencies are all resolved or no more different algorithms can be selected. We also show that when inconsistencies can be detected, our framework is able to improve high-level description when compared with single algorithms. In order for such complex and iterative processing being computationally tractable we also introduce a hardware platform based on reconfigurable VLSI that is well suited as the platform of the proposed approach. We show that the algorithm selected approach is ideally suited for either a hybrid type VLSI processor or for a Logic-In-Memory processing platform. © 2013, Springer-Verlag Berlin Heidelberg.",International Journal of Machine Learning and Cybernetics,10.1007/s13042-013-0197-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929077085&doi=10.1007%2fs13042-013-0197-x&partnerID=40&md5=daed99dba4cf86356d0c2ff6f6aa1559,2015,2021-07-20 15:49:42,2021-07-20 15:49:42
KIB8AQEF,journalArticle,2021,"Alattar, F.; Shaalan, K.",Using Artificial Intelligence to Understand What Causes Sentiment Changes on Social Media,"Sentiment Analysis tools allow decision-makers to monitor changes of opinions on social media towards entities, events, products, solutions, and services. These tools provide dashboards for tracking positive, negative, and neutral sentiments for platforms like Twitter where millions of users express their opinions on various topics. However, so far, these tools do not automatically extract reasons for sentiment variations, and that makes it difficult to conclude necessary actions by decision-makers. In this paper, we first compare performance of various Sentiment Analysis classifiers for short texts to select the top performer. Then we present a Filtered-LDA framework that significantly outperformed existing methods of interpreting sentiment variations on Twitter. The framework utilizes cascaded LDA Models with multiple settings of hyperparameters to capture candidate reasons that cause sentiment changes. Then it applies a filter to remove tweets that discuss old topics, followed by a Topic Model with a high Coherence Score to extract Emerging Topics that are interpretable by a human. Finally, a novel Twitter's sentiment reasoning dashboard is introduced to display the most representative tweet for each candidate reason. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3073657,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104630381&doi=10.1109%2fACCESS.2021.3073657&partnerID=40&md5=0893fd008c065578f6c01dcd3981cd4d,2021,2021-07-20 15:49:42,2021-07-20 15:49:42
VVV3WT9M,journalArticle,2020,"Dasgupta, I.; Guo, D.; Gershman, S.J.; Goodman, N.D.",Analyzing Machine-Learned Representations: A Natural Language Case Study,"As modern deep networks become more complex, and get closer to human-like capabilities in certain domains, the question arises as to how the representations and decision rules they learn compare to the ones in humans. In this work, we study representations of sentences in one such artificial system for natural language processing. We first present a diagnostic test dataset to examine the degree of abstract composable structure represented. Analyzing performance on these diagnostic tests indicates a lack of systematicity in representations and decision rules, and reveals a set of heuristic strategies. We then investigate the effect of training distribution on learning these heuristic strategies, and we study changes in these representations with various augmentations to the training set. Our results reveal parallels to the analogous representations in people. We find that these systems can learn abstract rules and generalize them to new contexts under certain circumstances—similar to human zero-shot reasoning. However, we also note some shortcomings in this generalization behavior—similar to human judgment errors like belief bias. Studying these parallels suggests new ways to understand psychological phenomena in humans as well as informs best strategies for building artificial intelligence with human-like language understanding. © 2020 Cognitive Science Society, Inc",Cognitive Science,10.1111/cogs.12925,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097923303&doi=10.1111%2fcogs.12925&partnerID=40&md5=dad2264ea4b2a90bf7be0d381181e603,2020,2021-07-20 15:49:42,2021-07-20 15:49:42
X566AA7M,journalArticle,2019,"Araujo Neto, J.P.; Pianto, D.M.; Ralha, C.G.",MULTS: A multi-cloud fault-tolerant architecture to manage transient servers in cloud computing,"The large-scale utilization of cloud computing resources has led to the emergence of cloud environment reliability as an important issue. In addition, cloud providers are negotiating unreliable virtual machines as a result of exploring unused resources offering them as transient servers - a lower price virtual machine service with resource revocations without user intervention. To increase the availability of transient servers, we propose a multi-cloud fault-tolerant architecture to provide a resilient environment using a scenario-based optimal checkpoint in a scheme to guarantee running processes with reduced user costs. The architecture combines a heuristic to extract information from a case-based reasoning and a statistical model to predict failure events helping to refine fault tolerance parameters. As a result, a cloud environment with better levels of reliability and reduced execution time is provided. Extensive simulations show high levels of accuracy reaching up to 92% survival prediction success rate and a gain of 74,58% of execution time reduction for long running applications. The results are promising, indicating that the proposed architecture can prevent revocation failures under realistic working conditions. © 2019 Elsevier B.V.",Journal of Systems Architecture,10.1016/j.sysarc.2019.101651,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074595559&doi=10.1016%2fj.sysarc.2019.101651&partnerID=40&md5=31dd5ba8a32923b49b4cc5977c45a9c3,2019,2021-07-20 15:49:42,2021-07-20 15:49:42
FVTHPJXZ,journalArticle,2018,"Zhu, H.; Paschalidis, I.C.; Hasselmo, M.E.",Neural circuits for learning context-dependent associations of stimuli,"The use of reinforcement learning combined with neural networks provides a powerful framework for solving certain tasks in engineering and cognitive science. Previous research shows that neural networks have the power to automatically extract features and learn hierarchical decision rules. In this work, we investigate reinforcement learning methods for performing a context-dependent association task using two kinds of neural network models (using continuous firing rate neurons), as well as a neural circuit gating model. The task allows examination of the ability of different models to extract hierarchical decision rules and generalize beyond the examples presented to the models in the training phase. We find that the simple neural circuit gating model, trained using response-based regulation of Hebbian associations, performs almost at the same level as a reinforcement learning algorithm combined with neural networks trained with more sophisticated back-propagation of error methods. A potential explanation is that hierarchical reasoning is the key to performance and the specific learning method is less important. © 2018 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2018.07.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052725288&doi=10.1016%2fj.neunet.2018.07.018&partnerID=40&md5=712bd41b26836157ecb216fc0308a760,2018,2021-07-20 15:49:42,2021-07-20 15:49:42
XMDAITPZ,journalArticle,2018,"Lee, H.-C.; Lee, S.-W.",Unified learning to enhance adaptive behavior of simulation objects,"Modeling and simulation are methods of validating new systems that are risky to be directly deployed in the real world. During the simulation, the simulation environment continuously changes and simulation objects correspondingly behave according to the changing situations. In general, modeling the behavior for all possible situations is extremely difficult when the rationale is unknown. Therefore, in order to adapt to the changing situation, it is important to recognize the rationale behind the behaviors of the simulation object. However, in many cases, even though the rationale is unknown or difficult to recognize, the simulation requires reasonable behaviors such as a commander’s decision in a war game simulation and a driver’s behavior in rush hours. In this study, we propose a new approach to determine the behavior of simulation objects under changing situations. The proposal is a unified learning approach that integrates two methods, data-driven and knowledge-driven approaches, which allow simulation objects to learn behavioral knowledge from experience as well as from domain experts performing the simulation and reuse verified knowledge. By combining both approaches, we supplement the shortcomings of one method with the strengths of the other. To verify our method, we apply the proposed approach to a military training simulation. © The Author(s) 2018.",Simulation,10.1177/0037549717753880,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043713096&doi=10.1177%2f0037549717753880&partnerID=40&md5=79142caf8118af6479f817f4f9895a89,2018,2021-07-20 15:49:42,2021-07-20 15:49:42
X362TRX2,journalArticle,2018,"Shen, Y.; Li, Y.; Deng, Y.; Zhang, J.; Yang, M.; Chen, J.; Si, S.; Lei, K.",Gastroenterology Ontology Construction Using Synonym Identification and Relation Extraction,"Ontology plays an increasingly important role in knowledge management and the semantic Web. However, ontology cannot perform well in realistic diagnosis reasoning unless it contains timely and accurate medical information and its individual items display all attributes of the categories they belong to. In this paper, we present a method that extracts synonyms along with concepts and their relationships for gastroenterology ontology construction. Specifically, we reuse the existing ontology as the basis for ontology completion. In addition, we conduct synonym identification through a combined application of global context features, local context features, and medical-specific features, and incorporate dependency information into deep neural networks for relation extraction. The extracted information is merged for ontology completion. Experimental results demonstrate that the proposed synonym identification and relation extraction method achieves the best performance compared with state-of-the-art methods and also builds a more complete ontology compared with existing gastroenterology disease ontologies. Our results are reproducible, and we will release the source code and ontology of this work after publication: https://github.com/shenyingpku/gastrointestinal-owl. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2862885,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051039880&doi=10.1109%2fACCESS.2018.2862885&partnerID=40&md5=3b55396920b13a189217dc11880ba181,2018,2021-07-20 15:49:42,2021-07-20 15:49:42
IR3AR3LJ,journalArticle,2016,"Suominen, A.; Toivanen, H.",Map of science with topic modeling: Comparison of unsupervised learning and human-assigned subject classification,"The delineation of coordinates is fundamental for the cartography of science, and accurate and credible classification of scientific knowledge presents a persistent challenge in this regard. We present a map of Finnish science based on unsupervised-learning classification, and discuss the advantages and disadvantages of this approach vis-à-vis those generated by human reasoning. We conclude that from theoretical and practical perspectives there exist several challenges for human reasoning-based classification frameworks of scientific knowledge, as they typically try to fit new-to-the-world knowledge into historical models of scientific knowledge, and cannot easily be deployed for new large-scale data sets. Automated classification schemes, in contrast, generate classification models only from the available text corpus, thereby identifying credibly novel bodies of knowledge. They also lend themselves to versatile large-scale data analysis, and enable a range of Big Data possibilities. However, we also argue that it is neither possible nor fruitful to declare one or another method a superior approach in terms of realism to classify scientific knowledge, and we believe that the merits of each approach are dependent on the practical objectives of analysis. © 2015 The Authors. Journal of the Association for Information Science and Technology published by Wiley Periodicals, Inc. on behalf of ASIS&T.",Journal of the Association for Information Science and Technology,10.1002/asi.23596,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987643700&doi=10.1002%2fasi.23596&partnerID=40&md5=e445658da2cfcb90f7d12ded0ce70a5a,2016,2021-07-20 15:49:42,2021-07-20 15:49:42
2XNEDU64,journalArticle,2016,"Yang, J.; Shi, Z.-K.; Wu, Z.-Y.",Towards automatic generation of as-built BIM: 3D building facade modeling and material recognition from images,"As-built building information model (BIM) is an urgent need of the architecture, engineering, construction and facilities management (AEC/FM) community. However, its creation procedure is still labor-intensive and far from maturity. Taking advantage of prevalence of digital cameras and the development of advanced computer vision technology, the paper proposes to reconstruct a building facade and recognize its surface materials from images taken from various points of view. These can serve as initial steps towards automatic generation of as-built BIM. Specifically, 3D point clouds are generated from multiple images using structure from motion method and then segmented into planar components, which are further recognized as different structural components through knowledge based reasoning. Windows are detected through a multilayered complementary strategy by combining detection results from every semantic layer. A novel machine learning based 3D material recognition strategy is presented. Binary classifiers are trained through support vector machines. Material type at a given 3D location is predicted by all its corresponding 2D feature points. Experimental results from three existing buildings validate the proposed system. © 2016, Institute of Automation, Chinese Academy of Sciences and Springer-Verlag Berlin Heidelberg.",International Journal of Automation and Computing,10.1007/s11633-016-0965-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84975256622&doi=10.1007%2fs11633-016-0965-7&partnerID=40&md5=c9ee1d23950b87ecba282207e596a72d,2016,2021-07-20 15:49:42,2021-07-20 15:49:42
H9L5LVIU,journalArticle,2014,"Poria, S.; Gelbukh, A.; Cambria, E.; Hussain, A.; Huang, G.-B.",EmoSenticSpace: A novel framework for affective common-sense reasoning,"Emotions play a key role in natural language understanding and sensemaking. Pure machine learning usually fails to recognize and interpret emotions in text accurately. The need for knowledge bases that give access to semantics and sentics (the conceptual and affective information) associated with natural language is growing exponentially in the context of big social data analysis. To this end, this paper proposes EmoSenticSpace, a new framework for affective common-sense reasoning that extends WordNet-Affect and SenticNet by providing both emotion labels and polarity scores for a large set of natural language concepts. The framework is built by means of fuzzy c-means clustering and support-vector-machine classification, and takes into account a number of similarity measures, including point-wise mutual information and emotional affinity. EmoSenticSpace was tested on three emotion-related natural language processing tasks, namely sentiment analysis, emotion recognition, and personality detection. In all cases, the proposed framework outperforms the state-of-the-art. In particular, the direct evaluation of EmoSenticSpace against psychological features provided in the benchmark ISEAR dataset shows a 92.15% agreement. © 2014 Elsevier B.V. All rights reserved.",Knowledge-Based Systems,10.1016/j.knosys.2014.06.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924596504&doi=10.1016%2fj.knosys.2014.06.011&partnerID=40&md5=585f7d103bc8c153315b79e1a70f1fa5,2014,2021-07-20 15:49:42,2021-07-20 15:49:42
2KHXWSJ8,journalArticle,2018,"Kroemer, O.; Leischnig, S.; Luettgen, S.; Peters, J.",A kernel-based approach to learning contact distributions for robot manipulation tasks,"Manipulation tasks often require robots to recognize interactions between objects. For example, a robot may need to determine if it has grasped an object properly or if one object is resting on another in a stable manner. These interactions usually depend on the contacts between the objects, with different distributions of contacts affording different interactions. In this paper, we address the problem of learning to recognize interactions between objects based on contact distributions. We present a kernel-based approach for representing the estimated contact distributions. The kernel can be used for various interactions, and it allows the robot to employ a variety of kernel methods from machine learning. The approach was evaluated on blind grasping, lifting, and stacking tasks. Using 30 training samples and the proposed kernel, the robot already achieved classification accuracies of 71.9, 85.93, and 97.5% for the blind grasping, lifting and stacking tasks respectively. The kernel was also used to cluster interactions using spectral clustering. The clustering method successfully differentiated between different types of interactions, including placing, inserting, and pushing. The contact points were extracted using tactile sensors or 3D point cloud models of the objects. The robot could construct small towers of assorted blocks using the classifier for the stacking task. © 2017, Springer Science+Business Media, LLC.",Autonomous Robots,10.1007/s10514-017-9651-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025440951&doi=10.1007%2fs10514-017-9651-z&partnerID=40&md5=70b39b14aa8f05ccc2b91b21d37ef196,2018,2021-07-20 15:49:42,2021-07-20 15:49:42
I353KYN3,journalArticle,2015,"Khanesar, M.A.; Kayacan, E.; Reyhanoglu, M.; Kaynak, O.",Feedback Error Learning Control of Magnetic Satellites Using Type-2 Fuzzy Neural Networks With Elliptic Membership Functions,"A novel type-2 fuzzy membership function (MF) in the form of an ellipse has recently been proposed in literature, the parameters of which that represent uncertainties are de-coupled from its parameters that determine the center and the support. This property has enabled the proposers to make an analytical comparison of the noise rejection capabilities of type-1 fuzzy logic systems with its type-2 counterparts. In this paper, a sliding mode control theory-based learning algorithm is proposed for an interval type-2 fuzzy logic system which benefits from elliptic type-2 fuzzy MFs. The learning is based on the feedback error learning method and not only the stability of the learning is proved but also the stability of the overall system is shown by adding an additional component to the control scheme to ensure robustness. In order to test the efficiency and efficacy of the proposed learning and the control algorithm, the trajectory tracking problem of a magnetic rigid spacecraft is studied. The simulations results show that the proposed control algorithm gives better performance results in terms of a smaller steady state error and a faster transient response as compared to conventional control algorithms. © 2013 IEEE.",IEEE Transactions on Cybernetics,10.1109/TCYB.2015.2388758,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925105202&doi=10.1109%2fTCYB.2015.2388758&partnerID=40&md5=b8ff49ec759498bcdbd61bbf719506a8,2015,2021-07-20 15:49:42,2021-07-20 15:49:42
9EIGTWBJ,journalArticle,2021,"Zhou, Y.",Image modal analysis in art design and image recognition using AI techniques,"Traditional art design models are inefficient and difficult to control the modal characteristics of images. Based on the actual needs of artistic design, this paper builds an intelligent model of artistic design image modal analysis based on machine learning technology and image recognition technology, and proposes a feature extraction method based on wavelet transform and fuzzy logic and an art image classification method based on rotating quaternion wavelet transform. Moreover, this paper calculates the activation intensity value corresponding to each fuzzy area in the fuzzy feature space and normalizes these activation intensity values to form the artistic image feature vector. In addition, this paper uses rotating quaternary wavelet transform to decompose the art image, and then calculate the energy and standard deviation of each decomposed sub-band coefficient. Finally, this paper uses support vector machines to recognize artistic images. The experimental research shows that the model constructed in this paper has certain effects. © 2021 - IOS Press. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-189526,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104367980&doi=10.3233%2fJIFS-189526&partnerID=40&md5=0a77561c85c21058d8d73486bd862295,2021,2021-07-20 15:49:42,2021-07-20 15:49:42
EGUVFWRJ,journalArticle,2020,"Neider, D.; Madhusudan, P.; Saha, S.; Garg, P.; Park, D.",A Learning-Based Approach to Synthesizing Invariants for Incomplete Verification Engines,"We propose a framework for synthesizing inductive invariants for incomplete verification engines, which soundly reduce logical problems in undecidable theories to decidable theories. Our framework is based on the counterexample guided inductive synthesis principle and allows verification engines to communicate non-provability information to guide invariant synthesis. We show precisely how the verification engine can compute such non-provability information and how to build effective learning algorithms when invariants are expressed as Boolean combinations of a fixed set of predicates. Moreover, we evaluate our framework in two verification settings, one in which verification engines need to handle quantified formulas and one in which verification engines have to reason about heap properties expressed in an expressive but undecidable separation logic. Our experiments show that our invariant synthesis framework based on non-provability information can both effectively synthesize inductive invariants and adequately strengthen contracts across a large suite of programs. This work is an extended version of a conference paper titled “Invariant Synthesis for Incomplete Verification Engines”. © 2020, The Author(s).",Journal of Automated Reasoning,10.1007/s10817-020-09570-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088013652&doi=10.1007%2fs10817-020-09570-z&partnerID=40&md5=e76dff3fac36586ae7acbaa94f573b72,2020,2021-07-20 15:49:42,2021-07-20 15:49:42
P4GNKQ8G,journalArticle,2018,"Brusey, J.; Hintea, D.; Gaura, E.; Beloe, N.",Reinforcement learning-based thermal comfort control for vehicle cabins,"Vehicle climate control systems aim to keep passengers thermally comfortable. However, current systems control temperature rather than thermal comfort and tend to be energy hungry, which is of particular concern when considering electric vehicles. This paper poses energy-efficient vehicle comfort control as a Markov Decision Process, which is then solved numerically using Sarsa(λ) and an empirically validated, single-zone, 1D thermal model of the cabin. The resulting controller was tested in simulation using 200 randomly selected scenarios and found to exceed the performance of bang-bang, proportional, simple fuzzy logic, and commercial controllers with 23%, 43%, 40%, 56% increase, respectively. Compared to the next best performing controller, energy consumption is reduced by 13% while the proportion of time spent thermally comfortable is increased by 23%. These results indicate that this is a viable approach that promises to translate into substantial comfort and energy improvements in the car. © 2017 Elsevier Ltd",Mechatronics,10.1016/j.mechatronics.2017.04.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018978690&doi=10.1016%2fj.mechatronics.2017.04.010&partnerID=40&md5=0ddf044d716ca8b4407559b44323b1f8,2018,2021-07-20 15:49:42,2021-07-20 15:49:42
QND4ZZTV,journalArticle,2013,"Gao, Y.; Tong, S.; Li, Y.",Adaptive fuzzy backstepping output feedback control for a class of uncertain stochastic nonlinear system in pure-feedback form,"This paper is concerned with the problem of adaptive fuzzy output feedback for a class of uncertain stochastic pure-feedback nonlinear systems with immeasurable states. With the help of fuzzy logic systems to approximate the unknown nonlinear functions, and a fuzzy state observer is designed to estimate the unmeasured states. By incorporating the filtered signals into the backstepping recursive design, a fuzzy adaptive output feedback control scheme is developed. It is proven that all the signals of the closed-loop system are bounded in probability, and also that the observer errors and the output of the system converge to a small neighborhood of the origin by appropriate choice of the design parameters. Simulation studies are included to illustrate the effectiveness of the proposed approach. © 2013 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2013.06.036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884209186&doi=10.1016%2fj.neucom.2013.06.036&partnerID=40&md5=fccdf95de6a816763b66a91e78251472,2013,2021-07-20 15:49:43,2021-07-20 15:49:43
3UGZAKJW,journalArticle,2013,"Cassidy, A.S.; Georgiou, J.; Andreou, A.G.","Design of silicon brains in the nano-CMOS era: Spiking neurons, learning synapses and neural architecture optimization","We present a design framework for neuromorphic architectures in the nano-CMOS era. Our approach to the design of spiking neurons and STDP learning circuits relies on parallel computational structures where neurons are abstracted as digital arithmetic logic units and communication processors. Using this approach, we have developed arrays of silicon neurons that scale to millions of neurons in a single state-of-the-art Field Programmable Gate Array (FPGA). We demonstrate the validity of the design methodology through the implementation of cortical development in a circuit of spiking neurons, STDP synapses, and neural architecture optimization. © 2013 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2013.05.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880823556&doi=10.1016%2fj.neunet.2013.05.011&partnerID=40&md5=6468de78ab7c2dbc1b460c956d85170f,2013,2021-07-20 15:49:43,2021-07-20 15:49:43
7XNSC7XQ,journalArticle,2018,"Zhang, S.; Stogin, W.; Alshurafa, N.",I sense overeating: Motif-based machine learning framework to detect overeating using wrist-worn sensing,"Obesity, caused primarily by overeating, is a preventable chronic disease yielding staggering healthcare costs. To detect overeating passively, a machine learning framework was designed to detect and accurately count the number of feeding gestures during an eating episode to characterize each eating episode with a feeding gesture count using a 6-axis inertial wrist-worn sensor. Moreover, detecting feeding gestures is useful to aid in end-of-day dietary recalls. It has been shown that feeding gesture count correlates with caloric intake; the more one eats, the more calories one is likely consuming. Recent research has shown promise in passively detecting feeding gestures, but this effort focuses on bridging detection of feeding gesture count and identifying overeating episodes. This paper presents results on three experiments: highly structured (participants pretending to eat), in-lab structured with confounding activities (participants eating while performing other scripted activities), and unstructured overeating (participants induced to overeat while watching television and eating their favorite foods). Our experiment successfully induced overeating in 50% of the participants, showing a correlation between feeding gesture count and caloric intake in unstructured eating (r=.79, p-value=.007). Results provide an approximate upper bound on feeding gesture classification using exact segmentation techniques, and show improvement when compared to prior sliding window techniques. Results also suggest the importance of stressing the challenge of accurate segmentation over identifying the accurate classification technique in detection of feeding gestures. Since participant-dependent models provide optimal results, a motif-based time-point fusion classification (MTFC) framework is proposed using spectral energy density, K-Spectral Centroid Clustering, symbolic aggregate approximation (SAX), a Random Forest classifier (trained on segmented motifs) and a time-point classifier fusion technique to show reliable classification of feeding gestures (75% F-measure), and a 94% accuracy of feeding gesture count in the unstructured eating experiment, resulting in a root mean square error of 2.9 feeding gestures. Mapping feeding gesture count to caloric intake, we obtain a rough estimate of whether participants overate while watching television. © 2017 Elsevier B.V.",Information Fusion,10.1016/j.inffus.2017.08.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034581732&doi=10.1016%2fj.inffus.2017.08.003&partnerID=40&md5=8e570065582a3efb6a122e1fe3ca15b9,2018,2021-07-20 15:49:43,2021-07-20 15:49:43
29SDDKS5,journalArticle,2017,"Ceron, J.D.; Lopez, D.M.; Ramirez, G.A.",A mobile system for sedentary behaviors classification based on accelerometer and location data,"Background Sedentary behaviors are associated to the development of noncommunicable diseases (NCD) such as cardiovascular diseases (CVD), type 2 diabetes, and cancer. Accelerometers and inclinometers have been used to estimate sedentary behaviors, however a major limitation is that these devices do not provide enough contextual information in order to recognize the specific sedentary behavior performed, e.g., sitting or lying watching TV, using the PC, sitting at work, driving, etc. Objective Propose and evaluate the precision of a mobile system for objectively measuring six sedentary behaviors using accelerometer and location data. Results The system is implemented as an Android Mobile App, which identifies individual's sedentary behaviors based on accelerometer data taken from the smartphone or a smartwatch, and symbolic location data obtained from Bluetooth Low Energy (BLE) beacons. The system infers sedentary behaviors by means of a supervised Machine Learning Classifier. The precision of the classification of five of the six studied sedentary behaviors exceeded 95% using accelerometer data from a smartwatch attached to the wrist and 98% using accelerometer data from a smartphone put into the pocket. Statistically significant improvement in the average precision of the classification due to the use of BLE beacons was found by comparing the precision of the classification using accelerometer data only, and BLE beacons localization technology. Conclusions The proposed system provides contextual information of specific sedentary behaviors by inferring with very high precision the physical location where the sedentary event occurs. Moreover, it was found that, when accelerometers are put in the user's pocket, instead of the wrist and, when symbolic location is inferred using BLE beacons; the precision in the classification is improved. In practice, the proposed system has the potential to contribute to the understanding of the context and determinants of sedentary behaviors, necessary for the implementation and monitoring of personalized noncommunicable diseases prevention programs, for instance, sending sedentary behavior alerts, or providing personalized recommendations on physical activity. The system could be used at work to promote active breaks and healthy habits. © 2017",Computers in Industry,10.1016/j.compind.2017.06.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021065666&doi=10.1016%2fj.compind.2017.06.005&partnerID=40&md5=4610a8c0c0ac57a46e47d24b419483e3,2017,2021-07-20 15:49:43,2021-07-20 15:49:43
VYPZ5GEV,journalArticle,2020,"Osorio, V.R.; Iyengar, R.; Yao, X.; Bhattachan, P.; Ragobar, A.; Dey, N.; Tripp, B.","37,000 human-planned robotic grasps with six degrees of freedom","Much recent work in grasp planning has focused on data-driven approaches, using deep learning to map from images to gripper configurations. However, this approach typically fails about once per ten attempts, limiting its practicality. We sought to better understand the degree to which such failures can be attributed to hardware versus control. To this end, we developed a naturalistic grasp demonstration system in which a gripper was fitted with a handle and moved by a human operator, while its trajectory was recorded with a motion tracker. The gripper's fingers were controlled with a joystick. We recorded roughly 37 K grasp demonstrations with this system. These grasps were almost always successful. In contrast with planar grasp planners that perform only top-down grasps by design, many of the human-planned grasps used a horizontal approach rather than a top-down approach. We analysed robustness of a subset of these human-planned grasps, and found that many were robust to gripper rotations of about π8 radians and translations of 3 cm (depending on the object). Consistent with past work, human operators tended to align the gripper aperture with objects' principal axes. We also tested robustness of grasps in which the gripper aperture was aligned exactly with the principal axes, and found that these heuristic grasps were even more robust than human-planned grasps. This suggests that humans used a partly symbolic grasp planning strategy, with somewhat imprecise control. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2020.2976295,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082000001&doi=10.1109%2fLRA.2020.2976295&partnerID=40&md5=5cc744e3410ce5edb9bb8d560604edaf,2020,2021-07-20 15:49:43,2021-07-20 15:49:43
MPCYHZ8U,journalArticle,2019,"Sharma, D.; Chandra, P.",A comparative analysis of soft computing techniques in software fault prediction model development,"In the process of software development, software fault prediction is a useful practice to ensure reliable and high quality software products. It plays a vital role in the process of software quality assurance. A high quality software product contains minimum number of faults and failures. Software fault prediction examines the vulnerability of software product towards faults. In this paper, a comparative analysis of various soft computing approaches in terms of the process of software fault prediction is considered. In addition, an analysis of various pros and cons of soft computing techniques in terms of software fault prediction process is also mentioned. The conclusive results show that the soft computing approach has the propensity to identify faults in the process of software development. © 2018, Bharati Vidyapeeth's Institute of Computer Applications and Management.",International Journal of Information Technology (Singapore),10.1007/s41870-018-0211-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091887728&doi=10.1007%2fs41870-018-0211-3&partnerID=40&md5=c3e700143c6609efae725ef57646bfdb,2019,2021-07-20 15:49:43,2021-07-20 15:49:43
4IV7C46M,journalArticle,2018,"Courtois, C.; Timmermans, E.",Cracking the tinder code: An experience sampling approach to the dynamics and impact of platform governing algorithms,"This article conceptualizes algorithmically-governed platforms as the outcomes of a structuration process involving three types of actors: platform owners/developers, platform users, and machine learning algorithms. This threefold conceptualization informs media effects research, which still struggles to incorporate algorithmic influence. It invokes insights into algorithmic governance from platform studies and (critical) studies in the political economy of online platforms. This approach illuminates platforms’ underlying technological and economic logics, which allows to construct hypotheses on how they appropriate algorithmic mechanisms, and how these mechanisms function. The present study tests the feasibility of experience sampling to test such hypotheses. The proposed methodology is applied to the case of mobile dating app Tinder. © 2018 International Communication Association.",Journal of Computer-Mediated Communication,10.1093/jcmc/zmx001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049003459&doi=10.1093%2fjcmc%2fzmx001&partnerID=40&md5=2d600ae489209b1008a66527a35af6fd,2018,2021-07-20 15:49:43,2021-07-20 15:49:43
RAJRQV6F,journalArticle,2017,"Rashmi, S.; Hanumanthappa, M.",Qualitative and quantitative study of syntactic structure: a grammar checker using part of speech tags,"One of the fascinating features of English language is its robust grammar construction and syntactic structure. Learning grammar is not difficult in the present era as there are many online tools available for grammar teaching. In spite of its abundance presence and relevance, when one takes a deep dive into finding the syntactic structure used for grammar checker, it is perhaps a complex paradigm. It is hence important to study the logic of defining the grammar rules. Therefore the objective of this paper is to describe the prototype of an efficient grammar checker and to design an interface to perform grammar check. The efficiency of the proposed algorithm is improved as compared to the existing methods. © 2017, Bharati Vidyapeeth's Institute of Computer Applications and Management.",International Journal of Information Technology (Singapore),10.1007/s41870-017-0016-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091859579&doi=10.1007%2fs41870-017-0016-9&partnerID=40&md5=68d8fcddab87d26f8d92b87e0b6bba93,2017,2021-07-20 15:49:43,2021-07-20 15:49:43
ICNUWGS5,journalArticle,2021,"Ni, Y.; Deng, Y.; Li, S.",PMBA: A Parallel MCMC Bayesian Computing Accelerator,"Bayesian computing, including sampling probability distributions, learning graphic model, and Bayesian reasoning, is a powerful class of machine learning algorithms with such wide applications as biologic computing, financial analysis, natural language processing, autonomous driving, and robotics. The central pattern of Bayesian computing is the Markov Chain Monte Carlo (MCMC) computing, which is compute-intensive and lacks explicit parallelism. In this work, we propose a parallel MCMC Bayesian computing accelerator (PMBA) architecture. Designed as a probabilistic computing platform with native support for efficient single-chain parallel Metropolis-Hastings based MCMC sampling, PMBA boosts the performance of probabilistic programs with a massive-parallelism microarchitecture. PMBA is equipped with on-chip random number generators as the built-in source of randomness. The sampling units of PMBA are designed for parallel random sampling through a customized SIMD pipeline supporting data synchronization every iteration. A respective computing framework supporting automatic parallelization and mapping of probabilistic programs is also developed. Evaluation results demonstrate that PMBA enables a 17-21 folds speedup over a TITAN X GPU on MCMC sampling workload. On probabilistic benchmarks, PMBA outperforms prior best solutions by factor of 3.6 to 10.3. An exemplar based visual category learning algorithm is implemented on PMBA to demonstrate its efficiency and effectiveness for complex statistical learning problems. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3076207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105078246&doi=10.1109%2fACCESS.2021.3076207&partnerID=40&md5=0e1aa91ebc3e51217f99a740d852535a,2021,2021-07-20 15:49:43,2021-07-20 15:49:43
6VBKWHLL,journalArticle,2019,"Xu, W.; Tan, Y.",Semi-supervised target-oriented sentiment classification,"Target-oriented aspect-based sentiment analysis (TABSA) is a sentiment classification task that requires performing fine-grained semantical reasoning about a given aspect. The amount of labeled data is usually insufficient for supervised learning because the manual annotation w.r.t. the aspects is both time-consuming and laborious. In this paper, we propose a novel semi-supervised method to derive and utilize the underlying sentiment of unlabeled samples via a deep generative model. This method assumes that when given the aspect, the sentence is generated by two stochastic variables, i.e., the context variable and the sentiment variable. By explicitly disentangling the representation into the context and sentiment, the meaning of sentiment variable can be kept clean during the training phase. An additional advantage is that the proposed method uses a standalone classifier, and as such, our system is able to integrate with various supervised models. In terms of the implementation, since capturing the conditional input is non-trivial for a sequential model, special structures are put forward and investigated. We conducted experiments on SemEval 2014 task 4 and the results indicate that our method effectively handles five kinds of advanced classifiers. The proposed method outperforms two general semi-supervised methods and achieves state-of-the-art performance on this benchmark. © 2019 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2019.01.059,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061439064&doi=10.1016%2fj.neucom.2019.01.059&partnerID=40&md5=d87a46b50196ead03cbf923a043f4f30,2019,2021-07-20 15:49:43,2021-07-20 15:49:43
YWSU5WXP,journalArticle,2021,"Costadopoulos, N.; Islam, M.Z.; Tien, D.",A knowledge discovery and visualisation method for unearthing emotional states from physiological data,"In this paper we propose a knowledge discovery and visualisation method for unearthing emotional states from physiological data typically available from wearable devices. In addition we investigate the viability of using a limited set of wearable sensors to extract decision tree rules which are representative of physiological changes taking place during emotional changes. Our method utilised a fusion of pre-processing and classification techniques using decision trees to discover logic rules relating to the valence and arousal emotional dimensions. This approach normalised the signal data in a manner that enabled accurate classification and generated logic rules for knowledge discovery. Furthermore, the use of three target classes for the emotional dimensions was effective at denoising the data and further enhancing classification and useful rule extraction. There are three key contributions in this work, firstly an exploration and validation of our knowledge discovery methodology, secondly successful extraction of high accuracy rules derived from physiological data and thirdly knowledge discovery and visualisation of relationships within-participant physiological data that can be inferred relating to emotions. Additionally, this work may be utilised in areas such as the medical sciences where interpretable rules are required for knowledge discovery. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",International Journal of Machine Learning and Cybernetics,10.1007/s13042-020-01205-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092597349&doi=10.1007%2fs13042-020-01205-4&partnerID=40&md5=61505607e95aabc45663996bac216ff5,2021,2021-07-20 15:49:43,2021-07-20 15:49:43
7N2EY3ZQ,journalArticle,2021,"Yang, Y.; Guan, J.; Huang, S.; Wan, W.; Xu, Y.; Liu, J.",End-to-End Rain Removal Network Based on Progressive Residual Detail Supplement,"Methods of rain removal based on deep learning have rapidly developed, and the image quality after rain removal is continuously improving. However, the results of most methods have some common problems, including a loss of details, a blurring of edges, and the existence of artifacts. To remove rain-related information more thoroughly and retain more edge details, this paper proposes an end-to-end rain removal network based on the progressive residual detail supplement (ERRN-PRDS) approach. The entire network structure is designed in an iterative manner to obtain higher-quality rain removal images from coarse to fine. In the network, a diamond residual block is constructed as the main module of iteration to learn the feature information of the background layer. Meanwhile, to keep more texture details in the background layer, a detail supplement mechanism is designed between the iterative layers to transfer more information to the next iterative operation. Experimental results show that this method can remove the rain information more completely and better retain the image edges compared with previous state-of-the-art methods. In addition, because of the sparsity of the detail injection, our network also achieves high-quality results for image denoising tasks. IEEE",IEEE Transactions on Multimedia,10.1109/TMM.2021.3068833,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103767853&doi=10.1109%2fTMM.2021.3068833&partnerID=40&md5=f9b8dbf06ee39df13817c8581c8cb61b,2021,2021-07-20 15:49:43,2021-07-20 15:49:43
CIQ4TSXD,journalArticle,2021,"Wen, H.; Gu, J.; Ma, J.; Yuan, L.; Jin, Z.",Probabilistic Load Forecasting via Neural Basis Expansion Model Based Prediction Intervals,"To narrow the width of prediction interval while guaranteeing coverage for probabilistic short term load forecasting, we propose a deep-learning forecasting model based on neural basis expansion analysis (N-BEATS). It takes load data as input, and feed the load sequence into three stacks. Each stack projects the load sequence on a set of basis vectors. Both the basis vectors and the corresponding coefficients are learned by the neural networks. A novel doubly residual stacking strategy is adopted, which decomposes forecasting task into three sub-problems, i.e., pattern characterization tasks corresponding to the stacks, under the assumption that load series can generally be represented by three patterns in subspaces with lower dimensions respectively. It removes redundant information in each stack, which guides the stack to concentrate on learning of one pattern. We further apply conformal quantile regression, which uses the residuals in a held-out validation, to calibrate constructed prediction interval for better theoretical coverage guarantee. Experiments based on load dataset provided by UT Dallas demonstrate improved performance of the proposed model in capturing the characteristics of load and providing narrow prediction intervals with nearly nominal coverage. IEEE",IEEE Transactions on Smart Grid,10.1109/TSG.2021.3066567,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103172650&doi=10.1109%2fTSG.2021.3066567&partnerID=40&md5=c3ffbe4d116e2777eb30db87597f3203,2021,2021-07-20 15:49:43,2021-07-20 15:49:43
EF927DJW,journalArticle,2020,"Ramesh Babu, M.; Veena, K.N.",Optimal DBN-based distributed attack detection model for Internet of Things,"This paper introduces a new detection mechanism for defending the cyberspace with a new logic that aiding the concept of deep learning. The process involves two phases, namely, feature extraction and classification. The initial phase is the feature extraction, in which the features are extracted from the given input data by the renowned principal component analysis (PCA). Subsequently, the extracted features are subjected to the classification phase, where the deep belief network (DBN) model is used. The DBN model classifies the presence of attacks like denial of service (DoS), probe, R2L, and U2R. In order to make the performance more excellent, this paper diverts the strategy to a new concept termed “Optimization Concept.” Here, the hidden neuron of DBN is optimally selected by a new algorithm termed novel mutation rate-based lion algorithm (NMR-LA), which is the modified model of lion algorithm (LA). The performance of proposed algorithm NMR-LA is compared over the conventional models in terms of both positive and negative measures like accuracy, sensitivity, specificity, precision, negative predictive value (NPV), F1 score and Mathews correlation coefficient (MCC), false-positive rate (FPR), false-negative rate (FNR), and false-discovery rate (FDR) and proves the betterments of proposed work. © 2020 John Wiley & Sons, Ltd.",International Journal of Communication Systems,10.1002/dac.4595,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090013790&doi=10.1002%2fdac.4595&partnerID=40&md5=411e6f6ce8f24f6a562a7caed5fbb179,2020,2021-07-20 15:49:43,2021-07-20 15:49:43
GJY6CMSS,journalArticle,2020,"Li, W.; Zhu, L.; Shi, Y.; Guo, K.; Cambria, E.",User reviews: Sentiment analysis using lexicon integrated two-channel CNN–LSTM​ family models,"Sentiment analysis, which refers to the task of detecting whether a textual item (e.g., a product review and a blog post) expresses a positive or negative opinion in general or about a given entity (e.g., a product, person, or policy), has received increasing attention in recent years. It serves as an important role in natural language processing. User generated content, like tourism reviews, developed dramatically during the past years, generating a large amount of unstructured data from which it is hard to obtain useful information. Due to the changes in textual order, sequence length and complicated logic, it is still a challenging task to predict the exact sentiment polarities of the user reviews, especially for fine-grained sentiment classification. In this paper, we first propose sentiment padding, a novel padding method compared with zero padding, making the input data sample of a consistent size and improving the proportion of sentiment information in each review. Inspired by the most recent studies with respect to neural networks, we propose deep learning based sentiment analysis models named lexicon integrated two-channel CNN–LSTM family models, combining CNN and LSTM/BiLSTM branches in a parallel manner. Experiments on several challenging datasets, like Stanford Sentiment Treebank, demonstrate that the proposed method outperforms many baseline methods. © 2020 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2020.106435,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086477447&doi=10.1016%2fj.asoc.2020.106435&partnerID=40&md5=27df94a6dff1dd2e99168f4e67644ff9,2020,2021-07-20 15:49:44,2021-07-20 15:49:44
KLR2L7TN,journalArticle,2020,"Véstias, M.P.; Duarte, R.P.; de Sousa, J.T.; Neto, H.C.",A fast and scalable architecture to run convolutional neural networks in low density FPGAs,"Deep learning and, in particular, convolutional neural networks (CNN) achieve very good results on several computer vision applications like security and surveillance, where image and video analysis are required. These networks are quite demanding in terms of computation and memory and therefore are usually implemented in high-performance computing platforms or devices. Running CNNs in embedded platforms or devices with low computational and memory resources requires a careful optimization of system architectures and algorithms to obtain very efficient designs. In this context, Field Programmable Gate Arrays (FPGA) can achieve this efficiency since the programmable hardware fabric can be tailored for each specific network. In this paper, a very efficient configurable architecture for CNN inference targeting any density FPGAs is described. The architecture considers fixed-point arithmetic and image batch to reduce computational, memory and memory bandwidth requirements without compromising network accuracy. The developed architecture supports the execution of large CNNs in any FPGA devices including those with small on-chip memory size and logic resources. With the proposed architecture, it is possible to infer an image in AlexNet in 4.3 ms in a ZYNQ7020 and 1.2 ms in a ZYNQ7045. © 2020 Elsevier B.V.",Microprocessors and Microsystems,10.1016/j.micpro.2020.103136,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085234927&doi=10.1016%2fj.micpro.2020.103136&partnerID=40&md5=a9b8743c4f28ebf53981d9275e2b4023,2020,2021-07-20 15:49:44,2021-07-20 15:49:44
5AF7AJPM,journalArticle,2019,"Zhang, W.; Quan, H.; Srinivasan, D.",An Improved Quantile Regression Neural Network for Probabilistic Load Forecasting,"Accurate and reliable load forecasting is essential for decision-making processes in the electric power industry. As the power industry transitions toward decarbonization, distributed energy systems, and integration of smart grid features, an increasing number of decision-making processes rely on uncertainty analysis of electric load. However, traditional point forecasting cannot address the uncertainties with only one forecasting value generated at each time step. As they are capable of representing uncertainties, probabilistic forecasts such as prediction intervals and quantile forecasts are preferred. Nevertheless, their practical application is limited partly due to the long training time of multiple probabilistic forecasting models. Traditional quantile regression neural network (QRNN) can train a single model for making quantile forecasts for multiple quantiles at one time. Whereas, the training cost is still unaffordable with large datasets. This paper proposes an improved QRNN (iQRNN) to address the issues of traditional QRNN, which incorporates popular techniques in deep learning areas. A case study on a publicly available dataset shows that not only can the proposed iQRNN generate remarkably superior quantile forecasts than state-of-the-art methods, but also the proposed iQRNN is more accurate, stable, and computationally efficient than traditional QRNN. © 2010-2012 IEEE.",IEEE Transactions on Smart Grid,10.1109/TSG.2018.2859749,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050614796&doi=10.1109%2fTSG.2018.2859749&partnerID=40&md5=c7d1764a69eed083be02107ef2f30923,2019,2021-07-20 15:49:44,2021-07-20 15:49:44
WP58EEYY,journalArticle,2018,"Le, T.H.N.; Quach, K.G.; Luu, K.; Duong, C.N.; Savvides, M.",Reformulating Level Sets as Deep Recurrent Neural Network Approach to Semantic Segmentation,"Variational Level Set (LS) has been a widely used method in medical segmentation. However, it is limited when dealing with multi-instance objects in the real world. In addition, its segmentation results are quite sensitive to initial settings and highly depend on the number of iterations. To address these issues and boost the classic variational LS methods to a new level of the learnable deep learning approaches, we propose a novel definition of contour evolution named Recurrent Level Set (RLS)1 to employ Gated Recurrent Unit under the energy minimization of a variational LS functional. The curve deformation process in RLS is formed as a hidden state evolution procedure and updated by minimizing an energy functional composed of fitting forces and contour length. By sharing the convolutional features in a fully end-to-end trainable framework, we extend RLS to Contextual RLS (CRLS) to address semantic segmentation in the wild. The experimental results have shown that our proposed RLS improves both computational time and segmentation accuracy against the classic variational LS-based method whereas the fully end-to-end system CRLS achieves competitive performance compared to the state-of-the-art semantic segmentation approaches.1Source codes will be publicly available. © 1992-2012 IEEE.",IEEE Transactions on Image Processing,10.1109/TIP.2018.2794205,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041373104&doi=10.1109%2fTIP.2018.2794205&partnerID=40&md5=1441004feea850bb05df52e044ef35ae,2018,2021-07-20 15:49:44,2021-07-20 15:49:44
NFQSEXXC,journalArticle,2019,"Huang, B.-Y.; Zhang, H.; Subramanyan, P.; Vizel, Y.; Gupta, A.; Malik, S.",Instruction-level abstraction (ILA): A uniform specification for system-on-chip (SOC) verification,"Modern Systems-on-Chip (SoC) designs are increasingly heterogeneous and contain specialized semi-programmable accelerators in addition to programmable processors. In contrast to the pre-accelerator era, when the ISA played an important role in verification by enabling a clean separation of concerns between software and hardware, verification of these “accelerator-rich” SoCs presents new challenges. From the perspective of hardware designers, there is a lack of a common framework for formal functional specification of accelerator behavior. From the perspective of software developers, there exists no unified framework for reasoning about software/hardware interactions of programs that interact with accelerators. This article addresses these challenges by providing a formal specification and high-level abstraction for accelerator functional behavior. It formalizes the concept of an Instruction Level Abstraction (ILA), developed informally in our previous work, and shows its application in modeling and verification of accelerators. This formal ILA extends the familiar notion of instructions to accelerators and provides a uniform, modular, and hierarchical abstraction for modeling software-visible behavior of both accelerators and programmable processors. We demonstrate the applicability of the ILA through several case studies of accelerators (for image processing, machine learning, and cryptography), and a general-purpose processor (RISC-V). We show how the ILA model facilitates equivalence checking between two ILAs, and between an ILA and its hardware finite-state machine (FSM) implementation. Further, this equivalence checking supports accelerator upgrades using the notion of ILA compatibility, similar to processor upgrades using ISA compatibility. © 2018 Association for Computing Machinery.",ACM Transactions on Design Automation of Electronic Systems,10.1145/3282444,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060144673&doi=10.1145%2f3282444&partnerID=40&md5=f27fc860026d9c7db72280945b08bba9,2019,2021-07-20 15:49:44,2021-07-20 15:49:44
BCIU4VB9,journalArticle,2018,"Sugiarto, I.; Conradt, J.",Modular design of a factor-graph-based inference engine on a System-On-Chip (SoC),"Factor graphs are probabilistic graphical frameworks for modeling complex and dynamic systems. They can be used in a broad range of application domains, from machine learning and robotics, to signal processing and digital communications. One important aspect that makes a factor graph very useful and very promising to be applied widely is its inference mechanism that is suitable for performing a complex model-based reasoning. However, its features have not fully explored and factor graphs are still used mainly as modeling tools that run on standard computers. Whereas in real applications such as robotics, one needs a practical implementation of such a framework. In this paper, we describe the development of a factor-graph-based inference engine that runs on a System-on-Chip (SoC). Running natively on a low level hardware, our factor graph engine delivers highest performance for real-time applications. We designed the embedded architecture so that it conveys important aspects such as modularity, scalability, flexibility and platform-friendly framework. The proposed architecture has customizable levels of parallelism as well as re-configurable modules that are extensible to accommodate large networks. We optimized the design to achieve high efficiency in terms of clock latency and resources consumption. We have tested our design on Xilinx Zynq-7000 SoCs and the implementation result demonstrates that the proposed framework can potentially be extended into a massively distributed probabilistic computing engine. © 2018 Elsevier B.V.",Microprocessors and Microsystems,10.1016/j.micpro.2018.04.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045660948&doi=10.1016%2fj.micpro.2018.04.002&partnerID=40&md5=234861c49657448e038dff5c5a0ce644,2018,2021-07-20 15:49:44,2021-07-20 15:49:44
SYHNMIZS,journalArticle,2017,"Hasuo, I.",Metamathematics for Systems Design: Comprehensive Transfer of Formal Methods Techniques to Cyber-Physical Systems,"This position paper describes the context, the goal, the strategy and the tactics of the ERATO MMSD project (2016–2022). The project aims at enhanced quality assurance measures for industry products like cars. In doing so, we follow a recent trend and exploit formal methods, a body of mathematical techniques originally developed for computer systems. However, there are fundamental gaps in application of formal methods to industry products: additional concerns in industry products such as continuous dynamics of physical components and quantitative measures such as probability, time, and cost make problems fundamentally different from those about software. Formal methods that accommodate these concerns is an active research area, which shows that it is a hard problem. There are several successful theoretical developments in this direction. They typically combine one individual technique with one specific concern, such as hybrid automata that extend automata with continuous dynamics. Our project aims to contribute to this hard problem in a unique way. In our project we will take a unique metamathematical strategy to bridging the gaps: instead of creating one technique for each concern, we want to find a meta-level theory that describes how to develop such techniques for many potential concerns in general. Through this strategy, together with our emphasis on real-world applications in industry, we expect a new prototype of applied mathematics will emerge. In this prototype, abstraction and genericity—characteristics of modern mathematics that are not often associated with application—are turned into crucial advantages in applications. © 2017, The Author(s).",New Generation Computing,10.1007/s00354-017-0023-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023765190&doi=10.1007%2fs00354-017-0023-1&partnerID=40&md5=e87d1347705bab490f05299141e31d9f,2017,2021-07-20 15:49:44,2021-07-20 15:49:44
29T7QI28,journalArticle,2021,"Yao, S.; Yang, J.-B.; Xu, D.-L.; Dark, P.",Probabilistic modeling approach for interpretable inference and prediction with data for sepsis diagnosis,"Sepsis is a serious disease that can cause death. It is important to predict sepsis within the early stages after the presence of sepsis symptoms. In this paper, a new probabilistic modeling approach is used to establish classifiers for sepsis diagnosis. This approach is characterized by unique strong interpretability, which is reflected in three aspects: (1) evidence acquisition based on likelihood analysis, (2) probabilistic rule-based inference, and (3) parameters optimization using machine learning algorithms. Four-fold cross-validation is used to train and validate classifiers established by the new approach and alternative ones. Results show that in terms of classification capability, the classifier established by the new approach generally performs better than the majority of alternative classifiers for sepsis diagnosis, and close to the best one. As the classifier also features an inherent interpretability, it can be used as a tool for supporting diagnostic decision-making in sepsis diagnosis. © 2021 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2021.115333,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107751374&doi=10.1016%2fj.eswa.2021.115333&partnerID=40&md5=8e88174cbb861d126726106a849c1bba,2021,2021-07-20 15:49:44,2021-07-20 15:49:44
Q4WLA2LH,journalArticle,2019,"Martínez-Villaseñor, L.; Ponce, H.",A concise review on sensor signal acquisition and transformation applied to human activity recognition and human–robot interaction,"Human activitiy recognition deals with the integration of sensing and reasoning aiming to understand better people’s actions. Moreover, it plays an important role in human interaction, human–robot interaction, and brain–computer interaction. When these approaches have to be developed, different efforts from signal processing and artificial intelligence are considered. In that sense, this article aims to present a concise review of signal processing in human activitiy recognition systems and describe two examples and applications both in human activity recognition and robotics: human–robot interaction and socialization, and imitation learning in robotics. In addition, it presents ideas and trends in the context of human activity recognition for human–robot interaction that are important when processing signals within that systems. © The Author(s) 2019.",International Journal of Distributed Sensor Networks,10.1177/1550147719853987,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067308617&doi=10.1177%2f1550147719853987&partnerID=40&md5=9361a2cc04b2dcf4c5d722459838a9a6,2019,2021-07-20 15:49:44,2021-07-20 15:49:44
YJ4UY62N,journalArticle,2018,"Li, T.; Luther, K.; North, C.",CrowdIA: Solving mysteries with crowdsourced sensemaking,"The increasing volume of text data is challenging the cognitive capabilities of expert analysts. Machine learning and crowdsourcing present new opportunities for large-scale sensemaking, but we must overcome the challenge of modeling the overall process so that many distributed agents can contribute to suitable components asynchronously and meaningfully. In this paper, we explore how to crowdsource the sensemaking process via a pipeline of modularized steps connected by clearly defined inputs and outputs. Our pipeline restructures and partitions information into ""context slices"" for individual workers. We implemented CrowdIA, a software platform to enable unsupervised crowd sensemaking using our pipeline. With CrowdIA, crowds successfully solved two mysteries, and were one step away from solving the third. The crowd’s intermediate results revealed their reasoning process and provided evidence that justifies their conclusions. We suggest broader possibilities to optimize each component, as well as to evaluate and refine previous intermediate analyses to improve the final result. © 2018 Association for Computing Machinery.",Proceedings of the ACM on Human-Computer Interaction,10.1145/3274374,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066423106&doi=10.1145%2f3274374&partnerID=40&md5=bb602473b399bb3c6cd149f5a306c491,2018,2021-07-20 15:49:44,2021-07-20 15:49:44
RI4Y9R5N,journalArticle,2015,"Ben Messaoud, M.; Leray, P.; Ben Amor, N.",SemCaDo: A serendipitous strategy for causal discovery and ontology evolution,"Within the last years, probabilistic causality has become a very active research topic in artificial intelligence and statistics communities. Due to its high impact in various applications involving reasoning tasks, machine learning researchers have proposed a number of techniques to learn Causal Bayesian Networks. Within the existing works in this direction, few studies have explicitly considered the role that decisional guidance might play to alternate between observational and experimental data processing. In this paper, we go further by introducing a serendipitous strategy to elucidate semantic background knowledge provided by the domain ontology to learn the causal structure of Bayesian Networks. We also complement our contribution with an enrichment process by which it will be possible to reuse these causal discoveries, support the evolving character of the semantic background and make an ontology evolution. Finally, the proposed method will be validated through simulations and real data analysis. © 2014 Elsevier B.V. All rights reserved.",Knowledge-Based Systems,10.1016/j.knosys.2014.12.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923047987&doi=10.1016%2fj.knosys.2014.12.006&partnerID=40&md5=843c5d8ac1dbe654ac0847c4fbffa869,2015,2021-07-20 15:49:44,2021-07-20 15:49:44
GIC4LEU4,journalArticle,2013,"Dsouza, S.; Gal, Y.K.; Pasquier, P.; Abdallah, S.; Rahwan, I.",Reasoning about goal revelation in human negotiation,"This article studies how people reveal private information in strategic settings in which participants need to negotiate over resources but are uncertain about each other's objectives. The study compares two negotiation protocols that differ in whether they allow participants to disclose their objectives in a repeated negotiation setting of incomplete information. Results show that most people agree to reveal their goals when asked, and this leads participants to more beneficial agreements. Machine learning was used to model the likelihood that people reveal their goals in negotiation, and this model was used to make goal request decisions in the game. In simulation, use of this model is shown to outperform people making the same type of decisions. These results demonstrate the benefit of this approach towards designing agents to negotiate with people under incomplete information. © 2001-2011 IEEE.",IEEE Intelligent Systems,10.1109/MIS.2011.93,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880665809&doi=10.1109%2fMIS.2011.93&partnerID=40&md5=0b4bef3569f1b205ceee506706e80fae,2013,2021-07-20 15:49:45,2021-07-20 15:49:45
DBNFFD67,journalArticle,2021,"Es-sabery, F.; Hair, A.; Qadir, J.; Sainz-de-Abajo, B.; Garcia-Zapirain, B.; De La Torre-Diez, I.",Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier,"At present, with the growing number of Web 2.0 platforms such as Instagram, Facebook, and Twitter, users honestly communicate their opinions and ideas about events, services, and products. Owing to this rise in the number of social platforms and their extensive use by people, enormous amounts of data are produced hourly. However, sentiment analysis or opinion mining is considered as a useful tool that aims to extract the emotion and attitude from the user-posted data on social media platforms by using different computational methods to linguistic terms and various Natural Language Processing (NLP). Therefore, enhancing text sentiment classification accuracy has become feasible, and an interesting research area for many community researchers. In this study, a new Fuzzy Deep Learning Classifier (FDLC) is suggested for improving the performance of data-sentiment classification. Our proposed FDLC integrates Convolutional Neural Network (CNN) to build an effective automatic process for extracting the features from collected unstructured data and Feedforward Neural Network (FFNN) to compute both positive and negative sentimental scores. Then, we used the Mamdani Fuzzy System (MFS) as a fuzzy classifier to classify the outcomes of the two used deep (CNN+FFNN) learning models in three classes, which are: Neutral, Negative, and Positive. Also, to prevent the long execution time taking by our hybrid proposed FDLC, we have implemented our proposal under the Hadoop cluster. An experimental comparative study between our FDLC and some other suggestions from the literature is performed to demonstrate our offered classifier&#x2019;s effectiveness. The empirical result proved that our FDLC performs better than other classifiers in terms of true positive rate, true negative rate, false positive rate, false negative rate, error rate, precision, classification rate, kappa statistic, F1-score and time consumption, complexity, convergence, and stability. CCBY",IEEE Access,10.1109/ACCESS.2021.3053917,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100468874&doi=10.1109%2fACCESS.2021.3053917&partnerID=40&md5=396f7ce7ea930aa7a7d3b8af2dc2c3fa,2021,2021-07-20 15:49:45,2021-07-20 15:49:45
2YIHESWB,journalArticle,2019,"Zhao, Y.; Li, H.; Wan, S.; Sekuboyina, A.; Hu, X.; Tetteh, G.; Piraud, M.; Menze, B.",Knowledge-Aided Convolutional Neural Network for Small Organ Segmentation,"Accurate and automatic organ segmentation is critical for computer-aided analysis towards clinical decision support and treatment planning. State-of-the-art approaches have achieved remarkable segmentation accuracy on large organs, such as the liver and kidneys. However, most of these methods do not perform well on small organs, such as the pancreas, gallbladder, and adrenal glands, especially when lacking sufficient training data. This paper presents an automatic approach for small organ segmentation with limited training data using two cascaded steps - localization and segmentation. The localization stage involves the extraction of the region of interest after the registration of images to a common template and during the segmentation stage, a voxel-wise label map of the extracted region of interest is obtained and then transformed back to the original space. In the localization step, we propose to utilize a graph-based groupwise image registration method to build the template for registration so as to minimize the potential bias and avoid getting a fuzzy template. More importantly, a novel knowledge-aided convolutional neural network is proposed to improve segmentation accuracy in the second stage. This proposed network is flexible and can combine the effort of both deep learning and traditional methods, consequently achieving better segmentation relative to either of individual methods. The ISBI 2015 VISCERAL challenge dataset is used to evaluate the presented approach. Experimental results demonstrate that the proposed method outperforms cutting-edge deep learning approaches, traditional forest-based approaches, and multi-atlas approaches in the segmentation of small organs. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2019.2891526,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064011984&doi=10.1109%2fJBHI.2019.2891526&partnerID=40&md5=7a9d65aa931edc52acde9f654c260346,2019,2021-07-20 15:49:45,2021-07-20 15:49:45
J7VK433C,journalArticle,2021,"Souza, P.V.C.; Guimaraes, A.J.; Araujo, V.S.; Lughofer, E.",An intelligent Bayesian hybrid approach to help autism diagnosis,"This paper proposes a Bayesian hybrid approach based on neural networks and fuzzy systems to construct fuzzy rules to assist experts in detecting features and relations regarding the presence of autism in human beings. The model proposed in this paper works with a database generated through mobile devices that deals with diagnoses of autistic characteristics in human beings who answer a series of questions in a mobile application. The Bayesian model works with the construction of Gaussian fuzzy neurons in the first and logical neurons in the second layer of the model to form a fuzzy inference system connected to an artificial neural network that activates a robust output neuron. The new fuzzy neural network model was compared with traditional state-of-the-art machine learning models based on high-dimensional based on real-world data sets comprising the autism occurrence in children, adults, and adolescents. The results (97.73- Children/94.32-Adolescent/97.28-Adult) demonstrate the efficiency of our new method in determining children, adolescents, and adults with autistic traits (being among the top performers among all ML models tested), can generate knowledge about the dataset through fuzzy rules. © 2021, The Author(s).",Soft Computing,10.1007/s00500-021-05877-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106441025&doi=10.1007%2fs00500-021-05877-0&partnerID=40&md5=a92ae10e92e9bac56143eee3d1b54f62,2021,2021-07-20 15:49:45,2021-07-20 15:49:45
92N6EIMN,journalArticle,2020,"Vélez, D.; Ayuso, A.; Perales-González, C.; Rodríguez, J.T.",Churn and Net Promoter Score forecasting for business decision-making through a new stepwise regression methodology,"Companies typically have to make relevant decisions regarding their clients’ fidelity and retention on the basis of analytical models developed to predict both their churn probability and Net Promoter Score (NPS). Although the predictive capability of these models is important, interpretability is a crucial factor to look for as well, because the decisions to be made from their results have to be properly justified. In this paper, a novel methodology to develop analytical models balancing predictive performance and interpretability is proposed, with the aim of enabling a better decision-making. It proceeds by fitting logistic regression models through a modified stepwise variable selection procedure, which automatically selects input variables while keeping their business logic, previously validated by an expert. In synergy with this procedure, a new method for transforming independent variables in order to better deal with ordinal targets and avoiding some logistic regression issues with outliers and missing data is also proposed. The combination of these two proposals with some competitive machine-learning methods earned the leading position in the NPS forecasting task of an international university talent challenge posed by a well-known global bank. The application of the proposed methodology and the results it obtained at this challenge are described as a case-study. © 2020 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2020.105762,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082480132&doi=10.1016%2fj.knosys.2020.105762&partnerID=40&md5=c37e849fb057f6c8df7f470564ff59a7,2020,2021-07-20 15:49:45,2021-07-20 15:49:45
NLVPL5XZ,journalArticle,2020,"Letizia, N.A.; Tonello, A.M.",Segmented Generative Networks: Data Generation in the Uniform Probability Space,"Recent advancements in generative networks have shown that it is possible to produce real-world-like data using deep neural networks. Some implicit probabilistic models that follow a stochastic procedure to directly generate data have been introduced to overcome the intractability of the posterior distribution. However, the ability to model data requires deep knowledge and understanding of its statistical dependence–which can be preserved and studied in appropriate latent spaces. In this article, we present a segmented generation process through linear and nonlinear manipulations in the same-dimensional latent space where data are projected to. Inspired by the known stochastic method to generate correlated data, we develop a segmented approach for the generation of dependent data, exploiting the concept of copula. The generation process is split into two frames: one embedding the covariance or copula information in the uniform probability space, and the other embedding the marginal distribution information in the sample domain. The proposed network structure, referred to as a segmented generative network (SGN), also provides an empirical method to sample directly from implicit copulas. To show its generality, we evaluate the presented approach in three application scenarios: a toy example, handwritten digits, and face image generation. CCBY",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2020.3042380,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098783054&doi=10.1109%2fTNNLS.2020.3042380&partnerID=40&md5=ba1c84655e3bbae647d4e8a500d82c03,2020,2021-07-20 15:49:45,2021-07-20 15:49:45
ZKE7ZYGZ,journalArticle,2020,"Pereira, G.; Moreschi, B.",Artificial intelligence and institutional critique 2.0: unexpected ways of seeing with computer vision,"During 2018, as part of a research project funded by the Deviant Practice Grant, artist Bruno Moreschi and digital media researcher Gabriel Pereira worked with the Van Abbemuseum collection (Eindhoven, NL), reading their artworks through commercial image-recognition (computer vision) artificial intelligences from leading tech companies. The main takeaways were: somewhat as expected, AI is constructed through a capitalist and product-focused reading of the world (values that are embedded in this sociotechnical system); and that this process of using AI is an innovative way for doing institutional critique, as AI offers an untrained eye that reveals the inner workings of the art system through its glitches. This paper aims to regard these glitches as potentially revealing of the art system, and even poetic at times. We also look at them as a way of revealing the inherent fallibility of the commercial use of AI and machine learning to catalogue the world: it cannot comprehend other ways of knowing about the world, outside the logic of the algorithm. But, at the same time, due to their “glitchy” capacity to level and reimagine, these faulty readings can also serve as a new way of reading art; a new way for thinking critically about the art image in a moment when visual culture has changed form to hybrids of human–machine cognition and “machine-to-machine seeing”. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",AI and Society,10.1007/s00146-020-01059-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090963043&doi=10.1007%2fs00146-020-01059-y&partnerID=40&md5=cc9da78181597ae6671c84595d60b4d7,2020,2021-07-20 15:49:45,2021-07-20 15:49:45
J4VTX6X2,journalArticle,2019,"Jiang, Y.; Zhao, K.; Xia, K.; Xue, J.; Zhou, L.; Ding, Y.; Qian, P.",A Novel Distributed Multitask Fuzzy Clustering Algorithm for Automatic MR Brain Image Segmentation,"Artificial intelligence algorithms have been used in a wide range of applications in clinical aided diagnosis, such as automatic MR image segmentation and seizure EEG signal analyses. In recent years, many machine learning-based automatic MR brain image segmentation methods have been proposed as auxiliary methods of medical image analysis in clinical treatment. Nevertheless, many problems regarding precise medical images, which cannot be effectively utilized to improve partition performance, remain to be solved. Due to the poor contrast in grayscale images, the ambiguity and complexity of MR images, and individual variability, the performance of classic algorithms in medical image segmentation still needs improvement. In this paper, we introduce a distributed multitask fuzzy c-means (MT-FCM) clustering algorithm for MR brain image segmentation that can extract knowledge common among different clustering tasks. The proposed distributed MT-FCM algorithm can effectively exploit information common among different but related MR brain image segmentation tasks and can avoid the negative effects caused by noisy data that exist in some MR images. Experimental results on clinical MR brain images demonstrate that the distributed MT-FCM method demonstrates more desirable performance than the classic signal task method. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Medical Systems,10.1007/s10916-019-1245-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063523091&doi=10.1007%2fs10916-019-1245-1&partnerID=40&md5=f3a9c1c7bc9e8797eaf4f2b70fda6e9b,2019,2021-07-20 15:49:45,2021-07-20 15:49:45
2FASFVKX,journalArticle,2019,"Sharma, R.K.; Issac, B.; Kalita, H.K.",Intrusion detection and response system inspired by the defense mechanism of plants,"The security of resources in a corporate network is always important to the organization. For this reason, different techniques, such as firewall and intrusion detection systems, are important. Years of long research have resulted in the contribution of different advancements in these techniques. Artificial intelligence, machine learning techniques, soft computing techniques, and bio-inspired techniques have been efficient in detecting advanced network attacks. However, very often, different new attacks are most successful in breaching these detection techniques. This very reason has been a motivation for us to explore the biological aspects and its defense mechanisms for designing a secure network model. After much study, we have identified that plants have a very well-established and evolved detection and a response mechanism to pathogens. In this paper, we have proposed and implemented a network attack detection and a response model inspired by plants. It is a three-layered model in analogy to the three-layer defense mechanism of plants to pathogens. We have further tested the proposed model to different network attacks and have compared the results to the open-source intrusion detection system, Snort. The experimental results also establish that the model is competent to detect and trigger an automated response whenever required. © 2019 IEEE.",IEEE Access,10.1109/ACCESS.2019.2912114,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066839418&doi=10.1109%2fACCESS.2019.2912114&partnerID=40&md5=a93cce1d983430f06dc1a1fc6a7dc8ff,2019,2021-07-20 15:49:45,2021-07-20 15:49:45
NFU98JJG,journalArticle,2017,"Tong, D.; Qu, Y.R.; Prasanna, V.K.",Accelerating Decision Tree Based Traffic Classification on FPGA and Multicore Platforms,"Machine learning (ML) algorithms have been shown to be effective in classifying a broad range of applications in the Internet traffic. In this paper, we propose algorithms and architectures to realize online traffic classification using flow level features. First, we develop a traffic classifier based on C4.5 decision tree algorithm and Entropy-MDL (Minimum Description Length) discretization algorithm. It achieves an overall accuracy of 97.92 percent for classifying eight major applications. Next we propose approaches to accelerate the classifier on FPGA (Field Programmable Gate Array) and multicore platforms. We optimize the original classifier by merging it with discretization. Our implementation of this optimized decision tree achieves 7500+ Million Classifications Per Second (MCPS) on a state-of-the-art FPGA platform and 75-150 MCPS on two state-of-the-art multicore platforms. We also propose a divide and conquer approach to handle imbalanced decision trees. Our implementation of the divide-and-conquer approach achieves 10,000+ MCPS on a state-of-the-art FPGA platform and 130-340 MCPS on two state-of-the-art multicore platforms. We conduct extensive experiments on both platforms for various application scenarios to compare the two approaches. © 1990-2012 IEEE.",IEEE Transactions on Parallel and Distributed Systems,10.1109/TPDS.2017.2714661,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021808413&doi=10.1109%2fTPDS.2017.2714661&partnerID=40&md5=e572cae8016f9fe093af81685af60e3e,2017,2021-07-20 15:49:45,2021-07-20 15:49:45
36U36BVR,journalArticle,2019,"Chen, K.; Chen, K.; Wang, Q.; He, Z.; Hu, J.; He, J.",Short-Term Load Forecasting with Deep Residual Networks,"We present in this paper a model for forecasting short-term electric load based on deep residual networks. The proposed model is able to integrate domain knowledge and researchers' understanding of the task by virtue of different neural network building blocks. Specifically, a modified deep residual network is formulated to improve the forecast results. Further, a two-stage ensemble strategy is used to enhance the generalization capability of the proposed model. We also apply the proposed model to probabilistic load forecasting using Monte Carlo dropout. Three public datasets are used to prove the effectiveness of the proposed model. Multiple test cases and comparison with existing models show that the proposed model provides accurate load forecasting results and has high generalization capability. © 2010-2012 IEEE.",IEEE Transactions on Smart Grid,10.1109/TSG.2018.2844307,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048168835&doi=10.1109%2fTSG.2018.2844307&partnerID=40&md5=1b6a414b07053f59532e12191ae63cbc,2019,2021-07-20 15:49:45,2021-07-20 15:49:45
EILYXMTJ,journalArticle,2011,"Chuang, C.-L.",Case-based reasoning support for liver disease diagnosis,"Objectives: In Taiwan, as well as in the other countries around the world, liver disease has reigned over the list of leading causes of mortality, and its resistance to early detection renders the disease even more threatening. It is therefore crucial to develop an auxiliary system for diagnosing liver disease so as to enhance the efficiency of medical diagnosis and to expedite the delivery of proper medical treatment. Methods: The study accordingly integrated the case-based reasoning (CBR) model into several common classification methods of data mining techniques, including back-propagation neural network (BPN), classification and regression tree, logistic regression, and discriminatory analysis, in an attempt to develop a more efficient model for early diagnosis of liver disease and to enhance classification accuracy. To minimize possible bias, this study used a ten-fold cross-validation to select a best model for more precise diagnosis results and to reduce problems caused by false diagnosis. Results: Through a comparison of five single models, BPN and CBR emerged to be the top two methods in terms of overall performance. For enhancing diagnosis performance, CBR was integrated with other methods, and the results indicated that the accuracy and sensitivity of each CBR-added hybrid model were higher than those of each single model. Of all the CBR-added hybrid models, the BPN-CBR method took the lead in terms of diagnosis capacity with an accuracy rate of 95%, a sensitivity of 98%, and a specificity of 94%. Conclusions: After comparing the five single and hybrid models, the study found BPN-CBR the best model capable of helping physicians to determine the existence of liver disease, achieve an accurate diagnosis, diminish the possibility of a false diagnosis being given to sick people, and avoid the delay of clinical treatment. © 2011 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2011.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960927416&doi=10.1016%2fj.artmed.2011.06.002&partnerID=40&md5=9fc025001dbbbb628eda39bda2cc7f5c,2011,2021-07-20 15:49:45,2021-07-20 15:49:45
Q7XNBWBG,journalArticle,2020,"Mahouti, P.",Application of artificial intelligence algorithms on modeling of reflection phase characteristics of a nonuniform reflectarray element,"Reflectarray antennas (RAs) have the ability to combine the advantages of both traditional parabolic reflector and phased array antennas without the need for feed network designs. Microstrip reflectarrays (MRAs) have the advantages of being small size, light weighted, easy to prototyped, high gain, low side-lobe level, and a predetermined radiation pattern. These can be achieved by precise calculation of reflection phase at each RA unit independently with a phase compensation proportional to the distance from the feed. The challenging problem is to have a fast and high accurate unit element to be used in multidimension, multiobjective design optimization. Herein, artificial intelligence algorithms (AIAs) have been used for prediction of reflection phase characterization of an X band MRA unit element with respect to the geometrical design parameters. Firstly, a nonuniform unit RA has been designed in 3D electromagnetic (EM) simulation tool for creating the training validation data sets. Then, the data sets are given to the different types of AIA regression models such as multilayer perceptron, symbolic regression, and convolutional neural network. From the results of the validation data set, it can be concluded that the proposed models have sufficient accuracy that can be used in a computationally efficient design optimization process of a large-scale RA design. © 2019 John Wiley & Sons, Ltd.","International Journal of Numerical Modelling: Electronic Networks, Devices and Fields",10.1002/jnm.2689,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075899361&doi=10.1002%2fjnm.2689&partnerID=40&md5=64b1cd1abcfc74cf040fcbde05f7c8b0,2020,2021-07-20 15:49:45,2021-07-20 15:49:45
TNUW7Y7N,journalArticle,2021,"Zhu, J.; Zhang, G.; Zhou, S.; Li, K.",Relation-aware Siamese region proposal network for visual object tracking,"The backbone networks used in Siamese trackers are relatively shallow, such as AlexNet and VGGNet, resulting in insufficient features for tracking task. Therefore, this paper focuses on extracting more discriminative features to improve the performance of Siamese trackers. By comprehensive experimental validations, this goal is achieved through a simple yet effective framework referred as relation-aware Siamese region proposal network (Ra-SiamRPN). Firstly, the deep backbone network ResNet-50 is adopted to extract both low-level detail features and high-level semantic features of an image. Then we propose the feature fusion module (FFM), which can combine low-level detail features with high-level semantic features effectively. Furthermore, we propose the relation reasoning module (RRM) to perform the global relation reasoning in multiple disjoint regions. RRM can generate discriminative information to enhance the features generated by ResNet-50. Extensive experiments are conducted on the dataset OTB2015, VOT2016, VOT2018, UAV123 and LaSOT. The experiment results indicate that Ra-SiamRPN achieves competitive performance with the current advanced algorithms and shows good real-time performance. To be highlighted, in the experiments conducted on the large-scale dataset LaSOT, the success score and the normalized precision score of Ra-SiamRPN are 0.495 and 0.576, respectively. These performance indexes are better than the second best tracker MDNet 24.7% and 25.2%. © 2021, The Author(s), under exclusive licence to Springer Science+Business Media, LLC part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-021-10574-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100469708&doi=10.1007%2fs11042-021-10574-z&partnerID=40&md5=fb65315b29ffc17259287c4616fa3015,2021,2021-07-20 15:49:45,2021-07-20 15:49:45
LQMARCNF,journalArticle,2014,"Jiménez, F.; Sánchez, G.; Juárez, J.M.",Multi-objective evolutionary algorithms for fuzzy classification in survival prediction,"Objective: This paper presents a novel rule-based fuzzy classification methodology for survival/mortality prediction in severe burnt patients. Due to the ethical aspects involved in this medical scenario, physicians tend not to accept a computer-based evaluation unless they understand why and how such a recommendation is given. Therefore, any fuzzy classifier model must be both accurate and interpretable. Methods and materials: The proposed methodology is a three-step process: (1) multi-objective constrained optimization of a patient's data set, using Pareto-based elitist multi-objective evolutionary algorithms to maximize accuracy and minimize the complexity (number of rules) of classifiers, subject to interpretability constraints; this step produces a set of alternative (Pareto) classifiers; (2) linguistic labeling, which assigns a linguistic label to each fuzzy set of the classifiers; this step is essential to the interpretability of the classifiers; (3) decision making, whereby a classifier is chosen, if it is satisfactory, according to the preferences of the decision maker. If no classifier is satisfactory for the decision maker, the process starts again in step (1) with a different input parameter set. Results: The performance of three multi-objective evolutionary algorithms, niched pre-selection multi-objective algorithm, elitist Pareto-based multi-objective evolutionary algorithm for diversity reinforcement (ENORA) and the non-dominated sorting genetic algorithm (NSGA-II), was tested using a patient's data set from an intensive care burn unit and a standard machine learning data set from an standard machine learning repository. The results are compared using the hypervolume multi-objective metric. Besides, the results have been compared with other non-evolutionary techniques and validated with a multi-objective cross-validation technique. Our proposal improves the classification rate obtained by other non-evolutionary techniques (decision trees, artificial neural networks, Naive Bayes, and case-based reasoning) obtaining with ENORA a classification rate of 0.9298, specificity of 0.9385, and sensitivity of 0.9364, with 14.2 interpretable fuzzy rules on average. Conclusions: Our proposal improves the accuracy and interpretability of the classifiers, compared with other non-evolutionary techniques. We also conclude that ENORA outperforms niched pre-selection and NSGA-II algorithms. Moreover, given that our multi-objective evolutionary methodology is non-combinational based on real parameter optimization, the time cost is significantly reduced compared with other evolutionary approaches existing in literature based on combinational optimization. © 2014 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2013.12.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897096881&doi=10.1016%2fj.artmed.2013.12.006&partnerID=40&md5=76f64b90ac40326bb7dd33a3a01c7ad9,2014,2021-07-20 15:49:46,2021-07-20 15:49:46
PVP5P444,journalArticle,2021,"Chelali, M.; Kurtz, C.; Puissant, A.; Vincent, N.",Deep-STaR: Classification of image time series based on spatio-temporal representations,"Image time series (ITS) represent complex 3D (2D+t in practice) data that are now daily produced in various domains, from medical imaging to remote sensing. They contain rich spatio-temporal information allowing the observation of the evolution of a sensed scene over time. In this work, we focus on the classification task of ITS, as often available in remote sensing tasks. An underlying problem here is to consider jointly the spatial and the temporal dimensions of the data. We present Deep-STaR, a method to learn such features from ITS data to proceed to their classification. Instead of reasoning in the original 2D+t space, we investigate novel 2D planar data representations, containing both temporal and spatial information. Such representations are a novel way to structure the ITS, compatible with deep learning architectures. They are used to feed a convolutional neural network to learn spatio-temporal features with 2D convolutions, leading ultimately to classification decision. To enhance the explainability of the results, we also propose a post-hoc attention mechanism, enabled by this new approach, providing a semantic map giving some insights for the taken decision. Deep-STaR is evaluated on a remote sensing application, for the classification of agricultural crops from satellite ITS. The results highlight the benefice of this method, compared to the literature, and its interest to make easier the interpretation of ITS to understand spatio-temporal phenomena. © 2021 Elsevier Inc.",Computer Vision and Image Understanding,10.1016/j.cviu.2021.103221,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106887154&doi=10.1016%2fj.cviu.2021.103221&partnerID=40&md5=baae20b3a281e181a9f569d860f2d01a,2021,2021-07-20 15:49:46,2021-07-20 15:49:46
4A7HENN4,journalArticle,2021,"Jain, N.; Gupta, V.; Shubham, S.; Madan, A.; Chaudhary, A.; Santosh, K.C.",Understanding cartoon emotion using integrated deep neural network on large dataset,"Emotion is an instinctive or intuitive feeling as distinguished from reasoning or knowledge. It varies over time, since it is a natural instinctive state of mind deriving from one’s circumstances, mood, or relationships with others. Since emotions vary over time, it is important to understand and analyze them appropriately. Existing works have mostly focused well on recognizing basic emotions from human faces. However, the emotion recognition from cartoon images has not been extensively covered. Therefore, in this paper, we present an integrated Deep Neural Network (DNN) approach that deals with recognizing emotions from cartoon images. Since state-of-works do not have large amount of data, we collected a dataset of size 8 K from two cartoon characters: ‘Tom’ & ‘Jerry’ with four different emotions, namely happy, sad, angry, and surprise. The proposed integrated DNN approach, trained on a large dataset consisting of animations for both the characters (Tom and Jerry), correctly identifies the character, segments their face masks, and recognizes the consequent emotions with an accuracy score of 0.96. The approach utilizes Mask R-CNN for character detection and state-of-the-art deep learning models, namely ResNet-50, MobileNetV2, InceptionV3, and VGG 16 for emotion classification. In our study, to classify emotions, VGG 16 outperforms others with an accuracy of 96% and F1 score of 0.85. The proposed integrated DNN outperforms the state-of-the-art approaches. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-021-06003-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104948326&doi=10.1007%2fs00521-021-06003-9&partnerID=40&md5=3f450c558ba6ef7e2433322e170c16ae,2021,2021-07-20 15:49:46,2021-07-20 15:49:46
YPDTIZ6Y,journalArticle,2019,"Weng, Y.; Zhou, H.",Data augmentation computing model based on generative adversarial network,"The edge intelligent computing technology can reduce the delay and energy consumption of deep learning model reasoning through the collaborative terminal acquisition equipment and edge server. We apply the neural network to the edge computing and build a data augmentation computing model based on the sparse data volume. Then, we get an intelligent generative image after the network training to achieve the effect of enhancement computing. In this paper, we choose a relatively small number of national elements and the generative adversarial network (GAN) as the experimental data calculation set and network model. First, we normalize the preprocessing of the collected data to form the initial sample data set. Second, the model extracts the feature vector by input image to the convolution neural network (CNN) layer. After that, we use a random noise vector z which follows a Gaussian distribution as the initial input of the conditional generative adversarial network (CGAN). The feature vector extracted from the image is taken as a label and a condition constraint of the CGAN to train the parameters of the CGAN. Finally, the trained CGAN model is used to complete the data augmentation computing. A total of 350 samples were collected, and 97 sample images were actually applied for data augmentation. The enhanced data set of this model is as many as 1,700 samples, and it is found that the generated image is of good quality by using this data set for the peak signal-to-noise ratio (PSNR) detection, which is of innovative value in the standard of real samples. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2917207,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066751859&doi=10.1109%2fACCESS.2019.2917207&partnerID=40&md5=6552a15337a033d9642e91765d8fd9c9,2019,2021-07-20 15:49:46,2021-07-20 15:49:46
45DTTIEI,journalArticle,2019,"Hailesellasie, M.T.; Hasan, S.R.",MulNet: A Flexible CNN Processor with Higher Resource Utilization Efficiency for Constrained Devices,"Leveraging deep convolutional neural networks (DCNNs) for various application areas has become a recent inclination of many machine learning practitioners due to their impressive performance. Research trends show that the state-of-the-art networks are getting deeper and deeper and such networks have shown significant performance increase. Deeper and larger neural networks imply the increase in computational intensity and memory footprint. This is particularly a problem for inference-based applications on resource constrained computing platforms. On the other hand, field-programmable gate arrays (FPGAs) are becoming a promising choice in giving hardware solutions for most deep learning implementations due to their high-performance and low-power features. With the rapid formation of various state-of-the-art CNN architectures, a flexible CNN hardware processor that can handle different CNN architectures and yet customize itself to achieve higher resource efficiency and optimum performance is critically important. In this paper, a novel and highly flexible DCNN processor, MulNet, is proposed. MulNet can be used to process most regular state-of-the-art CNN variants aiming at maximizing resource utilization of a target device. A processing core with multiplier and without multiplier is employed to achieve that. We formulated optimum fixed-point quantization format for MulNet by analyzing layer-by-layer quantization error. We also created a power-of-2 quantization for multiplier-free (MF) processing core of MulNet. Both quantizations significantly reduced the memory space needed and the logic consumption in the target device. We utilized Xilinx Zynq SoCs to leverage the one die hybrid (CPU and FPGA) architecture. We devised a scheme that utilizes Zynq processing system (PS) for memory intensive layers and the Zynq programmable logic (PL) for computationally intensive layers. We implemented modified LeNet, CIFAR-10 full, ConvNet processor (CNP), MPCNN, and AlexNet to evaluate MulNet. Our architecture with MF processing cores shows the promising result, by saving 36%-72% on-chip memory and 10%-44% DSP48 IPs, compared to the architecture with cores implemented using the multiplier. Comparison with the state of the art showed a very promising 25- 40\times DSP48 and 25- 29\times on-chip memory reduction with up to 136.9-GOP/s performance and 88.49-GOP/s/W power efficiency. Hence, our results demonstrate that the proposed architecture can be very expedient for resource constrained devices. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2907865,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065072594&doi=10.1109%2fACCESS.2019.2907865&partnerID=40&md5=cdf656891c120206e13849b8d621f7d8,2019,2021-07-20 15:49:46,2021-07-20 15:49:46
MDNSITQN,journalArticle,2021,"Xing, Y.; Lv, C.; Cao, D.; Velenis, E.",Multi-scale driver behavior modeling based on deep spatial-temporal representation for intelligent vehicles,"The mutual understanding between driver and vehicle is critical to the realization of intelligent vehicles and customized interaction interface. In this study, a unified driver behavior modeling system toward multi-scale behavior recognition is proposed to enhance the driver behavior reasoning ability for intelligent vehicles. Specifically, the driver behavior recognition system is designed to simultaneously recognize the driver's physical and mental states based on a deep encoder-decoder framework. The model jointly learns to recognize three driver behaviors with different time scales: mirror checking and facial expression state, and two mental behaviors, including intention and emotion. The encoder module is designed based on a deep convolutional neural network (CNN) to capture spatial information from the input video stream. Then, several decoders for different driver states estimation are proposed with fully-connected (FC) and long short-term memory (LSTM) based recurrent neural networks (RNN). Two naturalistic datasets are used in this study to investigate the model performance, which is a local highway dataset, namely, CranData, and one public dataset from Brain4Cars. Based on the spatial–temporal representation of driver physical behavior, it shows that the observed physical behaviors can be used to model the latent mental behaviors through the proposed end-to-end learning process. The testing results on these two datasets show state-of-the-art results on mirror checking behavior, intention, and emotion recognition. With the proposed system, intelligent vehicles can gain a holistic understanding of the driver's physical and phycological behaviors to better collaborate and interact with the human driver, and the driver behavior reasoning system helps to reduce the conflicts between the human and vehicle automation. © 2021 Elsevier Ltd",Transportation Research Part C: Emerging Technologies,10.1016/j.trc.2021.103288,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109428121&doi=10.1016%2fj.trc.2021.103288&partnerID=40&md5=bd3ba09f09358dd06c6caeb93e33c5dd,2021,2021-07-20 15:49:46,2021-07-20 15:49:46
8Q4G6PH4,journalArticle,2020,"Wu, J.; Liu, X.; Hu, X.; Zhu, J.",PopMNet: Generating structured pop music melodies using neural networks,"Recently, many deep learning models have been proposed to generate symbolic melodies. However, generating pop music melodies with well organized structures remains to be challenging. In this paper, we present a melody structure-based model called PopMNet to generate structured pop music melodies. The melody structure is defined by pairwise relations, specifically, repetition and sequence, between all bars in a melody. PopMNet consists of a Convolutional Neural Network (CNN)-based Structure Generation Net (SGN) and a Recurrent Neural Network (RNN)-based Melody Generation Net (MGN). The former generates melody structures and the latter generates melodies conditioned on the structures and chord progressions. The proposed model is compared with four existing models AttentionRNN, LookbackRNN, MidiNet and Music Transformer. The results indicate that the melodies generated by our model contain much clearer structures compared to those generated by other models, as confirmed by human behavior experiments. © 2020 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2020.103303,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085619634&doi=10.1016%2fj.artint.2020.103303&partnerID=40&md5=99914eea5f7e6d18ce6897e4e1d50474,2020,2021-07-20 15:49:46,2021-07-20 15:49:46
PHNNLLIJ,journalArticle,2020,"Coulibaly, L.; Kamsu-Foguem, B.; Tangara, F.",Rule-based machine learning for knowledge discovering in weather data,"The Climate change trains regularly some phenomena threatening directly the environment and the humanity. In this context, meteorology plays a more important role in the control of these phenomena. It is thus important to search resources allowing to contribute to the improvement of the numerical model for the predictions of weather and climate. The objective of this work is to look for the weaknesses of the models in the simulation of exchanges between the surface and the atmosphere. These exchanges are quantified by sensible and latent heat fluxes. The preprocessing is done through the combined use of k-nearest neighbors algorithm (k-NN) and Autoregressive integrated moving average (ARIMA) model in order to estimate missing values. The processing is performed with the learning of the association rules and the knowledge extracted enables us to make some comparisons between observations and simulations by the numerical model. The postprocessing is made by logical and graphical reasoning that facilitates the visualization of links between the obtained rules. This method is deployed on a database containing measured variables (sensible and latent heat flux, temperature and humidity of the air, wind speed and direction, rain, global radiation, etc.) at the experimental site of the Centre de Recherches Atmosphériques (CRA) which is one of the two sites composing the Pyrenean Plateforme for the Observation of the Atmosphere (P2OA) in France. The obtained and expressed results in the form of association rules have made it possible to highlight that the differences between model and observations from a surface flux point of view are often concomitant with an important difference on global radiation. The expected profits are relative to the generation of knowledge useful for the improvement in the quality of the prediction with a better analysis of the important concomitant factors during errors on a weather model. © 2020 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2020.03.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082397453&doi=10.1016%2fj.future.2020.03.012&partnerID=40&md5=45afb9653cf70a420d3ac8b8c53191d9,2020,2021-07-20 15:49:46,2021-07-20 15:49:46
RY44CFEV,journalArticle,2011,"Tian, D.; Zeng, X.-J.; Keane, J.",Core-generating approximate minimum entropy discretization for rough set feature selection in pattern classification,"Rough set feature selection (RSFS) can be used to improve classifier performance. RSFS removes redundant attributes whilst retaining important ones that preserve the classification power of the original dataset. Reducts are feature subsets selected by RSFS. Core is the intersection of all the reducts of a dataset. RSFS can only handle discrete attributes, hence, continuous attributes need to be discretized before being input to RSFS. Discretization determines the core size of a discrete dataset. However, current discretization methods do not consider the core size during discretization. Earlier work has proposed core-generating approximate minimum entropy discretization (C-GAME) algorithm which selects the maximum number of minimum entropy cuts capable of generating a non-empty core within a discrete dataset. The contributions of this paper are as follows: (1) the C-GAME algorithm is improved by adding a new type of constraint to eliminate the possibility that only a single reduct is present in a C-GAME-discrete dataset; (2) performance evaluation of C-GAME in comparison to C4.5, multi-layer perceptrons, RBF networks and k-nearest neighbours classifiers on ten datasets chosen from the UCI Machine Learning Repository; (3) performance evaluation of C-GAME in comparison to Recursive Minimum Entropy Partition (RMEP), Chimerge, Boolean Reasoning and Equal Frequency discretization algorithms on the ten datasets; (4) evaluation of the effects of C-GAME and the other four discretization methods on the sizes of reducts; (5) an upper bound is defined on the total number of reducts within a dataset; (6) the effects of different discretization algorithms on the total number of reducts are analysed; (7) performance analysis of two RSFS algorithms (a genetic algorithm and Johnson's algorithm). © 2011 Elsevier Inc. All rights reserved.",International Journal of Approximate Reasoning,10.1016/j.ijar.2011.03.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955557689&doi=10.1016%2fj.ijar.2011.03.001&partnerID=40&md5=2747672334f6c3eb3ccb5f8a913b062e,2011,2021-07-20 15:49:46,2021-07-20 15:49:46
FH3M3IR6,journalArticle,2016,"Arriaga, J.G.; Sanchez, H.; Vallejo, E.E.; Hedley, R.; Taylor, C.E.",Identification of Cassin[U+05F3]s Vireo (Vireo cassinii) individuals from their acoustic sequences using an ensemble of learners,"The ability to identify individual birds can provide us with information on behavioral interactions between individuals and ecological interactions with the environment. Identifying individual animals has traditionally been a time - and cost - intensive exercise in the field. Most past efforts dedicated towards individual identification from its vocalizations have centered on the analysis of acoustic features. Here we present an alternative approach to this task, using an ensemble of learners to identify individual Cassin's Vireo from the structural properties of their vocalizations, using symbolic representations of its syntactic elements instead of the acoustic characteristics of the signal. We also test the ability of this ensemble of learners to identify individuals within a year and across years. We propose a new learner combination that confers the ensemble with the ability to handle outliers - unknown individuals not seen during training. After being trained with 9 individuals from one year (2014), the ensemble achieved 96% accuracy identifying samples from 13 individual birds from the same year and 95% with sequences from 8 individual birds from a previous year. Predicting individuals in one year using recordings from other years indicates effective generalization capabilities of the ensemble. © 2015 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2015.05.129,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962669642&doi=10.1016%2fj.neucom.2015.05.129&partnerID=40&md5=c721de700488e0451f59f77fb7ffa8b0,2016,2021-07-20 15:49:46,2021-07-20 15:49:46
N9RYRWWR,journalArticle,2019,"Hermessi, H.; Mourali, O.; Zagrouba, E.",Deep feature learning for soft tissue sarcoma classification in MR images via transfer learning,"Medical image analysis is motivated by deep learning emergence and computation power increase. Meanwhile, relevant deep features can significantly enhance learnable expert and intelligent systems performance and reduce diagnosis time and arduousness. This paper presents a deep learning-based radiomics framework for aided diagnosis of soft tissue sarcomas of the extremities. MR Images with histologically confirmed Liposarcoma (LPS) and Leiomyosarcomas (LMS) have been retrieved from the Cancer Imaging Archives database and pre-processed to recuperate ROIs from MR scans with delineated tumors. This study investigates the significance and impact of medical image fusion on deep feature learning based on transfer learning from the natural domain to the medical domain. Towards this end, we propose to fuse T1 with T2FS or STIR modalities using type-2 fuzzy sets in the non-subsampled shearlet domain. Being decomposed, low-frequency sub-images were selected using local energy and type-2 fuzzy entropy, while high frequencies were selected according to the maximum of the absolute value. Experimental results indicated that the proposed fusion framework outperformed the state-of-the-art fuzzy logic-based fusion techniques in terms of entropy and mutual information. Accordingly, we fine-tuned the pre-trained AlexNet deep convolutional neural network (CNN) with stochastic gradient descent (SGD). First, with the pre-processed dataset, and second with the fused images. As a result, the average classification accuracy using the augmented training data by image rotation and flipping was 97.17% with the raw data and 98.28% with the fused images, which highlighted the usefulness of complementary information for deep feature learning. One crucial concern was to investigate the depth of knowledge transferability. We incrementally fine-tuned the pre-trained CNN to assess the required level that achieves performance improvements in STS classification. Through layer-wise fine-tuning, our study further confirms the potential of middle and deep layers in performance improvement. Moreover, the transferability was concluded better than random weights. With the encouragement of classification results, our aided diagnosis framework may be in the pipeline to assist radiologists in classifying LPS and LMS. © 2018 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2018.11.025,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056853631&doi=10.1016%2fj.eswa.2018.11.025&partnerID=40&md5=8255f250ec38822652bb63e678d115be,2019,2021-07-20 15:49:46,2021-07-20 15:49:46
VLGR2Z6E,journalArticle,2019,"Barua, S.; Ahmed, M.U.; Ahlström, C.; Begum, S.","Automatic driver sleepiness detection using EEG, EOG and contextual information","The many vehicle crashes that are caused by driver sleepiness each year advocates the development of automated driver sleepiness detection (ADSD) systems. This study proposes an automatic sleepiness classification scheme designed using data from 30 drivers who repeatedly drove in a high-fidelity driving simulator, both in alert and in sleep deprived conditions. Driver sleepiness classification was performed using four separate classifiers: k-nearest neighbours, support vector machines, case-based reasoning, and random forest, where physiological signals and contextual information were used as sleepiness indicators. The subjective Karolinska sleepiness scale (KSS) was used as target value. An extensive evaluation on multiclass and binary classifications was carried out using 10-fold cross-validation and leave-one-out validation. With 10-fold cross-validation, the support vector machine showed better performance than the other classifiers (79% accuracy for multiclass and 93% accuracy for binary classification). The effect of individual differences was also investigated, showing a 10% increase in accuracy when data from the individual being evaluated was included in the training dataset. Overall, the support vector machine was found to be the most stable classifier. The effect of adding contextual information to the physiological features improved the classification accuracy by 4% in multiclass classification and by and 5% in binary classification. © 2018 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2018.07.054,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051410923&doi=10.1016%2fj.eswa.2018.07.054&partnerID=40&md5=f78c2b5cdfca2365cde1cf8c65e572cb,2019,2021-07-20 15:49:46,2021-07-20 15:49:46
CDKGQBNE,journalArticle,2015,"Chebel-Morello, B.; Haouchine, M.K.; Zerhouni, N.",Case-based maintenance: Structuring and incrementing the case base,"To avoid performance degradation and maintain the quality of results obtained by the case-based reasoning (CBR) systems, maintenance becomes necessary, especially for those systems designed to operate over long periods and which must handle large numbers of cases. CBR systems cannot be preserved without scanning the case base. For this reason, the latter must undergo maintenance operations. The techniques of case base's dimension optimization is the analog of instance reduction size methodology (in the machine learning community). This study links these techniques by presenting case-based maintenance in the framework of instance based reduction, and provides: first an overview of CBM studies, second, a novel method of structuring and updating the case base and finally an application of industrial case is presented. The structuring combines a categorization algorithm with a measure of competence CM based on competence and performance criteria. Since the case base must progress over time through the addition of new cases, an auto-increment algorithm is installed in order to dynamically ensure the structuring and the quality of a case base. The proposed method was evaluated through a case base from an industrial plant. In addition, an experimental study of the competence and the performance was undertaken on reference benchmarks. This study showed that the proposed method gives better results than the best methods currently found in the literature. © 2015 Elsevier B.V. All rights reserved.",Knowledge-Based Systems,10.1016/j.knosys.2015.07.034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941599462&doi=10.1016%2fj.knosys.2015.07.034&partnerID=40&md5=9bdc7677be1fef220acfdf7342ecf576,2015,2021-07-20 15:49:46,2021-07-20 15:49:46
V7PA757C,journalArticle,2012,"Malof, J.M.; Mazurowski, M.A.; Tourassi, G.D.",The effect of class imbalance on case selection for case-based classifiers: An empirical study in the context of medical decision support,"Case selection is a useful approach for increasing the efficiency and performance of case-based classifiers. Multiple techniques have been designed to perform case selection. This paper empirically investigates how class imbalance in the available set of training cases can impact the performance of the resulting classifier as well as properties of the selected set. In this study, the experiments are performed using a dataset for the problem of detecting breast masses in screening mammograms. The classification problem was binary and we used a k-nearest neighbor classifier. The classifier's performance was evaluated using the Receiver Operating Characteristic (ROC) area under the curve (AUC) measure. The experimental results indicate that although class imbalance reduces the performance of the derived classifier and the effectiveness of selection at improving overall classifier performance, case selection can still be beneficial, regardless of the level of class imbalance. © 2011 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2011.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82355169734&doi=10.1016%2fj.neunet.2011.07.002&partnerID=40&md5=2d7347f0ae8c39d0917c90c4ddb0c64b,2012,2021-07-20 15:49:46,2021-07-20 15:49:46
RXPU8Y6G,journalArticle,2021,"Haasler, I.; Singh, R.; Zhang, Q.; Karlsson, J.; Chen, Y.",Multi-marginal optimal transport and probabilistic graphical models,"We study multi-marginal optimal transport problems from a probabilistic graphical model perspective. We point out an elegant connection between the two when the underlying cost for optimal transport allows a graph structure. In particular, an entropy regularized multi-marginal optimal transport is equivalent to a Bayesian marginal inference problem for probabilistic graphical models with the additional requirement that some of the marginal distributions are specified. This relation on the one hand extends the optimal transport as well as the probabilistic graphical model theories, and on the other hand leads to fast algorithms for multi-marginal optimal transport by leveraging the well-developed algorithms in Bayesian inference. Several numerical examples are provided to highlight the results. IEEE",IEEE Transactions on Information Theory,10.1109/TIT.2021.3077465,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105605651&doi=10.1109%2fTIT.2021.3077465&partnerID=40&md5=f8f69e219e1defa36dc65f3d2f367743,2021,2021-07-20 15:49:46,2021-07-20 15:49:46
HXGDZZV5,journalArticle,2019,"Ghanem, R.G.; Soize, C.; Safta, C.; Huan, X.; Lacaze, G.; Oefelein, J.C.; Najm, H.N.",Design optimization of a scramjet under uncertainty using probabilistic learning on manifolds,"We demonstrate, on a scramjet combustion problem, a constrained probabilistic learning approach that augments physics-based datasets with realizations that adhere to underlying constraints and scatter. The constraints are captured and delineated through diffusion maps, while the scatter is captured and sampled through a projected stochastic differential equation. The objective function and constraints of the optimization problem are then efficiently framed as non-parametric conditional expectations. Different spatial resolutions of a large-eddy simulation filter are used to explore the robustness of the model to the training dataset and to gain insight into the significance of spatial resolution on optimal design. © 2019 Elsevier Inc.",Journal of Computational Physics,10.1016/j.jcp.2019.108930,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072205741&doi=10.1016%2fj.jcp.2019.108930&partnerID=40&md5=7d57d52ce0f40034f4f444fbe59432d4,2019,2021-07-20 15:49:46,2021-07-20 15:49:46
SUR85VZL,journalArticle,2019,"Melucci, M.",An Efficient Algorithm to Compute a Quantum Probability Space,"Learning algorithms based on probability organize the observed data in subsets corresponding to binary variables. In this paper, we address the problem of estimating one probability space given a set of observed data about nn variables or properties. One problem with estimating one single probability space is the exponential number of events. Approximation is one approach to addressing the problem of the exponential order of the number of events. Alternatively to approximation, we change paradigm - from classical, set-based probability spaces based on sets to quantum probability spaces based on vector subspaces. By changing paradigm, we leverage quantum probability and present an efficient algorithm to calculate a Quantum Probability Space (QPS) in only O(n4)O(n4) steps. © 2018 IEEE.",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2018.2863709,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051396367&doi=10.1109%2fTKDE.2018.2863709&partnerID=40&md5=0b519563bbda858454ee068e8bfc86af,2019,2021-07-20 15:49:47,2021-07-20 15:49:47
VGNQGC76,journalArticle,2018,"Pan, I.; Bester, D.",Fuzzy Bayesian Learning,"In this paper, we propose a novel approach for learning from data using rule-based fuzzy inference systems where the model parameters are estimated using Bayesian inference and Markov Chain Monte Carlo techniques. We show the applicability of the method for regression and classification tasks using synthetic datasets and also a real world example in the financial services industry. Then, we demonstrate how the method can be extended for knowledge extraction to select the individual rules in a Bayesian way which best explains the given data. Finally, we discuss the advantages and pitfalls of using this method over state-of-the-art techniques and highlight the specific class of problems where this would be useful. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2017.2746064,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028732547&doi=10.1109%2fTFUZZ.2017.2746064&partnerID=40&md5=5c642f83291f4b720e32b1490e065a7b,2018,2021-07-20 15:49:47,2021-07-20 15:49:47
PPC7X7Q3,journalArticle,2016,"Liu, Y.; Li, C.; Huang, T.; Wang, X.",Robust adaptive lag synchronization of uncertain fuzzy memristive neural networks with time-varying delays,"This paper investigates the problem of robust adaptive lag synchronization of uncertain fuzzy memristive neural networks with time-varying delays. The memristive neural networks combining fuzzy model with uncertain parameters are described at length. Based on the Lyapunov functional method and novel update laws for parameter uncertainties and controller gain, some sufficient conditions which guarantee the slave systems robust adaptive lag synchronized with the master systems are obtained. Finally, two simulated examples are presented to show the effectiveness and applications of the obtained results. © 2016 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2016.01.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962044996&doi=10.1016%2fj.neucom.2016.01.018&partnerID=40&md5=4e886646133a972b838f80f4031eae10,2016,2021-07-20 15:49:47,2021-07-20 15:49:47
WENU9MQH,journalArticle,2021,"Zhao, H.; Liu, Z.; Yao, X.; Yang, Q.",A machine learning-based sentiment analysis of online product reviews with a novel term weighting and feature selection approach,"Recently, online shopping has turned into a mainstream means for users to purchase as well as consume with the upsurge development of Internet technology. User satisfaction can be improved effectively by doing Sentiment Analysis (SA) of a large quantity of user reviews on e-commerce platforms. It is still challenging to envisage the accurate sentiment polarities of the user reviews because of the changes in sequence length, textual order, along with complicated logic. This paper proposes a new optimized Machine Learning (ML) algorithm called the Local Search Improvised Bat Algorithm based Elman Neural Network (LSIBA-ENN) for the SA of online product reviews. The proposed work of SA encompasses ‘4’ major steps: i) Data Collection (DC), ii) preprocessing, iii) Features Extraction (FE) or Term Weighting (TW), Feature Selection (FS), and polarity or Sentiment Classifications (SC). Initially, the Web Scrapping Tool (WST) is utilized to extract the customer reviews of the products for which the data is gathered as of the E-commerce websites. Next, preprocessing is carried out on the web scrap extracted data. Those preprocessed data go through TW and FS for additional processing by means of Log Term Frequency-based Modified Inverse Class Frequency (LTF-MICF) and Hybrid Mutation based Earth Warm Algorithm (HMEWA). Lastly, the HM-EWA data is rendered to the LSIBA-ENN, which classifies the customer reviews’ sentiment as positive, negative, and neutral. For the performance analysis of the proposed and prevailing classifiers, ‘2’ yardstick datasets are taken. The outcomes exhibit that the LSIBA-ENN attains the best performance in SC when weighted against the existing top-notch algorithms. The observations of the reviewer are exact. The prevailing ENN proffers recall of 87.79 when utilizing the proposed LTF-MICF scheme, whereas ENN only achieve 83.55, 84.03, 85.48, and 86.04 of recall whilst utilizing W2V, TF, TF-IDF, and TF-DFS schemes respectively. © 2021 Elsevier Ltd",Information Processing and Management,10.1016/j.ipm.2021.102656,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107911902&doi=10.1016%2fj.ipm.2021.102656&partnerID=40&md5=de3d8f340a6b44918422d9f2c8b2716e,2021,2021-07-20 15:49:47,2021-07-20 15:49:47
EMEZ44K2,journalArticle,2020,"Khalid, F.; Hasan, S.R.; Zia, S.; Hasan, O.; Awwad, F.; Shafique, M.",MacLeR: Machine Learning-Based Runtime Hardware Trojan Detection in Resource-Constrained IoT Edge Devices,"Traditional learning-based approaches for runtime hardware Trojan (HT) detection require complex and expensive on-chip data acquisition frameworks, and thus incur high area and power overhead. To address these challenges, we propose to leverage the power correlation between the executing instructions of a microprocessor to establish a machine learning (ML)-based runtime HT detection framework, called MacLeR. To reduce the overhead of data acquisition, we propose a single power-port current acquisition block using current sensors in time-division multiplexing, which increases accuracy while incurring reduced area overhead. We have implemented a practical solution by analyzing multiple HT benchmarks inserted in the RTL of a system-on-chip (SoC) consisting of four LEON3 processors integrated with other IPs, such as vga_lcd, RSA, AES, Ethernet, and memory controllers. Our experimental results show that compared to state-of-the-art HT detection techniques, MacLeR achieves 10% better HT detection accuracy (i.e., 96.256%) while incurring a $7\times $ reduction in area and power overhead (i.e., 0.025% of the area of the SoC and < 0.07% of the power of the SoC). In addition, we also analyze the impact of process variation (PV) and aging on the extracted power profiles and the HT detection accuracy of MacLeR. Our analysis shows that variations in fine-grained power profiles due to the HTs are significantly higher compared to the variations in fine-grained power profiles caused by the PVs and aging effects. Moreover, our analysis demonstrates that on average, the HT detection accuracy drops in MacLeR is less than 1% and 9% when considering only PV and PV with worst case aging, respectively, which is $\approx 10\times $ less than in the case of the state-of-the-art ML-based HT detection technique. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.3012236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096033010&doi=10.1109%2fTCAD.2020.3012236&partnerID=40&md5=82f18bc2095cbdaa4bf3ac405bb1843e,2020,2021-07-20 15:49:47,2021-07-20 15:49:47
NCLGD6EU,journalArticle,2018,"Alizadehsani, R.; Hosseini, M.J.; Khosravi, A.; Khozeimeh, F.; Roshanzamir, M.; Sarrafzadegan, N.; Nahavandi, S.",Non-invasive detection of coronary artery disease in high-risk patients based on the stenosis prediction of separate coronary arteries,"Background and objective: Cardiovascular diseases are an extremely widespread sickness and account for 17 million deaths in the world per annum. Coronary artery disease (CAD) is one of such diseases with an annual mortality rate of about 7 million. Thus, early diagnosis of CAD is of vital importance. Angiography is currently the modality of choice for the detection of CAD. However, its complications and costs have prompted researchers to seek alternative methods via machine learning algorithms. Methods: The present study proposes a novel machine learning algorithm. The proposed algorithm uses three classifiers for detection of the stenosis of three coronary arteries, i.e., left anterior descending (LAD), left circumflex (LCX) and right coronary artery (RCA) to get higher accuracy for CAD diagnosis. Results: This method was applied on the extension of Z-Alizadeh Sani dataset which contains demographic, examination, ECG, and laboratory and echo data of 500 patients. This method achieves an accuracy, sensitivity and specificity rates of 96.40%, 100% and 88.1%, respectively for the detection of CAD. To our knowledge, such high rates of accuracy and sensitivity have not been attained elsewhere before. Conclusion: This new algorithm reliably distinguishes those with normal coronary arteries from those with CAD which may obviate the need for angiography in the normal group. © 2018 Elsevier B.V.",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2018.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047058221&doi=10.1016%2fj.cmpb.2018.05.009&partnerID=40&md5=189a19c0ec7cf1e0098be099c78bdfdd,2018,2021-07-20 15:49:47,2021-07-20 15:49:47
IBWIWRRP,journalArticle,2018,"Pahlevan, A.; Qu, X.; Zapater, M.; Atienza, D.",Integrating Heuristic and Machine-Learning Methods for Efficient Virtual Machine Allocation in Data Centers,"Modern cloud data centers (DCs) need to tackle efficiently the increasing demand for computing resources and address the energy efficiency challenge. Therefore, it is essential to develop resource provisioning policies that are aware of virtual machine (VM) characteristics, such as CPU utilization and data communication, and applicable in dynamic scenarios. Traditional approaches fall short in terms of flexibility and applicability for large-scale DC scenarios. In this paper, we propose a heuristic- and a machine learning (ML)-based VM allocation method and compare them in terms of energy, quality of service (QoS), network traffic, migrations, and scalability for various DC scenarios. Then, we present a novel hyper-heuristic algorithm that exploits the benefits of both methods by dynamically finding the best algorithm, according to a user-defined metric. For optimality assessment, we formulate an integer linear programming (ILP)-based VM allocation method to minimize energy consumption and data communication, which obtains optimal results, but is impractical at runtime. Our results demonstrate that the ML approach provides up to 24% server-to-server network traffic improvement and reduces execution time by up to 480 × compared to conventional approaches, for large-scale scenarios. On the contrary, the heuristic outperforms the ML method in terms of energy and network traffic for reduced scenarios. We also show that the heuristic and ML approaches have up to 6% energy consumption overhead compared to ILP-based optimal solution. Our hyper-heuristic integrates the strengths of both the heuristic and the ML methods by selecting the best one during runtime. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2017.2760517,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031774157&doi=10.1109%2fTCAD.2017.2760517&partnerID=40&md5=ed06c75529660919029ceb200d260493,2018,2021-07-20 15:49:47,2021-07-20 15:49:47
AQD3DH8Z,journalArticle,2020,"George, S.; Santra, A.K.",Fuzzy Inspired Deep Belief Network for the Traffic Flow Prediction in Intelligent Transportation System Using Flow Strength Indicators,"Intelligent transportation system (ITS) is an advance leading edge technology that aims to deliver innovative services to different modes of transport and traffic management. Traffic flow prediction (TFP) is one of the key macroscopic parameters of traffic that supports traffic management in ITS. Growth of the real-time data in transportation from various modern equipments, technology, and other resources has led to generate big data, posing a huge concern to deal with. Recently, deep learning (DL) techniques have demonstrated the capability to extract comprehensive features efficiently, using multiple hidden layers, from such huge raw, unstructured, and nonlinear data. Nonlinearity in traffic data is the major cause of inaccuracy in TFP. In this article, we propose a flow strength indicator-based Chronological Dolphin Echolocation-Fuzzy, a bioinspired optimization method with fuzzy logic for incremental learning of deep belief network. Technical indicators provide flow strength features as an input to the model. Hidden layers of DL architecture consequently learn more features and propagate it as an input to next layer for supervised learning. The degree of membership to the features is identified by the membership functions, followed by weight optimization using Dolphin Echolocation algorithm to fit the model for the nonlinear data. Experiments performed on two different data sets, namely Traffic-major roads and performance measurement system-San Francisco (PEMS-SF), show good results for the proposed deep architecture. The analysis of the proposed method using log mean square error and log root mean square deviation acquires a minimum value of 2.4141 and 0.61 for the Traffic-major roads database taken for the time step duration of 1 year and a minimum value of 1.6691 and 0.5208 for PEMS-SF data set for the time step interval of 5 minutes, respectively. These positive results demonstrate key importance of our traffic flow model for the transportation system. © 2020, Mary Ann Liebert, Inc., publishers.",Big Data,10.1089/big.2019.0007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089808019&doi=10.1089%2fbig.2019.0007&partnerID=40&md5=4f2bda87b3a868a9916a1d38c8b2d7ea,2020,2021-07-20 15:49:47,2021-07-20 15:49:47
FRD963LA,journalArticle,2019,"Torrent-Fontbona, F.; Lopez, B.",Personalized adaptive CBR bolus recommender system for type 1 diabetes,"Type 1 diabetes mellitus (T1DM) is a chronic disease. Those who have it must administer themselves with insulin to control their blood glucose level. It is difficult to estimate the correct insulin dosage due to the complex glucose metabolism, which can lead to less than optimal blood glucose levels. This paper presents PepperRec, a case-based reasoning (CBR) bolus insulin recommender system capable of dealing with an unrestricted number of situations in which T1DM persons can find themselves. PepperRec considers several factors that affect glucose metabolism, such as data about the physical activity of the user, and can also cope with missing values for these factors. Based on CBR methodology, PepperRec uses new methods to adapt past recommendations to the current state of the user, and retains updated historical patient information to deal with slow and gradual changes in the patient over time (concept drift). The proposed approach is tested using the UVA/PADOVA simulator with 33 virtual subjects and compared with other methods in the literature, and with the default insulin therapy of the simulator. The achieved results demonstrate that PepperRec increases the amount of time the users are in their target glycaemic range, reduces the time spent below it, while maintaining, or even reducing, the time spent above it. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2018.2813424,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043465949&doi=10.1109%2fJBHI.2018.2813424&partnerID=40&md5=fc5a2f6332d075487056ea3d40d13643,2019,2021-07-20 15:49:47,2021-07-20 15:49:47
ZXWBZP3U,journalArticle,2021,"Dattachaudhuri, A.; Biswas, S.K.; Chakraborty, M.; Sarkar, S.",A transparent rule-based expert system using neural network,"Classification is one of the foremost machine learning tasks in this modern era. Neural Network (NN) is one of the powerful classification techniques. NN can achieve high classification accuracy on highly imbalanced and complex datasets, but lacks in explanation of its reasoning process which limits its applicability in various domains which require transparent decision along with good accuracy. There are some techniques which extract rules from NN and make it transparent; however, attribute pruning, rule pruning and class overlap algorithms are not sufficiently effective. Therefore, this paper proposes a rule extraction algorithm, called Transparent Rule Extraction using Neural Network (TRENN) to convert NN into white box with greater emphasis on attribute pruning and rule pruning. The proposed TRENN is a pedagogical approach and an extension of one of the existing algorithms named Rule Extraction from Neural Network using Classified and Misclassified data (RxNCM). The proposed TRENN extends the RxNCM with sequential floating backward search for feature and rule selection to improve the comprehensibility of the generated rules. Besides, the proposed TRENN uses probabilistic approach for the treatment of class overlapping problem in the rule updating phase instead of reclassification used in RxNCM where the overlap may persist. Experiments are conducted with eight real datasets collected from the UCI repository. Performance of the TRENN is measured with Precision, Recall, FP-Rate, F-measure, and local and global comprehensibility. It is observed from the experimental results that TRENN performs better than Re-RX, RxNCM and RxREN. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH, DE part of Springer Nature.",Soft Computing,10.1007/s00500-020-05547-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099871566&doi=10.1007%2fs00500-020-05547-7&partnerID=40&md5=0c6b47adaf45deb181742b9b1d4ca53f,2021,2021-07-20 15:49:47,2021-07-20 15:49:47
7K8EDLPY,journalArticle,2020,"Salawu, S.; He, Y.; Lumsden, J.",Approaches to Automated Detection of Cyberbullying: A Survey,"Research into cyberbullying detection has increased in recent years, due in part to the proliferation of cyberbullying across social media and its detrimental effect on young people. A growing body of work is emerging on automated approaches to cyberbullying detection. These approaches utilise machine learning and natural language processing techniques to identify the characteristics of a cyberbullying exchange and automatically detect cyberbullying by matching textual data to the identified traits. In this paper, we present a systematic review of published research (as identified via Scopus, ACM and IEEE Xplore bibliographic databases) on cyberbullying detection approaches. On the basis of our extensive literature review, we categorise existing approaches into 4 main classes, namely supervised learning, lexicon-based, rule-based, and mixed-initiative approaches. Supervised learning-based approaches typically use classifiers such as SVM and Naïve Bayes to develop predictive models for cyberbullying detection. Lexicon-based systems utilise word lists and use the presence of words within the lists to detect cyberbullying. Rule-based approaches match text to predefined rules to identify bullying, and mixed-initiatives approaches combine human-based reasoning with one or more of the aforementioned approaches. We found lack of labelled datasets and non-holistic consideration of cyberbullying by researchers when developing detection systems are two key challenges facing cyberbullying detection research. This paper essentially maps out the state-of-the-art in cyberbullying detection research and serves as a resource for researchers to determine where to best direct their future research efforts in this field. © 2010-2012 IEEE.",IEEE Transactions on Affective Computing,10.1109/TAFFC.2017.2761757,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031765182&doi=10.1109%2fTAFFC.2017.2761757&partnerID=40&md5=6dc63ff1c99bd06022b96705b1cbf3d1,2020,2021-07-20 15:49:47,2021-07-20 15:49:47
67WP7N8J,journalArticle,2019,"Hu, J.; Fang, J.; Du, Y.; Liu, Z.; Ji, P.",A security risk plan search assistant decision algorithm using deep neural network combined with two-stage similarity calculation,"In view of the nonlinearity and uncertainty of safety accident risk assessment, firstly, based on the deep neural network, the training criterion of the network is changed, and the triplet convolutional neural network with the similarity measure as the cost function is proposed. The inactive multi-scale set features are extracted from them, so that the semantic features obtained by learning are suitable for security risk image retrieval. In the image retrieval application, the training samples of the retrieved data set are not enough to train a large network, and the innovative application of migration learning to security risk image retrieval proposes to train the network with data sets similar to the retrieved data sets. Then based on the traditional nearest neighbor algorithm, this paper proposes a case similarity calculation method based on two-dimensional structure of structural similarity and attribute similarity, input the characteristic attribute value of the current emergency event, and conduct similar case retrieval. The final calculation returns the historical case and its solution that the user is most similar to the currently entered incident feature. The experiment proves that the maximum relative error between the output of the network and the expected output value is 5.17%, and the minimum relative error is 1.38%, which has high accuracy. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Personal and Ubiquitous Computing,10.1007/s00779-019-01236-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066334547&doi=10.1007%2fs00779-019-01236-x&partnerID=40&md5=f37233c98076399a724285ee1621a773,2019,2021-07-20 15:49:48,2021-07-20 15:49:48
WQ2MK3W6,journalArticle,2017,"Samek, W.; Binder, A.; Montavon, G.; Lapuschkin, S.; Müller, K.-R.",Evaluating the visualization of what a deep neural network has learned,"Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the 'importance' of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance. © 2016 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2016.2599820,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983621562&doi=10.1109%2fTNNLS.2016.2599820&partnerID=40&md5=c3ea2cc0981d5fe2e799795cfa63315b,2017,2021-07-20 15:49:48,2021-07-20 15:49:48
VB7U43PZ,journalArticle,2014,"Bashar, A.; Parr, G.; McClean, S.; Scotney, B.; Nauck, D.",Application of bayesian networks for autonomic network management,"The ever evolving telecommunication networks in terms of their technology, infrastructure, and supported services have always posed challenges to the network managers to come up with an efficient Network Management System (NMS) for effective network management. The need for automated and efficient management of the current networks, more specifically the Next Generation Network (NGN), is the subject addressed in this research. A detailed description of the management challenges in the context of current networks is presented and then this work enlists the desired features and characteristics of an efficient NMS. It then proposes that there is a need to apply Artificial Intelligence (AI) and Machine Learning (ML) approaches for enhancing and automating the functions of NMS. The first contribution of this work is a comprehensive survey of the AI and ML approaches applied to the domain of NM. The second contribution of this work is that it presents the reasoning and evidence to support the choice of Bayesian Networks (BN) as a viable solution for ML-based NMS. The final contribution of this work is that it proposes and implements three novel NM solutions based on the BN approach, namely BN-based Admission Control (BNAC), BN-based Distributed Admission Control (BNDAC) and BN-based Intelligent Traffic Engineering (BNITE), along with the description of algorithms underpinning the proposed framework. © 2013 Springer Science+Business Media New York.",Journal of Network and Systems Management,10.1007/s10922-013-9289-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897602196&doi=10.1007%2fs10922-013-9289-x&partnerID=40&md5=f62cf3ba5cfce54b8433e8444f75579b,2014,2021-07-20 15:49:48,2021-07-20 15:49:48
DFUTAX4J,journalArticle,2014,"Khater, M.; Murariu, D.; Gras, R.",Contemporary evolution and genetic change of prey as a response to predator removal,"The ecological effects of predator removal and its consequence on prey behavior have been investigated widely; however, predator removal can also cause contemporary evolution of prey resulting in prey genetic change. Here we tested the role of predator removal on the contemporary evolution of prey traits such as movement, reproduction and foraging. We use EcoSim simulation which allows complex intra- and inter-specific interactions, based on individual evolving behavioral models, as well as complex predator-prey dynamics and coevolution in spatially homogenous and heterogeneous worlds. We model organisms' behavior using fuzzy cognitive maps (FCM) that are coded in their genomes which has a clear semantics making reasoning about causality of any evolved behavior possible. We show that the contemporary evolution of prey behavior owing to predator removal is also accompanied by prey genetic change. We employed machine learning methods, now recognized as holding great promise for the advancement of our understanding and prediction of ecological phenomena. A classification algorithm was used to demonstrate the difference between genomes belonging to prey coevolving with predators and prey evolving in the absence of predation pressure. We argue that predator introductions to naive prey might be destabilizing if prey have evolved and adapted to the absence of predators. Our results suggest that both predator introductions and predator removal from an ecosystem have widespread effects on the survival and evolution of prey by altering their genomes and behavior, even after relatively short time intervals. Our study highlights the need to consider both ecological and evolutionary time scales, as well as the complex interplay of behaviors between trophic levels, in determining the outcomes of predator-prey interactions. © 2014 Elsevier B.V.",Ecological Informatics,10.1016/j.ecoinf.2014.02.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899838593&doi=10.1016%2fj.ecoinf.2014.02.005&partnerID=40&md5=846a4698c8d3ba273c08a5f3d540b218,2014,2021-07-20 15:49:48,2021-07-20 15:49:48
N4JQNWVX,journalArticle,2021,"Opiyo, S.; Okinda, C.; Zhou, J.; Mwangi, E.; Makange, N.",Medial axis-based machine-vision system for orchard robot navigation,"In this paper, we propose a novel medial axis-based machine vision technique for autonomous path tracking and navigation of an agricultural robot in orchards. In the proposed method, the raw color image captured by the onboard camera is converted to grayscale using the color index of vegetation extraction. The gray image is run through Gabor filters to extract the texture features before applying Principal Component Analysis technique to minimize the computational load during path coordinate extraction. The resulting image is clustered using K-means clustering algorithm with k = 2 as proved best based on Silhouette method. Navigation path is then extracted using medial axis algorithm from the binary image generated after k-means clustering. Fuzzy logic controller with two inputs, heading and offset, is used to smoothly track the medial axis during navigation. Experiment is carried out in an orchard environment to gauge the performance of the system. The results of field experiment clearly demonstrate that the proposed medial axis technique has the potential to accurately extract guidance path for robot navigation. The navigation performance of the system is quite satisfactory with a maximum trajectory tracking error and standard deviation of 14.6 mm and 6.8 mm respectively. The average root mean square error (RMSE) for the lateral deviation was 45.3 mm. © 2021 Elsevier B.V.",Computers and Electronics in Agriculture,10.1016/j.compag.2021.106153,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104879321&doi=10.1016%2fj.compag.2021.106153&partnerID=40&md5=9f79b275f907c4723dccf5c8e9f4eb43,2021,2021-07-20 15:49:48,2021-07-20 15:49:48
AYV5IK7J,journalArticle,2020,"Wen, G.; Philip Chen, C.L.; Li, W.N.",Simplified optimized control using reinforcement learning algorithm for a class of stochastic nonlinear systems,"In this work, a reinforcement learning (RL) based optimized control approach is developed by implementing tracking control for a class of stochastic nonlinear systems with unknown dynamic. The RL is constructed in identifier-actor-critic architecture, where the identifier aims for determining the stochastic system in mean square, the actor aims for executing the control action and the critic aims for evaluating the control performance. In almost all of the published RL-based optimal control, since both actor and critic updating laws are yielded on the basis of implementing gradient descent method to the square of Bellman residual error, these methods are very complex and are performed difficultly. By contrast, the proposed optimized control is obviously simple because the RL algorithm is derived based on the negative gradient of a simple positive function. Furthermore, the proposed approach can remove the assumption of persistence excitation, which is required for most RL based adaptive optimal control. Finally, based on the adaptive identifier, the system stability is proven by using the quadratic Lyapunov function rather than quartic Lyapunov function, which is usually required for most stochastic systems. Simulation further demonstrates that the optimized stochastic approach can achieve the desired control objective. © 2019",Information Sciences,10.1016/j.ins.2019.12.039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077658170&doi=10.1016%2fj.ins.2019.12.039&partnerID=40&md5=40a6188e1e6c9c17d685ff5a16a064c4,2020,2021-07-20 15:49:48,2021-07-20 15:49:48
JL7ESG3L,journalArticle,2015,"Semwal, V.B.; Chakraborty, P.; Nandi, G.C.",Less computationally intensive fuzzy logic (type-1)-based controller for humanoid push recovery,"In this study, we present a new type-1 fuzzy logic-based controller for push recovery by humanoids. The objective of this study is to develop an intelligent controller and to implement biologically inspired push recovery for humanoid robots. The fuzzy inference system takes two crisp values as inputs, which are fuzzified, before a number of rules are applied, and finally the output is defuzzified to convert it into a crisp value. We apply fuzzy rules to our model, which we simulate in an unstructured environment. The objective is to reduce the fuzzy rules and make the fuzzy inference set less computationally intensive and fast, as well as exploiting the advantages of easy trainability and high generalizability. The fuzzy logic-based controller can predict the requisite push recovery strategy and whether the robot will be able to recover or fall. The architecture has a hierarchical design. The first fuzzy inference system (FIS1) is based on two input variables: the force and the direction of motion, where the result depends on the magnitude of the force applied to the body and direction in which the body moves after being pushed. FIS1 can determine small, medium, and large forces in term of roll and pitch effects on the body. These outputs are the input variables employed by FIS2 to predict the push recovery strategy that will be applied, and eventually the robot will be able to recover from a push or fall. The term ""auto-leaning"" from human autonomy is introduced in the context of push recovery. We extend the Gordon model for balancing humanoids using fuzzy logic by considering the effects of roll, pitch, and yaw. This extends earlier research in the field of humanoid push recovery, where the force was only applied in one direction, to studies of pushing in different directions. In a cluttered environment, pushing is a very common experience, from which humans can readily recover whereas humanoid robots cannot. Humanoid robots have more degrees of freedom, thus finding solutions using alternative methods is not a simple task. The novel feature of this study is the introduction of an intuitive fuzzy logic-based learning approach and we show that it is fast and effective. This exhaustive fuzzy logic-based design is the main focus of this study. © 2014 Elsevier B.V.All rights reserved.",Robotics and Autonomous Systems,10.1016/j.robot.2014.09.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84915820260&doi=10.1016%2fj.robot.2014.09.001&partnerID=40&md5=1aee8a8e6ba76813432d7dd256efc27f,2015,2021-07-20 15:49:48,2021-07-20 15:49:48
EW3FXLIC,journalArticle,2014,"Maletzke, A.G.; Lee, H.D.; Alves Batista, G.E.A.P.; Coy, C.S.R.; Fagundes, J.J.; Chung, W.F.",Time series classification with motifs and characteristics,"In the last years, there is a huge increase of interest in application of time series. Virtually all human endeavors create time-oriented data, and the Data Mining community has proposed a large number of approaches to analyze such data. One of the most common tasks in Data Mining is classification, in which each time series should be associated to a class. Empirical evidence has shown that the nearest neighbor rule is very effective to classify time series data. However, the nearest neighbor classifier is unable to provide any form of explanation. In this chapter we describe a novel method to induce classifiers from time series data. Our approach uses standard Machine Learning classifiers using motifs and characteristics as features. We show that our approach can be very effective for classification, providing higher accuracy for most of the data sets used in an empirical evaluation. In addition, when used with symbolic models, such as decision trees, our approach provides very compact decision rules, leveraging knowledge discovery from time series. We also show two case studies with real world medical data.© Springer-Verlag Berlin Heidelberg 2014.",Studies in Computational Intelligence,10.1007/978-3-642-53737-0_8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958536785&doi=10.1007%2f978-3-642-53737-0_8&partnerID=40&md5=5a1d7a44e32d3c725c6f05dd62117e73,2014,2021-07-20 15:49:48,2021-07-20 15:49:48
KYHCHMIY,journalArticle,2019,"Chen, J.; Xiao, Z.; Wang, D.; Long, W.; Bai, J.; Havyarimana, V.",Stay Time Prediction for Individual Stay Behavior,"Stay time is important for understanding people's travel behavior and mobility motivation. In this paper, by leveraging private car trajectory data, we propose a novel systematic approach for implementing stay behavior detection and stay time prediction. Specifically, we first propose a fuzzy logic-based stay detection method for detecting stay behavior in a large-scale private car trajectory dataset. Then, we design a spatiotemporal feature extraction method called clustering and kernel (CaK) by considering the spatial similarity, temporal periodicity and spatiotemporal correlation of stay behavior data. Furthermore, we propose a stay time predictor (STP) based on gradient-boosting regression trees and a long short-term memory network that can estimate the future durations of private car users' stays in various scenarios. We perform extensive experiments based on two real-life trajectory datasets. The experimental results demonstrate that the STP achieves a predictive accuracy (specifically, the root-mean-square error) of 123.94 and R2 of 0.893 for stay time prediction of individual stays. This study provides a new perspective for understanding people's stay behavior. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2940545,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077746646&doi=10.1109%2fACCESS.2019.2940545&partnerID=40&md5=787581ec989b88a14d16bf8b1f760666,2019,2021-07-20 15:49:48,2021-07-20 15:49:48
6BP8LWJA,journalArticle,2018,"Medina-Quero, J.; Zhang, S.; Nugent, C.; Espinilla, M.",Ensemble classifier of long short-term memory with fuzzy temporal windows on binary sensors for activity recognition,"There are approaches that successfully recognize activities of daily living by using a trained classifier on feature vectors created from binary sensor data. Although these approaches have been successful, there are still open issues such as the evaluation of multiple temporal windows, ensembles of classifiers or unbalanced classes which need to be addressed in order to improve the performance of the real-time activity recognition process. In this paper, we present a methodology for Real-Time Activity Recognition based on the diverse fields of Machine Learning, including Fuzzy Logic and Recurrent Neural Networks. The methodology uses a long-term and short-term representation of binary-sensor activations based on Fuzzy Temporal Windows. The paper proposes an ensemble of activity-based classifiers for the purposes of balanced training, where each classifier in the ensemble is a Long Short-Term Memory. The approach was evaluated using two binary-sensor datasets of daily living activities and benchmarked against previous approaches based on the combination of sensor activation features. © 2018 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2018.07.068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051102091&doi=10.1016%2fj.eswa.2018.07.068&partnerID=40&md5=b3e38016d0a35500ff98e63a199dc749,2018,2021-07-20 15:49:48,2021-07-20 15:49:48
SCM476JK,journalArticle,2015,"Bui, T.D.; Nong, T.H.; Kien Dang, T.",Improving learning rule for fuzzy associative memory with combination of content and association,"FAM is an associative memory that uses operators of fuzzy logic and mathematical morphology (MM). FAMs possess important advantages including noise tolerance, unlimited storage, and one pass convergence. An important property, deciding FAM performance, is the ability to capture content of each pattern, and association of patterns. Existing FAMs capture either content or association of patterns well, but not both of them. They are designed to handle either erosive or dilative noise in distorted inputs but not both. Therefore, they cannot recall distorted input patterns very well when both erosive and dilative noises are present. In this paper, we propose a new FAM called content-association associative memory (ACAM) that stores both content and association of patterns. The weight matrix is formed with the weighted sum of output pattern and the difference between input and output patterns. Our ACAM can handle inputs with both erosive and dilative noises better than existing models. © 2014 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2014.01.063,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922140622&doi=10.1016%2fj.neucom.2014.01.063&partnerID=40&md5=70e21de0e3915fd309c7cf034b2e7e86,2015,2021-07-20 15:49:48,2021-07-20 15:49:48
PEX5HA9F,journalArticle,2014,"Dai, Y.S.; Xiang, Y.P.; Pan, Y.","Bionic autonomic nervous systems for self-defense against dos, spyware, malware, virus, and fishing","Computing systems and networks become increasingly large and complex with a variety of compromises and vulnerabilities. The network security and privacy are of great concern today, where self-defense against different kinds of attacks in an autonomous and holistic manner is a challenging topic. To address this problem, we developed an innovative technology called Bionic Autonomic Nervous System (BANS). The BANS is analogous to biological nervous system, which consists of basic modules like cyber axon, cyber neuron, peripheral nerve and central nerve. We also presented an innovative self-defense mechanism which utilizes the Fuzzy Logic, Neural Networks, and Entropy Awareness, etc. Equipped with the BANS, computer and network systems can intelligently self-defend against both known and unknown compromises/attacks including denial of services (DoS), spyware, malware, and virus. BANS also enabled multiple computers to collaboratively fight against some distributed intelligent attacks like DDoS. We have implemented the BANS in practice. Some case studies and experimental results exhibited the effectiveness and efficiency of the BANS and the self-defense mechanism. © 2014 ACM.",ACM Transactions on Autonomous and Adaptive Systems,10.1145/2567924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896971464&doi=10.1145%2f2567924&partnerID=40&md5=50ddac25c938cbaae81589c5dfc1a6bf,2014,2021-07-20 15:49:48,2021-07-20 15:49:48
CU2VYW85,journalArticle,2013,"Muralidharan, V.; Sugumaran, V.",Selection of discrete wavelets for fault diagnosis of monoblock centrifugal pump using the J48 algorithm,"Monoblock centrifugal pumps play an important role in a variety of engineering applications such as in the food industry, in wastewater treatment plants, in agriculture, in the oil and gas industry, in the paper and pulp industry, and others. Condition monitoring of the various mechanical components of centrifugal pumps becomes essential for increasing productivity and reducing the number of breakdowns. Vibration-based continuous monitoring and analysis using machine learning approaches are gaining momentum. Particularly, artificial neural networks and fuzzy logic have been employed for continuous monitoring and fault diagnosis. This article presents the use of the J48 algorithm for fault diagnosis through discrete wavelet features extracted from vibration signals of good and faulty conditions of the components of a centrifugal pump. The classification accuracies of different discrete wavelet families were calculated and compared in order to find the best wavelet for the fault diagnosis of the centrifugal pump. © 2013 Copyright Taylor and Francis Group, LLC.",Applied Artificial Intelligence,10.1080/08839514.2012.721694,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872310648&doi=10.1080%2f08839514.2012.721694&partnerID=40&md5=bd1e538a7b7d626696b20a1eef18459e,2013,2021-07-20 15:49:48,2021-07-20 15:49:48
Z937H2TE,journalArticle,2015,"Alhaddad, M.J.; Mohammed, A.; Kamel, M.; Hagras, H.",A genetic interval type-2 fuzzy logic-based approach for generating interpretable linguistic models for the brain P300 phenomena recorded via brain–computer interfaces,"One of the important areas of brain–computer interface (BCI) research is to identify event-related potentials (ERPs) which are spatial–temporal patterns of the brain activity that happen after presentation of a stimulus and before execution of a movement. One of the important ERPs is the P300 which is an endogenous component of ERPs with a latency of about 300 ms which is elicited by significant stimuli (visual, or auditory). Various machine learning-based classifiers have been used to predict the P300 events and relate them to the human intended activities. However, the vast majority of the employed techniques like Bayesian linear discriminant analysis (BLDA) and regularized fisher linear discriminant analysis (RFLDA) are black box models which are difficult to understand and analyse by a normal clinician. In addition, due to the inter- and intra-user uncertainties associated with the P300 events, most of the existing classifiers need to be trained for a specific user under specific circumstances and the classifier needs to be retrained for different users or change of circumstances. In this paper, we present an interval type-2 fuzzy logic-based classifier which is able to handle the users’ uncertainties to produce better prediction accuracies than other competing classifiers such as BLDA or RFLDA. In addition, the generated type-2 fuzzy classifier is learnt from data via genetic algorithms to produce a small number of rules with a rule length of only one antecedent to maximise the transparency and interpretability for the normal clinician. We also employ a feature selection system based on an ensemble neural networks recursive feature selection which is able to find the effective time instances within the effective sensors in relation to given P300 event. We will present various experiments which were performed on standard data sets and using real-data sets obtained from real subjects’ experiments performed in the BCI laboratory in King Abdulaziz University. It will be shown that the produced type-2 fuzzy logic-based classifier will learn simple rules which are easy to understand explaining the events in question. In addition, the produced type-2 fuzzy logic classifier will be able to give better accuracies when compared to BLDA or RFLDA on various human subjects on the standard and real-world data sets. © 2014, Springer-Verlag Berlin Heidelberg.",Soft Computing,10.1007/s00500-014-1312-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925291394&doi=10.1007%2fs00500-014-1312-y&partnerID=40&md5=c4716ff9de645171d8b0769b6534c9b2,2015,2021-07-20 15:49:48,2021-07-20 15:49:48
PSRZXLCY,journalArticle,2021,"Sousa, L.","Nonconventional Computer Arithmetic Circuits, Systems and Applications","Arithmetic plays a major role in a computer?s performance and efficiency. Building new computing platforms supported by the traditional binary arithmetic and silicon-based technologies to meet the requirements of today?s applications is becoming increasingly more challenging, regardless whether we consider embedded devices or high-performance computers. As a result, a significant amount of research effort has been devoted to the study of nonconventional number systems to investigate more efficient arithmetic circuits and improved computer technologies to facilitate the development of computational units that can meet the requirements of applications in emergent domains. This paper presents an overview of the state of the art in nonconventional computer arithmetic. Several different alternative computing models and emerging technologies are analyzed, such as nanotechnologies, superconductor devices, and biological- and quantum-based computing, and their applications to multiple domains are discussed. A comprehensive approach is followed in a survey of the logarithmic and residue number systems, the hyperdimensional and stochastic computation models, and the arithmetic for quantum- and DNA-based computing systems and techniques for approximate computing. Technologies, processors and systems addressing these nonconventional computer arithmetic systems are also reviewed, taking into consideration some of the most prominent applications, such as deep learning or postquantum cryptography. In the end, some conclusions are drawn, and directions for future research on nonconventional computer arithmetic are discussed. © 2001-2012 IEEE.",IEEE Circuits and Systems Magazine,10.1109/MCAS.2020.3027425,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100907202&doi=10.1109%2fMCAS.2020.3027425&partnerID=40&md5=4e28bd1bbcae11b9833284920481fa82,2021,2021-07-20 15:49:48,2021-07-20 15:49:48
BB4AB7CW,journalArticle,2020,"Orujov, F.; Maskeliūnas, R.; Damaševičius, R.; Wei, W.",Fuzzy based image edge detection algorithm for blood vessel detection in retinal images,"We developed a contour detection based image processing algorithm based on Mamdani (Type-2) fuzzy rules for detection of blood vessels in retinal fundus images. The method uses the green channel data from eye fundus images as input, Contrast-Limited Adaptive Histogram Equalization (CLAHE) for contrast enhancement, and median filter for background exclusion. The Mamdani (Type-2) fuzzy rules applied on image gradient value are used for edge detection. The results of experiments on the Digital Retinal Images for Vessel Extraction (DRIVE), STructured Analysis of the Retina (STARE) and CHASEdb datasets show the applicability of the proposed method as a flexible approach which can be adapted to numerous edge detection/contour based applications. We achieved an accuracy of 0.865 for STARE dataset, an accuracy of 0.939 for the DRIVE dataset, and the accuracy of 0.950 for the ChaseDB dataset. In relation to works of other authors, our method offered a similar performance, but it offers an improved dynamics and flexibility in formulation of the linguistic threshold criteria, which can be a leading factor in design of image processing systems with dynamic and flexible rules, such as Type-2 fuzzy rules would allow, offering an interesting alternative to currently widespread deep learning applications. © 2020 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2020.106452,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086455233&doi=10.1016%2fj.asoc.2020.106452&partnerID=40&md5=c1dd51e13952cf1926dda7a46412af03,2020,2021-07-20 15:49:48,2021-07-20 15:49:48
DN6KJIVI,journalArticle,2020,"Lu, P.; Zhang, H.; Peng, X.; Jin, X.",Learning the Relation between Interested Objects and Aesthetic Region for Image Cropping,"As one of the fundamental techniques for image editing, image cropping discards irrelevant contents and remains the pleasing portions of the image to enhance the overall composition and achieve better visual/aesthetic perception. In this paper, we primarily focus on improving the efficiency of automatic image cropping, and on further exploring its potential in public datasets with high accuracy. From this perspective, we propose a deep learning based framework to learn the objects composition from photos with high aesthetic qualities, where an interested object region is detected through a convolutional neural network (CNN) based on the saliency map. The features of the detected interested objects are then fed into a regression network to obtain the final cropping result. Unlike the conventional methods that multiple candidates are proposed and evaluated iteratively, only a single interested object region is produced in our model, which is mapped to the final output directly. Thus, low computational resources are required for the proposed approach. The experimental results on the public datasets show that as a weakly supervised method, the proposed network outperforms the other weakly supervised methods on FLMS and FCD datasets and achieves comparable results to the existing methods on CUHK dataset. Furthermore, the proposed method is more efficient than these methods, where the processing speed is as fast as 20ms per image. IEEE",IEEE Transactions on Multimedia,10.1109/TMM.2020.3029882,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098791747&doi=10.1109%2fTMM.2020.3029882&partnerID=40&md5=d287f0aaf1cb7c0c37593538c93dbf52,2020,2021-07-20 15:49:49,2021-07-20 15:49:49
HQGV5UZ4,journalArticle,2020,"Qian, T.; Liang, Y.; Li, Q.; Xiong, H.",Attribute Graph Neural Networks for Strict Cold Start Recommendation,"Rating prediction is a classic problem underlying recommender systems. It is traditionally tackled with matrix factorization. Recently, deep learning based methods, especially graph neural networks, have made impressive progress on this problem. Despite their effectiveness, existing methods focus on modeling the user-item interaction graph. The inherent drawback of such methods is that their performance is bound to the density of the interactions, which is usually of high sparsity. Moreover, for a cold start user/item that does not have any interactions, such methods are unable to learn the preference embedding of the user/item since there is no link to this user/item in the graph. In this work, we develop a novel framework Attribute Graph Neural Networks (AGNN) by exploiting the attribute graph rather than the commonly used interaction graph. This leads to the capability of learning embeddings for the cold start users/items. Our AGNN can produce the preference embedding for a cold user/item by learning on the distribution of attributes with an extended variational auto-encoder structure. Moreover, we propose a new graph neural network variant, i.e., gated-GNN, to effectively aggregate various attributes of different modalities in a neighborhood. Empirical results demonstrate that our model yields significant improvements for cold start recommendations. IEEE",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2020.3038234,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098790100&doi=10.1109%2fTKDE.2020.3038234&partnerID=40&md5=39ec89d19dd99ebfb9b108fdd051f845,2020,2021-07-20 15:49:49,2021-07-20 15:49:49
JMKKZSTB,journalArticle,2020,"Shen, R.; Tang, B.; Lodi, A.; Tramontani, A.; Ayed, I.B.",An ILP Model for Multi-Label MRFs with Connectivity Constraints,"Integer Linear Programming (ILP) formulations of multi-label Markov random fields (MRFs) models with global connectivity priors were investigated previously in computer vision. In these works, only Linear Programming (LP) relaxations [1] or simplified versions [2] of the problem were solved. This paper investigates the ILP of MRF with exact connectivity priors via a branch-and-cut method, which provably finds globally optimal solutions. It enforces connectivity priors iteratively by a cutting plane method, and provides feasible solutions with a guarantee on sub-optimality even if we terminate it earlier. The proposed ILP can be applied as a post-processing method on top of any existing multi-label segmentation approach. As it provides globally optimal solution, it can be used off-line to serve as quality check for any fast on-line algorithm. Furthermore, the scribble based model presented in this paper could be potentially used to generate ground-truth proposals for any deep learning based segmentation. We demonstrate the power and usefulness of our model by extensive experiments on the BSDS500 and PASCAL VOC dataset. The experiments show that our proposed model achieves great performance, yielding provably global optimum in most instances and that provably good optimization solutions also provide good segmentation accuracy, even with the limited computing time of few seconds. © 1992-2012 IEEE.",IEEE Transactions on Image Processing,10.1109/TIP.2020.2995056,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088106560&doi=10.1109%2fTIP.2020.2995056&partnerID=40&md5=6a11d67f5aab96827f19a5f5b5f7fe33,2020,2021-07-20 15:49:49,2021-07-20 15:49:49
LVKTK7F6,journalArticle,2020,"Wang, G.; Jia, Q.-S.; Qiao, J.; Bi, J.; Liu, C.",A sparse deep belief network with efficient fuzzy learning framework,"Deep belief network (DBN) is one of the most feasible ways to realize deep learning (DL) technique, and it has been attracting more and more attentions in nonlinear system modeling. However, DBN cannot provide satisfactory results in learning speed, modeling accuracy and robustness, which is mainly caused by dense representation and gradient diffusion. To address these problems and promote DBN's development in cross-models, we propose a Sparse Deep Belief Network with Fuzzy Neural Network (SDBFNN) for nonlinear system modeling. In this novel framework, the sparse DBN is considered as a pre-training technique to realize fast weight-initialization and to obtain feature vectors. It can balance the dense representation to improve its robustness. A fuzzy neural network is developed for supervised modeling so as to eliminate the gradient diffusion. Its input happens to be the obtained feature vector. As a novel cross-model, SDBFNN combines the advantages of both pre-training technique and fuzzy neural network to improve modeling capability. Its convergence is also analyzed as well. A benchmark problem and a practical problem in wastewater treatment are conducted to demonstrate the superiority of SDBFNN. The extensive experimental results show that SDBFNN achieves better performance than the existing methods in learning speed, modeling accuracy and robustness. © 2019 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2019.09.035,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073015074&doi=10.1016%2fj.neunet.2019.09.035&partnerID=40&md5=6bfde4b2aed5b3dec2ea75bf3f96855e,2020,2021-07-20 15:49:49,2021-07-20 15:49:49
3IAF74PE,journalArticle,2019,"Lei, P.; Liang, J.; Guan, Z.; Wang, J.; Zheng, T.",Acceleration of FPGA Based Convolutional Neural Network for Human Activity Classification Using Millimeter-Wave Radar,"Deep learning techniques have attracted much attention in the radar automatic target recognition. In this paper, we investigate an acceleration method of the convolutional neural network (CNN) on the field-programmable gate array (FPGA) for the embedded application of the millimeter-wave (mmW) radar-based human activity classification. Considering the micro-Doppler effect caused by a person's body movements, the spectrogram of mmW radar echoes is adopted as the CNN input. After that, according to the CNN architecture and the properties of the FPGA implementation, several parallel processing strategies are designed as well as data quantization and optimization of classification decision to accelerate the CNN execution. Finally, comparative experiments and discussions are carried out based on a measured dataset of nine individuals with four different actions by using a 77-GHz mmW radar. The results show that the proposed method not only maintains the high classification accuracy but also improves its execution speed, memory requirement, and power consumption. Specifically, compared with the implementation of the same network model on a graphics processing unit, it could achieve the speedup of about 30.42% at the cost of the classification accuracy loss of only 0.27%. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2926381,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069765470&doi=10.1109%2fACCESS.2019.2926381&partnerID=40&md5=1f9421926f6ef0ed838654744986310c,2019,2021-07-20 15:49:49,2021-07-20 15:49:49
NVBAZ8R2,journalArticle,2018,"Cene, V.H.; Balbinot, A.",Using the sEMG signal representativity improvement towards upper-limb movement classification reliability,"Several Machine Learning techniques have been employed to process sEMG signals in order to provide a reliable control biosignal. Although some papers report accuracy rates superior to 90%, there is a lack of more detailed reasoning for reliable systems capable of providing control signals to users that may, for instance, control a prosthetic device. In this paper, we combined two strategies in order to increase the representativity of the sEMG signals: (a) the use of a stochastic filter based on Antonyan Vardan Transform (AVT) prior the extraction of the signal features that reduces the stochastic behavior of the sEMG signal; and (b) a novel sEMG feature called Differential Enhanced Signal (DES), designed to increase the signal representativity in the sEMG transition sections where features based on time-domain are usually inefficient. Thus, using only RMS and DES features, we were able to mitigate the class overlap in the transition sections and consequentially increase the overall classification accuracy for training and testing of the system. Since a reliable output signal is desired to perform ultimate prosthetic control, a reliability metric was defined and evaluated, and once a non-reliable classification is detected, the system autonomously activates auxiliary methods based on post-processing and data discard to maintain the classification consistency. Three preliminary scenarios involving the AVT filter, a Wavelet filter and the unfiltered signal were compared in terms of accuracy rate to define the most efficient filtering technique. The signal representation using the combination of RMS and DES features was also compared to a set of Time Domain (TD) features to test its enhancement capabilities. The AVT-based filter and the DES feature were able to present higher accuracy rates in both accuracy scenarios tested. Three different databases including 60 subjects among amputees and non-amputees were used to appraise the system, which was able to reach a mean accuracy rate of 99.1% in the best-case scenario. © 2018 Elsevier Ltd",Biomedical Signal Processing and Control,10.1016/j.bspc.2018.07.014,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051248874&doi=10.1016%2fj.bspc.2018.07.014&partnerID=40&md5=323b9e80249a316faf4497f274b435f3,2018,2021-07-20 15:49:49,2021-07-20 15:49:49
AHUFHM6T,journalArticle,2017,"Khamis, M.A.; Ngo, H.Q.; Rudra, A.",Juggling functions inside a database,"We define and study the Functional Aggregate Query (FAQ) problem, which captures common computational tasks across very wide range of domains including relational databases, logic, matrix and tensor computation, probabilistic graphical models, constraint satisfaction, and signal processing. Simply put, an FAQ is a declarative way of defining a new function from a database of input functions. We present InsideOut, a dynamic programming algorithm, to evaluate an FAQ. The algorithm rewrites the input query into a set of easier-to-compute FAQ sub-queries. Each subquery is then evaluated using a worst-case optimal relational join algorithm. The topic of designing algorithms to optimally evaluate the classic multiway join problem has seen exciting developments in the past few years. Our framework tightly connects these new ideas in database theory with a vast number of application areas in a coherent manner, showing potentially that - with the right abstraction, blurring the distinction between data and computation - a good database engine can be a general purpose constraint solver, relational data store, graphical model inference engine, and matrix/tensor computation processor all at once. The InsideOut algorithm is very simple, as shall be described in this paper. Yet, in spite of solving an extremely general problem, its runtime either is as good as or improves upon the best known algorithm for the applications that FAQ specializes to. These corollaries include computational tasks in graphical model inference, matrix/tensor operations, relational joins, and logic. Better yet, InsideOut can be used within any database engine, because it is basically a principled way of rewriting queries. Indeed, it is already part of the LogicBlox database engine, helping efficiently answer traditional database queries, graphical model inference queries, and train a large class of machine learning models inside the database itself. © ACM 2017.",SIGMOD Record,10.1145/3093754.3093757,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019844298&doi=10.1145%2f3093754.3093757&partnerID=40&md5=ba7a2720378354a4ee2203aad50723ee,2017,2021-07-20 15:49:49,2021-07-20 15:49:49
L6AN9UPU,journalArticle,2014,"Siqueira, J.M.; Paço, T.A.; Silvestre, J.C.; Santos, F.L.; Falcão, A.O.; Pereira, L.S.",Generating fuzzy rules by learning from olive tree transpiration measurement - An algorithm to automatize Granier sap flow data analysis,"The present study aims at developing an intelligent system of automating data analysis and prediction embedded in a fuzzy logic algorithm (FAUSY) to capture the relationship between environmental variables and sap flow measurements (Granier method). Environmental thermal gradients often interfere with Granier sap flow measurements since this method uses heat as a tracer, thus introducing a bias in transpiration flux calculation. The FAUSY algorithm is applied to solve measurement problems and provides an approximate and yet effective way of finding the relationship between the environmental variables and the natural temperature gradient (NTG), which is too complex or too ill-defined for precise mathematical analysis. In the process, FAUSY extracts the relationships from a set of input-output environmental observations, thus general directions for algorithm-based machine learning in fuzzy systems are outlined. Through an iterative procedure, the algorithm plays with the learning or forecasting via a simulated model. After a series of error control iterations, the outcome of the algorithm may become highly refined and be able to evolve into a more formal structure of rules, facilitating the automation of Granier sap flow data analysis. The system presented herein simulates the occurrence of NTG with reasonable accuracy, with an average residual error of 2.53% for sap flux rate, when compared to data processing performed in the usual way. For practical applications, this is an acceptable margin of error given that FAUSY could correct NTG errors up to an average of 76% of the normal manual correction process. In this sense, FAUSY provides a powerful and flexible way of establishing the relationships between the environment and NTG occurrences. © 2013 Elsevier B.V.",Computers and Electronics in Agriculture,10.1016/j.compag.2013.11.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890715465&doi=10.1016%2fj.compag.2013.11.013&partnerID=40&md5=aa9d07fb12180ce85cdad5349275fbf3,2014,2021-07-20 15:49:49,2021-07-20 15:49:49
A3AYVYEE,journalArticle,2011,"López, B.; Pous, C.; Gay, P.; Pla, A.; Sanz, J.; Brunet, J.",EXiT CBR: A framework for case-based medical diagnosis development and experimentation,"Objective: Medical applications have special features (interpretation of results in medical metrics, experiment reproducibility and dealing with complex data) that require the development of particular tools. The eXiT*CBR framework is proposed to support the development of and experimentation with new case-based reasoning (CBR) systems for medical diagnosis. Method: Our framework offers a modular, heterogeneous environment that combines different CBR techniques for different application requirements. The graphical user interface allows easy navigation through a set of experiments that are pre-visualized as plots (receiver operator characteristics (ROC) and accuracy curves). This user-friendly navigation allows easy analysis and replication of experiments. Used as a plug-in on the same interface, eXiT*CBR can work with any data mining technique such as determining feature relevance. Results: The results show that eXiT*CBR is a user-friendly tool that facilitates medical users to utilize CBR methods to determine diagnoses in the field of breast cancer, dealing with different patterns implicit in the data. Conclusions: Although several tools have been developed to facilitate the rapid construction of prototypes, none of them has taken into account the particularities of medical applications as an appropriate interface to medical users. eXiT*CBR aims to fill this gap. It uses CBR methods and common medical visualization tools, such as ROC plots, that facilitate the interpretation of the results. The navigation capabilities of this tool allow the tuning of different CBR parameters using experimental results. In addition, the tool allows experiment reproducibility. © 2010 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2010.09.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952509718&doi=10.1016%2fj.artmed.2010.09.002&partnerID=40&md5=62338032877f901916521368ebba232b,2011,2021-07-20 15:49:49,2021-07-20 15:49:49
DKXFLVIF,journalArticle,2020,"Abdolkarimi, E.S.; Abaei, G.; Selamat, A.; Mosavi, M.R.",A hybrid Type-2 Fuzzy Logic System and Extreme Learning Machine for low-cost INS/GPS in high-speed vehicular navigation system,"Due to the combined navigation system consisting of both Inertial Navigation System (INS) and Global Positioning System (GPS) in a complementary mode which assure a reliable, accurate, and continuous navigation system, we use a GPS/INS navigation system in our research. Because of the conditions of navigation system such as low-cost MEMS-based inertial sensors with considerable uncertainty in INS sensors, a highly noisy real data, and a long term outage of GPS signals during our flight tests, we enhance the positioning speed and accuracy by an Extreme Learning Machine (ELM) with the features of excellent generalization performance and fast learning speed. However, the generalization capability of ELM usually destabilizes with uncertainty existing in the dataset. In order to fix this limitation, first, a Type-2 Fuzzy Logic System (T2-FLS) handles the uncertainties in GPS/INS data, and then the final output ends up to the ELM to train and predict INS positioning error. We verify the efficiency of the suggested method in the estimation of speed and accuracy in INS sensors error during GPS satellites outage, particularly in real-time applications with a high-speed vehicle. Then, to evaluate the overall performance of the proposed method, the achieved results are discussed and compared to other methods like Extended Kalman Filter (EKF), wavelet-ELM, and Adaptive Neuro-Fuzzy Inference System (ANFIS). The results present considerable achievement and open the door to the application of T2-FLS and ELM in GPS/INS integration even in severe conditions. © 2020 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2020.106447,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086505382&doi=10.1016%2fj.asoc.2020.106447&partnerID=40&md5=99d177f7c816739166d2c1279efb9477,2020,2021-07-20 15:49:49,2021-07-20 15:49:49
JPVDZ5Z3,journalArticle,2020,"Anter, A.M.; Huang, G.; Li, L.; Zhang, L.; Liang, Z.; Zhang, Z.",A New Type of Fuzzy-Rule-Based System with Chaotic Swarm Intelligence for Multiclassification of Pain Perception from fMRI,"Machine learning has been increasingly used in decoding brain states from functional magnetic resonance imaging (fMRI). One important application is to classify the levels of pain perception from patients' fMRI for clinical pain assessment. However, the huge number of fMRI features and the complex relationships between fMRI and pain levels affect the performance of pain classification models heavily. In this article, we introduce a new fuzzy-rule-based hybrid optimization approach for dimension reduction and multiclassification problems using chaotic map, crow search optimization (CSO), and self-organizing fuzzy logic prototype (SOFLP). The approach is named as CCSO-SOFLP. In the proposed approach, chaotic map-based CSO is employed to find the optimal features from ultra-high-dimensional fMRI, and the fuzzy-rule-based SOFLP is employed for multiclassification of pain levels. In this sense, CSO is provided to avoid being stuck in local minima and to increase the computational performance. On the other hand, multilayer SOFLP classifier can continuously learn from new data and identify prototypes from the observed data and use them to build fuzzy rules, to define a suitable local area for each prototype, and to avoid overlapping. The proposed approach is applied on a pain-evoked fMRI data set to classify the levels of pain. Results indicate that the proposed approach can decode levels of pain and identify predictive fMRI patterns with higher accuracy and convergence speed and shorter execution time. Therefore, the new type of fuzzy-rule-based system with chaotic swarm intelligence holds great potential to predict pain perception in clinical uses. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2020.2979150,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086082571&doi=10.1109%2fTFUZZ.2020.2979150&partnerID=40&md5=b2dc38d1413f60f1a8f4185589ba0902,2020,2021-07-20 15:49:49,2021-07-20 15:49:49
AQ2SXGIB,journalArticle,2021,"Pirmoradi, S.; Teshnehlab, M.; Zarghami, N.; Sharifi, A.",A self-organizing deep neuro-fuzzy system approach for classification of kidney cancer subtypes using miRNA genomics data,"Kidney cancer is a dangerous disease affecting many patients all over the world. Early-stage diagnosis and correct identification of kidney cancer subtypes play an essential role in the patient's survival; therefore, its subtypes diagnosis and classification are the main challenges in kidney cancer treatment. Medical studies have proved that miRNA dysregulation can increase the risk of cancer. Thus, in this paper, we propose a new machine learning approach for significant miRNAs identification and kidney cancer subtype classification to design an automatic diagnostic tool. The proposed method contains two main steps: feature selection and classification. First, we apply the feature selection algorithm to choose the candidate miRNAs for each subtype. The feature selection algorithm utilizes the AMGM measure to select significant miRNAs with high discriminant power. Next, the candidate miRNAs are fed to a classifier to evaluate the candidate features. In the classification step, the proposed self-organizing deep neuro-fuzzy system is employed to classify kidney cancer subgroups. The new deep neuro-fuzzy system consists of a deep structure in the rule layer and novel architecture in the fuzzifier layer. The proposed self-organizing deep neuro-fuzzy system can help us to overcome the main obstacles in the field of neuro-fuzzy system applications, such as the curse of dimensionality. The goal of this paper is to illustrate that the neuro-fuzzy system can very useful in high dimensional data, such as genomics data, using the proposed deep neuro-fuzzy system. The obtained results illustrated that our proposed method has succeeded in classifying kidney cancer subtypes with high accuracy based on the selected miRNAs. © 2021",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2021.106132,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106223951&doi=10.1016%2fj.cmpb.2021.106132&partnerID=40&md5=814cdd72f08c04238de0ea7ff0d75a75,2021,2021-07-20 15:49:49,2021-07-20 15:49:49
PMB8RHCD,journalArticle,2020,"Lavrač, N.; Škrlj, B.; Robnik-Šikonja, M.",Propositionalization and embeddings: two sides of the same coin,"Data preprocessing is an important component of machine learning pipelines, which requires ample time and resources. An integral part of preprocessing is data transformation into the format required by a given learning algorithm. This paper outlines some of the modern data processing techniques used in relational learning that enable data fusion from different input data types and formats into a single table data representation, focusing on the propositionalization and embedding data transformation approaches. While both approaches aim at transforming data into tabular data format, they use different terminology and task definitions, are perceived to address different goals, and are used in different contexts. This paper contributes a unifying framework that allows for improved understanding of these two data transformation techniques by presenting their unified definitions, and by explaining the similarities and differences between the two approaches as variants of a unified complex data transformation task. In addition to the unifying framework, the novelty of this paper is a unifying methodology combining propositionalization and embeddings, which benefits from the advantages of both in solving complex data transformation and learning tasks. We present two efficient implementations of the unifying methodology: an instance-based PropDRM approach, and a feature-based PropStar approach to data transformation and learning, together with their empirical evaluation on several relational problems. The results show that the new algorithms can outperform existing relational learners and can solve much larger problems. © 2020, The Author(s).",Machine Learning,10.1007/s10994-020-05890-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087028636&doi=10.1007%2fs10994-020-05890-8&partnerID=40&md5=3a7b38489dcaac3e5b5b7fff571226bb,2020,2021-07-20 15:49:50,2021-07-20 15:49:50
QBEUTFSX,journalArticle,2020,"Mack, J.; Purdy, R.; Rockowitz, K.; Inouye, M.; Richter, E.; Valancius, S.; Kumbhare, N.; Hassan, M.S.; Fair, K.; Mixter, J.; Akoglu, A.",RANC: Reconfigurable Architecture for Neuromorphic Computing,"Neuromorphic architectures have been introduced as platforms for energy efficient spiking neural network execution. The massive parallelism offered by these architectures has also triggered interest from non-machine learning application domains. In order to lift the barriers to entry for hardware designers and application developers we present RANC: a Reconfigurable Architecture for Neuromorphic Computing, an open-source highly flexible ecosystem that enables rapid experimentation with neuromorphic architectures in both software via C++ simulation and hardware via FPGA emulation. We present the utility of the RANC ecosystem by showing its ability to recreate behavior of the IBM&#x2019;s TrueNorth and validate with direct comparison to IBM&#x2019;s Compass simulation environment and published literature. RANC allows optimizing architectures based on application insights as well as prototyping future neuromorphic architectures that can support new classes of applications entirely. We demonstrate the highly parameterized and configurable nature of RANC by studying the impact of architectural changes on improving application mapping efficiency with quantitative analysis based on Alveo U250 FPGA. We present post routing resource usage and throughput analysis across implementations of Synthetic Aperture Radar classification and Vector Matrix Multiplication applications, and demonstrate a neuromorphic architecture that scales to emulating 259K distinct neurons and 73.3M distinct synapses. IEEE",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.3038151,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096849844&doi=10.1109%2fTCAD.2020.3038151&partnerID=40&md5=19a5d9434fb2e419bf378b7709cfefa0,2020,2021-07-20 15:49:50,2021-07-20 15:49:50
R9DU9FZ2,journalArticle,2019,"Kang, J.; Xiong, Z.; Niyato, D.; Xie, S.; Zhang, J.",Incentive Mechanism for Reliable Federated Learning: A Joint Optimization Approach to Combining Reputation and Contract Theory,"Federated learning is an emerging machine learning technique that enables distributed model training using local datasets from large-scale nodes, e.g., mobile devices, but shares only model updates without uploading the raw training data. This technique provides a promising privacy preservation for mobile devices while simultaneously ensuring high learning performance. The majority of existing work has focused on designing advanced learning algorithms with an aim to achieve better learning performance. However, the challenges, such as incentive mechanisms for participating in training and worker (i.e., mobile devices) selection schemes for reliable federated learning, have not been explored yet. These challenges have hindered the widespread adoption of federated learning. To address the above challenges, in this article, we first introduce reputation as the metric to measure the reliability and trustworthiness of the mobile devices. We then design a reputation-based worker selection scheme for reliable federated learning by using a multiweight subjective logic model. We also leverage the blockchain to achieve secure reputation management for workers with nonrepudiation and tamper-resistance properties in a decentralized manner. Moreover, we propose an effective incentive mechanism combining reputation with contract theory to motivate high-reputation mobile devices with high-quality data to participate in model learning. Numerical results clearly indicate that the proposed schemes are efficient for reliable federated learning in terms of significantly improving the learning accuracy. © 2014 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2019.2940820,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076730826&doi=10.1109%2fJIOT.2019.2940820&partnerID=40&md5=4c82c49445890f28198dbf154191fdce,2019,2021-07-20 15:49:50,2021-07-20 15:49:50
AZ2CXK8N,journalArticle,2019,"Elnaggar, R.; Chakrabarty, K.; Tahoori, M.B.",Hardware Trojan Detection Using Changepoint-Based Anomaly Detection Techniques,"There has been a growing trend in recent years to outsource various aspects of the semiconductor design and manufacturing flow to different parties spread across the globe. Such outsourcing increases the risk of adversaries adding malicious logic, referred to as hardware Trojans, to the original design. The increased complexity of modern microprocessors increases the difficulty in detecting hardware Trojans at early stages of design and manufacturing. Therefore, there is a need for run-time detection techniques to capture Trojans that escape detection at these stages. In this paper, we introduce a machine learning-based run-time hardware Trojan detection method for microprocessor cores. This approach uses changepoint-based anomaly detection algorithm to detect the activation of Trojans that introduce abnormal patterns in the data streams obtained from performance counters. It does not modify the original microprocessor design to integrate on-chip monitoring sensors. We evaluate our method by detecting the activation of Trojans that cause denial-of-service, the degradation of system performance, and change in functionality of a microprocessor core. Results obtained using the OpenSPARC T1 core and an field-programmable gate array (FPGA) prototyping framework show that the Trojan activation is detected with a true positive rate of above 99% and a false positive rate of 0% for most of the implemented Trojans. © 1993-2012 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2019.2925807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075630795&doi=10.1109%2fTVLSI.2019.2925807&partnerID=40&md5=bfe437412388c91dc5322249372133c0,2019,2021-07-20 15:49:50,2021-07-20 15:49:50
VHN8XVW3,journalArticle,2019,"Thangavelu, V.; Divakaran, D.M.; Sairam, R.; Bhunia, S.S.; Gurusamy, M.",DEFT: A Distributed IoT Fingerprinting Technique,"Identifying IoT devices connected to a network has multiple security benefits, such as deployment of behavior-based anomaly detectors, automated vulnerability patching of specific device types, dynamic attack mitigation, etc. In this paper, we look into the problem of IoT device identification at network level, in particular from an ISP's perspective. The simple solution of deploying a supervised machine learning algorithm at a centralized location in the network neither scales well nor can identify new devices. To tackle these challenges, we propose and develop a distributed device fingerprinting technique (DEFT), a distributed fingerprinting solution that addresses and exploits the presence of common devices, including new devices, across smart homes and enterprises in a network. A DEFT controller develops and maintains classifiers for fingerprinting, while gateways located closer to the IoT devices at homes perform device classification. Importantly, the controller and gateways coordinate to identify new devices in the network. DEFT is designed to be scalable and dynamic - it can be deployed, orchestrated, and controlled using software-defined networking and network function virtualization. DEFT is able to identify new device types automatically, while achieving high accuracy and low false positive rate. We demonstrate the effectiveness of DEFT by experimenting on data obtained from real-world IoT devices. © 2014 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2018.2865604,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051675308&doi=10.1109%2fJIOT.2018.2865604&partnerID=40&md5=eb29e05bc55add94fb58a8881ee5cd64,2019,2021-07-20 15:49:50,2021-07-20 15:49:50
Z7DF4QH9,journalArticle,2017,"Zerva, C.; Batista-Navarro, R.; Day, P.; Ananiadou, S.",Using uncertainty to link and rank evidence from biomedical literature for model curation,"Motivation: In recent years, there has been great progress in the field of automated curation of biomedical networks and models, aided by text mining methods that provide evidence from literature. Such methods must not only extract snippets of text that relate to model interactions, but also be able to contextualize the evidence and provide additional confidence scores for the interaction in question. Although various approaches calculating confidence scores have focused primarily on the quality of the extracted information, there has been little work on exploring the textual uncertainty conveyed by the author. Despite textual uncertainty being acknowledged in biomedical text mining as an attribute of text mined interactions (events), it is significantly understudied as a means of providing a confidence measure for interactions in pathways or other biomedical models. In this work, we focus on improving identification of textual uncertainty for events and explore how it can be used as an additional measure of confidence for biomedical models. Results: We present a novel method for extracting uncertainty from the literature using a hybrid approach that combines rule induction and machine learning. Variations of this hybrid approach are then discussed, alongside their advantages and disadvantages. We use subjective logic theory to combine multiple uncertainty values extracted from different sources for the same interaction. Our approach achieves F-scores of 0.76 and 0.88 based on the BioNLP-ST and Genia-MK corpora, respectively, making considerable improvements over previously published work. Moreover, we evaluate our proposed system on pathways related to two different areas, namely leukemia and melanoma cancer research. © The Author 2017. Published by Oxford University Press.",Bioinformatics,10.1093/bioinformatics/btx466,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039853755&doi=10.1093%2fbioinformatics%2fbtx466&partnerID=40&md5=8cf5f7d19ee10493d9ddf5b9f6619827,2017,2021-07-20 15:49:50,2021-07-20 15:49:50
FFA6YJEK,journalArticle,2021,"Mehta, D.; Dwivedi, A.; Patra, A.; Anand Kumar, M.",A transformer-based architecture for fake news classification,"In today’s post-truth world, the proliferation of propaganda and falsified news poses a deadly risk of misinforming the public on a variety of issues, either through traditional media or on social media. Information people acquire through these articles and posts tends to shape their world view and provides reasoning for choices they take in their day to day lives. Thus, fake news can definitely be a malicious force, having massive real-world consequences. In this paper, we focus on classifying fake news using models based on a natural language processing framework, Bidirectional Encoder Representations from Transformers, also known as BERT. We fine-tune BERT for specific domain datasets and also make use of human justification and metadata for added performance in our models. We determine that the deep-contextualizing nature of BERT is effective for this task and obtain significant improvement over binary classification, and minimal yet important improvement in six-label classification in comparison with previously explored models. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Austria, part of Springer Nature.",Social Network Analysis and Mining,10.1007/s13278-021-00738-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104553125&doi=10.1007%2fs13278-021-00738-y&partnerID=40&md5=2353aadeb27fad21ba9c00a2e966bc59,2021,2021-07-20 15:49:50,2021-07-20 15:49:50
KCAJT2R9,journalArticle,2021,"Gao, Q.; Zeng, H.; Li, G.; Tong, T.",Graph Reasoning-Based Emotion Recognition Network,"Semantic information from images can be used to improve the performance of deep learning methods in recognizing human emotions. In this paper, we propose a novel framework based on the graph convolutional network for emotion recognition by utilizing the semantic relationships of different regions. First, we extract the salient image regions within video frame clips by using the bottom-up attention module to construct the node features of a graph. Then, we build the graphs containing the node features and the semantic correlations of nodes by using the graph convolutional network. For refinement, each node feature of graph vectors is enhanced via a gated recurrent unit consisting of gate and memory units to remove redundant feature information. Experimental results show that our proposed method achieves superior performance over state-of-the-art approaches for the emotion recognition on the CEAR and AFEW datasets. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3048693,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099093255&doi=10.1109%2fACCESS.2020.3048693&partnerID=40&md5=4e76496c1e5e2602b2f6ae8283de4190,2021,2021-07-20 15:49:50,2021-07-20 15:49:50
IJH8N8RU,journalArticle,2020,"Vasilopoulos, V.; Pavlakos, G.; Bowman, S.L.; Caporale, J.D.; Daniilidis, K.; Pappas, G.J.; Koditschek, D.E.",Reactive Semantic Planning in Unexplored Semantic Environments Using Deep Perceptual Feedback,"This letter presents a reactive planning system that enriches the topological representation of an environment with a tightly integrated semantic representation, achieved by incorporating and exploiting advances in deep perceptual learning and probabilistic semantic reasoning. Our architecture combines object detection with semantic SLAM, affording robust, reactive logical as well as geometric planning in unexplored environments. Moreover, by incorporating a human mesh estimation algorithm, our system is capable of reacting and responding in real time to semantically labeled human motions and gestures. New formal results allow tracking of suitably non-adversarial moving targets, while maintaining the same collision avoidance guarantees. We suggest the empirical utility of the proposed control architecture with a numerical study including comparisons with a state-of-the-art dynamic replanning algorithm, and physical implementation on both a wheeled and legged platform in different settings with both geometric and semantic goals. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2020.3001496,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090935119&doi=10.1109%2fLRA.2020.3001496&partnerID=40&md5=502675ebadb4635e4b770fe677c66960,2020,2021-07-20 15:49:50,2021-07-20 15:49:50
G3SGBPHK,journalArticle,2021,"Zheng, Z.; Yu, Z.; Zheng, H.; Yang, Y.; Shen, H.T.",One-Shot Image-to-Image Translation via Part-Global Learning with a Multi-adversarial Framework,"It is well known that humans can learn and recognize objects effectively from several limited image samples. However, learning from just a few images is still a tremendous challenge for existing main-stream deep neural networks. Inspired by analogical reasoning in the human mind, a feasible strategy is to translate the abundant images of a rich source domain to enrich the relevant yet different target domain with insufficient image data. To achieve this goal, we propose a novel, effective multi-adversarial framework (MA) based on part-global learning, which accomplishes the one-shot cross-domain image-to-image translation. In specific, we first devise a part-global adversarial training scheme to provide an efficient way for feature extraction and prevent discriminators being over-fitted. Then, a multi-adversarial mechanism is employed to enhance the image-to-image translation ability to unearth the high-level semantic representation. Moreover, a balanced adversarial loss function is presented, which aims to balance the training data and stabilize the training process. Extensive experiments demonstrate that the proposed approach can obtain impressive results on various datasets between two extremely imbalanced image domains and outperform state-of-the-art methods on one-shot image-to-image translation. Our code will be published with the paper. IEEE",IEEE Transactions on Multimedia,10.1109/TMM.2021.3053775,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100456082&doi=10.1109%2fTMM.2021.3053775&partnerID=40&md5=b7627e78847e1e2a302adb4aeb339022,2021,2021-07-20 15:49:50,2021-07-20 15:49:50
BEF8RKUD,journalArticle,2020,"Jiang, H.; Gao, F.; Xu, X.; Huang, F.; Zhu, S.",Attentive and ensemble 3D dual path networks for pulmonary nodules classification,"Automated pulmonary nodules classification aims at predicting whether a candidate nodule is benign or malignant. It is of great significance for computer-aided diagnosis of lung cancer. Despite the substantial progress achieved by existing methods, several challenges remain, including the lack of fine-grained representations, the interpretability of the reasoning procedure, and the trade-off between true-positive rate and false-positive rate. To tackle these challenges, in this work, we present a novel pulmonary nodule classification framework via attentive and ensemble 3D Dual Path Networks. Specially, we first devise a contextual attention mechanism to model the contextual correlations among adjacent locations, which improves the representativeness of deep features. Second, we employ a spatial attention mechanism to automatically locate the regions essential for nodule classification. Finally, we employ an ensemble of several models to improve the prediction robustness. Extensive experiments are conducted on the LIDC-IDRI database. Results demonstrate the effectiveness of the proposed techniques and the superiority of our model over previous state-of-the-art. © 2019 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2019.03.103,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069691521&doi=10.1016%2fj.neucom.2019.03.103&partnerID=40&md5=fa6b7da14750700864ad680526915c13,2020,2021-07-20 15:49:50,2021-07-20 15:49:50
W48BEJXZ,journalArticle,2018,"Phoemphon, S.; So-In, C.; Nguyen, T.G.",An enhanced wireless sensor network localization scheme for radio irregularity models using hybrid fuzzy deep extreme learning machines,"Localization is one of the key challenges facing wireless sensor networks (WSNs), particularly in the absence of global positioning equipment such as GPS. However, equipping WSNs with GPS sensors entails the additional costs of hardware logic and increased power consumption, thereby lowering the lifetime of the sensor, which is normally operated on a non-rechargeable battery. Range-free-based localization schemes have shown promise compared to range-based approaches as preferred and cost-effective solutions. Typical range-free localization algorithms have a key advantage: simplicity. However, their precision must be improved, especially under varying node densities, sensing coverage conditions, and topology diversity. Thus, this work investigates the probable integration of two soft-computing techniques, namely, Fuzzy Logic (FL) and Extreme Learning Machines (ELMs), with the goal of enhancing the approximate localization precision while considering the above factors. In stark contrast to ELMs, FL methods yield high accuracy under low node density and limited coverage conditions. In addition, as a hybrid scheme, extra steps are integrated to compensate for the effects of irregular topology (i.e., noisy signal density due to obstacles). Signal and weight are normalized during the fuzzy states, while the ELM uses a deep learning concept to adjust the signal coverage, including the spring force error estimation enhancement. The performance of our hybrid scheme is evaluated via simulations that demonstrate the scheme’s effectiveness compared with other soft-computing-based range-free localization schemes. © 2016, Springer Science+Business Media New York.",Wireless Networks,10.1007/s11276-016-1372-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84991384183&doi=10.1007%2fs11276-016-1372-2&partnerID=40&md5=1e8a49fe4a3f2394b9bc34d38b6fb263,2018,2021-07-20 15:49:50,2021-07-20 15:49:50
APS8RFU7,journalArticle,2018,"Dai, J.; Hu, H.; Wu, W.-Z.; Qian, Y.; Huang, D.",Maximal-discernibility-pair-based approach to attribute reduction in fuzzy rough sets,"Attribute reduction is one of the biggest challenges encountered in computational intelligence, data mining, pattern recognition, and machine learning. Effective in feature selection as the rough set theory is, it can only handle symbolic attributes. In order to overcome this drawback, the fuzzy rough set model is proposed, which is an extended model of rough sets and is able to deal with imprecision and uncertainty in both symbolic and numerical attributes. The existing attribute selection algorithms based on the fuzzy rough set model mainly take the angle of 'attribute set,' which means they define the object function representing the predictive ability for an attribute subset with regard to the domain of discourse, rather than following the view of an 'object pair.' Algorithms from the viewpoint of the object pair can ignore the object pairs that are already discerned by the selected attribute subsets and, thus, need only to deal with part of object pairs instead of the whole object pairs from the discourse, which makes such algorithms more efficient in attribute selection. In this paper, we propose the concept of reduced maximal discernibility pairs, which directly adopts the perspective of the object pair in the framework of the fuzzy rough set model. Then, we develop two attribute selection algorithms, named as reduced maximal discernibility pairs selection and weighted reduced maximal discernibility pair selection, based on the reduced maximal discernibility pairs. Experiment results show that the proposed algorithms are effective and efficient in attribute selection. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2017.2768044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032745701&doi=10.1109%2fTFUZZ.2017.2768044&partnerID=40&md5=1ea7596027be18bc2191216d30721d6b,2018,2021-07-20 15:49:50,2021-07-20 15:49:50
5GXN9L8P,journalArticle,2014,"ElBedwehy, M.N.; Ghoneim, M.E.; Hassanien, A.E.; Azar, A.T.",A computational knowledge representation model for cognitive computers,"The accumulating data are easy to store but the ability of understanding and using it does not keep track with its growth. So researches focus on the nature of knowledge processing in the mind. This paper proposes a semantic model (CKRMCC) based on cognitive aspects that enables cognitive computer to process the knowledge as the human mind and find a suitable representation of that knowledge. In cognitive computer, knowledge processing passes through three major stages: knowledge acquisition and encoding, knowledge representation, and knowledge inference and validation. The core of CKRMCC is knowledge representation, which in turn proceeds through four phases: prototype formation phase, discrimination phase, generalization phase, and algorithm development phase. Each of those phases is mathematically formulated using the notions of real-time process algebra. The performance efficiency of CKRMCC is evaluated using some datasets from the well-known UCI repository of machine learning datasets. The acquired datasets are divided into training and testing data that are encoded using concept matrix. Consequently, in the knowledge representation stage, a set of symbolic rule is derived to establish a suitable representation for the training datasets. This representation will be available in a usable form when it is needed in the future. The inference stage uses the rule set to obtain the classes of the encoded testing datasets. Finally, knowledge validation phase is validating and verifying the results of applying the rule set on testing datasets. The performances are compared with classification and regression tree and support vector machine and prove that CKRMCC has an efficient performance in representing the knowledge using symbolic rules. © 2014, Springer-Verlag London.",Neural Computing and Applications,10.1007/s00521-014-1614-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920254979&doi=10.1007%2fs00521-014-1614-0&partnerID=40&md5=f20d038f5f1a0daf1f0919b33a1d8445,2014,2021-07-20 15:49:50,2021-07-20 15:49:50
FTNZJN6M,journalArticle,2012,"Subagdja, B.; Tan, A.-H.",IFALCON: A neural architecture for hierarchical planning,"Hierarchical planning is an approach of planning by composing and executing hierarchically arranged predefined plans on the fly to solve some problems. This approach commonly relies on a domain expert providing all semantic and structural knowledge. One challenge is how the system deals with incomplete ill-defined knowledge while the solution can be achieved on the fly. Most symbolic-based hierarchical planners have been devised to allow the knowledge to be described expressively. However, in some cases, it is still difficult to produce the appropriate knowledge due to the complexity of the problem domain especially if the missing knowledge must be acquired online. This paper presents a novel neural-based model of hierarchical planning that can seek and acquire new plans online if the necessary knowledge are lacking. It enables all propositions and descriptions of plans to be computed and learnt simultaneously as inherent features of the model rather than discretely processed like in most symbolic approaches. Using a multi-channel adaptive resonance theory (fusion ART) neural network as the basic building block of the architecture and a new representation technique called gradient encoding, the so-called iFALCON architecture can capture and manipulate sequential and hierarchical relations of plans on the fly. Case studies using blocks world domain and an agent in Unreal Tournament video game demonstrate that the model can be used to execute, plan, and discover new plans through experiences. © 2012 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2012.01.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859110846&doi=10.1016%2fj.neucom.2012.01.008&partnerID=40&md5=2b3baeb64392e14eeec5333c4823a302,2012,2021-07-20 15:49:50,2021-07-20 15:49:50
4F8R7F94,journalArticle,2020,"Gerami Seresht, N.; Lourenzutti, R.; Fayek, A.R.",A fuzzy clustering algorithm for developing predictive models in construction applications,"Fuzzy inference systems (FISs) are a predictive modeling technique based on fuzzy sets that utilize approximate reasoning to mimic the decision-making process of human experts. There are several expert- and data-driven methods for developing FISs, among which fuzzy clustering algorithms are the most frequently used data-driven methods. This paper introduces a new fuzzy clustering algorithm for developing FISs in construction applications that addresses two limitations of existing fuzzy clustering algorithms: the lack of capacity to determine the number of clusters automatically from the characteristics of the data, and the poor performance in predictive modeling of highly dimensional problems. Existing fuzzy clustering algorithms are limited in construction applications since determining the number of clusters based on subjective expert judgment reduces the accuracy of the resulting FIS, and construction systems are often highly dimensional with a large number of inputs affecting the system outputs. The fuzzy clustering algorithm proposed in this paper determines the number of clusters automatically based on the characteristics of the data, specifically the non-linearity observed within clusters, and assigns weights to the rules of FISs to improve their accuracy in highly dimensional problems. This paper advances the state-of-the-art of fuzzy clustering and contributes to construction modeling by providing a new data-driven technique for developing FISs that suits the characteristics of construction problems. © 2020 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2020.106679,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090024902&doi=10.1016%2fj.asoc.2020.106679&partnerID=40&md5=3e80314e6077e7034615ed0f3c51c6ae,2020,2021-07-20 15:49:50,2021-07-20 15:49:50
NQDEGBEH,journalArticle,2020,"Perelman, B.S.; Evans, Iii, A.W.; Schaefer, K.E.",Where Do You Think You're Going?: Characterizing spatial mental models from planned routes,"Route planning is a critical behavior for human-intelligent agent (H-IA) team mobility. The scientific community has made major advances in improving route planner optimality and speed. However, human factors, such as the ability to predict and understand teammates' actions and goals, are necessary for trust development in H-IA teams. Trust is especially critical when agents' behaviors do not match human team members' expectations, or the human cannot understand the agent's underlying reasoning process. To address this issue, the artificial intelligence community has pushed toward creating human-like agent behaviors using machine learning. The problem with this approach is that we do not yet have a clear understanding of what constitutes human-like behavior across the breadth of tasks that H-IA teams undertake. This article describes an investigation and comparison of human and agent route planning behaviors, the interplay between humans and agents in collaborative planning, and the role of trust in this collaborative process. Finally, we propose a data-driven methodology for characterizing and visualizing differences among routes planned by humans and agents. This methodology provides a means to advance compatible mental model metrics and theory by informing targeted transparency manipulations, thereby improving the speed and quality of routes produced by H-IA teams. © 2020 ACM.",ACM Transactions on Human-Robot Interaction,10.1145/3385008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092709227&doi=10.1145%2f3385008&partnerID=40&md5=0e955dd72fe25ed1f04674ee5d9fd411,2020,2021-07-20 15:49:51,2021-07-20 15:49:51
GBM4QD8T,journalArticle,2019,"Atif, M.; Latif, S.; Ahmad, R.; Kiani, A.K.; Qadir, J.; Baig, A.; Ishibuchi, H.; Abbas, W.",Soft Computing Techniques for Dependable Cyber-Physical Systems,"Cyber-physical systems (CPSs) were envisaged as a way to manipulate the objects in the physical world through computer intelligence. This is usually done by providing a communication bridge between actuation and computing elements. This sought after control is hampered not only by the unavoidable certainty found in the physical world but also by the limitations of contemporary communication networks. These limitations hamper fine-grained control of elements that may be separated by large-scale distances. In this regard, soft computing is an emerging paradigm that can help to manage the unreliability of CPS by using techniques, including fuzzy systems, neural networks, evolutionary computation, probabilistic reasoning, and rough sets. We present a comprehensive contemporary review of soft computing techniques for CPS dependability modeling, analysis, and improvement. This paper provides an overview of CPS applications, explores the foundations of dependability engineering, and highlights the potential role of soft computing techniques for CPS dependability with various case studies while also identifying common pitfalls and future directions. In addition, this paper provides a comprehensive survey of the use of various soft computing techniques for making CPS dependable. This paper is timely due to the increasingly central role that CPSs are beginning to play in modern societies and the need to leverage all the relevant methodologies and tools (such as those provided by soft computing) for the development of highly dependable CPS. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2920317,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067395194&doi=10.1109%2fACCESS.2019.2920317&partnerID=40&md5=703f51e0bdc28b5c91b5a9c7b9b3a6ee,2019,2021-07-20 15:49:51,2021-07-20 15:49:51
AAPFG6CK,journalArticle,2016,"Anagnostopoulos, C.",Quality-optimized predictive analytics,"On-line statistical and machine learning analytic tasks over large-scale contextual data streams coming from e.g., wireless sensor networks, Internet of Things environments, have gained high popularity nowadays due to their significance in knowledge extraction, regression and classification tasks, and, more generally, in making sense from large-scale streaming data. The quality of the received contextual information, however, impacts predictive analytics tasks especially when dealing with uncertain data, outliers data, and data containing missing values. Low quality of received contextual data significantly spoils the progressive inference and on-line statistical reasoning tasks, thus, bias is introduced in the induced knowledge, e.g., classification and decision making. To alleviate such situation, which is not so rare in real time contextual information processing systems, we propose a progressive time-optimized data quality-aware mechanism, which attempts to deliver contextual information of high quality to predictive analytics engines by progressively introducing a certain controlled delay. Such a mechanism progressively delivers high quality data as much as possible, thus eliminating possible biases in knowledge extraction and predictive analysis tasks. We propose an analytical model for this mechanism and show the benefits stem from this approach through comprehensive experimental evaluation and comparative assessment with quality-unaware methods over real sensory multivariate contextual data. © 2016, Springer Science+Business Media New York.",Applied Intelligence,10.1007/s10489-016-0807-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976300444&doi=10.1007%2fs10489-016-0807-x&partnerID=40&md5=08b5a85b7a0f4a8a6317d93167e361b9,2016,2021-07-20 15:49:51,2021-07-20 15:49:51
GJTIA9YR,journalArticle,2019,"Watson, D.",The Rhetoric and Reality of Anthropomorphism in Artificial Intelligence,"Artificial intelligence (AI) has historically been conceptualized in anthropomorphic terms. Some algorithms deploy biomimetic designs in a deliberate attempt to effect a sort of digital isomorphism of the human brain. Others leverage more general learning strategies that happen to coincide with popular theories of cognitive science and social epistemology. In this paper, I challenge the anthropomorphic credentials of the neural network algorithm, whose similarities to human cognition I argue are vastly overstated and narrowly construed. I submit that three alternative supervised learning methods—namely lasso penalties, bagging, and boosting—offer subtler, more interesting analogies to human reasoning as both an individual and a social phenomenon. Despite the temptation to fall back on anthropomorphic tropes when discussing AI, however, I conclude that such rhetoric is at best misleading and at worst downright dangerous. The impulse to humanize algorithms is an obstacle to properly conceptualizing the ethical challenges posed by emerging technologies. © 2019, The Author(s).",Minds and Machines,10.1007/s11023-019-09506-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073681664&doi=10.1007%2fs11023-019-09506-6&partnerID=40&md5=74ca5be4edab24daa126b2e03e7b75bf,2019,2021-07-20 15:49:51,2021-07-20 15:49:51
96YWDJ2H,journalArticle,2011,"Belaïd, A.; D'Andecy, V.P.; Hamza, H.; Belaïd, Y.",Administrative document analysis and structure,"This chapter reports our knowledge about the analysis and recognition of scanned administrative documents. Regarding essentially the administrative paper flow with new and continuous arrivals, all the conventional techniques reserved to static databases modeling and recognition are doomed to failure. For this purpose, a new technique based on the experience was investigated giving very promising results. This technique is related to the case-based reasoning already used in data mining and various problems of machine learning. After the presentation of the context related to the administrative document flow and its requirements in a real time processing, we present a case based reasonning for invoice processing. The case corresponds to the co-existence of a problem and its solution. The problem in an invoice corresponds to a local structure such as the keywords of an address or the line patterns in the amounts table, while the solution is related to their content. This problem is then compared to a document case base using graph probing. For this purpose, we proposed an improvement of an already existing neural network called Incremental Growing Neural Gas. © 2011 Springer-Verlag Berlin Heidelberg.",Studies in Computational Intelligence,10.1007/978-3-642-22913-8_3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80455129757&doi=10.1007%2f978-3-642-22913-8_3&partnerID=40&md5=528c919801f5fca7b36cd438e58e0dee,2011,2021-07-20 15:49:51,2021-07-20 15:49:51
G2HF5E7D,journalArticle,2020,"Novaes, M.P.; Carvalho, L.F.; Lloret, J.; Proenca, Jr., M.L.",Long short-term memory and fuzzy logic for anomaly detection and mitigation in software-defined network environment,"Computer networks become complex and dynamic structures. As a result of this fact, the configuration and the managing of this whole structure is a challenging activity. Software-Defined Networks(SDN) is a new network paradigm that, through an abstraction of network plans, seeks to separate the control plane and data plane, and tends as an objective to overcome the limitations in terms of network infrastructure configuration. As in the traditional network environment, the SDN environment is also liable to security vulnerabilities. This work presents a system of detection and mitigation of Distributed Denial of Service (DDoS) attacks and Portscan attacks in SDN environments (LSTM-FUZZY). The LSTM-FUZZY system presented in this work has three distinct phases: characterization, anomaly detection, and mitigation. The system was tested in two scenarios. In the first scenario, we applied IP flows collected from the SDN Floodlight controllers through emulation on Mininet. On the other hand, in the second scenario, the CICDDoS 2019 dataset was applied. The results gained show that the efficiency of the system to assist in network management, detect and mitigate the occurrence of the attacks. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2992044,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084959271&doi=10.1109%2fACCESS.2020.2992044&partnerID=40&md5=453fa5c2782a2a1035238a0353e49bc8,2020,2021-07-20 15:49:51,2021-07-20 15:49:51
7EYVZITW,journalArticle,2019,"Fan, C.; Bao, S.; Tao, Y.; Li, B.; Zhao, C.",Fuzzy Reinforcement Learning for Robust Spectrum Access in Dynamic Shared Networks,"The persistent increases of wireless terminals have brought about diverse shared networks, where robust and efficient spectrum reuse among heterogeneous users is of critical importance while still remains as a challenging task for practical application. In this paper, we study the problem of robust spectrum access (RSA) in a canonical wireless shared network (WSN) with fully considering the inherent dynamics of the wireless environment. The non-static features of WSNs result in uncertain channel state information (CSI) and complicated coupling interference, which can't be directly formulated as the well-accepted crisp game model, rendering most existing perfect CSI relied approaches inefficient or even unfeasible. To address this, by interpreting the estimated CSI with uncertainty as fuzzy number, a novel framework referred to as a non-cooperative fuzzy game (NC-FG) is adopted, whereby the user utility is mapped as a fuzzy value via the user-defined fuzzy utility function. Based on the derived property of the NC-FG that fuzzy Nash equilibrium (FNE) exists, a fuzzy-logic inspired reinforcement learning (FLRL) algorithm is proposed to achieve the FNE solutions of the constructed NC-FG to obtain the RSA in dynamic WSN, with which both the iterative learning and decision making procedures are implemented in a fuzzy-space, thus the sensitiveness of our scheme to environmental variations is alleviated. Finally, numerical simulations are provided to demonstrate the convergence, effectiveness, and superiority of our proposed FLRL algorithm in dynamic WSNs. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2939000,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072584402&doi=10.1109%2fACCESS.2019.2939000&partnerID=40&md5=0a1e37ae9bc387d76453df726df2a1ed,2019,2021-07-20 15:49:51,2021-07-20 15:49:51
HBDAPVNA,journalArticle,2015,"Kaalia, R.; Kumar, A.; Srinivasan, A.; Ghosh, I.",An Ab Initio Method for Designing Multi-Target Specific Pharmacophores using Complementary Interaction Field of Aspartic Proteases,"For past few decades, key objectives of rational drug discovery have been the designing of specific and selective ligands for target proteins. Infectious diseases like malaria are continuously becoming resistant to traditional medicines, which inculcates need for new approaches to design inhibitors for antimalarial targets. A novel method for ab initio designing of multi target specific pharmacophores using the interaction field maps of active sites of multiple proteins has been developed to design 'specificity' pharmacophores for aspartic proteases. The molecular interaction field grid maps of active sites of aspartic proteases (plasmepsin II & IV from Plasmodium falciparum, plasmepsin from Plasmodium vivax, pepsin & cathepsin D from human) are calculated and common pharmacophoric features for favourable binding spots in active sites are extracted in the form of cliques of graphs using inductive logic programming (ILP). The two pharmacophore ensembles are constructed from largest common cliques by imposing size of receptor active site (L) and domain-specific receptor-ligand information (S). The overlap of chemical space between two ensembles and the results of virtual screening of inhibitor database with known activities show that this method can design efficient pharmacophores with no prior ligand information. © 2015 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.",Molecular Informatics,10.1002/minf.201400157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933522612&doi=10.1002%2fminf.201400157&partnerID=40&md5=202eb343e90d145b7187f40bd1642a08,2015,2021-07-20 15:49:51,2021-07-20 15:49:51
XCZY3LAE,journalArticle,2011,"Smole, D.; Čeh, M.; Podobnikar, T.",Evaluation of inductive logic programming for information extraction from natural language texts to support spatial data recommendation services,"In this article we analyze a well-known and extensively researched problem: how to find all datasets, on the one hand, and on the other hand only those that are of value to the user when dealing with a specific spatially oriented task. In analogy with existing approaches to a similar problem from other fields of human endeavor, we call this software solution 'a spatial data recommendation service.' In its final version, this service should be capable of matching requests created in the user's mind with the content of the existing datasets, while taking into account the user's preferences obtained from the user's previous use of the service. As a result, the service should recommend a list of datasets best suited to the user's needs. In this regard, we consider metadata, particularly natural language definitions of spatial entities, a crucial piece of the solution. To be able to use this information in the process of matching the user's request with the dataset content, this information must be semantically preprocessed. To automate this task we have applied a machine learning approach. With inductive logic programming (ILP) our system learns rules that identify and extract values for the five most frequent relations/properties found in Slovene natural language definitions of spatial entities. The initially established quality criterion for identifying and extracting information was met in three out of five examples. Therefore we conclude that ILP offers a promising approach to developing an information extraction component of a spatial data recommendation service. © 2011 Copyright Taylor and Francis Group, LLC.",International Journal of Geographical Information Science,10.1080/13658816.2011.556640,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859113383&doi=10.1080%2f13658816.2011.556640&partnerID=40&md5=4102f9f503ca5be1b5a2371fc696052c,2011,2021-07-20 15:49:51,2021-07-20 15:49:51
KSAHMDRK,journalArticle,2020,"Wen, N.; Zhang, F.",Extended Factorization Machines for Sequential Recommendation,"Users' historical activities are usually contained in real-life sequential recommendation systems to predict their future behaviors. In this situation, traditional Factorization Machines (FMs) approaches may be not suitable. Recently, a new surge of interest aims to use recurrent neural networks(RNN) to encode users' dynamic features with temporal characteristics. However, most of these works fail to reproduce computational simplicity of FMs. In this paper, we propose an architecture of extended-FM for sequential recommendation, which presents temporal feature interactions in an explicit way as traditional FM's formula. Our approach also allows us to accomplish computation of the model in linear time. Furthermore, we merge extended-FM into higher-order interaction framework without significant changes to the deeper models themselves. We conduct comprehensive experiments on two real-world datasets. The results demonstrate that extended-FM outperforms traditional FMs as well as deep learning feature combination models on sequential recommendation tasks. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2977231,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081627731&doi=10.1109%2fACCESS.2020.2977231&partnerID=40&md5=bc633f9bdf0f08866e8eab565efac5f3,2020,2021-07-20 15:49:52,2021-07-20 15:49:52
XAN69YHK,journalArticle,2021,"Das, S.; Ghosh, I.D.; Chattopadhyay, A.","An efficient deep sclera recognition framework with novel sclera segmentation, vessel extraction and gaze detection","Sclera recognition is a promising ocular biometric modality because of contact-less, gaze-independent image acquisition in visible light. Moreover, it is unaffected even if the subjects are wearing contact lenses in eyes. However, it is a difficult task because several steps are required, each of which must be performed accurately and efficiently. In this work, sclera recognition is performed in the following steps, namely, segmentation of sclera region, extraction of sclera vasculature pattern, detection of gaze direction and finally comparison of two vasculature patterns for matching and recognition. The proposed segmentation model DSeg is based on well-known deep learning model UNet and reduces model complexity by creating a Knowledge Base of sclera and non-sclera colors. DSeg is a lightweight and environment-friendly model, which outperforms UNet in terms of speed, efficiency and accuracy. Two rule-based unsupervised vessel extraction methods require prior sclera segmentation and exhibit competing recognition performance to a supervised deep model for vessel extraction, which does not require prior sclera segmentation. A novel deep recognition model is proposed which compares two vessel structures taking into account their affine-transformation, and produces a single Boolean output to decide whether the structures match or not. The model does not require post logic in the matching process. The model is further improved to detect errors in prediction. We achieve best recognition rates with low false-acceptance-rates for two sets of training and validation, using the publicly available dataset SBVPI and the best achieved AUC score is 0.98. © 2021 Elsevier B.V.",Signal Processing: Image Communication,10.1016/j.image.2021.116349,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107796193&doi=10.1016%2fj.image.2021.116349&partnerID=40&md5=ce85c8ae9ded790974e692ea5f30a428,2021,2021-07-20 15:49:52,2021-07-20 15:49:52
EE8L92DA,journalArticle,2021,"Acevedo, A.; Merino, A.; Boldú, L.; Molina, Á.; Alférez, S.; Rodellar, J.",A new convolutional neural network predictive model for the automatic recognition of hypogranulated neutrophils in myelodysplastic syndromes,"Background: Dysplastic neutrophils commonly show at least 2/3 reduction of the content of cytoplasmic granules by morphologic examination. Recognition of less granulated dysplastic neutrophils by human eyes is difficult and prone to inter-observer variability. To tackle this problem, we proposed a new deep learning model (DysplasiaNet) able to automatically recognize the presence of hypogranulated dysplastic neutrophils in peripheral blood. Methods: Eight models were generated by varying convolutional blocks, number of layer nodes and fully connected layers. Each model was trained for 20 epochs. The five most accurate models were selected for a second stage, being trained again from scratch for 100 epochs. After training, cut-off values were calculated for a granularity score that discerns between normal and dysplastic neutrophils. Furthermore, a threshold value was obtained to quantify the minimum proportion of dysplastic neutrophils in the smear to consider that the patient might have a myelodysplastic syndrome (MDS). The final selected model was the one with the highest accuracy (95.5%). Results: We performed a final proof of concept with new patients not involved in previous steps. We reported 95.5% sensitivity, 94.3% specificity, 94% precision, and a global accuracy of 94.85%. Conclusions: The primary contribution of this work is a predictive model for the automatic recognition in an objective way of hypogranulated neutrophils in peripheral blood smears. We envision the utility of the model implemented as an evaluation tool for MDS diagnosis integrated in the clinical laboratory workflow. © 2021 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2021.104479,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106276741&doi=10.1016%2fj.compbiomed.2021.104479&partnerID=40&md5=62407ed659b7431e55dc5195d4edbe4f,2021,2021-07-20 15:49:52,2021-07-20 15:49:52
FL52RRMU,journalArticle,2021,"Thangakrishnan, M.S.; Ramar, K.",Automated Hand-drawn sketches retrieval and recognition using regularized Particle Swarm Optimization based deep convolutional neural network,"One of the most popular and rising research area of image processing is free hand-drawn sketch recognition and its retrieval. Enlarger number of methods is introduced to retrieve the sketch images but it made few complexity issues and their performance often degraded. So, in this paper, we proposed an effective method of Regularized Particle Swarm Optimization Based Deep Convolutional Neural Network (RPSO-DCNN) algorithm to retrieve the performance of free hand-drawn sketches. In feature extraction, the Regularized Particle Swarm Optimization (RPSO) model that aim is to produce an optimal evolutionary deep learning result. Therefore, the free hand-drawn sketch image classification and its retrieval are performed by Support Vector Machine and Levenshtein distance-based fuzzy k-nearest neighbour (L-FkNN) algorithms. Hence, this work can bring in communication between human and computer. Experimentally, the simulation work of the proposed RPSO-DCNN model is implemented in the running software of MATLAB. The sketch images are chosen from the TU-Berlin dataset, Sketch dataset, SHREC13 dataset, Flickr dataset and Sketchy dataset. Aiming is to facilitate the performance of the proposed RPSO-DCNN model with various kinds of state of art methods such as H-CNN, Fuzzy, CNN, MARQS and TCVD. The experimental result demonstrates that, the proposed RPSO-DCNN accomplish the optimal accuracy with different state-of-art methods. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-020-02248-9,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087491850&doi=10.1007%2fs12652-020-02248-9&partnerID=40&md5=f5f575445ce27a58bc700157c39fcafd,2021,2021-07-20 15:49:52,2021-07-20 15:49:52
C4GS3V5K,journalArticle,2021,"Wu, W.; Huang, Z.; Zeng, J.; Fan, K.",A fast decision-making method for process planning with dynamic machining resources via deep reinforcement learning,"Mass customized production brings great uncertainty to the computer-aided process planning (CAPP). Current CAPP methods based on heuristic optimization assume in advance that manufacturing resources are static and make a deterministic plan that cannot cope with the uncertainty of the manufacture environment. As a promising method in solving complex and dynamic decision-making problems, deep reinforcement learning is employed in this paper for process planning, aiming at promoting the response speed by exploiting the reusability and expandability of past decision-making experiences. To simplify the decision procedure, two different types of decisions, operation sequencing and resource selection, are fused into one by integrating environment states and agent behaviors in a matrix manner. Then, a masking algorithm is developed to screen out currently inexecutable machining operations at each decision step and process planning datasets are generated for training and testing according to the actual processing logic. Next, the Monte Carlo method and the deep learning algorithm are utilized to evaluate and improve the process policy, respectively. Finally, the searching capability of the proposed method for both static and dynamic manufacturing resources are tested in case studies, and the results are discussed. It is shown that the proposed approach can solve the planning problem more efficiently compared with current optimization-based approaches. © 2020 The Society of Manufacturing Engineers",Journal of Manufacturing Systems,10.1016/j.jmsy.2020.12.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099500807&doi=10.1016%2fj.jmsy.2020.12.015&partnerID=40&md5=2bda22af09b70b7a54139085668eb65c,2021,2021-07-20 15:49:52,2021-07-20 15:49:52
HXLUKGHU,journalArticle,2020,"Cao, D.; Bai, G.",DNN-based surrogate modeling-based feasible performance reliability design methodology for aircraft engine,"The risks and costs of developing a new aeroengine are fundamentally depending on the performance design final proposal. Thus, this paper presents a novel aeroengine performance design methodology that is committed to managing the effectiveness and economic availability of the design proposal. To reach such a target, the presented methodology formulates the traditional thermal cycle design problem as a reliability-based fuzzy optimization. The performance reliability is predicted by the deep neural network (DNN)-based surrogate models while a hyperparameter tuning technique is proposed to find the optimal DNN topology for better implementing a particular deep learning task. The testing results imply the DNN models with optimized topology possess remarkable function approximation capability in global so that achieves significantly higher prediction accuracy. Moreover, the DNN-based surrogate models only cost nearly 0.003% as much computing time as MC simulation (2.3591 sec vs 64746 sec, for 20 samples). Such kind of remarkably higher computational efficiency facilitates the optimization for reliability-based fitness calculation. The efficiency of the presented methodology can be further verified by abundant feasible cycle proposals. The obtained cycle solutions can achieve expected reliability (&#x003E;95%) in all reference states without unnecessary performance redundancy. Besides, the diversity of feasible cycle solutions contributes to the selection of best proposal associated with engineering situation. The presented effort is favorable to acquire a more cost-efficient design proposal and enrich thermodynamic system design theory. CCBY",IEEE Access,10.1109/ACCESS.2020.3044949,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85098758869&doi=10.1109%2fACCESS.2020.3044949&partnerID=40&md5=87ce1bc05ea9a3bbb018bb6e037630a0,2020,2021-07-20 15:49:52,2021-07-20 15:49:52
VK934TGV,journalArticle,2020,"Yang, X.; Li, F.; Liu, H.",TTL-IQA: Transitive Transfer Learning based no-reference Image Quality Assessment,"Image quality assessment (IQA) based on deep learning faces the overfitting problem due to limited training samples available in existing IQA databases. Transfer learning is a plausible solution to the problem, in which the shared features derived from the large-scale Imagenet source domain could be transferred from the original recognition task to the intended IQA task. However, the Imagenet source domain and the IQA target domain as well as their corresponding tasks are not directly related. In this paper, we propose a new transitive transfer learning method for no-reference image quality assessment (TTL-IQA). First, the architecture of the multi-domain transitive transfer learning for IQA is developed to transfer the Imagenet source domain to the auxiliary domain, and then to the IQA target domain. Second, the auxiliary domain and the auxiliary task are constructed by a new generative adversarial network based on distortion translation (DT-GAN). Furthermore, a TTL network of the semantic features transfer (SFTnet) is proposed to optimize the shared features for the TTL-IQA. Experiments are conducted to evaluate the performance of the proposed method on various IQA databases, including the LIVE, TID2013, CSIQ, LIVE multiply distorted and LIVE challenge. The results show that the proposed method significantly outperforms the state-of-the-art methods. In addition, our proposed method demonstrates a strong generalization ability. IEEE",IEEE Transactions on Multimedia,10.1109/TMM.2020.3040529,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097201809&doi=10.1109%2fTMM.2020.3040529&partnerID=40&md5=3430baee6fc1f58235f82f9e8dbca799,2020,2021-07-20 15:49:52,2021-07-20 15:49:52
EAQYD8H8,journalArticle,2020,"Zhang, X.; Fang, F.; Wang, J.",Probabilistic Solar Irradiation Forecasting based on Variational Bayesian Inference with Secure Federated Learning,"The irradiation forecasting technology is important for the effective utilization of solar power. Existing irradiation forecasting methods have achieved excellent performance with a massive amount of data in a centralized way. However, concerns about privacy protection and data security, which may arise in the process of data collection and transmission from distributed points to the centralized server, pose challenges to current forecasting methods. In this paper, a novel federated probabilistic forecasting scheme of solar irradiation is proposed based on deep learning, variational Bayesian inference, and federated learning. In this scheme, the training data are stored and computed in local IoT devices, only forecasting models are shared. Two real-world datasets from SolarGIS and NSRDB, and one benchmark dataset of Folsom are used to verify the feasibility and performance of the federated-based scheme. Comprehensive case studies are conducted to analyze the performance of the proposed scheme in multi-horizon. And the effects of its components are evaluated. Compared with other state-of-the-art probabilistic centralized models, when data can be shared, the proposed scheme achieves competitive forecasting performance on the basis of data privacy protection; when data-sharing is unavailable, due to the cooperative nature inherent (model-sharing) of federated learning, the performance advantage of the proposed scheme is more obvious. IEEE",IEEE Transactions on Industrial Informatics,10.1109/TII.2020.3035807,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096832954&doi=10.1109%2fTII.2020.3035807&partnerID=40&md5=69af73e5bc95a263e851775ec89844fa,2020,2021-07-20 15:49:52,2021-07-20 15:49:52
8KSVN7FP,journalArticle,2019,"Yu, X.; Tan, Y.-A.; Sun, Z.; Liu, J.; Liang, C.; Zhang, Q.",A fault-tolerant and energy-efficient continuous data protection system,"Storage reliability of massive amounts of data is the basis for deep learning, and continuous data protection (CDP) technology is an effective means of ensuring it. The method of storing CDP historical data, represented by TRAP-Array parity logs, effectively reduces the overheads of storage capacity, but it increases the risk of historical data loss due to the absence of the RAID protection mechanism for the parity data. This paper proposes a design method for a fault-tolerant and energy-efficient continuous data protection system (FTEECDP), which is composed of SSD mirror array, TRAP parity generation logic and S-RAID disk array. Taking the SSD mirror array as the source data volume of the system to provide the I/O service of the upper applications, FTEECDP improves the throughput of the system. This method also takes S-RAID as the CDP log volume to store the TRAP parity logs. When the disk data error occurs, the error data will be recovered from the data redundancy information of S-RAID, thus protecting the TRAP parity chain from being destroyed and improving the reliability of the TRAP parity logs. The experiment shows that in the continuous data protection system, the CDP logs stored as the TRAP parity can greatly reduce the storage space overheads and reduce the system energy consumption. In addition, with the disk scheduling algorithm, S-RAID shifts the disk grouping without any data requests into a standby state, thus further reducing the energy consumption of the system. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-018-0726-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049588630&doi=10.1007%2fs12652-018-0726-2&partnerID=40&md5=ea20246487ee5a33b6496355687289c5,2019,2021-07-20 15:49:52,2021-07-20 15:49:52
2DXCL6ZI,journalArticle,2020,"Kumar, V.S.; Boulanger, D.",Automated Essay Scoring and the Deep Learning Black Box: How Are Rubric Scores Determined?,"This article investigates the feasibility of using automated scoring methods to evaluate the quality of student-written essays. In 2012, Kaggle hosted an Automated Student Assessment Prize contest to find effective solutions to automated testing and grading. This article: a) analyzes the datasets from the contest – which contained hand-graded essays – to measure their suitability for developing competent automated grading tools; b) evaluates the potential for deep learning in automated essay scoring (AES) to produce sophisticated testing and grading algorithms; c) advocates for thorough and transparent performance reports on AES research, which will facilitate fairer comparisons among various AES systems and permit study replication; d) uses both deep neural networks and state-of-the-art NLP tools to predict finer-grained rubric scores, to illustrate how rubric scores are determined from a linguistic perspective, and to uncover important features of an effective rubric scoring model. This study’s findings first highlight the level of agreement that exists between two human raters for each rubric as captured in the investigated essay dataset, that is, 0.60 on average as measured by the quadratic weighted kappa (QWK). Only one related study has been found in the literature which also performed rubric score predictions through models trained on the same dataset. At best, the predictive models had an average agreement level (QWK) of 0.53 with the human raters, below the level of agreement among human raters. In contrast, this research’s findings report an average agreement level per rubric with the two human raters’ resolved scores of 0.72 (QWK), well beyond the agreement level between the two human raters. Further, the AES system proposed in this article predicts holistic essay scores through its predicted rubric scores and produces a QWK of 0.78, a competitive performance according to recent literature where cutting-edge AES tools generate agreement levels between 0.77 and 0.81, results computed as per the same procedure as in this article. This study’s AES system goes one step further toward interpretability and the provision of high-level explanations to justify the predicted holistic and rubric scores. It contends that predicting rubric scores is essential to automated essay scoring, because it reveals the reasoning behind AIED-based AES systems. Will building AIED accountability improve the trustworthiness of the formative feedback generated by AES? Will AIED-empowered AES systems thoroughly mimic, or even outperform, a competent human rater? Will such machine-grading systems be subjected to verification by human raters, thus paving the way for a human-in-the-loop assessment mechanism? Will trust in new generations of AES systems be improved with the addition of models that explain the inner workings of a deep learning black box? This study seeks to expand these horizons of AES to make the technique practical, explainable, and trustable. © 2020, International Artificial Intelligence in Education Society.",International Journal of Artificial Intelligence in Education,10.1007/s40593-020-00211-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091086122&doi=10.1007%2fs40593-020-00211-5&partnerID=40&md5=122d97093647eeedc47e4498b1fa0229,2020,2021-07-20 15:49:52,2021-07-20 15:49:52
U29KKX7J,journalArticle,2021,"de Souza, R.W.R.; Silva, D.S.; Passos, L.A.; Roder, M.; Santana, M.C.; Pinheiro, P.R.; de Albuquerque, V.H.C.",Computer-assisted Parkinson's disease diagnosis using fuzzy optimum- path forest and Restricted Boltzmann Machines,"Parkinson's disease (PD) is a progressive neurodegenerative illness associated with motor skill disorders, affecting thousands of people, mainly elderly, worldwide. Since its symptoms are not clear and commonly confused with other diseases, providing early diagnosis is a challenging task for traditional methods. In this context, computer-aided assistance is an alternative method for a fast and automatic diagnosis, accelerating the treatment and alleviating an excessive effort from professionals. Moreover, the most recent studies proposing a solution to this problem lack in computational efficiency, prediction power, reliability among other factors. Therefore, this work proposes a Fuzzy Optimum Path Forest for automated PD identification, which is based on fuzzy logic and graph-based framework theory. Experiments consider a dataset composed of features extracted from hand-drawn images using Restricted Boltzmann Machines, and results are compared with baseline models such as Support Vector Machines, KNN, and the standard OPF classifier. Results show that the proposed model outperforms the baselines in most cases, suggesting the Fuzzy OPF as a viable alternative to deal with PD detection problems. © 2021 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2021.104260,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100719703&doi=10.1016%2fj.compbiomed.2021.104260&partnerID=40&md5=4a989192b969f46f31c1bf5aa6ca6c92,2021,2021-07-20 15:49:52,2021-07-20 15:49:52
9PJ63IMU,journalArticle,2020,"Ma, Y.; Lu, C.; Sinopoli, B.; Zeng, S.",Exploring Edge Computing for Multitier Industrial Control,"Industrial automation traditionally relies on local controllers implemented on microcontrollers or programmable logic controllers. With the emergence of edge computing, however, industrial automation evolves into a distributed two-tier computing architecture comprising local controllers and edge servers that communicate over wireless networks. Compared to local controllers, edge servers provide larger computing capacity at the cost of data loss over wireless networks. This article presents switching multitier control (SMC) to exploit edge computing for industrial control. SMC dynamically optimizes control performance by switching between local and edge controllers in response to changing network conditions. SMC employs a data-driven approach to derive switching policies based on classification models trained based on simulations while guaranteeing system stability based on an extended Simplex approach tailored for two-tier platforms. To evaluate the performance of industrial control over edge computing platforms, we have developed WCPS-EC, a real-time hybrid simulator that integrates simulated plants, real computing platforms, and real or simulated wireless networks. In a case study of an industrial robotic control system, SMC significantly outperformed both a local controller and an edge controller in face of varying data loss in a wireless network. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.3012648,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096033800&doi=10.1109%2fTCAD.2020.3012648&partnerID=40&md5=2c986a87cf2f65da0bc9ea356489c947,2020,2021-07-20 15:49:52,2021-07-20 15:49:52
M6MWATK2,journalArticle,2018,"Zhang, J.; Williams, S.O.; Wang, H.",Intelligent computing system based on pattern recognition and data mining algorithms,"The integration of intelligent system mainly includes the application of intelligent technology, such as artificial intelligence and computational intelligence method, which is used in different levels of the system. This paper introduces the application and technology of several intelligent system integrations, the advantages and disadvantages of learning theory and expert system. Neural network is applied in intelligent systems and we use scope reviewed several new development of intelligent technology, plus this paper describes the development direction of the intelligent system. This paper introduces the basic concepts of data mining, including data mining technology, artificial intelligence, machine learning, statistical analysis, fuzzy logic, pattern recognition and artificial neural networks and other technologies. We analyze the structure of the general algorithm of data mining, and classify the data mining technology in details, including more than 10 techniques of decision tree technology, neural network technology, rough set and fuzzy set. Finally, the research directions of data mining in artificial intelligence, e-commerce applications and mobile communication computing are discussed. © 2017 Elsevier Inc.",Sustainable Computing: Informatics and Systems,10.1016/j.suscom.2017.10.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034587707&doi=10.1016%2fj.suscom.2017.10.010&partnerID=40&md5=95709003e570d27b8a75225b637e29f3,2018,2021-07-20 15:49:52,2021-07-20 15:49:52
L59HZ77Q,journalArticle,2017,"Chakraborty, R.S.; Jeldi, R.R.; Saha, I.; Mathew, J.",Binary Decision Diagram Assisted Modeling of FPGA-Based Physically Unclonable Function by Genetic Programming,"We present a computationally efficient technique to build concise and accurate computational models for large (60 or more inputs, 1 output) Boolean functions, only a very small fraction of whose truth table is known during model building. We use Genetic Programming with Boolean logic operators, and enhance the accuracy of the technique using Reduced Ordered Binary Decision Diagram based representations of Boolean functions, whereby we exploit their canonical forms. We demonstrate the effectiveness of the proposed technique by successfully modeling several common Boolean functions, and ultimately by accurately modeling a 63-input Physically Unclonable Function circuit design on Xilinx Field Programmable Gate Array. We achieve better accuracy (at lesser computational overhead) in predicting truth table entries not seen during model building, than a previously proposed machine learning based modeling technique for similar Physically Unclonable Function circuits using Support Vector Machines. The success of this modeling technique has important implications in determining the acceptability of Physically Unclonable Functions as useful hardware security primitives, in applications such as anti-counterfeiting of integrated circuits. © 1968-2012 IEEE.",IEEE Transactions on Computers,10.1109/TC.2016.2603498,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027306895&doi=10.1109%2fTC.2016.2603498&partnerID=40&md5=f97af2090dd6c4a8d5096e48a4921293,2017,2021-07-20 15:49:52,2021-07-20 15:49:52
Z3Y3BTDB,journalArticle,2015,"Shell, J.; Coupland, S.",Fuzzy Transfer Learning: Methodology and application,"Producing a methodology that is able to predict output using a model is a well studied area in Computational Intelligence (CI). However, a number of real-world applications require a model but have little or no data available of the specific environment. Predominantly, standard machine learning approaches focus on a need for training data for such models to come from the same domain as the target task. Such restrictions can severely reduce the data acquisition making it extremely costly, or in certain situations, impossible. This impedes the ability of these approaches to model such environments. It is on this particular problem that this paper is focussed. In this paper two concepts, Transfer Learning (TL) and Fuzzy Logic (FL) are combined in a framework, Fuzzy Transfer Learning (FuzzyTL), to address the problem of learning tasks that have no prior direct contextual knowledge. Through the use of a FL based learning method, uncertainty that is evident in dynamic environments is represented. By applying a TL approach through the combining of labelled data from a contextually related source task, and little or no unlabelled data from a target task, the framework is shown to be able to accomplish predictive tasks using models learned from contextually different data. ©2014 Published by Elsevier Inc. All rights reserved.",Information Sciences,10.1016/j.ins.2014.09.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84922440761&doi=10.1016%2fj.ins.2014.09.004&partnerID=40&md5=4e76aad13354c61f42a447601454419a,2015,2021-07-20 15:49:53,2021-07-20 15:49:53
RK7826XH,journalArticle,2014,"Ghofrani, F.; Helfroush, M.S.; Danyali, H.; Kazemi, K.",Improving the performance of machine learning algorithms using fuzzy-based features for medical x-ray image classification,"This paper proposes a novel approach for medical x-ray image classification using fuzzification of Contourlet-based Center Symmetric Local Binary Patterns (CCS-LBPs). The proposed classification method consists of three stages. In the first stage, local features are obtained by partitioning each image into 25 overlapping sub-images, computing the 2-level contourlet transform of each subimage and extracting CS-LBPs from each resulting subband. In the second stage, fuzzy logic using reduced CCS-LBPs is employed to determine the degree of membership of subimages to each class. Finally, in order to assign images to their respective classes, we utilize membership values as the input of classifiers such as support vector machine (SVM) and k-nearest neighbor (K-NN). This work makes a major contribution to improve the performance of these classifiers. We conducted experiments on a subset of IRMA dataset to evaluate the effectiveness of our classification scheme. Experimental results reveal that the proposed scheme not only achieves a very good performance but also learns well even with a small number of training images. © 2014 - IOS Press and the authors.",Journal of Intelligent and Fuzzy Systems,10.3233/IFS-141273,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84915760666&doi=10.3233%2fIFS-141273&partnerID=40&md5=dea194a787eb947f9cb95cee3910aa33,2014,2021-07-20 15:49:53,2021-07-20 15:49:53
HFJACF3N,journalArticle,2020,"Zhou, Z.; Li, S.; Qin, G.; Folkert, M.; Jiang, S.; Wang, J.",Multi-Objective-Based Radiomic Feature Selection for Lesion Malignancy Classification,"Objective: accurately classifying the malignancy of lesions detected in a screening scan is critical for reducing false positives. Radiomics holds great potential to differentiate malignant from benign tumors by extracting and analyzing a large number of quantitative image features. Since not all radiomic features contribute to an effective classifying model, selecting an optimal feature subset is critical. Methods: this work proposes a new multi-objective based feature selection (MO-FS) algorithm that considers sensitivity and specificity simultaneously as the objective functions during feature selection. For MO-FS, we developed a modified entropy-based termination criterion that stops the algorithm automatically rather than relying on a preset number of generations. We also designed a solution selection methodology for multi-objective learning that uses the evidential reasoning approach (SMOLER) to automatically select the optimal solution from the Pareto-optimal set. Furthermore, we developed an adaptive mutation operation to generate the mutation probability in MO-FS automatically. Results: we evaluated the MO-FS for classifying lung nodule malignancy in low-dose CT and breast lesion malignancy in digital breast tomosynthesis. Conclusion: the experimental results demonstrated that the feature set selected by MO-FS achieved better classification performance than features selected by other commonly used methods. Significance: the proposed method is general and more effective radiomic feature selection strategy. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2019.2902298,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072975005&doi=10.1109%2fJBHI.2019.2902298&partnerID=40&md5=6dd50bf0300404755c4261bb4e488ef2,2020,2021-07-20 15:49:53,2021-07-20 15:49:53
6AFQRCB4,journalArticle,2021,"Selvachandran, G.; Quek, S.G.; Lan, L.T.H.; Son, L.H.; Giang, N.L.; Ding, W.; Abdel-Basset, M.; De Albuquerque, V.H.C.",A New Design of Mamdani Complex Fuzzy Inference System for Multiattribute Decision Making Problems,"This article proposes the Mamdani complex fuzzy inference system (Mamdani CFIS) to improve performance of the classical FIS and complex FIS. The applicability of the proposed CFIS is demonstrated by applying it to six commonly available datasets from UCI Machine Learning under the comparison with Mamdani FIS and the Adaptive Neuro Complex Fuzzy Inference System (ANCFIS). It is successfully proven that the proposed Mamdani CFIS is computationally less expensive and presents a more efficient method to handle time-series data and time-periodic phenomena, among all the fuzzy IS found thus far in the literature. Furthermore, the novelty of CFIS mainly lies in its implementation of the complex number throughout the entire procedures of computation. This gives much greater flexibility of implementing unexpected, nonlinear fluctuations. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2019.2961350,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103881370&doi=10.1109%2fTFUZZ.2019.2961350&partnerID=40&md5=8f199e8a4bb2b20bc65a54bcc907e051,2021,2021-07-20 15:49:53,2021-07-20 15:49:53
BVVX8YF3,journalArticle,2020,"Firat, M.; Crognier, G.; Gabor, A.F.; Hurkens, C.A.J.; Zhang, Y.",Column generation based heuristic for learning classification trees,"This paper explores the use of Column Generation (CG) techniques in constructing univariate binary decision trees for classification tasks. We propose a novel Integer Linear Programming (ILP) formulation, based on root-to-leaf paths in decision trees. The model is solved via a Column Generation based heuristic. To speed up the heuristic, we use a restricted instance data by considering a subset of decision splits, sampled from the solutions of the well-known CART algorithm. Extensive numerical experiments show that our approach is competitive with the state-of-the-art ILP-based algorithms. In particular, the proposed approach is capable of handling big data sets with tens of thousands of data rows. Moreover, for large data sets, it finds solutions competitive to CART. © 2019",Computers and Operations Research,10.1016/j.cor.2019.104866,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076856329&doi=10.1016%2fj.cor.2019.104866&partnerID=40&md5=3b9a99b4d4042479b54cac36680d9ebd,2020,2021-07-20 15:49:53,2021-07-20 15:49:53
S49KWVJ7,journalArticle,2019,"He, C.; Liu, Y.; Yao, T.; Xu, F.; Hu, Y.; Zheng, J.",A fast learning algorithm based on extreme learning machine for regular fuzzy neural network,The regular fuzzy neural network (RFNN) is a kind of fuzzy neural network by fuzzifying the feed-forward neural network. The RFNN can directly deal with the language information and it has the merits of fuzzy system and neural network. It is presented a fast learning algorithm based on the extreme learning machine (ELM) for the RFNN in this paper. The RFNN referred here is a three-layer feed-forward fuzzy neural network and the connected weights in the RFNN are all fuzzy numbers. A simulation example is given to approximately realize the fuzzy if-Then rules by the RFNN. The results show that the RFNN trained by the proposed algorithm has good performance and approximation ability. © 2019-IOS Press and the authors. All rights reserved.,Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-18046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064672921&doi=10.3233%2fJIFS-18046&partnerID=40&md5=98fb6f1a7da01c86c46b95cbf0aba45f,2019,2021-07-20 15:49:53,2021-07-20 15:49:53
FBFBQDX2,journalArticle,2017,"Bordes, J.-B.; Davoine, F.; Xu, P.; Denœux, T.",Evidential grammars: A compositional approach for scene understanding. Application to multimodal street data,"Automatic scene understanding from multimodal data is a key task in the design of fully autonomous vehicles. The theory of belief functions has proved effective for fusing information from several sensors at the superpixel level. Here, we propose a novel framework, called evidential grammars, which extends stochastic grammars by replacing probabilities by belief functions. This framework allows us to fuse local information with prior and contextual information, also modeled as belief functions. The use of belief functions in a compositional model is shown to allow for better representation of the uncertainty on the priors and for greater flexibility of the model. The relevance of our approach is demonstrated on multi-modal traffic scene data from the KITTI benchmark suite. © 2017 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2017.06.020,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026532806&doi=10.1016%2fj.asoc.2017.06.020&partnerID=40&md5=194600240f0bf3c617a806bc09664f30,2017,2021-07-20 15:49:53,2021-07-20 15:49:53
53KYX4X9,journalArticle,2015,"Mathew, J.; Chakraborty, R.S.; Sahoo, D.P.; Yang, Y.; Pradhan, D.K.",A novel memristor based physically unclonable function,"Memristor is an exciting new addition to the repertoire of fundamental circuit elements. Alternatives to many security protocols originally employing traditional mathematical cryptography involve novel hardware security primitives, such as Physically Unclonable Functions (PUFs). In this paper, we first introduce a novel hybrid memristor-CMOS XOR/XNOR logic circuit that offers several advantages such as combinational circuit behavior, simpler operation and lower hardware overhead than existing solutions. Then, we use this XOR circuit as a component to design a hybrid memristor-CMOS PUF circuit and demonstrate its effectiveness through extensive simulations of environmental and process variation effects. The proposed PUF circuit has substantially lesser hardware overhead than previously proposed memristor-based PUF circuits, while being resistant against machine learning based modelling attacks. The proposed PUF can be conveniently used in many security applications and protocols based on hardware-intrinsic security. © 2015 Elsevier B.V.","Integration, the VLSI Journal",10.1016/j.vlsi.2015.05.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944077019&doi=10.1016%2fj.vlsi.2015.05.005&partnerID=40&md5=c83a15bfd06dfeb6319a44416453423a,2015,2021-07-20 15:49:53,2021-07-20 15:49:53
99BM72LD,journalArticle,2014,"Chen, D.; Zhang, D.",Structure of feature spaces related to fuzzy similarity relations as kernels,"Fuzzy similarity relations have been considered as kernels in the domain of machine learning in previous works. As a supplementary of this consideration, this paper focuses on construction of feature space related to fuzzy similarity relations as indefinite kernels. First we generalize the Mercer Theorem from positive definite kernels to fuzzy similarity relations. With this generalization we construct the feature space related to a fuzzy similarity relation as a particular Krein space. The proposed feature space is a naturally generalization of the existing Pseudo-Euclidean space from finite to infinite universe of discourses. At the end of this paper we develop an open problem to set up the connection between transitivity and the positive definiteness of a fuzzy similarity relation. Results in this paper complete the mathematical foundation for further applications of fuzzy similarity relations under the framework of kernel tricks.",Fuzzy Sets and Systems,10.1016/j.fss.2013.08.017,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891871674&doi=10.1016%2fj.fss.2013.08.017&partnerID=40&md5=107a70b0888b0f9c54aecd0e728b1f17,2014,2021-07-20 15:49:53,2021-07-20 15:49:53
E2Z893VQ,journalArticle,2020,"Gu, P.; Xie, X.; Li, S.; Niu, D.; Zheng, H.; Malladi, K.T.; Xie, Y.",DLUX: a LUT-based Near-Bank Accelerator for Data Center Deep Learning Training Workloads,"The frequent data movement between the processor and the memory has become a severe performance bottleneck for deep neural network (DNN) training workloads in data centers. To solve this off-chip memory access challenge, the 3D stacking processing-in-memory (3D-PIM) architecture provides a viable solution. However, existing 3D-PIM designs for DNN training suffer from the limited memory bandwidth in the base logic die. To overcome this obstacle, integrating the DNN related logic near each memory bank becomes a promising yet challenging solution, since naively implementing the floating-point (FP) unit and the cache in the memory die incurs large area overhead. To address these problems, we propose DLUX, a high performance and energy-efficient 3D-PIM accelerator for DNN training using the near-bank architecture. From the hardware perspective, to support the FP multiplier with low area overhead, an in-DRAM lookup table (LUT) mechanism is invented. Then, we propose to use a small scratchpad buffer together with a lightweight transformation engine to exploit the locality and enable flexible data layout without the expensive cache. From the software aspect, we split the mapping/scheduling tasks during DNN training into intra-layer and inter-layer phases. During the intra-layer phase, to maximize data reuse in the LUT buffer and the scratchpad buffer, achieve high concurrency, and reduce data movement among banks, a 3D-PIM customized loop tiling technique is adopted. During the inter-layer phase, efficient techniques are invented to ensure the input-output data layout consistency and realize the forward-backward layout transposition. Experiment results show that DLUX can reduce FP32 multiplier area overhead by 60% against the direct implementation. Compared with a Tesla V100 GPU, end-to-end evaluations show that DLUX can provide on average 6.3&#x00D7; speedup and 42&#x00D7; energy efficiency improvement. IEEE",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.3021336,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090440162&doi=10.1109%2fTCAD.2020.3021336&partnerID=40&md5=b38b5e15c61838a6bded3a02aba95afb,2020,2021-07-20 15:49:54,2021-07-20 15:49:54
2IIUUQKI,journalArticle,2019,"Hoe, D.H.K.",Bayesian inference using stochastic logic: A study of buffering schemes for mitigating autocorrelation,"Bayesian inference has become near ubiquitous in the design of algorithms for machine learning and autonomous robotic systems as this method provides a rigorous mathematical framework for the probabilistically handling of data with elements of uncertainty. As conventional computer architectures are inefficient in their implementation of probabilistic approaches to inference, stochastic computation has recently emerged as a viable alternative. Recent research has demonstrated that Bayes' rule can be directly implemented with a Muller C-element using stochastic computing. However, the switching inertia at the output due to the inherent memory effect in the C-element limits the accuracy of this approach. Previous methods for reducing this autocorrelation effect have ranged in complexity from counter-based regeneration to bit-reshuffling with memory buffers. Simplified buffering techniques that function independently of the state of C-element are proposed in this work and the effectiveness of the autocorrelation mitigation is evaluated. In addition, the use of multi-input C-elements are shown to add design flexibility for implementing Bayesian inference. Detailed numerical simulations and analysis of configurations that range from a simple cascade of gates to tree structures with large multi-input C-elements demonstrate the viability and limitations of the proposed simplified buffering approaches. These results are significant for their potential for enabling compact implementations of stochastic Bayesian approaches as control lines and random number generators can be shared across multiple C-elements. © 2019 Elsevier Inc.",International Journal of Approximate Reasoning,10.1016/j.ijar.2019.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066822055&doi=10.1016%2fj.ijar.2019.05.007&partnerID=40&md5=a271bf9f5fc5ac5a70343e9bfc2632ed,2019,2021-07-20 15:49:54,2021-07-20 15:49:54
CWDWGM8M,journalArticle,2019,"Hsieh, J.-G.; Jeng, J.-H.; Lin, Y.-L.; Kuo, Y.-S.",Single index fuzzy neural networks using locally weighted polynomial regression,"The novel single index fuzzy neural network models are proposed in this paper for general machine learning problems. The proposed models are different from the usual fuzzy neural network models in that the output nodes of the networks are replaced by (nonparametric) single index models. Specifically, instead of pre-specifying the output activation functions as in the usual models, they are re-estimated adaptively during the training process via “Loess” (“LOcal regrESSion”), a powerful (nonparametric) scatterplot smoother. These estimated activation functions are not necessarily the usual sigmoidal or identity functions. It is interesting to find that in many cases the estimated output activation functions are well approximated by simple polynomial or generalized hyperbolic tangent functions. These problem-tailored simple functions can, if necessary, then be used as the actual output activation functions for neural network training and prediction. Particle swarm optimization, a commonly used evolutionary computation technique, is adopted in this study to search the optimal connection weights of the neural networks. The main advantages of the single index fuzzy neural network models are that they are well suited in situations when one lacks the information about the probability distribution of the response and it is not necessary to specify the output activation functions of the neural networks. Simulation results show that the proposed models usually provide better fits than the usual models for the data at hand. © 2019 Elsevier B.V.",Fuzzy Sets and Systems,10.1016/j.fss.2019.02.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062340898&doi=10.1016%2fj.fss.2019.02.010&partnerID=40&md5=4caea19a718f1782a96a1b6e76fe2d3e,2019,2021-07-20 15:49:54,2021-07-20 15:49:54
SKYFJ6YL,journalArticle,2019,"Afifi, S.; GholamHosseini, H.; Sinha, R.",A system on chip for melanoma detection using FPGA-based SVM classifier,"Support Vector Machine (SVM) is a robust machine learning model that shows high accuracy with different classification problems, and is widely used for various embedded applications. However, implementation of embedded SVM classifiers is challenging, due to the inherent complicated computations required. This motivates implementing the SVM on hardware platforms for achieving high performance computing at low cost and power consumption. Melanoma is the most aggressive form of skin cancer that increases the mortality rate. We aim to develop an optimized embedded SVM classifier dedicated for a low-cost handheld device for early detection of melanoma at the primary healthcare. In this paper, we propose a hardware/software co-design for implementing the SVM classifier onto FPGA to realize melanoma detection on a chip. The implemented SVM on a recent hybrid FPGA (Zynq) platform utilizing the modern UltraFast High-Level Synthesis design methodology achieves efficient melanoma classification on chip. The hardware implementation results demonstrate classification accuracy of 97.9%, and a significant hardware acceleration rate of 21 with only 3% resources utilization and 1.69 W for power consumption. These results show that the implemented system on chip meets crucial embedded system constraints of high performance and low resources utilization, power consumption, and cost, while achieving efficient classification with high classification accuracy. © 2018 Elsevier B.V.",Microprocessors and Microsystems,10.1016/j.micpro.2018.12.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059594690&doi=10.1016%2fj.micpro.2018.12.005&partnerID=40&md5=70b1a3b27bbfdaae255b8e164633a466,2019,2021-07-20 15:49:54,2021-07-20 15:49:54
LMBS87C6,journalArticle,2019,"Zhang, R.; Chen, X.; Wen, S.; Zheng, X.; Ding, Y.",Using AI to Attack VA: A stealthy spyware against voice assistances in smart phones,"Voice Assistants (VAs) are increasingly popular for human-computer interaction (HCI) smartphones. To help users automatically conduct various tasks, these tools usually come with high privileges and are able to access sensitive system resources. A comprised VA is a stepping stone for attackers to hack into users' phones. Prior work has experimentally demonstrated that VAs can be a promising attack point for HCI tools. However, the state-of-the-art approaches require ad-hoc mechanisms to activate VAs that are non-trivial to trigger in practice and are usually limited to specific mobile platforms. To mitigate the limitations faced by the state-of-the-art, we propose a novel attack approach, namely Vaspy, which crafts the users' ""activation voice"" by silently listening to users' phone calls. Once the activation voice is formed, Vaspy can select a suitable occasion to launch an attack. Vaspy embodies a machine learning model that learns suitable attacking times to prevent the attack from being noticed by the user. We implement a proof-of-concept spyware and test it on a range of popular Android phones. The experimental results demonstrate that this approach can silently craft the activation voice of the users and launch attacks. In the wrong hands, a technique like Vaspy can enable automated attacks to HCI tools. By raising awareness, we urge the community and manufacturers to revisit the risks of VAs and subsequently revise the activation logic to be resilient to the style of attacks proposed in this work. © 2019 IEEE.",IEEE Access,10.1109/ACCESS.2019.2945791,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077807710&doi=10.1109%2fACCESS.2019.2945791&partnerID=40&md5=5c32864150aa498f6dc23c79d3f05219,2019,2021-07-20 15:49:54,2021-07-20 15:49:54
ETGGRAXZ,journalArticle,2019,"Junhua, D.; Yi-An, Z.; Dong, Z.; Lixiang, Z.; Lin, Z.",Particle swarm optimization based multi-task parallel reinforcementlearning algorithm,"Transfer learning has been identified as conducive to improving the speed of machine learning in many areas. In multi-task reinforcement learning, transfer learning can assist the transfer of experiences between different tasks. The research conducted in this article is focused on two aspects. On the one hand, multi-task parallel transfer learning can improve the learning speed of parallel learning tasks. On the other hand, the learning of the current optimal experience can help the target point rewards to be transmitted to the starting point. The value of this self-learning can also accelerate the convergence speed of the reinforcement learning. According to the research into these two aspects, this paper uses the idea of particle swarm optimization (PSO) to conduct self-learning and interactive learning in multi-task parallel learning. In this paper, a new multi-task learning algorithm named PSO-MTPRL (Multi-Task Parallel Reinforcement Learning based on PSO) is proposed. Based on the idea of PSO algorithm, the Boltzmann strategy, Self-Learning Process (SLP) and Interactive Learning Process (ILP) are selected probabilistically. Based on the characteristic exhibited by reinforcement learning, segmented learning model is recommended. In the early learning stages, the complete Boltzmann exploration strategy is applied, and B-SLP-ILP (Boltzmann-SLP- ILP) learning procedure is conducted exclusively in the middle stage of the learning. In the late learning stages, Boltzmann exploration is involved again. The segmented learning model can help ensure the balance of the exploration and exploitation, in addition to ensuring that all tasks convergence. © 2019 - IOS Press and the authors. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-190209,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077439927&doi=10.3233%2fJIFS-190209&partnerID=40&md5=9997fa16056e3568f3b48784f2aca14c,2019,2021-07-20 15:49:54,2021-07-20 15:49:54
BNMJ7ASL,journalArticle,2018,"Vijay, S.A.A.; GaneshKumar, P.",Fuzzy Expert System based on a Novel Hybrid Stem Cell (HSC) Algorithm for Classification of Micro Array Data,"In the growing scenario, microarray data is extensively used since it provides a more comprehensive understanding of genetic variants among diseases. As the gene expression samples have high dimensionality it becomes tedious to analyze the samples manually. Hence an automated system is needed to analyze these samples. The fuzzy expert system offers a clear classification when compared to the machine learning and statistical methodologies. In fuzzy classification, knowledge acquisition would be a major concern. Despite several existing approaches for knowledge acquisition much effort is necessary to enhance the learning process. This paper proposes an innovative Hybrid Stem Cell (HSC) algorithm that utilizes Ant Colony optimization and Stem Cell algorithm for designing fuzzy classification system to extract the informative rules to form the membership functions from the microarray dataset. The HSC algorithm uses a novel Adaptive Stem Cell Optimization (ASCO) to improve the points of membership function and Ant Colony Optimization to produce the near optimum rule set. In order to extract the most informative genes from the large microarray dataset a method called Mutual Information is used. The performance results of the proposed technique evaluated using the five microarray datasets are simulated. These results prove that the proposed Hybrid Stem Cell (HSC) algorithm produces a precise fuzzy system than the existing methodologies. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Medical Systems,10.1007/s10916-018-0910-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042438324&doi=10.1007%2fs10916-018-0910-0&partnerID=40&md5=24c5b833b7863075a2af5558b02cd38c,2018,2021-07-20 15:49:54,2021-07-20 15:49:54
JDCYBJST,journalArticle,2018,"Saleh, E.; Błaszczyński, J.; Moreno, A.; Valls, A.; Romero-Aroca, P.; de la Riva-Fernández, S.; Słowiński, R.",Learning ensemble classifiers for diabetic retinopathy assessment,"Diabetic retinopathy is one of the most common comorbidities of diabetes. Unfortunately, the recommended annual screening of the eye fundus of diabetic patients is too resource-consuming. Therefore, it is necessary to develop tools that may help doctors to determine the risk of each patient to attain this condition, so that patients with a low risk may be screened less frequently and the use of resources can be improved. This paper explores the use of two kinds of ensemble classifiers learned from data: fuzzy random forest and dominance-based rough set balanced rule ensemble. These classifiers use a small set of attributes which represent main risk factors to determine whether a patient is in risk of developing diabetic retinopathy. The levels of specificity and sensitivity obtained in the presented study are over 80%. This study is thus a first successful step towards the construction of a personalized decision support system that could help physicians in daily clinical practice. © 2017 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2017.09.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030629171&doi=10.1016%2fj.artmed.2017.09.006&partnerID=40&md5=e95a962380b954459e3063de94d13caf,2018,2021-07-20 15:49:54,2021-07-20 15:49:54
A79JUUC4,journalArticle,2018,"Antonini, M.; Vecchio, M.; Antonelli, F.; Ducange, P.; Perera, C.",Smart audio sensors in the internet of things edge for anomaly detection,"Everyday objects are becoming smart enough to directly connect to other nearby and remote objects and systems. These objects increasingly interact with machine learning applications that perform feature extraction and model inference in the cloud. However, this approach poses several challenges due to latency, privacy, and dependency on network connectivity between data producers and consumers. To alleviate these limitations, computation should be moved as much as possible towards the IoT edge, that is on gateways, if not directly on data producers. In this paper, we propose a design framework for smart audio sensors able to record and pre-process raw audio streams, before wirelessly transmitting the computed audio features to a modular IoT gateway. In this paper, an anomaly detection algorithm executed as a micro-service is capable of analyzing the received features, hence detecting audio anomalies in real-time. First, to assess the effectiveness of the proposed solution, we deployed a real smart environment showcase. More in detail, we adopted two different anomaly detection algorithms, namely Elliptic Envelope and Isolation Forest, that were purposely trained and deployed on an affordable IoT gateway to detect anomalous sound events happening in an office environment. Then, we numerically compared both the deployments, in terms of end-to-end latency and gateway CPU load, also deriving some ideal capacity bounds. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2877523,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055720811&doi=10.1109%2fACCESS.2018.2877523&partnerID=40&md5=27b971f3e5a0cea8013fe03c62a40700,2018,2021-07-20 15:49:54,2021-07-20 15:49:54
EJ2WGC8Y,journalArticle,2015,"Tan, S.C.; Watada, J.; Ibrahim, Z.; Khalid, M.",Evolutionary fuzzy ARTMAP neural networks for classification of semiconductor defects,"Wafer defect detection using an intelligent system is an approach of quality improvement in semiconductor manufacturing that aims to enhance its process stability, increase production capacity, and improve yields. Occasionally, only few records that indicate defective units are available and they are classified as a minority group in a large database. Such a situation leads to an imbalanced data set problem, wherein it engenders a great challenge to deal with by applying machine-learning techniques for obtaining effective solution. In addition, the database may comprise overlapping samples of different classes. This paper introduces two models of evolutionary fuzzy ARTMAP (FAM) neural networks to deal with the imbalanced data set problems in a semiconductor manufacturing operations. In particular, both the FAM models and hybrid genetic algorithms are integrated in the proposed evolutionary artificial neural networks (EANNs) to classify an imbalanced data set. In addition, one of the proposed EANNs incorporates a facility to learn overlapping samples of different classes from the imbalanced data environment. The classification results of the proposed evolutionary FAM neural networks are presented, compared, and analyzed using several classification metrics. The outcomes positively indicate the effectiveness of the proposed networks in handling classification problems with imbalanced data sets. © 2012 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2014.2329097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928104623&doi=10.1109%2fTNNLS.2014.2329097&partnerID=40&md5=edfa0cff04629b1fadaf2371ffb46571,2015,2021-07-20 15:49:54,2021-07-20 15:49:54
6ERRVYX3,journalArticle,2014,"Hutter, F.; Xu, L.; Hoos, H.H.; Leyton-Brown, K.",Algorithm runtime prediction: Methods & evaluation,"Perhaps surprisingly, it is possible to predict how long an algorithm will take to run on a previously unseen input, using machine learning techniques to build a model of the algorithm's runtime as a function of problem-specific instance features. Such models have important applications to algorithm analysis, portfolio-based algorithm selection, and the automatic configuration of parameterized algorithms. Over the past decade, a wide variety of techniques have been studied for building such models. Here, we describe extensions and improvements of existing models, new families of models, and - perhaps most importantly - a much more thorough treatment of algorithm parameters as model inputs. We also comprehensively describe new and existing features for predicting algorithm runtime for propositional satisfiability (SAT), travelling salesperson (TSP) and mixed integer programming (MIP) problems. We evaluate these innovations through the largest empirical analysis of its kind, comparing to a wide range of runtime modelling techniques from the literature. Our experiments consider 11 algorithms and 35 instance distributions; they also span a very wide range of SAT, MIP, and TSP instances, with the least structured having been generated uniformly at random and the most structured having emerged from real industrial applications. Overall, we demonstrate that our new models yield substantially better runtime predictions than previous approaches in terms of their generalization to new problem instances, to new algorithms from a parameterized space, and to both simultaneously. © 2013 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2013.10.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887848457&doi=10.1016%2fj.artint.2013.10.003&partnerID=40&md5=9bff03e7ec9cc14d0c92375727e07e0f,2014,2021-07-20 15:49:54,2021-07-20 15:49:54
ZQZXF47Y,journalArticle,2013,"Zhou, G.; Lu, Z.; Peng, Y.",L1-graph construction using structured sparsity,"As a powerful model to represent the data, graph has been widely applied to many machine learning tasks. More notably, to address the problems associated with the traditional graph construction methods, sparse representation has been successfully used for graph construction, and one typical work is L1-graph. However, since L1-graph often establishes only part of all the valuable connections between different data points due to its tendency to ignore the intrinsic structure hidden among the data, it fails to exploit such important information for the subsequent machine learning. Besides, the high computational costs of L1-graph prevent it from being applied to large scale high-dimensional datasets. In this paper, we construct a new graph, called the k-nearest neighbor (k-NN) fused Lasso graph, which is different from the traditional L1-graph because of its successful incorporation of the structured sparsity into the graph construction process and its applicability to large complex datasets. More concretely, to induce the structured sparsity, a novel regularization term is defined and reformulated into a matrix form to fit in the sparse representation step of L1-graph construction, and the k-NN method and kernel method are employed to deal with large complex datasets. Experimental results on several complex image datasets demonstrate the promising performance of our k-NN fused Lasso graph and also its advantage over the traditional L1-graph in the task of spectral clustering. © 2013 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2013.03.045,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882848347&doi=10.1016%2fj.neucom.2013.03.045&partnerID=40&md5=0dcaff233fb202f5312717c15fd156bf,2013,2021-07-20 15:49:54,2021-07-20 15:49:54
MWQ7HTAN,journalArticle,2014,"Xu, H.; Zhao, Y.; Barbic, J.",Implicit multibody penalty-baseddistributed contact,"The penalty method is a simple and popular approach to resolving contact in computer graphics and robotics. Penalty-based contact, however, suffers from stability problems due to the highly variable and unpredictable net stiffness, and this is particularly pronounced in simulations with time-varying distributed geometrically complex contact. We employ semi-implicit integration, exact analytical contact gradients, symbolic Gaussian elimination and a SVD solver to simulate stable penalty-based frictional contact with large, time-varying contact areas, involving many rigid objects and articulated rigid objects in complex conforming contact and self-contact. We also derive implicit proportional-derivative control forces for real-time control of articulated structures with loops. We present challenging contact scenarios such as screwing a hexbolt into a hole, bowls stacked in perfectly conforming configurations, and manipulating many objects using actively controlled articulated mechanisms in real time. © 1995-2012 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2014.2312013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905244811&doi=10.1109%2fTVCG.2014.2312013&partnerID=40&md5=6e2abf61d751947913ff0b2adcff5560,2014,2021-07-20 15:49:54,2021-07-20 15:49:54
GWWKKASR,journalArticle,2012,"Rebelo, A.; Fujinaga, I.; Paszkiewicz, F.; Marcal, A.R.S.; Guedes, C.; Cardoso, J.S.",Optical music recognition: state-of-the-art and open issues,"For centuries, music has been shared and remembered by two traditions: aural transmission and in the form of written documents normally called musical scores. Many of these scores exist in the form of unpublished manuscripts and hence they are in danger of being lost through the normal ravages of time. To preserve the music some form of typesetting or, ideally, a computer system that can automatically decode the symbolic images and create new scores is required. Programs analogous to optical character recognition systems called optical music recognition (OMR) systems have been under intensive development for many years. However, the results to date are far from ideal. Each of the proposed methods emphasizes different properties and therefore makes it difficult to effectively evaluate its competitive advantages. This article provides an overview of the literature concerning the automatic analysis of images of printed and handwritten musical scores. For self-containment and for the benefit of the reader, an introduction to OMR processing systems precedes the literature overview. The following study presents a reference scheme for any researcher wanting to compare new OMR algorithms against well-known ones. © 2012, Springer-Verlag London Limited.",International Journal of Multimedia Information Retrieval,10.1007/s13735-012-0004-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984956369&doi=10.1007%2fs13735-012-0004-6&partnerID=40&md5=4d4549bd3d03d4f868a7b2fd242d9047,2012,2021-07-20 15:49:54,2021-07-20 15:49:54
Z2WC4XMJ,journalArticle,2021,"Zheng, Q.; Tian, X.; Yang, M.; Su, H.",CLMIP: cross-layer manifold invariance based pruning method of deep convolutional neural network for real-time road type recognition,"Recently, deep learning based models have demonstrated the superiority in a variety of visual tasks like object detection and instance segmentation. In practical applications, deploying advanced networks into real-time applications such as autonomous driving is still challenging due to expensive computational cost and memory footprint. In this paper, to reduce the size of deep convolutional neural network (CNN) and accelerate its reasoning, we propose a cross-layer manifold invariance based pruning method named CLMIP for network compression to help it complete real-time road type recognition in low-cost vision system. Manifolds are higher-dimensional analogues of curves and surfaces, which can be self-organized to reflect the data distribution and characterize the relationship between data. Therefore, we hope to guarantee the generalization ability of deep CNN by maintaining the consistency of the data manifolds of each layer in the network, and then remove the parameters with less influence on the manifold structure. Therefore, CLMIP can be regarded as a tool to further investigate the dependence of model structure on network optimization and generalization. To the best of our knowledge, this is the first time to prune deep CNN based on the invariance of data manifolds. During experimental process, we use the python based keyword crawler program to collect 102 first-view videos of car cameras, including 137 200 images (320 × 240) of four road scenes (urban road, off-road, trunk road and motorway). Finally, the classification results have demonstrated that CLMIP can achieve state-of-the-art performance with a speed of 26 FPS on NVIDIA Jetson Nano. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Multidimensional Systems and Signal Processing,10.1007/s11045-020-00736-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088009659&doi=10.1007%2fs11045-020-00736-x&partnerID=40&md5=fba88853800c1685d84281b78e6d64ed,2021,2021-07-20 15:49:54,2021-07-20 15:49:54
AZ7KBEIP,journalArticle,2020,"Pise, A.; Vadapalli, H.; Sanders, I.",Facial emotion recognition using temporal relational network: an application to E-learning,"E-learning enables the dissemination of valuable academic information to all users regardless of where they are situated. One of the challenges faced by e-learning systems is the lack of constant interaction between the user and the system. This observability feature is an essential feature of a typical classroom setting and a means of detecting or observing feature reactions and thus such features in the form of expressions should be incorporated into an e-learning platform. The proposed solution is the implementation of a deep-learning-based facial image analysis model to estimate the learning affect and to reflect on the level of student engagement. This work proposes the use of a Temporal Relational Network (TRN), for identifying the changes in the emotions on students’ faces during e-learning session. It is observed that TRN sparsely samples individual frames and then learns their causal relations, which is much more efficient than sampling dense frames and convolving them. In this paper, single-scale and multi-scale temporal relations are considered to achieve the proposed goal. Furthermore, a Multi-Layer Perceptron (MLP) is also tested as a baseline classifier. The proposed framework is end-to-end trainable for video-based Facial Emotion Recognition (FER). The proposed FER model was tested on the open-source DISFA+ database. The TRN based model showed a significant reduction in the length of the feature set which were effective in recognizing expressions. It is observed that the multi-scale TRN has produced better accuracy than the single-scale TRN and MLP with an accuracy of 92.7%, 89.4%, and 86.6% respectively. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-020-10133-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095980740&doi=10.1007%2fs11042-020-10133-y&partnerID=40&md5=c902d44456beb5bc294f375d7c00507a,2020,2021-07-20 15:49:55,2021-07-20 15:49:55
9NJ63HP5,journalArticle,2020,"Maldonado, J.A.; Marcos, M.; Fernández-Breis, J.T.; Giménez-Solano, V.M.; Legaz-García, M.D.C.; Martínez-Salvador, B.",CLIN-IK-LINKS: A platform for the design and execution of clinical data transformation and reasoning workflows,"Background and Objective: Effective sharing and reuse of Electronic Health Records (EHR) requires technological solutions which deal with different representations and different models of data. This includes information models, domain models and, ideally, inference models, which enable clinical decision support based on a knowledge base and facts. Our goal is to develop a framework to support EHR interoperability based on transformation and reasoning services intended for clinical data and knowledge. Methods: Our framework is based on workflows whose primary components are reusable mappings. Key features are an integrated representation, storage, and exploitation of different types of mappings for clinical data transformation purposes, as well as the support for the discovery of new workflows. The current framework supports mappings which take advantage of the best features of EHR standards and ontologies. Our proposal is based on our previous results and experience working with both technological infrastructures. Results: We have implemented CLIN-IK-LINKS, a web-based platform that enables users to create, modify and delete mappings as well as to define and execute workflows. The platform has been applied in two use cases: semantic publishing of clinical laboratory test results; and implementation of two colorectal cancer screening protocols. Real data have been used in both use cases. Conclusions: The CLIN-IK-LINKS platform allows the composition and execution of clinical data transformation workflows to convert EHR data into EHR and/or semantic web standards. Having proved its usefulness to implement clinical data transformation applications of interest, CLIN-IK-LINKS can be regarded as a valuable contribution to improve the semantic interoperability of EHR systems. © 2020",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2020.105616,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087430673&doi=10.1016%2fj.cmpb.2020.105616&partnerID=40&md5=81f7620a4a2ca344aad42e13e66bfb0d,2020,2021-07-20 15:49:55,2021-07-20 15:49:55
TQXLG867,journalArticle,2014,"Zhang, X.; Hu, B.; Ma, X.; Moore, P.; Chen, J.",Ontology driven decision support for the diagnosis of mild cognitive impairment,"In recent years, mild cognitive impairment (MCI) has attracted significant attention as an indicator of high risk for Alzheimer's disease (AD), and the diagnosis of MCI can alert patient to carry out appropriate strategies to prevent AD. To avoid subjectivity in diagnosis, we propose an ontology driven decision support method which is an automated procedure for diagnosing MCI through magnetic resonance imaging (MRI). In this approach, we encode specialized MRI knowledge into an ontology and construct a rule set using machine learning algorithms. Then we apply these two parts in conjunction with reasoning engine to automatically distinguish MCI patients from normal controls (NC). The rule set is trained by MRI data of 187 MCI patients and 177 normal controls selected from Alzheimer's Disease Neuroimaging Initiative (ADNI) using C4.5 algorithm. By using a 10-fold cross validation, we prove that the performance of C4.5 with 80.2% sensitivity is better than other algorithms, such as support vector machine (SVM), Bayesian network (BN) and back propagation (BP) neural networks, and C4.5 is suitable for the construction of reasoning rules. Meanwhile, the evaluation results suggest that our approach would be useful to assist physicians efficiently in real clinical diagnosis for the disease of MCI. © 2014 Elsevier Ireland Ltd.",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2013.12.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894252987&doi=10.1016%2fj.cmpb.2013.12.023&partnerID=40&md5=a68b77c0205d6e3fc61b7215b23a093e,2014,2021-07-20 15:49:55,2021-07-20 15:49:55
FH8HR3JH,journalArticle,2020,"Hedeya, M.A.; Eid, A.H.; Abdel-Kader, R.F.",A super-learner ensemble of deep networks for vehicle-type classification,"Automatic vehicle-type classification plays an imperative role in the development of efficient Intelligent Transportation Systems (ITS). In this paper, a super-learner ensemble is proposed for the vehicle-type classification problem. A densely connected single-split super learner is utilized to exploit the strengths and diminish the weaknesses of the individual base learners ResNet50, Xception, and DenseNet. The super learner aims to learn fusion weights in a data-adaptive manner to obtain the optimal combination of the base learners. The proposed method is simple, robust, and enhances the discrimination capabilities among the similarly-looking classes without requiring any hand-crafted features or logical reasoning. The proposed method is evaluated using two of the most challenging publicly available traffic surveillance datasets: the MIOvision Traffic Camera Dataset (MIO-TCD) and the Beijing Institute of Technology's (BIT) vehicle classification dataset. Three variants of the super learner ensemble: RXD-CV-CW, RXD-CV-CW-NCW and Augmented-RXD, were examined on the MIO-TCD dataset with variations in applying class weights and data augmentation during training. RXD-CV-CW-NCW and Augmented-RXD share the third place among the published state-of-the-art methods reported in the MIO-TCD classification challenge. Augmented-RXD generalizes to the classes in common between the two datasets without degrading its performance on the MIO-TCD dataset. Both variants achieved an overall accuracy of 97.94%, and a Cohen Kappa score of 96.78%. In addition, the super-learner variants that we trained on the BIT-Vehicle dataset images achieved overall accuracies of up to 97.62%. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2997286,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086073784&doi=10.1109%2fACCESS.2020.2997286&partnerID=40&md5=3d64f112f31030a142e36fe93b9cf06a,2020,2021-07-20 15:49:55,2021-07-20 15:49:55
D3AHAVXB,journalArticle,2020,"Ming, Y.; Xu, P.; Cheng, F.; Qu, H.; Ren, L.",ProtoSteer: Steering Deep Sequence Model with Prototypes,"Recently we have witnessed growing adoption of deep sequence models (e.g. LSTMs) in many application domains, including predictive health care, natural language processing, and log analysis. However, the intricate working mechanism of these models confines their accessibility to the domain experts. Their black-box nature also makes it a challenging task to incorporate domain-specific knowledge of the experts into the model. In ProtoSteer (Prototype Steering), we tackle the challenge of directly involving the domain experts to steer a deep sequence model without relying on model developers as intermediaries. Our approach originates in case-based reasoning, which imitates the common human problem-solving process of consulting past experiences to solve new problems. We utilize ProSeNet (Prototype Sequence Network), which learns a small set of exemplar cases (i.e., prototypes) from historical data. In ProtoSteer they serve both as an efficient visual summary of the original data and explanations of model decisions. With ProtoSteer the domain experts can inspect, critique, and revise the prototypes interactively. The system then incorporates user-specified prototypes and incrementally updates the model. We conduct extensive case studies and expert interviews in application domains including sentiment analysis on texts and predictive diagnostics based on vehicle fault logs. The results demonstrate that involvements of domain users can help obtain more interpretable models with concise prototypes while retaining similar accuracy. © 1995-2012 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2019.2934267,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075630282&doi=10.1109%2fTVCG.2019.2934267&partnerID=40&md5=7eab1cda9540bfbde4a816ebf424cd64,2020,2021-07-20 15:49:55,2021-07-20 15:49:55
Q9I3P66H,journalArticle,2012,"Peula, J.M.; Urdiales, C.; Herrero, I.; Fernandez-Carmona, M.; Sandoval, F.",Case-based reasoning emulation of persons for wheelchair navigation,"Objective: Testing is a key stage in system development, particularly in systems such as a wheelchair, in which the final user is typically a disabled person. These systems have stringent safety requirements, requiring major testing with many different individuals. The best would be to have the wheelchair tested by many different end users, as each disability affects driving skills in a different way. Unfortunately, from a practical point of view it is difficult to engage end users as beta testers. Hence, testing often relies on simulations. Naturally, these simulations need to be as realistic as possible to make the system robust and safe before real tests can be accomplished. This work presents a tool to automatically test wheelchairs through realistic emulation of different wheelchair users. Methods and materials: Our approach is based on extracting meaningful data from real users driving a power wheelchair autonomously. This data is then used to train a case-based reasoning (CBR) system that captures the specifics of the driver via learning. The resulting case-base is then used to emulate the driving behavior of that specific person in more complex situations or when a new assistive algorithm needs to be tested. CBR returns user's motion commands appropriate for each specific situation to add the human component to shared control systems. Results: The proposed system has been used to emulate several power wheelchair users presenting different disabilities. Data to create this emulation was obtained from previous wheelchair navigation experiments with 35 volunteer in-patients presenting different degrees of disability. CBR was trained with a limited number of scenarios for each volunteer. Results proved that: (i) emulated and real users returned similar paths in the same scenario (maximum and mean path deviations are equal to 23 and 10. cm, respectively) and similar efficiency; (ii) we established the generality of our approach taking a new path not present in the training traces; (iii) the emulated user is more realistic - path and efficiency are less homogeneous and smooth - than potential field approaches; and (iv) the system adequately emulates in-patients - maximum and mean path deviations are equal to 19 and 8.3. cm approximately and efficiencies are similar - with specific disabilities (apraxia and dementia) obtaining different behaviors during emulation for each of the in-patients, as expected. Conclusions: The proposed system adequately emulates the driving behavior of people with different disabilities in indoor scenarios. This approach is suitable to emulate real users' driving behaviors for early testing stages of assistive navigation systems. © 2012 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2012.08.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869093768&doi=10.1016%2fj.artmed.2012.08.007&partnerID=40&md5=e68a674fa71767ac30373b95a86796a3,2012,2021-07-20 15:49:55,2021-07-20 15:49:55
G5RFKW4J,journalArticle,2014,"Tarei, M.E.; Abdollahi, B.; Nakhaei, M.",A fuzzy imperialistic competitive algorithm for optimizing convex functions,"Purpose: The purpose of this paper is to describe imperialist competitive algorithm (ICA), a novel socio-politically inspired optimization strategy for proposing a fuzzy variant of this algorithm. ICA is a meta-heuristic algorithm for dealing with different optimization tasks. The basis of the algorithm is inspired by imperialistic competition. It attempts to present the social policy of imperialisms (referred to empires) to control more countries (referred to colonies) and use their sources. If one empire loses its power, among the others making a competition to take possession of it. Design/methodology/approach: In fuzzy imperialist competitive algorithm (FICA), the colonies have a degree of belonging to their imperialists and the top imperialist, as in fuzzy logic, rather than belonging completely to just one empire therefore the colonies move toward the superior empire and their relevant empires. Simultaneously for balancing the exploration and exploitation abilities of the ICA. The algorithms are used for optimization have shortcoming to deal with accuracy rate and local optimum trap and they need complex tuning procedures. FICA is proposed a way for optimizing convex function with high accuracy and avoiding to trap in local optima rather than using original ICA algorithm by implementing fuzzy logic on it. Findings: Therefore several solution procedures, including ICA, FICA, genetic algorithm, particle swarm optimization, tabu search and simulated annealing optimization algorithm are considered. Finally numerical experiments are carried out to evaluate the effectiveness of models as well as solution procedures. Test results present the suitability of the proposed fuzzy ICA for convex functions with little fluctuations. Originality/value: The proposed evolutionary algorithm, FICA, can be used in diverse areas of optimization problems where convex functions properties are appeared including, industrial planning, resource allocation, scheduling, decision making, pattern recognition and machine learning (optimization techniques; fuzzy logic; convex functions). © Emerald Group Publishing Limited.",International Journal of Intelligent Computing and Cybernetics,10.1108/IJICC-12-2013-0052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84902441958&doi=10.1108%2fIJICC-12-2013-0052&partnerID=40&md5=a745b55cd4c0b88919e5b821c9b5a8c9,2014,2021-07-20 15:49:55,2021-07-20 15:49:55
BXK2DCLV,journalArticle,2012,"El-Sebakhy, E.A.; Asparouhov, O.; Abdulraheem, A.-A.; Al-Majed, A.-A.; Wu, D.; Latinski, K.; Raharja, I.",Functional networks as a new data mining predictive paradigm to predict permeability in a carbonate reservoir,"Permeability prediction has been a challenge to reservoir engineers due to the lack of tools that measure it directly. The most reliable data of permeability obtained from laboratory measurements on cores do not provide a continuous profile along the depth of the formation. Recently, researchers utilized statistical regression, neural networks, and fuzzy logic to estimate both permeability and porosity from well logs. Unfortunately, due to both uncertainty and imprecision, the developed predictive modelings are less accurate compared to laboratory experimental core data. This paper presents functional networks as a novel approach to forecast permeability using well logs in a carbonate reservoir. The new intelligence paradigm helps to overcome the most common limitations of the existing modeling techniques in statistics, data mining, machine learning, and artificial intelligence communities. To demonstrate the usefulness of the functional networks modeling strategy, we briefly describe its learning algorithm through simple distinct examples. Comparative studies were carried out using real-life industry wireline logs to compare the performance of the new framework with the most popular modeling schemes, such as linear/nonlinear regression, neural networks, and fuzzy logic inference systems. The results show that the performance of functional networks (separable and generalized associativity) architecture with polynomial basis is accurate, reliable, and outperforms most of the existing predictive data mining modeling approaches. Future work can be achieved using different structure of functional networks with different basis, interaction terms, ensemble and hybrid strategies, different clustering, and outlier identification techniques within different oil and gas challenge problems, namely, 3D passive seismic, identification of lithofacies types, history matching, rock mechanics, viscosity, risk assessment, and reservoir characterization. © 2012 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2012.01.157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861186454&doi=10.1016%2fj.eswa.2012.01.157&partnerID=40&md5=af7955c63d55dd928db8041185dae4ce,2012,2021-07-20 15:49:55,2021-07-20 15:49:55
FW53B46D,journalArticle,2011,"Hoiem, D.; Savarese, S.",Representations and techniques for 3D object recognition and scene interpretation,"One of the grand challenges of artificial intelligence is to enable computers to interpret 3D scenes and objects from imagery. This book organizes and introduces major concepts in 3D scene and object representation and inference from still images, with a focus on recent efforts to fuse models of geometry and perspective with statistical machine learning. The book is organized into three sections: (1) Interpretation of Physical Space; (2) Recognition of 3D Objects; and (3) Integrated 3D Scene Interpretation. The first discusses representations of spatial layout and techniques to interpret physical scenes from images. The second section introduces representations for 3D object categories that account for the intrinsically 3D nature of objects and provide robustness to change in viewpoints. The third section discusses strategies to unite inference of scene geometry and object pose and identity into a coherent scene interpretation. Each section broadly surveys important ideas from cognitive science and artificial intelligence research, organizes and discusses key concepts and techniques from recent work in computer vision, and describes a few sample approaches in detail. Newcomers to computer vision will benefit from introductions to basic concepts, such as single-view geometry and image classification, while experts and novices alike may find inspiration from the book's organization and discussion of the most recent ideas in 3D scene understanding and 3D object recognition. Specific topics include: mathematics of perspective geometry; visual elements of the physical scene, structural 3D scene representations; techniques and features for image and region categorization; historical perspective, computational models, and datasets and machine learning techniques for 3D object recognition; inferences of geometrical attributes of objects, such as size and pose; and probabilistic and feature-passing approaches for contextual reasoning about 3D objects and scenes. © 2011 by Morgan & Claypool.",Synthesis Lectures on Artificial Intelligence and Machine Learning,10.2200/S00370ED1V01Y201107AIM015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051955582&doi=10.2200%2fS00370ED1V01Y201107AIM015&partnerID=40&md5=4a7a52c5a13a0a07575008fdc91779a2,2011,2021-07-20 15:49:55,2021-07-20 15:49:55
VRPF2WKT,journalArticle,2020,"Rajeswar, S.; Mannan, F.; Golemo, F.; Parent-Lévesque, J.; Vazquez, D.; Nowrouzezahrai, D.; Courville, A.",Pix2Shape: Towards Unsupervised Learning of 3D Scenes from Images Using a View-Based Representation,"We infer and generate three-dimensional (3D) scene information from a single input image and without supervision. This problem is under-explored, with most prior work relying on supervision from, e.g., 3D ground-truth, multiple images of a scene, image silhouettes or key-points. We propose Pix2Shape, an approach to solve this problem with four component: (i) an encoder that infers the latent 3D representation from an image, (ii) a decoder that generates an explicit 2.5D surfel-based reconstruction of a scene—from the latent code—(iii) a differentiable renderer that synthesizes a 2D image from the surfel representation, and (iv) a critic network trained to discriminate between images generated by the decoder-renderer and those from a training distribution. Pix2Shape can generate complex 3D scenes that scale with the view-dependent on-screen resolution, unlike representations that capture world-space resolution, i.e., voxels or meshes. We show that Pix2Shape learns a consistent scene representation in its encoded latent space, and that the decoder can then be applied to this latent representation in order to synthesize the scene from a novel viewpoint. We evaluate Pix2Shape with experiments on the ShapeNet dataset as well as on a novel benchmark we developed – called 3D-IQTT—to evaluate models based on their ability to enable 3d spatial reasoning. Qualitative and quantitative evaluation demonstrate Pix2Shape’s ability to solve scene reconstruction, generation and understanding tasks. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",International Journal of Computer Vision,10.1007/s11263-020-01322-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084204610&doi=10.1007%2fs11263-020-01322-1&partnerID=40&md5=4cea70edbd00d85f68c6ee1ee4bb805f,2020,2021-07-20 15:49:55,2021-07-20 15:49:55
CI34H7DF,journalArticle,2020,"Song, J.Y.; Chung, J.J.Y.; Fouhey, D.F.; Lasecki, W.S.",C-Reference: Improving 2D to 3D Object Pose Estimation Accuracy via Crowdsourced Joint Object Estimation,"Converting widely-available 2D images and videos, captured using an RGB camera, to 3D can help accelerate the training of machine learning systems in spatial reasoning domains ranging from in-home assistive robots to augmented reality to autonomous vehicles. However, automating this task is challenging because it requires not only accurately estimating object location and orientation, but also requires knowing currently unknown camera properties (e.g., focal length). A scalable way to combat this problem is to leverage people's spatial understanding of scenes by crowdsourcing visual annotations of 3D object properties. Unfortunately, getting people to directly estimate 3D properties reliably is difficult due to the limitations of image resolution, human motor accuracy, and people's 3D perception (i.e., humans do not ""see"" depth like a laser range finder). In this paper, we propose a crowd-machine hybrid approach that jointly uses crowds' approximate measurements of multiple in-scene objects to estimate the 3D state of a single target object. Our approach can generate accurate estimates of the target object by combining heterogeneous knowledge from multiple contributors regarding various different objects that share a spatial relationship with the target object. We evaluate our joint object estimation approach with 363 crowd workers and show that our method can reduce errors in the target object's 3D location estimation by over 40%, while requiring only $35$% as much human time. Our work introduces a novel way to enable groups of people with different perspectives and knowledge to achieve more accurate collective performance on challenging visual annotation tasks. © 2020 ACM.",Proceedings of the ACM on Human-Computer Interaction,10.1145/3392858,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085640694&doi=10.1145%2f3392858&partnerID=40&md5=55e3e292b32f8136f021ef7c44196f3a,2020,2021-07-20 15:49:55,2021-07-20 15:49:55
FCSKVF7D,journalArticle,2018,"Draskovic, D.; Cvetanovic, M.; Nikolic, B.",SAIL—Software system for learning AI algorithms,"Artificial intelligence (AI) comprises a large spectrum of groups of algorithms: heuristic algorithms for search and planning, formal methods for representation of knowledge and reasoning, algorithms for machine learning and many more. Since these algorithms are complex, there is a need for a system which would enable their application both in everyday work and education processes. This paper describes a software system for learning AI algorithms called SAIL (Software System for AI Learning), which can be used both on computers and mobile devices. The paper gives examples of lab exercises and self-study tasks that through graphic representation and detailed procedures help students master this area. Students can enter their examples into the system and obtain correct solutions for those examples. At any point when an example is simulated, a student can proceed to the next step or go back to the previous one, save the current simulation as a file, or print the detailed procedure as a task solution. SAIL helps lecturers go through the syllabus more efficiently and improve class material, while at the same time it helps students get a better grasp of implemented algorithms. SAIL can also benefit software engineers, who can select and simulate an adequate algorithm to solve a specific problem. The results of the SAIL system are verified within the AI introductory course at the School of Electrical Engineering University of Belgrade and they are presented in this paper. © 2018 Wiley Periodicals, Inc.",Computer Applications in Engineering Education,10.1002/cae.21988,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053854396&doi=10.1002%2fcae.21988&partnerID=40&md5=c9d7b78057fae08fb18373fa445e6757,2018,2021-07-20 15:49:55,2021-07-20 15:49:55
ZPJN9R8B,journalArticle,2018,"Abou-Moustafa, K.; Ferrie, F.P.",Local generalized quadratic distance metrics: Application to the k-nearest neighbors classifier,"Finding the set of nearest neighbors for a query point of interest appears in a variety of algorithms for machine learning and pattern recognition. Examples include k nearest neighbor classification, information retrieval, case-based reasoning, manifold learning, and nonlinear dimensionality reduction. In this work, we propose a new approach for determining a distance metric from the data for finding such neighboring points. For a query point of interest, our approach learns a generalized quadratic distance (GQD) metric based on the statistical properties in a “small” neighborhood for the point of interest. The locally learned GQD metric captures information such as the density, curvature, and the intrinsic dimensionality for the points falling in this particular neighborhood. Unfortunately, learning the GQD parameters under such a local learning mechanism is a challenging problem with a high computational overhead. To address these challenges, we estimate the GQD parameters using the minimum volume covering ellipsoid (MVCE) for a set of points. The advantage of the MVCE is two-fold. First, the MVCE together with the local learning approach approximate the functionality of a well known robust estimator for covariance matrices. Second, computing the MVCE is a convex optimization problem which, in addition to having a unique global solution, can be efficiently solved using a first order optimization algorithm. We validate our metric learning approach on a large variety of datasets and show that the proposed metric has promising results when compared with five algorithms from the literature for supervised metric learning. © Springer-Verlag Berlin Heidelberg 2017.",Advances in Data Analysis and Classification,10.1007/s11634-017-0286-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018705803&doi=10.1007%2fs11634-017-0286-x&partnerID=40&md5=42da4425370866257ab6e77eb0c76ed5,2018,2021-07-20 15:49:56,2021-07-20 15:49:56
JVQZUJEK,journalArticle,2015,"Gjoreski, H.; Kaluža, B.; Gams, M.; Milić, R.; Luštrek, M.",Context-based ensemble method for human energy expenditure estimation,"Monitoring human energy expenditure (EE) is important in many health and sports applications, since the energy expenditure directly reflects the intensity of physical activity. The actual energy expenditure is unpractical to measure; therefore, it is often estimated from the physical activity measured with accelerometers and other sensors. Previous studies have demonstrated that using a person's activity as the context in which the EE is estimated, and using multiple sensors, improves the estimation. In this study, we go a step further by proposing a context-based reasoning method that uses multiple contexts provided by multiple sensors. The proposed Multiple Contexts Ensemble (MCE) approach first extracts multiple features from the sensor data. Each feature is used as a context for which multiple regression models are built using the remaining features as training data: for each value of the context feature, a regression model is trained on a subset of the dataset with that value. When evaluating a data sample, the models corresponding to the context (feature) values in the evaluated sample are assembled into an ensemble of regression models that estimates the EE of the user. Experiments showed that the MCE method outperforms (in terms of lower root means squared error and lower mean absolute error): (i) five single-regression approaches (linear and non-linear); (ii) two ensemble approaches: Bagging and Random subspace; (iii) an approach that uses artificial neural networks trained on accelerometer-data only; and (iv) BodyMedia (a state-of-the-art commercial EE-estimation device). © 2015 Elsevier B.V. All rights reserved.",Applied Soft Computing Journal,10.1016/j.asoc.2015.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947487609&doi=10.1016%2fj.asoc.2015.05.001&partnerID=40&md5=3eff7ed521b7d2f13aa5b47dc1cd77f2,2015,2021-07-20 15:49:56,2021-07-20 15:49:56
MD55YRLU,journalArticle,2021,"Singh, A.; Singh, T.D.; Bandyopadhyay, S.",Attention based video captioning framework for Hindi,"In recent times, active research is going on for bridging the gap between computer vision and natural language. In this paper, we attempt to address the problem of Hindi video captioning. In a linguistically diverse country like India, it is important to provide a means which can help in understanding the visual entities in native languages. In this work, we employ a hybrid attention mechanism by extending the soft temporal attention mechanism with a semantic attention to make the system able to decide when to focus on visual context vector and semantic input. The visual context vector of the input video is extracted using 3D convolutional neural network (3D CNN) and a Long Short-Term Memory (LSTM) recurrent network with attention module is used for decoding the encoded context vector. We experimented on a dataset built in-house for Hindi video captioning by translating MSR- VTT dataset followed by post-editing. Our system achieves 0.369 CIDEr score and 0.393 METEOR score and outperformed other baseline models including RMN (Reasoning Module Networks)-based model. © 2021, The Author(s), under exclusive licence to Springer-Verlag GmbH Germany, part of Springer Nature.",Multimedia Systems,10.1007/s00530-021-00816-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108083408&doi=10.1007%2fs00530-021-00816-3&partnerID=40&md5=53fa83ed65fe93786409ea1a07984c68,2021,2021-07-20 15:49:56,2021-07-20 15:49:56
QCXT9L8Y,journalArticle,2021,"Patsantzis, S.; Muggleton, S.H.",Top program construction and reduction for polynomial time Meta-Interpretive learning,"Meta-Interpretive Learners, like most ILP systems, learn by searching for a correct hypothesis in the hypothesis space, the powerset of all constructible clauses. We show how this exponentially-growing search can be replaced by the construction of a Top program: the set of clauses in all correct hypotheses that is itself a correct hypothesis. We give an algorithm for Top program construction and show that it constructs a correct Top program in polynomial time and from a finite number of examples. We implement our algorithm in Prolog as the basis of a new MIL system, Louise, that constructs a Top program and then reduces it by removing redundant clauses. We compare Louise to the state-of-the-art search-based MIL system Metagol in experiments on grid world navigation, graph connectedness and grammar learning datasets and find that Louise improves on Metagol’s predictive accuracy when the hypothesis space and the target theory are both large, or when the hypothesis space does not include a correct hypothesis because of “classification noise” in the form of mislabelled examples. When the hypothesis space or the target theory are small, Louise and Metagol perform equally well. © 2021, The Author(s).",Machine Learning,10.1007/s10994-020-05945-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100768489&doi=10.1007%2fs10994-020-05945-w&partnerID=40&md5=66951ffdaf7b26613c5cda76541d4504,2021,2021-07-20 15:49:56,2021-07-20 15:49:56
92UBNL7Y,journalArticle,2018,"Karyotis, C.; Doctor, F.; Iqbal, R.; James, A.; Chang, V.",A fuzzy computational model of emotion for cloud based sentiment analysis,"This paper presents a novel emotion modeling methodology for incorporating human emotion into intelligent computer systems. The proposed approach includes a method to elicit emotion information from users, a new representation of emotion (AV-AT model) that is modelled using a genetically optimized adaptive fuzzy logic technique, and a framework for predicting and tracking user's affective trajectory over time. The fuzzy technique is evaluated in terms of its ability to model affective states in comparison to other existing machine learning approaches. The performance of the proposed affect modeling methodology is tested through the deployment of a personalised learning system, and series of offline and online experiments. A hybrid cloud intelligence infrastructure is used to conduct large-scale experiments to analyze user sentiments and associated emotions, using data from a million Facebook users. A performance analysis of the infrastructure on processing, analyzing, and data storage has been carried out, illustrating its viability for large-scale data processing tasks. A comparison of the proposed emotion categorizing approach with Facebook's sentiment analysis API demonstrates that our approach can achieve comparable performance. Finally, discussions on research contributions to cloud intelligence using sentiment analysis, emotion modeling, big data, and comparisons with other approaches are presented in detail. © 2017 Elsevier Inc.",Information Sciences,10.1016/j.ins.2017.02.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013177499&doi=10.1016%2fj.ins.2017.02.004&partnerID=40&md5=448e6b18c9dd63c6a3878a78c0e8da37,2018,2021-07-20 15:49:56,2021-07-20 15:49:56
W9T6B52M,journalArticle,2017,"Bartocci, E.; Bortolussi, L.; Brázdil, T.; Milios, D.; Sanguinetti, G.",Policy learning in continuous-time Markov decision processes using Gaussian Processes,"Continuous-time Markov decision processes provide a very powerful mathematical framework to solve policy-making problems in a wide range of applications, ranging from the control of populations to cyber–physical systems. The key problem to solve for these models is to efficiently compute an optimal policy to control the system in order to maximise the probability of satisfying a set of temporal logic specifications. Here we introduce a novel method based on statistical model checking and an unbiased estimation of a functional gradient in the space of possible policies. Our approach presents several advantages over the classical methods based on discretisation techniques, as it does not assume the a-priori knowledge of a model that can be replaced by a black-box, and does not suffer from state-space explosion. The use of a stochastic moment-based gradient ascent algorithm to guide our search considerably improves the efficiency of learning policies and accelerates the convergence using the momentum term. We demonstrate the strong performance of our approach on two examples of non-linear population models: an epidemiology model with no permanent recovery and a queuing system with non-deterministic choice. © 2017 Elsevier B.V.",Performance Evaluation,10.1016/j.peva.2017.08.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029590224&doi=10.1016%2fj.peva.2017.08.007&partnerID=40&md5=ec67075dc71ced996972d06a7b37d544,2017,2021-07-20 15:49:57,2021-07-20 15:49:57
ZAHAYM6I,journalArticle,2017,"Chun, B.-G.; Condie, T.; Chen, Y.; Cho, B.; Chung, A.; Curino, C.; Douglas, C.; Interlandi, M.; Jeon, B.; Jeong, J.S.; Lee, G.; Lee, Y.; Majestro, T.; Malkhi, D.; Matusevych, S.; Myers, B.; Mykhailova, M.; Narayanamurthy, S.; Noor, J.; Ramakrishnan, R.; Rao, S.; Sears, R.; Sezgin, B.; Um, T.; Wang, J.; Weimer, M.; Yang, Y.",Apache REEF: Retainable evaluator execution framework,"Resource Managers like YARN and Mesos have emerged as a critical layer in the cloud computing system stack, but the developer abstractions for leasing cluster resources and instantiating application logic are very low level. This flexibility comes at a high cost in terms of developer effort, as each application must repeatedly tackle the same challenges (e.g., fault tolerance, task scheduling and coordination) and reimplement common mechanisms (e.g., caching, bulk-data transfers). This article presents REEF, a development framework that provides a control plane for scheduling and coordinating task-level (data-plane) work on cluster resources obtained from a Resource Manager. REEF provides mechanisms that facilitate resource reuse for data caching and state management abstractions that greatly ease the development of elastic data processing pipelines on cloud platforms that support a Resource Manager service. We illustrate the power of REEF by showing applications built atop: a distributed shell application, a machine-learning framework, a distributed in-memory caching system, and a port of the CORFU system. REEF is currently an Apache top-level project that has attracted contributors from several institutions and it is being used to develop several commercial offerings such as the Azure Stream Analytics service.",ACM Transactions on Computer Systems,10.1145/3132037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032886342&doi=10.1145%2f3132037&partnerID=40&md5=a94c03e787a825d5ff7ddd8b1fcb89e1,2017,2021-07-20 15:49:57,2021-07-20 15:49:57
XF4LSBE2,journalArticle,2013,"Zhang, Y.; Zhang, F.; Jin, Z.; Bakos, J.D.",An FPGA-based accelerator for frequent itemset mining,"In this article we describe a Field Programmable Gate Array (FPGA)-based coprocessor architecture for Frequent Itemset Mining (FIM). FIM is a common data mining task used to find frequently occurring subsets amongst a database of sets. FIM is a nonnumerical, data intensive computation and is used in machine learning and computational biology. FIM is particularly expensive - in terms of execution time and memory - when performed on large and/or sparse databases or when applied using a low appearance frequency threshold. Because of this, the development of increasingly efficient FIM algorithms and their mapping to parallel architectures is an active field. Previous attempts to accelerate FIM using FPGAs have relied on performance-limiting strategies such as iterative database loading and runtime logic unit reconfiguration. In this article, we present a novel architecture to implement Eclat, a well-known FIM algorithm. Unlike previous efforts, our technique does not impose limits on the maximum set size as a function of available FPGA logic resources and our design scales well to multiple FPGAs. In addition to a novel hardware design, we also present a corresponding compression scheme for intermediate results that are stored in onchip memory. On a four-FPGA board, experimental results show up to 68X speedup compared to a highly optimized software implementation. © 2013 ACM.",ACM Transactions on Reconfigurable Technology and Systems,10.1145/2457443.2457445,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877918621&doi=10.1145%2f2457443.2457445&partnerID=40&md5=2efef3bc822c2be7015346357a5353bd,2013,2021-07-20 15:49:57,2021-07-20 15:49:57
SEBI9T47,journalArticle,2021,"Xie, W.",Risk spillover in financial markets based on support vector quantile regression,"In terms of financial market risk research, with the rapid popularization of non-linear perspectives and the improvement of theoretical reasoning, scholars have slowly broken through the cage of linear ideas and derived new and more practical methods from non-linear perspectives to make up for the shortcomings of traditional research. Based on the support vector classification regression algorithm, this research combines the typical facts and characteristics of financial markets, from the perspective of quantile regression and SVR intelligent technology in computer science, to explore the research method of financial market risk spillover effects from a nonlinear perspective. Moreover, this research integrates statistical research, machine learning and other related research methods, and applies them to the measurement of financial risk spillover effects. The empirical analysis shows that the method proposed in this paper has certain effects, and financial risk analysis can be performed based on the risk spillover effect measurement model constructed in this paper. © 2021-IOS Press. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-189230,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100548999&doi=10.3233%2fJIFS-189230&partnerID=40&md5=710896452e5dafb16da26560d2cfcf54,2021,2021-07-20 15:49:57,2021-07-20 15:49:57
CCSF2N44,journalArticle,2018,"Schmidtke, H.R.",A survey on verification strategies for intelligent transportation systems,"As intelligent systems are increasingly entering everyday life, in domains such as transportation, resource distribution, health care, or retail, developing suitable verification mechanisms for such systems becomes vital. From a formal point of view, the employed intelligent sensor actuator systems (ISAS) constituting such intelligent systems combine three different technologies: control systems, distributed systems, and learning and reasoning. While each of the parent domains features tested and proven verification methods, simply combining the tasks unfortunately leads to a combinatorial explosion of complexity. This paper presents an overview and classification of currently employed techniques for handling ISAS in terms of: cyber-physical systems, intelligent autonomous robots, or intelligent agents. The article argues that each of the three classical perspectives misses one important characteristic of ISAS and proposes to combine the three for a full solution. The paper argues that in particular two mechanisms are promising: an intelligent environments perspective that verifies local safety and techniques for context-aware monitoring that allow a mobile system to leverage context-awareness to reduce complexity for self-monitoring tasks. © 2018, Springer Nature Switzerland AG.",Journal of Reliable Intelligent Environments,10.1007/s40860-018-0070-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062694920&doi=10.1007%2fs40860-018-0070-5&partnerID=40&md5=b35e6a90288baa46956561d19c363967,2018,2021-07-20 15:49:57,2021-07-20 15:49:57
9YMU8X4H,journalArticle,2018,"Peres, R.S.; Dionisio Rocha, A.; Leitao, P.; Barata, J.",IDARTS – Towards intelligent data analysis and real-time supervision for industry 4.0,"The manufacturing industry represents a data rich environment, in which larger and larger volumes of data are constantly being generated by its processes. However, only a relatively small portion of it is actually taken advantage of by manufacturers. As such, the proposed Intelligent Data Analysis and Real-Time Supervision (IDARTS) framework presents the guidelines for the implementation of scalable, flexible and pluggable data analysis and real-time supervision systems for manufacturing environments. IDARTS is aligned with the current Industry 4.0 trend, being aimed at allowing manufacturers to translate their data into a business advantage through the integration of a Cyber-Physical System at the edge with cloud computing. It combines distributed data acquisition, machine learning and run-time reasoning to assist in fields such as predictive maintenance and quality control, reducing the impact of disruptive events in production. © 2018 Elsevier B.V.",Computers in Industry,10.1016/j.compind.2018.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050319341&doi=10.1016%2fj.compind.2018.07.004&partnerID=40&md5=215a060849b112439c5abf81349f7efc,2018,2021-07-20 15:49:57,2021-07-20 15:49:57
348VAKIK,journalArticle,2016,"Huang, M.-W.; Lin, W.-C.; Chen, C.-W.; Ke, S.-W.; Tsai, C.-F.; Eberle, W.",Data preprocessing issues for incomplete medical datasets,"While there is an ample amount of medical information available for data mining, many of the datasets are unfortunately incomplete – missing relevant values needed by many machine learning algorithms. Several approaches have been proposed for the imputation of missing values, using various reasoning steps to provide estimations from the observed data. One of the important steps in data mining is data preprocessing, where unrepresentative data is filtered out of the data to be mined. However, none of the related studies about missing value imputation consider performing a data preprocessing step before imputation. Therefore, the aim of this study is to examine the effect of two preprocessing steps, feature and instance selection, on missing value imputation. Specifically, eight different medical-related datasets are used, containing categorical, numerical and mixed types of data. Our experimental results show that imputation after instance selection can produce better classification performance than imputation alone. In addition, we will demonstrate that imputation after feature selection does not have a positive impact on the imputation result. © 2016 Wiley Publishing Ltd",Expert Systems,10.1111/exsy.12155,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84973897271&doi=10.1111%2fexsy.12155&partnerID=40&md5=a12b4f8af874adced8180ba02a5a27b9,2016,2021-07-20 15:49:57,2021-07-20 15:49:57
68E67JBJ,journalArticle,2014,"Zhao, J.; Sun, S.; Liu, X.; Sun, J.; Yang, A.",A Novel Biologically Inspired Visual Saliency Model,"The paper focuses on the modeling of visual saliency. We present a novel model to simulate the two stages of visual processing that are involved in attention. Firstly, the proto-object features are extracted in the pre-attentive stage. On the one hand, the salient pixels and regions are extracted. On the other hand, the semantic proto-objects, which involve all possible states of the observer’s memories such as face, person, car, and text, are detected. Then, the support vector machines are utilized to simulate the learning process. As a consequence, the association between the proto-object features and the salient information is established. A visual attention model is built via the method of machine learning, and the saliency information of a new image can be obtained by the way of reasoning. To validate the model, the eye fixations prediction problem on the MIT dataset is studied. Experimental results indicate that the proposed model effectively improves the predictive accuracy rates compared with other approaches. © 2014, Springer Science+Business Media New York.",Cognitive Computation,10.1007/s12559-014-9266-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84916217657&doi=10.1007%2fs12559-014-9266-z&partnerID=40&md5=43766b5184b0c9f5c91054245b321815,2014,2021-07-20 15:49:57,2021-07-20 15:49:57
9EM4HHN9,journalArticle,2020,"Lee, D.; Arigi, A.M.; Kim, J.",Algorithm for Autonomous Power-Increase Operation Using Deep Reinforcement Learning and a Rule-Based System,"The power start-up operation of a nuclear power plant (NPP) increases the reactor power to the full-power condition for electricity generation. Compared to full-power operation, the power-increase operation requires significantly more decision-making and therefore increases the potential for human errors. While previous studies have investigated the use of artificial intelligence (AI) techniques for NPP control, none of them have addressed making the relatively complicated power-increase operation fully autonomous. This study focused on developing an algorithm for converting all the currently manual activities in the NPP power-increase process to autonomous operations. An asynchronous advantage actor-critic, which is a type of deep reinforcement learning method, and a long short-term memory network were applied to the operator tasks for which establishing clear rules or logic was challenging, while a rule-based system was developed for those actions, which could be described by simple logic (such as if-then logic). The proposed autonomous power-increase control algorithm was trained and validated using a compact nuclear simulator (CNS). The simulation results were used to evaluate the algorithm's ability to control the parameters within allowable limits, and the proposed power-increase control algorithm was proven capable of identifying an acceptable operation path for increasing the reactor power from 2% to 100% at a specified rate of power increase. In addition, the pattern of operation that resulted from the autonomous control simulation was found to be identical to that of the established operation strategy. These results demonstrate the potential feasibility of fully autonomous control of the NPP power-increase operation. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3034218,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096334589&doi=10.1109%2fACCESS.2020.3034218&partnerID=40&md5=078a6eb453e7f53920a945105cb01d4c,2020,2021-07-20 15:49:57,2021-07-20 15:49:57
VF4ASSDH,journalArticle,2012,"Jonnalagadda, S.; Cohen, T.; Wu, S.; Gonzalez, G.",Enhancing clinical concept extraction with distributional semantics,"Extracting concepts (such as drugs, symptoms, and diagnoses) from clinical narratives constitutes a basic enabling technology to unlock the knowledge within and support more advanced reasoning applications such as diagnosis explanation, disease progression modeling, and intelligent analysis of the effectiveness of treatment. The recent release of annotated training sets of de-identified clinical narratives has contributed to the development and refinement of concept extraction methods. However, as the annotation process is labor-intensive, training data are necessarily limited in the concepts and concept patterns covered, which impacts the performance of supervised machine learning applications trained with these data. This paper proposes an approach to minimize this limitation by combining supervised machine learning with empirical learning of semantic relatedness from the distribution of the relevant words in additional unannotated text.The approach uses a sequential discriminative classifier (Conditional Random Fields) to extract the mentions of medical problems, treatments and tests from clinical narratives. It takes advantage of all Medline abstracts indexed as being of the publication type "" clinical trials"" to estimate the relatedness between words in the i2b2/VA training and testing corpora. In addition to the traditional features such as dictionary matching, pattern matching and part-of-speech tags, we also used as a feature words that appear in similar contexts to the word in question (that is, words that have a similar vector representation measured with the commonly used cosine metric, where vector representations are derived using methods of distributional semantics). To the best of our knowledge, this is the first effort exploring the use of distributional semantics, the semantics derived empirically from unannotated text often using vector space models, for a sequence classification task such as concept extraction. Therefore, we first experimented with different sliding window models and found the model with parameters that led to best performance in a preliminary sequence labeling task.The evaluation of this approach, performed against the i2b2/VA concept extraction corpus, showed that incorporating features based on the distribution of words across a large unannotated corpus significantly aids concept extraction. Compared to a supervised-only approach as a baseline, the micro-averaged F-score for exact match increased from 80.3% to 82.3% and the micro-averaged F-score based on inexact match increased from 89.7% to 91.3%. These improvements are highly significant according to the bootstrap resampling method and also considering the performance of other systems. Thus, distributional semantic features significantly improve the performance of concept extraction from clinical narratives by taking advantage of word distribution information obtained from unannotated data. © 2011 Elsevier Inc.",Journal of Biomedical Informatics,10.1016/j.jbi.2011.10.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84856376731&doi=10.1016%2fj.jbi.2011.10.007&partnerID=40&md5=dd1c6de31c50ddec07807a6dffbf7b17,2012,2021-07-20 15:49:57,2021-07-20 15:49:57
F8JIS6ID,journalArticle,2021,"Ferigo, D.; Camoriano, R.; Viceconte, P.M.; Calandriello, D.; Traversaro, S.; Rosasco, L.; Pucci, D.",On the Emergence of Whole-body Strategies from Humanoid Robot Push-recovery Learning,"Balancing and push-recovery are essential capabilities enabling humanoid robots to solve complex locomotion tasks. In this context, classical control systems tend to be based on simplified physical models and hard-coded strategies. Although successful in specific scenarios, this approach requires demanding tuning of parameters and switching logic between specifically-designed controllers for handling more general perturbations. We apply model-free Deep Reinforcement Learning for training a general and robust humanoid push-recovery policy in a simulation environment. Our method targets high-dimensional whole-body humanoid control and is validated on the iCub humanoid. Reward components incorporating expert knowledge on humanoid control enable fast learning of several robust behaviors by the same policy, spanning the entire body. We validate our method with extensive quantitative analyses in simulation, including out-of-sample tasks which demonstrate policy robustness and generalization, both key requirements towards real-world robot deployment. CCBY",IEEE Robotics and Automation Letters,10.1109/LRA.2021.3076955,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105025081&doi=10.1109%2fLRA.2021.3076955&partnerID=40&md5=f5e11a1c6576ed78a24e9b9540bfbd67,2021,2021-07-20 15:49:58,2021-07-20 15:49:58
M3IVSFR3,journalArticle,2021,"Nguyen, D.; Vadaine, R.; Hajduch, G.; Garello, R.; Fablet, R.",GeoTrackNet–A Maritime Anomaly Detector Using Probabilistic Neural Network Representation of AIS Tracks and A Contrario Detection,"Representing maritime traffic patterns and detecting anomalies from them are key to vessel monitoring and maritime situational awareness. We propose a novel approach–referred to as GeoTrackNet–for maritime anomaly detection from AIS data streams. Our model exploits state-of-the-art neural network schemes to learn a probabilistic representation of AIS tracks and a contrario detection to detect abnormal events. The neural network provides a new means to capture complex and heterogeneous patterns in vessels' behaviours, while the a contrario detector takes into account the fact that the learnt distribution may be location-dependent. Experiments on a real AIS dataset comprising more than 4.2 million AIS messages demonstrate the relevance of the proposed method compared with state-of-the-art schemes. IEEE",IEEE Transactions on Intelligent Transportation Systems,10.1109/TITS.2021.3055614,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100851321&doi=10.1109%2fTITS.2021.3055614&partnerID=40&md5=91c31d5eff2d501e6e806acb57e4cc44,2021,2021-07-20 15:49:58,2021-07-20 15:49:58
L23RWW7A,journalArticle,2021,"Gomez-Garcia, C.A.; Askar-Rodriguez, M.A.; Velasco-MEDINA, J.",Platform for Healthcare Promotion and Cardiovascular Disease Prevention,"This article presents the hardware-software design and implementation of an open, integrated, and scalable healthcare platform oriented to multiple point-care scenarios for healthcare promotion and cardiovascular disease prevention. The platform has the capability to provide continuous monitoring, extended device integration, strategies based on artificial intelligence for the information analysis and cybersecurity support, delivering a secure end-to-end hardware-software solution. This platform is used to perform the remote patient health monitoring and supervision by doctors, triage procedures in hospitals, or self-care monitoring using personal devices such as tablets and cellphones. The proposed hardware architecture facilitates the integration of biomedical data acquired from different health-point cares, collecting relevant information for the detection of cardiovascular risk through deep-learning algorithms. All these characteristics make our development a strong tool to perform epidemiological profiling and future implementation of strategies for comprehensive cardiovascular risk intervention. The components of the platform are described, and their main functionalities are highlighted. IEEE",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2021.3051967,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099731934&doi=10.1109%2fJBHI.2021.3051967&partnerID=40&md5=f8577e1cc3b29c96e40621c7a917349f,2021,2021-07-20 15:49:58,2021-07-20 15:49:58
XI2GTBQV,journalArticle,2018,"Hifny, Y.",Hybrid LSTM/MaxEnt Networks for Arabic Syntactic Diacritics Restoration,"Restoring syntactic diacritics is an essential task for Arabic text-to-speech systems. Syntactic diacritic mark is defined as the last diacritic mark of the stem of a whitespace delimited word. It is usually assigned based on the syntax of the Arabic language. In this paper, we formulate the problem as a tagging problem and propose the use of long short-term memory (LSTM) networks to assign the syntactic diacritics for a sentence of Arabic words. These LSTM networks were augmented with sparse direct connections between the input and output layers of the tagger (i.e., maximum entropy (MaxEnt) connections). On the Arabic tree bank task, this hybrid LSTM/MaxEnt approach achieves competitive results to the state-of-the-art systems. © 2018 IEEE.",IEEE Signal Processing Letters,10.1109/LSP.2018.2865098,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052585404&doi=10.1109%2fLSP.2018.2865098&partnerID=40&md5=6687363c7dfc7558625b4698bea150e8,2018,2021-07-20 15:49:58,2021-07-20 15:49:58
FJXILTFY,journalArticle,2021,"Usman, M.",Recent progress in atomistic modelling and simulations of donor spin qubits in silicon,"Electron or nuclear spins associated with dopant atoms, such as phosphorus impurities in silicon (Si:P), have been shown to form excellent qubits with promising potential for scale-up towards a fault-tolerant quantum computer architecture. The remarkable progress in the design and characterisation of Si:P qubits and quantum gates has been led by recent experimental demonstrations. Equally importantly, advances in theoretical modelling and simulations over a number of years have underpinned the experimental efforts through the fundamental understanding of dopant physics and by providing crucial interpretation of the experimental evidence. This brief review article provides highlights of our research on developing atomistic theoretical methods and their application to the understanding, characterisation and scale-up of Si:P qubits in silicon. We have established a state-of-the-art theoretical framework which is capable of performing electronic structure simulations over millions of atoms. This includes a comprehensive set of central-cell corrections within atomistic tight-binding theory to simulate dopant energy spectra and electronic wave functions with high precision. When integrated with Bardeen's tunnelling formalism and Chen's derivative rule, the theoretical simulations were able to reproduce the measured spatially resolved scanning tunnelling microscope (STM) images of dopant wave functions, providing an unprecedented access to the dopant physics in silicon. A systematic examination of the STM image features (brightness and symmetry) allowed pinpointing of the dopant atom positions in silicon lattice with an exact atom precision and for dopant depths up to 5 nm below the silicon surface. The scale-up of the metrology technique was demonstrated by training a machine learning algorithm such as convolutional neural network. For the design and implementation of high-fidelity two-qubit quantum gates, we investigated exchange interaction between dopant pairs and showed that the application of a small lattice strain could provide a full control in the presence of one-lattice site donor position variations. The state-of-the-art computational capability developed by our team is a culmination of more than five years of research efforts – it has been well-benchmarked against several different experimental measurements and is expected to play an important role in design and characterisation of quantum gates and scale-up architectures in the coming years. © 2021 Elsevier B.V.",Computational Materials Science,10.1016/j.commatsci.2021.110280,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103560678&doi=10.1016%2fj.commatsci.2021.110280&partnerID=40&md5=2043f25ab278ba6608e49d714d89b6f1,2021,2021-07-20 15:49:58,2021-07-20 15:49:58
FLQDE4IR,journalArticle,2018,"Garcia-Arroyo, J.L.; Garcia-Zapirain, B.",Recognition of pigment network pattern in dermoscopy images based on fuzzy classification of pixels,"Background and Objective: One of the most relevant dermoscopic patterns is the pigment network. An innovative method of pattern recognition is presented for its detection in dermoscopy images. Methods: It consists of two steps. In the first one, by means of a supervised machine learning process and after performing the extraction of different colour and texture features, a fuzzy classification of pixels into the three categories present in the pattern's definition (“net” “hole” and “other”) is carried out. This enables the three corresponding fuzzy sets to be created and, as a result, the three probability images that map them out are generated. In the second step, the pigment network pattern is characterised from a parameterisation process –derived from the system specification– and the subsequent extraction of different features calculated from the combinations of image masks extracted from the probability images, corresponding to the alpha-cuts obtained from the fuzzy sets. Results: The method was tested on a database of 875 images –by far the largest used in the state of the art to detect pigment network– extracted from a public Atlas of Dermoscopy, obtaining AUC results of 0.912 and 88%% accuracy, with 90.71%% sensitivity and 83.44%% specificity. Conclusion: The main contribution of this method is the very design of the algorithm, highly innovative, which could also be used to deal with other pattern recognition problems of a similar nature. Other contributions are: 1. The good performance in discriminating between the pattern and the disturbing artefacts –which means that no prior preprocessing is required in this method– and between the pattern and other dermoscopic patterns; 2. It puts forward a new methodological approach for work of this kind, introducing the system specification as a required step prior to algorithm design and development, being this specification the basis for a required parameterisation –in the form of configurable parameters (with their value ranges) and set threshold values– of the algorithm and the subsequent conducting of the experiments. © 2017 The Authors",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2017.10.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042706069&doi=10.1016%2fj.cmpb.2017.10.005&partnerID=40&md5=a1c296981f3b63895f33fccd51aa0300,2018,2021-07-20 15:49:58,2021-07-20 15:49:58
V8HCAUG7,journalArticle,2016,"Pham, N.K.; Kumar, A.; Singh, A.K.; Khin, M.M.A.",Leakage aware resource management approach with machine learning optimization framework for partially reconfigurable architectures,"Shrinking size of transistors has enabled us to integrate more and more logic elements into FPGA chips leading to higher computing power. However, it also brings a serious concern to the leakage power dissipation of the FPGA devices. One of the major reasons for leakage power dissipation in FPGA is the utilization of prefetching technique to minimize the reconfiguration overhead (delay) in Partially Reconfigurable (PR) FPGAs. This technique creates delays between the reconfiguration and execution parts of a task, which may lead up to 38% leakage power of FPGA since the SRAM-cells containing reconfiguration information cannot be powered down. In this work, a resource management approach (RMA) containing scheduling, placement and post-placement stages has been proposed to address the aforementioned issue. In scheduling stage, a leakage-aware priority function is derived to cope with the leakage power. The placement stage uses a cost function that allows designers to determine the desired trade-off between performance and leakage-saving. The post-placement stage employs a heuristic approach to close the gaps between reconfiguration and execution of tasks, hence further reduce leakage waste. To further examine the trade-off between performance (schedule length) and leakage waste, we propose a framework to utilize the Genetic Algorithm (GA) for exploring the design space and obtaining Pareto optimal design points. Addressing the time-consuming limitation of GA, we apply Regression technique and Clustering algorithm to build predictive models for the Pareto fronts using a training task graph dataset. Experiments show that our approach can achieve large leakage savings for both synthetic and real-life applications with acceptable extended deadline. Furthermore, different variants of the proposed approach can reduce leakage power by 40–65% when compared to a performance-driven approach and by 15–43% when compared to state-of-the-art works. It's also proven that our Machine Learning Optimization framework can estimate the Pareto front for new coming task graphs 10x faster than well-established GA approach with only 10% degradation in quality. © 2016 Elsevier B.V.",Microprocessors and Microsystems,10.1016/j.micpro.2016.09.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84999266385&doi=10.1016%2fj.micpro.2016.09.012&partnerID=40&md5=ca6d5fbe810e659ffcf076bba85ea942,2016,2021-07-20 15:49:58,2021-07-20 15:49:58
4NERELMX,journalArticle,2017,"Ibrahim, M.-H.; Pal, C.; Pesant, G.",Improving probabilistic inference in graphical models with determinism and cycles,"Many important real-world applications of machine learning, statistical physics, constraint programming and information theory can be formulated using graphical models that involve determinism and cycles. Accurate and efficient inference and training of such graphical models remains a key challenge. Markov logic networks (MLNs) have recently emerged as a popular framework for expressing a number of problems which exhibit these properties. While loopy belief propagation (LBP) can be an effective solution in some cases; unfortunately, when both determinism and cycles are present, LBP frequently fails to converge or converges to inaccurate results. As such, sampling based algorithms have been found to be more effective and are more popular for general inference tasks in MLNs. In this paper, we introduce Generalized arc-consistency Expectation Maximization Message-Passing (GEM-MP), a novel message-passing approach to inference in an extended factor graph that combines constraint programming techniques with variational methods. We focus our experiments on Markov logic and Ising models but the method is applicable to graphical models in general. In contrast to LBP, GEM-MP formulates the message-passing structure as steps of variational expectation maximization. Moreover, in the algorithm we leverage the local structures in the factor graph by using generalized arc consistency when performing a variational mean-field approximation. Thus each such update increases a lower bound on the model evidence. Our experiments on Ising grids, entity resolution and link prediction problems demonstrate the accuracy and convergence of GEM-MP over existing state-of-the-art inference algorithms such as MC-SAT, LBP, and Gibbs sampling, as well as convergent message passing algorithms such as the concave–convex procedure, residual BP, and the L2-convex method. © 2016, The Author(s).",Machine Learning,10.1007/s10994-016-5585-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979279248&doi=10.1007%2fs10994-016-5585-5&partnerID=40&md5=c296e3791f6ed8955b0ec61dc836e23b,2017,2021-07-20 15:49:58,2021-07-20 15:49:58
ZZWDD226,journalArticle,2015,"Magliacane, S.; Stutz, P.; Groth, P.; Bernstein, A.","foxPSL: A Fast, Optimized and eXtended PSL implementation","In this paper, we describe foxPSL, a fast, optimized and extended implementation of Probabilistic Soft Logic (PSL) based on the distributed graph processing framework Signal/Collect. PSL is one of the leading formalisms of statistical relational learning, a recently developed field of machine learning that aims at representing both uncertainty and rich relational structures, usually by combining logical representations with probabilistic graphical models. PSL can be seen as both a probabilistic logic and a template language for hinge-loss Markov Random Fields, a type of continuous Markov Random fields (MRF) in which Maximum a Posteriori inference is very efficient, since it can be formulated as a constrained convex minimization problem, as opposed to a discrete optimization problem for standard MRFs. From the logical perspective, a key feature of PSL is the capability to represent soft truth values, allowing the expression of complex domain knowledge, like degrees of truth, in parallel with uncertainty. foxPSL supports the full PSL pipeline from problem definition to a distributed solver that implements the Alternating Direction Method of Multipliers (ADMM) consensus optimization. It provides a Domain Specific Language that extends standard PSL with a class system and existential quantifiers, allowing for efficient grounding. Moreover, it implements a series of configurable optimizations, like optimized grounding of constraints and lazy inference, that improve grounding and inference time. We perform an extensive evaluation, comparing the performance of foxPSL to a state-of-the-art implementation of ADMM consensus optimization in GraphLab, and show an improvement in both inference time and solution quality. Moreover, we evaluate the impact of the optimizations on the execution time and discuss the trade-offs related to each optimization. © 2015 The Authors.",International Journal of Approximate Reasoning,10.1016/j.ijar.2015.05.012,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945462720&doi=10.1016%2fj.ijar.2015.05.012&partnerID=40&md5=2cad361e4594f8f9218d296080f98829,2015,2021-07-20 15:49:58,2021-07-20 15:49:58
HRZSN48B,journalArticle,2020,"Ali, M.Q.; Majeed, H.",Difficult first strategy GP: an inexpensive sampling technique to improve the performance of genetic programming,"Genetic programming (GP) is a top performer in solving classification and clustering problems, in general and symbolic regression problems, in particular. GP has produced impressive results and has outperformed human generated results for 76 different problems taken from 22 different fields.There remain a number of significant open issues despite its impressive results. Among them are high computational cost, premature convergence and high error rate. These issues must be addressed for GP to realise its full potential. In this paper a simple and cost effective technique called Difficult First Strategy-GP (DFS-GP) is proposed to address the aforementioned problems. The proposed technique involves pre-processing and sampling steps. In the pre-processing step, difficult to evolve data points by GP from the given data set are marked and in the sampling step, they are introduced in the evolutionary run by using two newly defined sampling techniques, called difficult points first and difficulty proportionate selection.These techniques are biased towards selecting difficult data points during the initial stage of a run and of easy points in the latter stage of a run. This ensures that GP does not ignore difficult-to-evolve data points during a run. Experiments have shown that GP coupled with DFS avoids premature convergence and attained higher fitness than standard GP using same fitness evaluations.Performance of the proposed technique was evaluated on three commonly known metrics, which are convergence speed, fitness and variance in the best results. Our results have shown that the proposed setups had achieved 10–15% better fitness values than Standard GP. Furthermore, the proposed setups had consistently generated better quality solutions on all the problems and utilized 30–50% less computations to match the best performance of Standard GP. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Evolutionary Intelligence,10.1007/s12065-020-00355-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079699305&doi=10.1007%2fs12065-020-00355-2&partnerID=40&md5=90494b54d7f1c930f69f141cae955bd6,2020,2021-07-20 15:49:58,2021-07-20 15:49:58
R25YF8JD,journalArticle,2020,"Kalita, K.; Mukhopadhyay, T.; Dey, P.; Haldar, S.",Genetic programming-assisted multi-scale optimization for multi-objective dynamic performance of laminated composites: the advantage of more elementary-level analyses,"High-fidelity multi-scale design optimization of many real-life applications in structural engineering still remains largely intractable due to the computationally intensive nature of numerical solvers like finite element method. Thus, in this paper, an alternate route of metamodel-based design optimization methodology is proposed in multi-scale framework based on a symbolic regression implemented using genetic programming (GP) coupled with d-optimal design. This approach drastically cuts the computational costs by replacing the finite element module with appropriately constructed robust and efficient metamodels. Resulting models are compact, have good interpretability and assume a free-form expression capable of capturing the non-linearly, complexity and vastness of the design space. Two robust nature-inspired optimization algorithms, viz. multi-objective genetic algorithm and multi-objective particle swarm optimization, are used to generate Pareto optimal solutions for several test problems with varying complexity. TOPSIS, a multi-criteria decision-making approach, is then applied to choose the best alternative among the Pareto optimal sets. Finally, the applicability of GP in efficiently tackling multi-scale optimization problems of composites is investigated, where a real-life scenario is explored by varying fractions of pertinent engineering materials to bring about property changes in the final composite structure across two different scales. The study reveals that a microscale optimization leads to better optimized solutions, demonstrating the advantage of carrying out a multi-scale optimization without any additional computational burden. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-019-04280-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067840928&doi=10.1007%2fs00521-019-04280-z&partnerID=40&md5=1f016086f54ac86b07f2ace26b809d4e,2020,2021-07-20 15:49:58,2021-07-20 15:49:58
MAD3P9IY,journalArticle,2020,"D'Angelo, G.; Palmieri, F.",Knowledge elicitation based on genetic programming for non destructive testing of critical aerospace systems,"In non-destructive testing of aerospace structures’ defects, the tests reliability is a crucial issue for guaranteeing security of both aircrafts and passengers. Most of the widely recognized approaches rely on precision and reliability of testing equipment, but also the methods and techniques used for processing measurement results, in order to detect defects, may heavily influence the overall quality of the testing process. The effectiveness of such methods strongly depends on specific field knowledge that is definitely not easy to be formalized and codified within the results processing practices. Although many studies have been conducted in this direction, such issue is yet an open-problem. This work describes the use of Genetic Programming for the diagnosis and modeling of aerospace structural defects. The resulting approach aims at extracting such knowledge by providing a mathematical model of the considered defects, which can be used for recognizing other similar ones. Eddy-Current Testing has been selected as a case study in order to assess both the performance and functionality of the whole framework, and a publicly available dataset of specific measures for aircraft structures has been considered. The experimental results put into evidence the effectiveness of the proposed approach in building reliable models of the aforementioned defects, so that it can be considered a successful option for building the knowledge needed by tools for controlling the quality of critical aerospace systems. © 2019 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2019.09.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072279374&doi=10.1016%2fj.future.2019.09.007&partnerID=40&md5=dddf248627728a07e254d13e7d9b595b,2020,2021-07-20 15:49:58,2021-07-20 15:49:58
6DWCY3EC,journalArticle,2012,"Mehler, A.; Lücking, A.; Menke, P.",Assessing cognitive alignment in different types of dialog by means of a network model,"We present a network model of dialog lexica, called TiTAN (Two-layer Time-Aligned Network) series. TiTAN series capture the formation and structure of dialog lexica in terms of serialized graph representations. The dynamic update of TiTAN series is driven by the dialog-inherent timing of turn-taking. The model provides a link between neural, connectionist underpinnings of dialog lexica on the one hand and observable symbolic behavior on the other. On the neural side, priming and spreading activation are modeled in terms of TiTAN networking. On the symbolic side, TiTAN series account for cognitive alignment in terms of the structural coupling of the linguistic representations of dialog partners. This structural stance allows us to apply TiTAN in machine learning of data of dialogical alignment. In previous studies, it has been shown that aligned dialogs can be distinguished from non-aligned ones by means of TiTAN -based modeling. Now, we simultaneously apply this model to two types of dialog: task-oriented, experimentally controlled dialogs on the one hand and more spontaneous, direction giving dialogs on the other. We ask whether it is possible to separate aligned dialogs from non-aligned ones in a type-crossing way. Starting from a recent experiment (. Mehler, Lücking, & Menke, 2011a), we show that such a type-crossing classification is indeed possible. This hints at a structural fingerprint left by alignment in networks of linguistic items that are routinely co-activated during conversation. © 2012 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2012.02.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861762399&doi=10.1016%2fj.neunet.2012.02.013&partnerID=40&md5=26a67baacbef087092153f8439bfccda,2012,2021-07-20 15:49:58,2021-07-20 15:49:58
XUB4AK5U,journalArticle,2011,"Fidalgo-Merino, R.; Núñez, M.",Self-adaptive induction of regression trees,"A new algorithm for incremental construction of binary regression trees is presented. This algorithm, called SAIRT, adapts the induced model when facing data streams involving unknown dynamics, like gradual and abrupt function drift, changes in certain regions of the function, noise, and virtual drift. It also handles both symbolic and numeric attributes. The proposed algorithm can automatically adapt its internal parameters and model structure to obtain new patterns, depending on the current dynamics of the data stream. SAIRT can monitor the usefulness of nodes and can forget examples from selected regions, storing the remaining ones in local windows associated to the leaves of the tree. On these conditions, current regression methods need a careful configuration depending on the dynamics of the problem. Experimentation suggests that the proposed algorithm obtains better results than current algorithms when dealing with data streams that involve changes with different speeds, noise levels, sampling distribution of examples, and partial or complete changes of the underlying function. © 2011 IEEE.",IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2011.19,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959517343&doi=10.1109%2fTPAMI.2011.19&partnerID=40&md5=1e3701968a1cff8514d86c6fa9ce4161,2011,2021-07-20 15:49:58,2021-07-20 15:49:58
Y56IDMX3,journalArticle,2018,"Kim, E.-H.; Oh, S.-K.; Pedrycz, W.",Design of double fuzzy clustering-driven context neural networks,"In this study, we introduce a novel category of double fuzzy clustering-driven context neural networks (DFCCNNs). The study is focused on the development of advanced design methodologies for redesigning the structure of conventional fuzzy clustering-based neural networks. The conventional fuzzy clustering-based neural networks typically focus on dividing the input space into several local spaces (implied by clusters). In contrast, the proposed DFCCNNs take into account two distinct local spaces called context and cluster spaces, respectively. Cluster space refers to the local space positioned in the input space whereas context space concerns a local space formed in the output space. Through partitioning the output space into several local spaces, each context space is used as the desired (target) local output to construct local models. To complete this, the proposed network includes a new context layer for reasoning about context space in the output space. In this sense, Fuzzy C-Means (FCM) clustering is useful to form local spaces in both input and output spaces. The first one is used in order to form clusters and train weights positioned between the input and hidden layer, whereas the other one is applied to the output space to form context spaces. The key features of the proposed DFCCNNs can be enumerated as follows: (i) the parameters between the input layer and hidden layer are built through FCM clustering. The connections (weights) are specified as constant terms being in fact the centers of the clusters. The membership functions (represented through the partition matrix) produced by the FCM are used as activation functions located at the hidden layer of the “conventional” neural networks. (ii) Following the hidden layer, a context layer is formed to approximate the context space of the output variable and each node in context layer means individual local model. The outputs of the context layer are specified as a combination of both weights formed as linear function and the outputs of the hidden layer. The weights are updated using the least square estimation (LSE)-based method. (iii) At the output layer, the outputs of context layer are decoded to produce the corresponding numeric output. At this time, the weighted average is used and the weights are also adjusted with the use of the LSE scheme. From the viewpoint of performance improvement, the proposed design methodologies are discussed and experimented with the aid of benchmark machine learning datasets. Through the experiments, it is shown that the generalization abilities of the proposed DFCCNNs are better than those of the conventional FCNNs reported in the literature. © 2018 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2018.03.018,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046336298&doi=10.1016%2fj.neunet.2018.03.018&partnerID=40&md5=86290d990a097dcf11db86d690c6acb1,2018,2021-07-20 15:49:58,2021-07-20 15:49:58
JA5HHBV2,journalArticle,2020,"Nápoles, G.; Jastrzębska, A.; Mosquera, C.; Vanhoof, K.; Homenda, W.",Deterministic learning of hybrid Fuzzy Cognitive Maps and network reduction approaches,"Hybrid artificial intelligence deals with the construction of intelligent systems by relying on both human knowledge and historical data records. In this paper, we approach this problem from a neural perspective, particularly when modeling and simulating dynamic systems. Firstly, we propose a Fuzzy Cognitive Map architecture in which experts are requested to define the interaction among the input neurons. As a second contribution, we introduce a fast and deterministic learning rule to compute the weights among input and output neurons. This parameterless learning method is based on the Moore–Penrose inverse and it can be performed in a single step. In addition, we discuss a model to determine the relevance of weights, which allows us to better understand the system. Last but not least, we introduce two calibration methods to adjust the model after the removal of potentially superfluous weights. © 2020 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2020.01.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078744842&doi=10.1016%2fj.neunet.2020.01.019&partnerID=40&md5=1adf862ed129e7abc0864e917908816c,2020,2021-07-20 15:49:59,2021-07-20 15:49:59
D2NEZW6A,journalArticle,2019,"Abdel-Basset, M.; Mohamed, M.; Elhoseny, M.; Son, L.H.; Chiclana, F.; Zaied, A.E.-N.H.",Cosine similarity measures of bipolar neutrosophic set for diagnosis of bipolar disorder diseases,"Similarity plays a significant implicit or explicit role in various fields. In some real applications in decision making, similarity may bring counterintuitive outcomes from the decision maker's standpoint. Therefore, in this research, we propose some novel similarity measures for bipolar and interval-valued bipolar neutrosophic set such as the cosine similarity measures and weighted cosine similarity measures. The propositions of these similarity measures are examined, and two multi-attribute decision making techniques are presented based on proposed measures. For verifying the feasibility of proposed measures, two numerical examples are presented in comparison with the related methods for demonstrating the practicality of the proposed method. Finally, we applied the proposed measures of similarity for diagnosing bipolar disorder diseases. © 2019 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2019.101735,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074299137&doi=10.1016%2fj.artmed.2019.101735&partnerID=40&md5=46e3785fe04e6457b1b660e230eb84e1,2019,2021-07-20 15:49:59,2021-07-20 15:49:59
2YIRBV9M,journalArticle,2019,"Bu, X.",Actor-Critic Reinforcement Learning Control of Non-Strict Feedback Nonaffine Dynamic Systems,"The most focuses of the existing actor-critic reinforcement learning control (ARLC) are on dealing with continuous affine systems or discrete nonaffine systems. In this paper, I propose a new ARLC method for continuous nonaffine dynamic systems subject to unknown dynamics and external disturbances. A new input-to-state stable system is developed to establish an augmented dynamic system, from which I further get a strict-feedback affine model that is convenient for control designing based on a model transformation approach. The Nussbaum function is connected with a fuzzy approximation to devise an actor network whose tracking performance is further enhanced via strengthening signals generated by a fuzzy critic network. The stability of the closed-loop control system is guaranteed by the Lyapunov synthesis. Finally, the comparison simulation results are presented to verify the design. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2917141,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066970337&doi=10.1109%2fACCESS.2019.2917141&partnerID=40&md5=dfba3bd4a4ad9ea32b5b4a50f08b0e5e,2019,2021-07-20 15:49:59,2021-07-20 15:49:59
4JHZW82D,journalArticle,2018,"Danish Lohani, Danishlohani@cs.sau.ac.in, Q.M.; Solanki, R.; Muhuri, P.K.",Novel Adaptive Clustering Algorithms Based on a Probabilistic Similarity Measure over Atanassov Intuitionistic Fuzzy Set,"This paper presents a novel probabilistic similarity measure (PSM) for Atanassov intuitionistic fuzzy sets. It then exploits PSM to propose an adaptive probabilistic similarity degree and develops the novel probabilistic λ-cutting algorithm for clustering. Further, the probabilistic distance measure (obtained from the PSM) is used to develop a new clustering technique, which we have named 'probabilistic intuitionistic fuzzy c-mean (PIFCM) algorithm'. Simulation experiments have been conducted over a variety of datasets including UCI machine learning datasets and real-world car dataset. The results obtained have been thoroughly compared with other well-known clustering techniques such as fuzzy c-mean (FCM), intuitionistic fuzzy c-mean, association coefficient method, and λ-cutting method. Based upon the experimental results, it can be concluded that our probabilistic λ-cutting algorithm and PIFCM algorithm outperform their existing counterparts. © 1993-2012 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2018.2848245,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048599468&doi=10.1109%2fTFUZZ.2018.2848245&partnerID=40&md5=9a1817b65c5336343b9073974e1dbf0f,2018,2021-07-20 15:49:59,2021-07-20 15:49:59
EHJTC4QH,journalArticle,2017,"Quost, B.; Denœux, T.; Li, S.",Parametric classification with soft labels using the evidential EM algorithm: linear discriminant analysis versus logistic regression,"Partially supervised learning extends both supervised and unsupervised learning, by considering situations in which only partial information about the response variable is available. In this paper, we consider partially supervised classification and we assume the learning instances to be labeled by Dempster–Shafer mass functions, called soft labels. Linear discriminant analysis and logistic regression are considered as special cases of generative and discriminative parametric models. We show that the evidential EM algorithm can be particularized to fit the parameters in each of these models. We describe experimental results with simulated data sets as well as with two real applications: K-complex detection in sleep EEGs signals and facial expression recognition. These results confirm the interest of using soft labels for classification as compared to potentially erroneous crisp labels, when the true class membership is partially unknown or ill-defined. © 2017, Springer-Verlag GmbH Germany, part of Springer Nature.",Advances in Data Analysis and Classification,10.1007/s11634-017-0301-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033482933&doi=10.1007%2fs11634-017-0301-2&partnerID=40&md5=2b081fedf10595334bf058e6cc4b8310,2017,2021-07-20 15:49:59,2021-07-20 15:49:59
RVMEBURW,journalArticle,2016,"Song, B.; Jiang, Z.; Liu, L.",Automated experiential engineering knowledge acquisition through Q&A contextualization and transformation,"Experiential knowledge (EK) in the brain of proficient engineers is an important asset for manufacturing enterprises. As a kind of tacit knowledge, EK is hard to describe clearly and often requires a lot of human efforts to be acquired in a computer-operable form. In this paper we propose a context-aware mechanism to acquire EK in an automatic and timely manner. The proposal comprises a formal description of EK using ontology and default logic, a machine learning-based method that discovers Q&A from the context of collaborative engineering tasks, and a semantic mapping step transforming the discovered Q&A into ontological concepts and relations. An application case shows that the EK of a group of engineers collaborating over a finite element analysis task can be automatically captured from their desktop information flow. The effectiveness of the proposed method with respect to other knowledge acquisition approaches is demonstrated through quantitative and qualitative comparison. © 2016 Elsevier Ltd. All rights reserved.",Advanced Engineering Informatics,10.1016/j.aei.2016.06.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976354880&doi=10.1016%2fj.aei.2016.06.002&partnerID=40&md5=8550baa7726a266dc5f94dff5fc4e5a0,2016,2021-07-20 15:49:59,2021-07-20 15:49:59
FN6ZI9C4,journalArticle,2015,"Fong, S.; Deb, S.; Chaudhary, A.",A review of metaheuristics in robotics,"Metaheuristics have a substantial history in fine-tuning machine learning algorithms. They gained tremendous popularity in many application domains. Robotics on the other hand is a wide research discipline that embraces artificial intelligence in a complex individually-thinking robot and distributed robots. Recently, metaheuristics made a significant impact on the application areas of collaborating robotics. This new trend of collaborating robotics, offers the possibility of enhanced task performance, high reliability, low unit complexity and decreased cost over traditional robotic systems. Collaborating robots however are more than just networks of independent agents; they are potentially reconfigurable networks of communicating agents capable of coordinated sensing and interaction with the environment. On the conceptual level, these bots can be empowered by the logics of metaheuristic algorithms which share the same functionalities and capabilities. This paper reviews the recent advances of metaheuristic algorithms on robotics applications. A taxonomy is provided as a reference for robotics designers. © 2015 Elsevier Ltd",Computers and Electrical Engineering,10.1016/j.compeleceng.2015.01.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924709362&doi=10.1016%2fj.compeleceng.2015.01.009&partnerID=40&md5=1cebbcf662b5563f2b944a535adefb08,2015,2021-07-20 15:49:59,2021-07-20 15:49:59
54LE8HAY,journalArticle,2014,"Davtalab, R.; Dezfoulian, M.H.; Mansoorizadeh, M.",Multi-level fuzzy min-max neural network classifier,"In this paper a multi-level fuzzy min-max neural network classifier (MLF), which is a supervised learning method, is described. MLF uses basic concepts of the fuzzy min-max (FMM) method in a multi-level structure to classify patterns. This method uses separate classifiers with smaller hyperboxes in different levels to classify the samples that are located in overlapping regions. The final output of the network is formed by combining the outputs of these classifiers. MLF is capable of learning nonlinear boundaries with a single pass through the data. According to the obtained results, the MLF method, compared to the other FMM networks, has the highest performance and the lowest sensitivity to maximum size of the hyperbox parameter θ, with a training accuracy of 100% in most cases. © 2012 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2013.2275937,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897647724&doi=10.1109%2fTNNLS.2013.2275937&partnerID=40&md5=2f4ee25e2029aaa9701d9971f67eb640,2014,2021-07-20 15:49:59,2021-07-20 15:49:59
AYSKV65J,journalArticle,2014,"Matlin, E.; Agrawal, M.; Stoker, D.",Non-invasive recognition of poorly resolved integrated circuit elements,"We present a non-invasive method for recognition of components in a digital CMOS integrated circuit (IC). We use a confocal infrared laser scanning optical microscope to collect multimodal images through the backside of the IC. Individual modes correspond to passive reflectivity measurements or active measurements, such as light-induced voltage alteration. The modes are registered and stored in a multidimensional data cube. We apply a machine learning algorithm using a binary representation to identify a variety of data structures from transistors to entire logic cells. Because of the compact representation, objects can be detected rapidly. We show that by increasing the number of imaging modes used to develop the descriptor, we can significantly increase recognition accuracy. The approach allows recognition of poorly resolved components, whose primary distinguishing features are below traditional optical resolution limits, and is general enough to be applied to multiple design processes. We believe this represents a significant step toward a fully non-invasive IC reverse engineering system. © 2005-2012 IEEE.",IEEE Transactions on Information Forensics and Security,10.1109/TIFS.2013.2297518,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894121699&doi=10.1109%2fTIFS.2013.2297518&partnerID=40&md5=dfba6f494b2520465f180dab02a00779,2014,2021-07-20 15:49:59,2021-07-20 15:49:59
Y9JQT33E,journalArticle,2012,"Daliri, M.R.",A hybrid automatic system for the diagnosis of lung cancer based on genetic algorithm and fuzzy extreme learning machines,"An automatic system for the diagnosis of lung cancer has been proposed in this manuscript. The proposed method is based on combination of genetic algorithm (GA) for the feature selection and newly proposed approach, namely the extreme learning machines (ELM) for the classification of lung cancer data. The dimension of the feature space is reduced by the GA in this scheme and the effective features are selected in this way. The data are then fed to a fuzzy inference system (FIS) which is trained by the fuzzy extreme learning machines approach. The results on real data indicate that the proposed system is very effective in the diagnosis of lung cancer and can be used for clinical applications. © Springer Science+Business Media, LLC 2011.",Journal of Medical Systems,10.1007/s10916-011-9806-y,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863210448&doi=10.1007%2fs10916-011-9806-y&partnerID=40&md5=0f1f023312cc9c9c62b4378c451b01e6,2012,2021-07-20 15:49:59,2021-07-20 15:49:59
PKBF7NDP,journalArticle,2021,"Hu, X.; Zhao, Y.; Deng, L.; Liang, L.; Zuo, P.; Ye, J.; Lin, Y.; Xie, Y.",Practical Attacks on Deep Neural Networks by Memory Trojaning,"Deep neural network (DNN) accelerators are widely deployed in computer vision, speech recognition, and machine translation applications, in which attacks on DNNs have become a growing concern. This article focuses on exploring the implications of hardware Trojan attacks on DNNs. Trojans are one of the most challenging threat models in hardware security where adversaries insert malicious modifications to the original integrated circuits (ICs), leading to malfunction once being triggered. Such attacks can be conducted by adversaries because modern ICs commonly include third-party intellectual property (IP) blocks. Previous studies design hardware Trojans to attack DNNs with the assumption that adversaries have full knowledge or manipulation of the DNN systems' victim model and toolchain in addition to the hardware platforms, yet such a threat model is strict, limiting their practical adoption. In this article, we propose a memory Trojan methodology that implants the malicious logics merely into the memory controllers of DNN systems without the necessity of toolchain manipulation or accessing to the victim model and thus is feasible for practical uses. Specifically, we locate the input image data among the massive volume of memory traffics based on memory access patterns and propose a Trojan trigger mechanism based on detecting the geometric feature in input images. Extensive experiments show that the proposed trigger mechanism is effective even in the presence of environmental noises and preprocessing operations. Furthermore, we design and implement the payload and verify that the proposed Trojan technique can effectively conduct both untargeted and targeted attacks on DNNs. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.2995347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085758557&doi=10.1109%2fTCAD.2020.2995347&partnerID=40&md5=e0c1bf4b79a1136b2a495c4970ec7382,2021,2021-07-20 15:49:59,2021-07-20 15:49:59
AS3TKGGI,journalArticle,2021,"Li, Y.; Liu, Y.; Guo, Y.; Liao, X.; Hu, B.; Yu, T.",Spatio-Temporal-Spectral Hierarchical Graph Convolutional Network With Semisupervised Active Learning for Patient-Specific Seizure Prediction,"Graph theory analysis using electroencephalogram (EEG) signals is currently an advanced technique for seizure prediction. Recent deep learning approaches, which fail to fully explore both the characterizations in EEGs themselves and correlations among different electrodes simultaneously, generally neglect the spatial or temporal dependencies in an epileptic brain and, thus, produce suboptimal seizure prediction performance consequently. To tackle this issue, in this article, a patient-specific EEG seizure predictor is proposed by using a novel spatio-temporal-spectral hierarchical graph convolutional network with an active preictal interval learning scheme (STS-HGCN-AL). Specifically, since the epileptic activities in different brain regions may be of different frequencies, the proposed STS-HGCN-AL framework first infers a hierarchical graph to concurrently characterize an epileptic cortex under different rhythms, whose temporal dependencies and spatial couplings are extracted by a spectral-temporal convolutional neural network and a variant self-gating mechanism, respectively. Critical intrarhythm spatiotemporal properties are then captured and integrated jointly and further mapped to the final recognition results by using a hierarchical graph convolutional network. Particularly, since the preictal transition may be diverse from seconds to hours prior to a seizure onset among different patients, our STS-HGCN-AL scheme estimates an optimal preictal interval patient dependently via a semisupervised active learning strategy, which further enhances the robustness of the proposed patient-specific EEG seizure predictor. Competitive experimental results validate the efficacy of the proposed method in extracting critical preictal biomarkers, indicating its promising abilities in automatic seizure prediction. IEEE",IEEE Transactions on Cybernetics,10.1109/TCYB.2021.3071860,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107201098&doi=10.1109%2fTCYB.2021.3071860&partnerID=40&md5=d5058ec4367e45b5a1c74dfea18dcd36,2021,2021-07-20 15:49:59,2021-07-20 15:49:59
5ZN6Z7BI,journalArticle,2019,"Yin, S.; Tang, S.; Lin, X.; Ouyang, P.; Tu, F.; Liu, L.; Wei, S.",A High Throughput Acceleration for Hybrid Neural Networks with Efficient Resource Management on FPGA,"Deep learning is the amazing technology which has promoted the development of artificial intelligence and achieved many amazing successes in intelligent fields. Convolution-based layers (CLs), fully connected layers (FLs) and recurrent layers (RLs) are three types of layers in classic neural networks. Most intelligent tasks are implemented by the hybrid neural networks (hybrid-NNs), which are commonly composed of different layer-blocks (LBs) of CLs, FLs, and RLs. Because the CLs require the most computation in hybrid-NNs, many field-programmable gate array (FPGA)-based accelerators focus on CLs acceleration and have demonstrated great performance. However, the CLs accelerators lead to an underutilization of FPGA resources in the acceleration of the whole hybrid-NN. To fully exploit the logic resources and the memory bandwidth in the acceleration of CLs/FLs/RLs, we propose an FPGA resource efficient mapping mechanism for hybrid-NNs. The mechanism first improves the utilization of DSPs by integrating multiple small bit-width operations on one DSP. Then the LB-level spatial mapping is used to exploit the complementary features between different neural networks in the hybrid-NN. We evaluate the mapping mechanism by implementing four hybrid-NNs on Xilinx Virtex7 690T FPGA. The proposed mechanism achieves a peak performance of 1805.8 giga operations per second (GOPs). With the analysis on resource utilization and throughput, the proposed method exploits more computing power in FPGA and achieves up to 4.13 × higher throughput than the state-of-the-art acceleration. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2821561,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044737461&doi=10.1109%2fTCAD.2018.2821561&partnerID=40&md5=a4aee2ac92bc67a989a95f0ba0a8e78d,2019,2021-07-20 15:49:59,2021-07-20 15:49:59
R9CKVUCN,journalArticle,2018,"Shao, W.; Luo, H.; Zhao, F.; Ma, Y.; Zhao, Z.; Crivello, A.",Indoor Positioning Based on Fingerprint-Image and Deep Learning,"Wi-Fi and magnetic field fingerprinting have been a hot topic in indoor positioning researches because of their ubiquity and location-related features. Wi-Fi signals can provide rough initial positions, and magnetic fields can further improve the positioning accuracies, therefore many researchers have tried to combine the two signals for high-accuracy indoor localization. Currently, state-of-the-art solutions design separate algorithms to process different indoor signals. Outputs of these algorithms are generally used as inputs of data fusion strategies. These methods rely on computationally expensive particle filters, labor-intensive feature analysis, and time-consuming parameter tuning to achieve better accuracies. Besides, particle filters need to estimate the moving directions of particles, limiting smartphone orientation to be stable, and aligned with the user's moving directions. In this paper, we adopted a convolutional neural network (CNN) to implement an accurate and orientation-free positioning system. Inspired by the state-of-the-art image classification methods, we design a novel hybrid location image using Wi-Fi and magnetic field fingerprints, and then a CNN is employed to classify the locations of the fingerprint images. In order to prevent the overfitting problem of the positioning CNN on limited training datasets, we also propose to divide the learning process into two steps to adopt proper learning strategies for different network branches. We show that the CNN solution is able to automatically learn location patterns, thus significantly lower the workforce burden of designing a localization system. Our experimental results convincingly reveal that the proposed positioning method achieves an accuracy of about 1 m under different smartphone orientations, users, and use patterns. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2018.2884193,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057790544&doi=10.1109%2fACCESS.2018.2884193&partnerID=40&md5=69bff7ec156d04e9ccea1e21a795a7a1,2018,2021-07-20 15:50:00,2021-07-20 15:50:00
UL2UAG63,journalArticle,2020,"Malinka, F.; Železný, F.; Kléma, J.",Finding semantic patterns in omics data using concept rule learning with an ontology-based refinement operator,"Background: Identification of non-trivial and meaningful patterns in omics data is one of the most important biological tasks. The patterns help to better understand biological systems and interpret experimental outcomes. A well-established method serving to explain such biological data is Gene Set Enrichment Analysis. However, this type of analysis is restricted to a specific type of evaluation. Abstracting from details, the analyst provides a sorted list of genes and ontological annotations of the individual genes; the method outputs a subset of ontological terms enriched in the gene list. Here, in contrary to enrichment analysis, we introduce a new tool/framework that allows for the induction of more complex patterns of 2-dimensional binary omics data. This extension allows to discover and describe semantically coherent biclusters. Results: We present a new rapid method called sem1R that reveals interpretable hidden rules in omics data. These rules capture semantic differences between two classes: a target class as a collection of positive examples and a non-target class containing negative examples. The method is inspired by the CN2 rule learner and introduces a new refinement operator that exploits prior knowledge in the form of ontologies. In our work this knowledge serves to create accurate and interpretable rules. The novel refinement operator uses two reduction procedures: Redundant Generalization and Redundant Non-potential, both of which help to dramatically prune the rule space and consequently, speed-up the entire process of rule induction in comparison with the traditional refinement operator as is presented in CN2. Conclusions: Efficiency and effectivity of the novel refinement operator were tested on three real different gene expression datasets. Concretely, the Dresden Ovary Dataset, DISC, and m2816 were employed. The experiments show that the ontology-based refinement operator speeds-up the pattern induction drastically. The algorithm is written in C++ and is published as an R package available at http://github.com/fmalinka/sem1r. © 2020 The Author(s).",BioData Mining,10.1186/s13040-020-00219-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85092138340&doi=10.1186%2fs13040-020-00219-6&partnerID=40&md5=b6561d541e3a6453e891e76b44418d20,2020,2021-07-20 15:50:00,2021-07-20 15:50:00
VFJGSECU,journalArticle,2016,"Conţiu, Ş.; Groza, A.",Improving remote sensing crop classification by argumentation-based conflict resolution in ensemble learning,"The acquisition of data through remote sensing has become of great importance in precision agriculture, as it covers large geographical areas faster and cheaper than ground inspections. The challenge is to develop technical solutions that can benefit from both huge amounts of raw data extracted from satellite images, but also from the robust amount of knowledge refined during centuries of agricultural practice. Aiming to accurately classify crops from satellite images, we developed a hybrid intelligent system that can exploit both agricultural expert knowledge and machine learning algorithms. As the crop raw data is characterized by heterogeneity, we drive our attention to ensemble learners, while expert knowledge is encapsulated within a rule-based system. Vote-based methods for solving conflicts between ensemble's base learners have difficulties in classifying exceptional cases correctly and also to give the rationale behind their decision. The conceptual research question is on conflict resolution in ensemble learning. To deal with debatable cases in ensemble learning and to increase transparency in such debatable decisions, our hypothesis is that argumentation could be more effective than voting-based methods. The main contribution is that voting system in ensemble learning is substituted by an argumentation-base conflict resolutor. Prospective decisions of base classifiers are presented to an argumentative system based on defeasible logic that performs dialectical reasoning on pros and cons against a classification decision. The system computes a recommendation considering both the rules extracted from base learners and the available expert knowledge. The investigated case study deals with crop classification into four classes: corn, soybean, cotton, and rice. The test site used for the experiment is an area of 20 square kilometers in the New Madrid County, southeast of the Missouri State, USA. The results show that our approach increases classification accuracy compared to the voting-based method for conflict resolution in an ensemble learner comprising of three base classifiers: a decision tree, a neural network, and a support vector machine algorithm. We also argue that combining ensemble learning and argumentation fits the decision patterns of human agents, who first collect various opinions and then perform dialectical reasoning on these opinions. We think that the people who can benefit from the conceptual instrumentation presented in this work are decision makers in domains characterized by high data availability, robust expert knowledge, and a need for justifying the rationale behind decisions. © 2016 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2016.07.037,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980322412&doi=10.1016%2fj.eswa.2016.07.037&partnerID=40&md5=c6cd44fdb927b7b54fcbcbe6bd7b4ee9,2016,2021-07-20 15:50:00,2021-07-20 15:50:00
E7VNR2JD,journalArticle,2021,"Butt, M.A.; Riaz, F.; Mehmood, Y.; Akram, S.",REEEC-AGENT: human driver cognition and emotions-inspired rear-end collision avoidance method for autonomous vehicles,"Rear-end collision detection and avoidance is one of the most crucial driving tasks of self-driving vehicles. Mathematical models and fuzzy logic-based methods have recently been proposed to improve the effectiveness of the rear-end collision detection and avoidance systems in autonomous vehicles (AVs). However, these methodologies do not tackle real-time object detection and response problems in dense/dynamic road traffic conditions due to their complex computation and decision-making structures. In our previous work, we presented an affective computing-inspired Enhanced Emotion Enabled Cognitive Agent (EEEC_Agent), which is capable of rear-end collision avoidance using artificial human driver emotions. However, the architecture of the EEEC_Agent is based on an ultrasonic sensory system which follows three-state driving strategies without considering the neighbor vehicles types. To address these issues, in this paper we propose an extended version of the EEEC_Agent which contains human driver-inspired dynamic driving mode controls for autonomous vehicles. In addition, a novel end-to-end learning-based motion planner has been devised to perceive the surrounding environment and regulate driving tasks accordingly. The real-time in-field experiments performed using a prototype AV demonstrate the effectiveness of this proposed rear-end collision avoidance system. © The Author(s) 2021.",Simulation,10.1177/00375497211004721,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104256380&doi=10.1177%2f00375497211004721&partnerID=40&md5=470469474091ba2a5fe8051742f3a5f6,2021,2021-07-20 15:50:00,2021-07-20 15:50:00
FS2QA79A,journalArticle,2020,"Sanchez Gorostiza, F.; Gonzalez-Longatt, F.M.",Deep Reinforcement Learning-Based Controller for SOC Management of Multi-Electrical Energy Storage System,"The ongoing reduction of the total rotational inertia in modern power systems brings about faster frequency dynamics that must be limited to maintain a secure and economical operation. Electrical energy storage systems (EESSs) have become increasingly attractive to provide fast frequency response services due to their response times. However, proper management of their finite energy reserves is required to ensure timely and secure operation. This paper proposes a deep reinforcement learning (DRL) based controller to manage the state of charge (SOC) of a Multi-EESS (M-EESS), providing frequency response services to the power grid. The proposed DRL agent is trained using an actor-critic method called Deep Deterministic Policy Gradients (DDPG) that allows for continuous action and smoother SOC control of the M-EESS. Deep neural networks (DNNs) are used to represent the actor and critic policies. The proposed strategy comprises granting the agent a constant reward for each time step that the SOC is within a specific band of its target value combined with a substantial penalty if the SOC reaches its minimum or maximum allowable values. The proposed controller is compared to benchmark DRL methods and other control techniques, i.e., Fuzzy Logic and a traditional PID control. Simulation results show the effectiveness of the proposed approach. © 2010-2012 IEEE.",IEEE Transactions on Smart Grid,10.1109/TSG.2020.2996274,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094842130&doi=10.1109%2fTSG.2020.2996274&partnerID=40&md5=34a15c4acf5af7ddb70239728d28a2e6,2020,2021-07-20 15:50:00,2021-07-20 15:50:00
Y59NN2TS,journalArticle,2020,"Faraone, J.; Kumm, M.; Hardieck, M.; Zipf, P.; Liu, X.; Boland, D.; Leong, P.H.W.",AddNet: Deep Neural Networks Using FPGA-Optimized Multipliers,"Low-precision arithmetic operations to accelerate deep-learning applications on field-programmable gate arrays (FPGAs) have been studied extensively, because they offer the potential to save silicon area or increase throughput. However, these benefits come at the cost of a decrease in accuracy. In this article, we demonstrate that reconfigurable constant coefficient multipliers (RCCMs) offer a better alternative for saving the silicon area than utilizing low-precision arithmetic. RCCMs multiply input values by a restricted choice of coefficients using only adders, subtractors, bit shifts, and multiplexers (MUXes), meaning that they can be heavily optimized for FPGAs. We propose a family of RCCMs tailored to FPGA logic elements to ensure their efficient utilization. To minimize information loss from quantization, we then develop novel training techniques that map the possible coefficient representations of the RCCMs to neural network weight parameter distributions. This enables the usage of the RCCMs in hardware, while maintaining high accuracy. We demonstrate the benefits of these techniques using AlexNet, ResNet-18, and ResNet-50 networks. The resulting implementations achieve up to 50% resource savings over traditional 8-bit quantized networks, translating to significant speedups and power savings. Our RCCM with the lowest resource requirements exceeds 6-bit fixed point accuracy, while all other implementations with RCCMs achieve at least similar accuracy to an 8-bit uniformly quantized design, while achieving significant resource savings. © 1993-2012 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2019.2939429,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077816344&doi=10.1109%2fTVLSI.2019.2939429&partnerID=40&md5=c357fcf43f1f26b2ae72672993157406,2020,2021-07-20 15:50:00,2021-07-20 15:50:00
S5HNKTP4,journalArticle,2015,"Ma, Z.","Towards computational models of animal cognition, an introduction for computer scientists","The last few years of the twentieth century witnessed the emerging convergence of biology and computer science and this trend has been accelerating since then. The study of animal behavior or behavior biology has been one of the major contributors for this convergence. Behavior is fascinating because it is the response of an organism to internal and external signals and it is controlled by complex interactions among nerves, the sensory and the motor systems. To some extent, behavior is similar to the output (or response) of a computer system or a network node if we consider an animal brain as a computer node. This paper is the first in a two-part series in which I review the state-of-the-art research in behavior biology inspired computing and communication, with the first part focusing on animal cognition and the second part on animal communication (Ma, 2014). The present article also assumes the task of presenting a general introduction on behavior biology literature, which sets a foundation for synthesizing both parts of the series but the synthesis will be performed in the second part of the series. I sets three objectives in this 'cognition' part: (i) to present a brief overview on the literature of behavior biology for computer scientists; (ii) to summarize the state-of-the-art studies in several cognitive aspects of animal behavior: focusing on emerging research in cognitive ecology, social learning and innovation, as well as animal logics; (iii) to review some important existing studies inspired by animal behavior and further present a perspective on the future research. These cognition-related topics offer insights for research fields such as machine learning, human computer interactions (HCI), brain computer interfaces (BCIs), evolutionary computing, pervasive computing, etc. In perspective, I suggest that the interaction between behavioral biology and computer science should be bidirectional, and a new subject, behavioral informatics, or more general computational behavior biology, should be developed by the cooperative efforts between biologists and computer scientists. © 2014 Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2014.08.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920160425&doi=10.1016%2fj.cogsys.2014.08.001&partnerID=40&md5=078ee8c9be171854bfc97884e40d55dd,2015,2021-07-20 15:50:00,2021-07-20 15:50:00
TLV4LBCV,journalArticle,2021,"Tan, H.; Tarasov, V.; Jarfors, A.E.W.; Seifeddine, S.",A design of fuzzy inference systems to predict tensile properties of as-cast alloy,"In this study, a design of Mamdani type fuzzy inference systems is presented to predict tensile properties of as-cast alloy. To improve manufacturing of light weight cast components, understanding of mechanical properties of cast components under load is important. The ability of deterministic models to predict the performance of a cast component is limited due to the uncertainty and imprecision in casting data. Mamdani type fuzzy inference systems are introduced as a promising solution. Compared to other artificial intelligence approaches, Mandani type fuzzy models allow for a better result interpretation. The fuzzy inference systems were designed from data and experts’ knowledge and optimized using a genetic algorithm. The experts’ knowledge was used to set up the values for the inference engine and initial values for the database parameters. The rule base was automatically generated from the data which were collected from casting and tensile testing experiments. A genetic algorithm with real-valued coding was used to optimize the database parameters. The quality of the constructed systems was evaluated by comparing predicted and actual tensile properties, including yield strength, Y.modulus, and ultimate tensile strength, of as-case alloy from two series of casting and tensile testing experimental data. The obtained results showed that the quality of the systems has satisfactory accuracy and is similar to or better than several machine learning methods. The evaluation results also demonstrated good reliability and stability of the approach. © 2021, The Author(s).",International Journal of Advanced Manufacturing Technology,10.1007/s00170-020-06502-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100088104&doi=10.1007%2fs00170-020-06502-4&partnerID=40&md5=3170ff2154b3d895b0490130fa28fea6,2021,2021-07-20 15:50:00,2021-07-20 15:50:00
V9XFUV45,journalArticle,2021,"Sun, W.; Wang, Z.; Zhang, G.",A QoS-guaranteed intelligent routing mechanism in software-defined networks,"With the development of the Internet of Things (IoT), the network is required to guarantee the differential Quality of Service (QoS) requirements of the various data flows of various IoT services. Software-defined network (SDN) is envisioned as a promising technique to guarantee the QoS requirements of different services, through separating the control logic from data planes of networks. In order to guarantee the QoS requirements of data flows in SDNs, in this paper we investigate the problem of intelligent routing in SDNs, by leveraging a novel data flow classification method. Combining a variety of machine learning algorithms, a data flow classification method called MACCA2-RF&RF is proposed, in order to identify the data flow category and obtain the QoS requirements. The link parameter is newly designed considering multiple QoS requirements. According to the link parameter, the QoS-guaranteed path selection algorithm is then proposed, which can select QoS guaranteed routing path for different data flows with different QoS requirements. Aiming at the situation that the link is congested, local routing change algorithm is then proposed which only adjusts the links before and after the congested link instead of the entire path. Based on the above, a QoS-guaranteed intelligent routing mechanism called QI-RM in SDN is finally proposed in this paper to achieve QoS guarantee for data flows. The simulation results show that the MACCA2-RF&RF can classify data flows efficiently with an identification accuracy of 99.73%, and the QI-RM can guarantee the QoS requirements of data flows before and after link congestion. © 2020",Computer Networks,10.1016/j.comnet.2020.107709,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097582857&doi=10.1016%2fj.comnet.2020.107709&partnerID=40&md5=dcc52d134500f8844d9d27d0dfde2851,2021,2021-07-20 15:50:00,2021-07-20 15:50:00
IG2LL9P9,journalArticle,2021,"Mothukuri, V.; Khare, P.; Parizi, R.M.; Pouriyeh, S.; Dehghantanha, A.; Srivastava, G.",Federated Learning-based Anomaly Detection for IoT Security Attacks,"The Internet of Things (IoT) is made up of billions of physical devices connected to the Internet via networks that perform tasks independently with less human intervention. Such brilliant automation of mundane tasks requires a considerable amount of user data in digital format, which in turn makes IoT networks an open-source of Personally Identifiable Information data for malicious attackers to steal, manipulate and perform nefarious activities. Huge interest has developed over the past years in applying machine learning (ML)-assisted approaches in the IoT security space. However, the assumption in many current works is that big training data is widely available and transferable to the main server because data is born at the edge and is generated continuously by IoT devices. This is to say that classic ML works on the legacy set of entire data located on a central server, which makes it the least preferred option for domains with privacy concerns on user data. To address this issue, we propose federated learning (FL)-based anomaly detection approach to proactively recognize intrusion in IoT networks using decentralized on-device data. Our approach uses federated training rounds on Gated Recurrent Units (GRUs) models and keeps the data intact on local IoT devices by sharing only the learned weights with the central server of the FL. Also, the approach&#x2019;s ensembler part aggregates the updates from multiple sources to optimize the global ML model&#x2019;s accuracy. Our experimental results demonstrate that our approach outperforms the classic/centralized machine learning (non-FL) versions in securing the privacy of user data and provides an optimal accuracy rate in attack detection. IEEE",IEEE Internet of Things Journal,10.1109/JIOT.2021.3077803,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105874470&doi=10.1109%2fJIOT.2021.3077803&partnerID=40&md5=91fe0976cd0273e9fc75a699b4aacf10,2021,2021-07-20 15:50:00,2021-07-20 15:50:00
N9G2VTYN,journalArticle,2020,"Jia, Y.; Kwong, S.; Wang, R.",Applying Exponential Family Distribution to Generalized Extreme Learning Machine,"The learning algorithm of an extreme learning machine (ELM) has two fundamental steps: 1) random nonlinear feature transformation and 2) least squares learning. Since the probabilistic interpretation for a sample by the least squares method follows a Gaussian distribution, there are two limitations in ELM caused by the second step: 1) it may be inaccurate to handle binary classification problems, since the output of a binary dataset has a distribution far from Gaussian and 2) it may have difficulties in dealing with nontraditional data types (such as count data, ordinal data, etc.), which also do not follow Gaussian distribution. In order to solve the above-mentioned problems, this paper proposes a generalized ELM (GELM) framework by applying the exponential family distribution (EFD) to the output layer node of ELM. It simplifies the design of ELM models for task-specific output domains with different data types. We propose a unified learning paradigm for all the models under this GELM framework with different distributions in EFD, and prove that traditional ELM is a special instance of GELM by setting the output distribution as a Gaussian distribution (GELM-Gaussian). We also prove that the training of GELM-Gaussian can be finished in one iteration, in this case, GELM-Gaussian does not slow down the training speed of traditional ELM. Besides, we propose the kernel version of GELM, which can also be concretized to different models by applying different EFDs. Experimental comparisons demonstrate that GELM can give more accurate probabilistic interpretation to binary classification and GELM has a great potential in dealing with a broader range of machine learning tasks. © 2013 IEEE.","IEEE Transactions on Systems, Man, and Cybernetics: Systems",10.1109/TSMC.2017.2788005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040990433&doi=10.1109%2fTSMC.2017.2788005&partnerID=40&md5=e06373093a1745609242185617717f25,2020,2021-07-20 15:50:00,2021-07-20 15:50:00
VKKINIJI,journalArticle,2020,"Bahrami, M.; Bozkaya, B.; Balcisoy, S.",Using Behavioral Analytics to Predict Customer Invoice Payment,"Experiences from various industries show that companies may have problems collecting customer invoice payments. Studies report that almost half of the small- and medium-sized enterprise and business-to-business invoices in the United States and United Kingdom are paid late. In this study, our aim is to understand customer behavior regarding invoice payments, and propose an analytical approach to learning and predicting payment behavior. Our logic can then be embedded into a decision support system where decision makers can make predictions regarding future payments, and take actions as necessary toward the collection of potentially unpaid debt, or adjust their financial plans based on the expected invoice-to-cash amount. In our analysis, we utilize a large data set with more than 1.6 million customers and their invoice and payment history, as well as various actions (e.g., e-mail, short message service, phone call) performed by the invoice-issuing company toward customers to encourage payment. We use supervised and unsupervised learning techniques to help predict whether a customer will pay the invoice or outstanding balance by the next due date based on the actions generated by the company and the customer's response. We propose a novel behavioral scoring model used as an input variable to our predictive models. Among the three machine learning approaches tested, we report the results of logistic regression that provides up to 97% accuracy with or without preclustering of customers. Such a model has a high potential to help decision makers in generating actions that contribute to the financial stability of the company in terms of cash flow management and avoiding unnecessary corporate lines of credit. © Copyright 2020, Mary Ann Liebert, Inc., publishers 2020.",Big Data,10.1089/big.2018.0116,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079908614&doi=10.1089%2fbig.2018.0116&partnerID=40&md5=fdbde10b8e58dae8d208bf3c3bb49f01,2020,2021-07-20 15:50:00,2021-07-20 15:50:00
7MJ8L6MH,journalArticle,2020,"Burgholzer, L.; Wille, R.",Advanced Equivalence Checking for Quantum Circuits,"In the not-so-distant future, quantum computing will change the way we tackle certain problems. It promises to dramatically speed-up many chemical, financial, cryptographical, and machine-learning applications. However, in order to capitalize on those promises, complex design flows composed of steps such as compilation, decomposition, mapping, or transpilation need to be employed before being able to execute a conceptual quantum algorithm on an actual device. This results in many descriptions at various levels of abstraction which may significantly differ from each other. The complexity of the underlying design problems makes it ever more important to not only provide efficient solutions for the single steps, but also to verify that the originally intended functionality is preserved throughout all levels of abstraction. This motivates methods for equivalence checking of quantum circuits. However, most existing methods for this are inspired by equivalence checking in the classical realm and have merely been extended to support quantum circuits (i.e., circuits which do not only rely on 0&#x2019;s and 1&#x2019;s, but also employ superposition and entanglement). In this work, we propose an advanced methodology which takes the different paradigms of quantum circuits not only as a burden, but as an opportunity. In fact, the proposed methodology explicitly utilizes characteristics unique to quantum computing in order to overcome the shortcomings of existing approaches. We show that, by exploiting the reversibility of quantum circuits, complexity can be kept feasible in many cases. Moreover, we show that, in contrast to the classical realm, simulation is very powerful in verifying quantum circuits. Experimental evaluations confirm that the resulting methodology allows one to conduct equivalence checking dramatically faster than ever before&#x2014;in many cases just a single simulation run is sufficient. An implementation of the proposed equivalence checking flow is publicly available at. IEEE",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.3032630,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096139131&doi=10.1109%2fTCAD.2020.3032630&partnerID=40&md5=5e0e3cb2db93a1b1e7bbe1d9ad1cae9e,2020,2021-07-20 15:50:00,2021-07-20 15:50:00
77Z9THN8,journalArticle,2020,"Zhijun, W.; Qing, X.; Jingjie, W.; Meng, Y.; Liang, L.",Low-Rate DDoS Attack Detection Based on Factorization Machine in Software Defined Network,"As the Software Define Network (SDN) adopts centralized control logic, it is vulnerable to various types of Distributed Denial of Service (DDoS) attacks. At present, almost all the research work focuses on high-rate DDoS attack against the SDN control layer. Moreover, most of the existing detection methods are effective for high-rate DDoS attack detection of the control layer, while a low-rate DDoS attack against the SDN data layer is highly concealed, and the detection accuracy against this kind of attack is low. In order to improve the detection accuracy of the low-rate DDoS attack against the SDN data layer, this paper studies the mechanism of such attacks, and then proposes a multi-feature DDoS attack detection method based on Factorization Machine (FM). The features extracted from the flow rules are used to detect low-rate DDoS attacks, and the detection of low-rate DDoS attacks based on FM machine learning algorithms is implemented. The experimental results show that the method can effectively detect the low-rate DDoS attack against the SDN data layer, and the detection accuracy reaches 95.80 percent. Because FM algorithm can achieve fine-grained detection for low-rate DDoS attack, which provides a reliable condition for defending against such attacks. Finally, this paper proposes a defense method based on dynamic deletion of flow rules, and carries out experimental simulation and analysis to prove the effectiveness of the defense method, and the success rate of forwarding normal packets reached 97.85 percent. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2967478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079788532&doi=10.1109%2fACCESS.2020.2967478&partnerID=40&md5=b88cffb6f482302cb5bd22e892f3190c,2020,2021-07-20 15:50:00,2021-07-20 15:50:00
AQJ2TTNF,journalArticle,2019,"Ahn, K.U.; Park, S.H.; Hwang, S.; Choi, S.; Park, C.S.",Optimal control strategies of eight parallel heat pumps using Gaussian process emulator,"This study describes the development of the optimal control strategies of eight parallel heat pumps in an existing building. The building consists of seven floors above ground and two floors underground with a total floor area of 22,440 m2. The chilled water generated by each of the eight parallel heat pumps runs through a common primary pipe to multiple air-handling units in the building. Because only one flowmeter and two thermometers (entering and exiting) are installed in the primary pipe, the heat removal rate and efficiency of each heat pump are unknown. The existing control of the heat pumps is as follows: if the chilled water return temperature in the primary pipe becomes greater than a predetermined temperature, the controller increases the number of operating heat pumps. The heat removal rate and efficiency of each heat pump were first identified using a Gaussian process (GP) machine-learning algorithm to develop the optimal control strategy of the eight heat pumps. Two GP models, one for estimating the heat removal rate and the other for estimating the coefficient of performance (COP), were developed based on the measured data for 27 days in July at the sampling time of 15 min. After developing the GP models, the authors applied a COP-based sequencing control strategy to the eight parallel heat pumps. The new optimal control strategy is to switch on the heat pumps in order from highest to lowest COP. Compared with the existing control logic, the new optimal control can reduce energy consumption by 20.9%. © 2019, © 2019 International Building Performance Simulation Association (IBPSA).",Journal of Building Performance Simulation,10.1080/19401493.2019.1597924,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063470462&doi=10.1080%2f19401493.2019.1597924&partnerID=40&md5=8fa749d582816bdcfcf9a25496121da0,2019,2021-07-20 15:50:00,2021-07-20 15:50:00
36ZJ4Z45,journalArticle,2019,"Vimala, S.; Khanaa, V.; Nalini, C.",A study on supervised machine learning algorithm to improvise intrusion detection systems for mobile ad hoc networks,"The security inside the network correspondence is a noteworthy concern. Being the way that information is considered as the profitable asset of an association, giving security against the intruders is exceptionally fundamental. Intrusion Detection Systems tries to recognize security assaults of intruders by researching a few information records saw in forms on the network. In this paper, Intrusion Detection Classification of attacks is done by NNIDS, TSVID and DF-IDS. The proposed algorithms are trained and tested using KDD Cup 1999 dataset. This paper has presented a novel method for an adaptive fault tolerant mobile agent based intrusion detection system. At first the classification of attacks is done by TSVID Classification algorithm. The TSVID makes use of RBF kernel and iterative learning mechanism. Next the classification is done by using NNIDS that makes use of neural network based approach. Advantages of NNIDS method is that it can successfully handle both qualitative, quantitative data, and it handles multiple criteria and easier to understand. Then, finally, the classification is done by using iterative learning mechanism and DF-IDS gives successful results of classification. The performances of the proposed algorithm are evaluated using the classification metrics such as detection rate and accuracy. Comparison graphs of attack detection rate and false-alarm rate reveals that the obtained results of anticipated methods achieve greater detection rate and less computational time for the classification of attacks and protocols. The proposed study is a classification based approach for combining several networks in intrusion detection systems. In evaluation of this model, it has been demonstrated that there is a significant improvement in real time performance without sacrificing efficiency. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Cluster Computing,10.1007/s10586-018-2686-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046006731&doi=10.1007%2fs10586-018-2686-x&partnerID=40&md5=3aed67cc6260cc8774cefa73dfd24e98,2019,2021-07-20 15:50:01,2021-07-20 15:50:01
ZH4FMZ2Y,journalArticle,2017,"Pratama, M.; Lu, J.; Lughofer, E.; Zhang, G.; Er, M.J.",An Incremental Learning of Concept Drifts Using Evolving Type-2 Recurrent Fuzzy Neural Networks,"The age of online data stream and dynamic environments results in the increasing demand of advanced machine learning techniques to deal with concept drifts in large data streams. Evolving fuzzy systems (EFS) are one of recent initiatives from the fuzzy system community to resolve the issue. Existing EFSs are not robust against data uncertainty, temporal system dynamics, and the absence of system order, because a vast majority of EFSs are designed in the type-1 feedforward network architecture. This paper aims to solve the issue of data uncertainty, temporal behavior, and the absence of system order by developing a novel evolving recurrent fuzzy neural network, called evolving type-2 recurrent fuzzy neural network (eT2RFNN). eT2RFNN is constructed in a new recurrent network architecture, featuring double recurrent layers. The new recurrent network architecture evolves a generalized interval type-2 fuzzy rule, where the rule premise is built upon the interval type-2 multivariate Gaussian function, whereas the rule consequent is crafted by the nonlinear wavelet function. The eT2RFNN adopts a holistic concept of evolving systems, where the fuzzy rule can be automatically generated, pruned, merged, and recalled in the single-pass learning mode. eT2RFNN is capable of coping with the problem of high dimensionality because it is equipped with online feature selection technology. The efficacy of eT2RFNN was experimentally validated using artificial and real-world data streams and compared with prominent learning algorithms. eT2RFNN produced more reliable predictive accuracy, while retaining lower complexity than its counterparts. © 2017 IEEE.",IEEE Transactions on Fuzzy Systems,10.1109/TFUZZ.2016.2599855,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032441894&doi=10.1109%2fTFUZZ.2016.2599855&partnerID=40&md5=c2134edab87d683e9ddd75e9b37bab3e,2017,2021-07-20 15:50:01,2021-07-20 15:50:01
IDUIU65B,journalArticle,2017,"Janghorbani, A.; Moradi, M.H.",Fuzzy Evidential Network and Its Application as Medical Prognosis and Diagnosis Models,"Uncertainty is one of the important facts of the medical knowledge. Medical prognosis and diagnosis, as the essential parts of medical knowledge, is affected by different aspects of uncertainty, which must be managed. In the previous studies, different theories such as Bayesian probability theory, evidence theory, and fuzzy set theory have been developed to represent and manage different aspects of uncertainty. Recently, hybrid frameworks are suggested to deal with various types of uncertainty in a single framework. Evidential networks are general frameworks for dealing explicitly with total and partial ignorance and offer powerful combination rule of contradictory evidence. In this framework, the fuzziness of linguistic variables is neglected while these variables commonly appear in the medical domain knowledge and different sources of medical information. In addition, the evidential network parameters are determined based on the experts’ knowledge and no data-driven algorithm is provided to learn these parameters. In this study, a novel hybrid framework called fuzzy evidential network was suggested to manage the imprecision and epistemic uncertainty of medical prognosis and diagnosis. Also, a data-driven algorithm based on the fuzzy set theory and the fuzzy maximum likelihood is provided to learn the network parameters from clinical databases. The performance of the proposed framework as various prognosis and diagnosis models, compared with well-known machine learning algorithms and the results showed its superiority. Also, an evidential method is suggested to handle the missing values and its results were compared with KNN imputation method. © 2017 Elsevier Inc.",Journal of Biomedical Informatics,10.1016/j.jbi.2017.07.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85023640248&doi=10.1016%2fj.jbi.2017.07.004&partnerID=40&md5=e0f08b2ecaba4994b09ba8b737e1e55e,2017,2021-07-20 15:50:01,2021-07-20 15:50:01
VMI82L49,journalArticle,2015,"DiTomaso, D.; Kodi, A.K.; Louri, A.; Bunescu, R.",Resilient and Power-Efficient Multi-Function Channel Buffers in Network-on-Chip Architectures,"Network-on-Chips (NoCs) are quickly becoming the standard communication paradigm for the growing number of cores on the chip. While NoCs can deliver sufficient bandwidth and enhance scalability, NoCs suffer from high power consumption due to the router microarchitecture and communication channels that facilitate inter-core communication. As technology keeps scaling down in the nanometer regime, unpredictable device behavior due to aging, infant mortality, design defects, soft errors, aggressive design, and process-voltage-temperature variations, will increase and will result in a significant increase in faults (both permanent and transient) and hardware failures. In this paper, we propose QORE - a fault tolerant NoC architecture with Multi-Function Channel (MFC) buffers. The use of MFC buffers and their associated control (link and fault controllers) enhance fault-tolerance by allowing the NoC to dynamically adapt to faults at the link level and reverse propagation direction to avoid faulty links. Additionally, MFC buffers reduce router power and improve performance by eliminating in-router buffering. We utilize a machine learning technique in our link controllers to predict the direction of traffic flow in order to more efficiently reverse links. Our simulation results using real benchmarks and synthetic traffic mixes show that QORE improves speedup by 1.3x and throughput by 2.3x when compared to state-of-the art fault tolerant NoCs designs such as Ariadne and Vicis. Moreover, using Synopsys Design Compiler, we also show that network power in QORE is reduced by 21 percent with minimal control overhead. © 1968-2012 IEEE.",IEEE Transactions on Computers,10.1109/TC.2015.2401013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84946943970&doi=10.1109%2fTC.2015.2401013&partnerID=40&md5=e240d873e71dc85e4d2445e5efaa791c,2015,2021-07-20 15:50:01,2021-07-20 15:50:01
AQQF8AXY,journalArticle,2021,"Ghorpade, S.N.; Zennaro, M.; Chaudhari, B.S.",IoT-based hybrid optimized fuzzy threshold ELM model for localization of elderly persons,"Due to the quickly aging population, the number of elderly persons is rapidly increasing, posing significant challenges for monitoring and assisting them in indoor and outdoor settings. Although some techniques are available for the indoor localization of elderly persons, in the coming years, outdoor localization will be an essential part of society. Different approaches such as GPS, range-based, and range-free have been developed for outdoor localization. However, the localization accuracy and precision is still a significant challenge. For accurate and low-cost localization, we propose a novel IoT-based range-based localization for smart city applications. Using the extreme learning machine (ELM), fuzzy system, and modified swarm intelligence, a hybrid optimized fuzzy threshold ELM (HOF℡M) algorithm is developed. The particle swarm grey wolf optimization is used to identify the direction of the moving sensor node. A fuzzy weighted centroid is used to optimize the consequences of irregular movement of the nodes. Lastly, an optimized threshold extreme learning machine and weighted mean are applied to localize the moving nodes accurately. Our algorithm outperforms the existing algorithms in terms of average location error ratio (ALER), the number of localized nodes, and the computational time. The results show that ALER reduces by at least 48.07% in comparison with the other algorithms. The proposed algorithm also localizes at least 7.25% additional nodes and has a computationally efficient operation. © 2021 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2021.115500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85109083490&doi=10.1016%2fj.eswa.2021.115500&partnerID=40&md5=c666b119a437e6b908a3cdb9a64f5b76,2021,2021-07-20 15:50:01,2021-07-20 15:50:01
5GYB232I,journalArticle,2020,"Das, P.; Kapoor, H.K.",nZESPA: A Near-3D-Memory Zero Skipping Parallel Accelerator for CNNs,"CNNs are one of the most popular machine learning tools for computer vision. The ubiquitous use in several applications with its high computation-cost has made it lucrative for optimization through accelerated architecture. State-of-the-art has either exploited the parallelism of CNNs, or eliminated computations through sparsity or used near-memory processing (NMP) to accelerate the CNNs. We introduce NMP-fully-sparse architecture, which acquires all three capabilities. The proposed architecture is parallel and hence processes the independent CNN tasks concurrently. To exploit the sparsity, the proposed system employs a dataflow, namely Near-3D-Memory Zero Skipping Parallel dataflow or nZESPA dataflow. This dataflow maintains the compressed-sparse encoding of data that skips all ineffectual zero-valued computations of CNNs. We design a custom accelerator which employs the nZESPA dataflow. The grids of nZESPA modules are integrated into the logic layer of the Hybrid Memory Cube. This integration saves a significant amount of off-chip communications while implementing the concept of NMP. We compare the proposed architecture with three other architectures which either do not exploit sparsity (NMP-dense) or do not employ NMP (traditional-fully-sparse) or do not include both (traditional-dense). The proposed system outperforms the baselines in terms of performance and energy consumption while executing CNN inference. IEEE",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.3022330,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090976029&doi=10.1109%2fTCAD.2020.3022330&partnerID=40&md5=5758ccc9ab8676baf8d3c0f8be36f31f,2020,2021-07-20 15:50:01,2021-07-20 15:50:01
JLJJXV7W,journalArticle,2020,"Wang, P.; Zheng, X.; Ku, J.; Wang, C.",Multiple-instance learning approach via bayesian extreme learning machine,"Multiple-instance learning (MIL) can solve supervised learning tasks, where only a bag of multiple instances is labeled, instead of a single instance. It is considerably important to develop effective and efficient MIL algorithms, because real-world datasets usually contain large instances. Known for its good generalization performance, MIL based on extreme learning machines (ELM-MIL) has proven to be more efficient than several typical MIL classification methods. ELM-MIL selects the most qualified instances from each bag through a single hidden layer feedforward network (SLFN) and trains modified ELM models to update the output weights. This learning approach often performs susceptible to the number of hidden nodes and can easily suffer from over-fitting problem. Using Bayesian inferences, this study introduces a Bayesian ELM (BELM)-based MIL algorithm (BELM-MIL) to address MIL classification problems. First, weight self-learning method based on a Bayesian network is applied to determine the weights of instance features. The most qualified instances are then selected from each bag to represent the bag. Second, BELM can improve the classification model via regularization of automatic estimations to reduce possible over-fitting during the calibration process. Experiments and comparisons are conducted with several competing algorithms on Musk datasets, images datasets, and inductive logic programming datasets. Superior classification accuracy and performance are demonstrated by BELM-MIL. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2984271,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083457069&doi=10.1109%2fACCESS.2020.2984271&partnerID=40&md5=416278aa0da541a4ae6222149eb26b2e,2020,2021-07-20 15:50:01,2021-07-20 15:50:01
XIREBJX5,journalArticle,2018,"Wong, G.Y.; Leung, F.H.F.; Ling, S.-H.",A hybrid evolutionary preprocessing method for imbalanced datasets,"Imbalanced datasets are commonly encountered in real-world classification problems. Many machine learning algorithms are originally designed for well-balanced datasets, therefore re-sampling has become an important step to pre-process imbalanced data. This aims to balance the datasets by increasing the samples of the smaller class or decreasing the samples of the larger class, which are known as over-sampling and under-sampling, respectively. In this paper, a sampling strategy that is based on both over-sampling and under-sampling is proposed, in which the new samples of the smaller class are created based on fuzzy logic. Improvement of the datasets is done by the evolutionary computational method of Cross-generational elitist selection, Heterogeneous recombination and Cataclysmic mutation (CHC) that under-samples both the minority and majority samples. Consequently, a hybrid preprocessing method is proposed to re-sample imbalanced datasets. The evaluation is done by applying the Support Vector Machine (SVM), C4.5 decision tree and nearest neighbor rule to train a classification model from the re-sampled training sets. From the experimental results, it can be seen that our proposed method improves both the F−measure and AUC. The over-sampling rate and complexity of the classification model are also compared. Our proposed method is found to be superior to all other methods under comparison and it is more robust in different classifiers. © 2018",Information Sciences,10.1016/j.ins.2018.04.068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046463531&doi=10.1016%2fj.ins.2018.04.068&partnerID=40&md5=b59235d086c9434a371faed9ff66f541,2018,2021-07-20 15:50:01,2021-07-20 15:50:01
N5MPB5XC,journalArticle,2016,"Yi, Y.; Liao, Y.; Wang, B.; Fu, X.; Shen, F.; Hou, H.; Liu, L.",FPGA based spike-time dependent encoder and reservoir design in neuromorphic computing processors,"In this paper, we propose a Field Programmable Gate Array (FPGA) platform for spike time dependent encoder and dynamic reservoir in neuromorphic computing processors. Neuromorphic computing processors represent a type of non-traditional architecture encompassing evolutionary systems and hold great promise for many important engineering and scientific applications. The reservoir computing approach with dynamic reservoir is a paradigm in machine learning whose processing capabilities rely on the dynamical behavior of Recurrent Neural Networks (RNNs). It has made swift progress and development in both the theoretical realm and its subsequent implementations. However, most of the implementations are based on software, due to the difficulties in performing real-time training of the output weights on hardware platforms. The reservoir computing approach implemented in this paper utilizes the Echo State Network (ESN) architecture which includes a reservoir and its consequent training process. It can be trained and implemented in FPGA without any software cooperation. As FPGA is a digital logic platform and the information entered into a RNN is analog, we also propose an encoding circuit and an Analog to Digital Converter (ADC) to bridge this divide. The proposed method given for the realization of the neuromorphic computing chips, therefore, provides a viable option. © 2016",Microprocessors and Microsystems,10.1016/j.micpro.2016.03.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962531699&doi=10.1016%2fj.micpro.2016.03.009&partnerID=40&md5=fa2916073a8ab550bbb867692cd1de45,2016,2021-07-20 15:50:01,2021-07-20 15:50:01
DI3TCXVJ,journalArticle,2012,"Lo, J.T.H.","A cortex-like learning machine for temporal hierarchical pattern clustering, detection, and recognition","A learning machine, called a clustering interpreting probabilistic associative memory (CIPAM), is proposed. CIPAM consists of a clusterer and an interpreter. The clusterer is a recurrent hierarchical neural network of unsupervised processing units (UPUs). The interpreter is a number of supervised processing units (SPUs) that branch out from the clusterer. Each processing unit (PU), UPU or SPU, comprises ""dendritic encoders"" for encoding inputs to the PU, ""synapses"" for storing resultant codes, a ""nonspiking neuron"" for generating inhibitory graded signals to modulate neighboring spiking neurons, ""spiking neurons"" for computing the subjective probability distribution (SPD) or the membership function, in the sense of fuzzy logic, of the label of said inputs to the PU and generating spike trains with the SPD or membership function as the firing rates, and a masking matrix for maximizing generalization. While UPUs employ unsupervised covariance learning mechanisms, SPUs employ supervised ones. They both also have unsupervised accumulation learning mechanisms. The clusterer of CIPAM clusters temporal and spatial data. The interpreter interprets the resultant clusters, effecting detection and recognition of temporal and hierarchical causes. © 2011 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2011.04.046,https://www.scopus.com/inward/record.uri?eid=2-s2.0-82655162088&doi=10.1016%2fj.neucom.2011.04.046&partnerID=40&md5=8de1c58cc682ad5cf4fa1952da4e0c82,2012,2021-07-20 15:50:01,2021-07-20 15:50:01
X4SDTAN3,journalArticle,2021,"Tal, O.; Liu, Y.; Huang, J.; Yu, X.; Aljbawi, B.",Neural Attention Frameworks for Explainable Recommendation,"Neural attention, an emerging technique used to identify important inputs within neural networks, have become increasingly popular in the area of recommender systems. Not only allowing to better identify what defines users and items, attention-based recommender systems are further able to provide accompanying explanations. However, these representations usually capture only part of users' preferences and items' attributes, resulting in limited reasoning and accuracy. We therefore propose Dual Attention Recommender with Items and Attributes (DARIA), a novel approach able to combine two dependable neural attention mechanisms to better justify its suggestions. Utilizing the personalized history of users, DARIA identifies the most relevant past activities while considering the real-world features that contributed to the similarity. In addition, we adopt the novel approach of self-attention and introduce Self-Attention Recommender based on Attributes and History (SARAH). As a variation to DARIA, SARAH utilizes two self-attention components to describe users by their most characteristic past activities and items by their best depicting attributes. Various experiments establish the significant improvement of SARAH and DARIA over seven key baselines in diverse recommendation settings. By comparing our two proposed frameworks, we demonstrate the potential benefit of applying self-attention in different scenarios. © 1989-2012 IEEE.",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2019.2953157,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104016069&doi=10.1109%2fTKDE.2019.2953157&partnerID=40&md5=d8d41cd4ed0b67f1e42335cdc4fa70cb,2021,2021-07-20 15:50:01,2021-07-20 15:50:01
33PCBE9R,journalArticle,2021,"Gan, Z.; Zeng, B.; Cheng, L.; Liu, S.; Yang, H.; Xu, M.; Ding, M.",RoRePo: Detecting the role information and relative position information for contexts in multi-turn dialogue generation,"In multi-turn dialogue generation, dialogue contexts have been shown to have an important influence on the reasoning of the next round of dialogue. A multi-turn dialogue between two people should be able to give a reasonable response according to the relevant context. However, the widely used hierarchical recurrent encoder-decoder model and the latest model that detecting the relevant contexts with self-attention are facing the same problem. Their given response doesn't match the identity of the current speaker, which we call it role ambiguity. In this paper, we propose a new model, named RoRePo, to tackle this problem by detecting the role information and relative position information. Firstly, as a part of the decoder input, we add a role embedding to identity different speakers. Secondly, we incorporate self-attention mechanism with relative position representation to dialogue context understanding. Besides, the design of our model architecture considers the influence of latent variables in generating more diverse responses. Experimental results of our evaluations on the DailyDialog and DSTC7_AVSD datasets show that our proposed model advances in multi-turn dialogue generation. © 2021 - IOS Press. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-202641,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104954784&doi=10.3233%2fJIFS-202641&partnerID=40&md5=65b21ad445dde58d2fa5781dc4c7fe96,2021,2021-07-20 15:50:01,2021-07-20 15:50:01
QEPNNDJP,journalArticle,2020,"Lambeta, M.; Chou, P.-W.; Tian, S.; Yang, B.; Maloon, B.; Most, V.R.; Stroud, D.; Santos, R.; Byagowi, A.; Kammerer, G.; Jayaraman, D.; Calandra, R.",DIGIT: A Novel Design for a Low-Cost Compact High-Resolution Tactile Sensor with Application to In-Hand Manipulation,"Despite decades of research, general purpose in-hand manipulation remains one of the unsolved challenges of robotics. One of the contributing factors that limit current robotic manipulation systems is the difficulty of precisely sensing contact forces - sensing and reasoning about contact forces are crucial to accurately control interactions with the environment. As a step towards enabling better robotic manipulation, we introduce DIGIT, an inexpensive, compact, and high-resolution tactile sensor geared towards in-hand manipulation. DIGIT improves upon past vision-based tactile sensors by miniaturizing the form factor to be mountable on multi-fingered hands, and by providing several design improvements that result in an easier, more repeatable manufacturing process, and enhanced reliability. We demonstrate the capabilities of the DIGIT sensor by training deep neural network model-based controllers to manipulate glass marbles in-hand with a multi-finger robotic hand. To provide the robotic community access to reliable and low-cost tactile sensors, we open-source the DIGIT design at www.digit.ml. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2020.2977257,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083775425&doi=10.1109%2fLRA.2020.2977257&partnerID=40&md5=c017a55a1c560a85318c0c45ab217ce8,2020,2021-07-20 15:50:01,2021-07-20 15:50:01
VQNFQPS3,journalArticle,2020,"Kim, S.; Hwang, G.; Bae, H.-M.",Bat-G2 net: Bat-inspired graphical visualization network guided by radiated ultrasonic call,"In this paper, a noise-immune Bat-inspired Graphical visualization network Guided by the radiated ultrasonic call (Bat-G2 net) that can reconstruct 3D shapes of a target from ultrasonic echoes is presented. The Bat-G2 net achieves noise-resiliency by emulating bat's auditory system that processes echoes along with the highly correlated radiated ultrasonic call (RUC). In order to extract the information contained in the echoes robustly and effectively, two implementation ideas have been applied to the Bat-G2 net: (1) RUC-guided attention, and (2) non-local attention. The Bat-G2 net is trained with ECHO-4CH dataset acquired by a custom-made Bat-I sensor. Noise-resistant property of the Bat-G2 net is demonstrated by comparing the reconstructed images with those from current state-of-the-art ultrasonic image reconstruction network under low SNR conditions. This study clearly demonstrates the implementation feasibility of the new modality of 'seeing by hearing' in practical environments. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",IEEE Access,10.1109/ACCESS.2020.3031297,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102898717&doi=10.1109%2fACCESS.2020.3031297&partnerID=40&md5=4e6289e09efab2a708ba4f5f885af5e1,2020,2021-07-20 15:50:01,2021-07-20 15:50:01
ASL7CQFB,journalArticle,2020,"Pan, L.; Chen, J.; Liu, S.; Ngo, C.; Kan, M.; Chua, T.",A Hybrid Approach for Detecting Prerequisite Relations in Multi-modal Food Recipes,"Modeling the structure of culinary recipes is the core of recipe representation learning. Current approaches mostly focus on extracting the workflow graph from recipes based on text descriptions. Process images, which constitute an important part of cooking recipes, has rarely been investigated in recipe structure modeling. We study this recipe structure problem from a multi-modal learning perspective, by proposing a prerequisite tree to represent recipes with cooking images at a step-level gran-ularity. We propose a simple-yet-effective two-stage framework to automatically construct the prerequisite tree for a recipe by (1) utilizing a trained classifier to detect pairwise prerequisite relations that fuses multi-modal features as input; then (2) applying different strategies (greedy method, maximum weight, and beam search) to build the tree structure. Experiments on the MM-ReS dataset demonstrates the advantages of introducing process images for recipe structure modeling. Also, compared with neural methods which require large numbers of training data, we show that our two-stage pipeline can achieve promising results using only 400 labeled prerequisite trees as training data. IEEE",IEEE Transactions on Multimedia,10.1109/TMM.2020.3042706,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097927563&doi=10.1109%2fTMM.2020.3042706&partnerID=40&md5=87c0ca91d1f46f05d040c226a326cbda,2020,2021-07-20 15:50:01,2021-07-20 15:50:01
ZHFRBUDM,journalArticle,2020,"Martel, D.R.; Lysy, M.; Laing, A.C.",Predicting population level hip fracture risk: a novel hierarchical model incorporating probabilistic approaches and factor of risk principles,"Fall-related hip fractures are a major public health issue. While individual-level risk assessment tools exist, population-level predictive models could catalyze innovation in large-scale interventions. This study presents a hierarchical probabilistic model that predicts population-level hip fracture risk based on Factor of Risk (FOR) principles. Model validation demonstrated that FOR output aligned with a published dataset categorized by sex and hip fracture status. The model predicted normalized FOR for 100000 individuals simulating the Canadian older-adult population. Predicted hip fracture risk was higher for females (by an average of 38%), and increased with age (by15% per decade). Potential applications are discussed. © 2020, © 2020 Informa UK Limited, trading as Taylor & Francis Group.",Computer Methods in Biomechanics and Biomedical Engineering,10.1080/10255842.2020.1793331,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088369633&doi=10.1080%2f10255842.2020.1793331&partnerID=40&md5=411a234e3cea35d2ccc48f5f8b06709d,2020,2021-07-20 15:50:01,2021-07-20 15:50:01
FF927AM8,journalArticle,2018,"Huang, J.; Hmelo-Silver, C.E.; Jordan, R.; Gray, S.; Frensley, T.; Newman, G.; Stern, M.J.",Scientific discourse of citizen scientists: Models as a boundary object for collaborative problem solving,"In this study, we examine the nature of scientific discourse among participants enrolled in two citizen science projects as they engage in collaborative modeling and problem solving. Specifically, we explore the nature of their conversation as they use, the Mental Modeler, an online collaborative modeling tool to facilitate science engagement, systems-thinking and reasoning. This paper applies an analytical approach that uses visual representations and the discourse around building these representations to understand the shifts in scientific discourse and interpret complex interaction patterns between participants and facilitators in the computer-based learning environment. Findings suggest that the Mental Modeler serves as a boundary object that empowers participants and facilitators to collaboratively engage with scientific topics and practices through the development of scientific discourse and deeper learning in problem solving contexts. Findings shows patterns for group modeling in terms of shifting from bottom-up level to top-down level of discussions. In addition, the results show greater group collaboration and interaction when participants engage in the discussion by using the Mental Modeler to plan and model the citizen science projects. © 2018 Elsevier Ltd",Computers in Human Behavior,10.1016/j.chb.2018.04.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045307904&doi=10.1016%2fj.chb.2018.04.004&partnerID=40&md5=3ff1c98e31f7c265d03e56005526e7e8,2018,2021-07-20 15:50:01,2021-07-20 15:50:01
SJK8UYNS,journalArticle,2021,"Khan, N.; Saleem, M.R.; Lee, D.; Park, M.-W.; Park, C.",Utilizing safety rule correlation for mobile scaffolds monitoring leveraging deep convolution neural networks,"Falls from height (FFH) are still a leading cause of fatalities in the construction industry, which also includes scaffolding-related accidents. Despite regular safety inspections, numerous scaffolding-related accidents occur at the construction site. The current safety monitoring practices are not only impractical but infeasible due to dynamicity of construction environment. Since a separate computer training and detection process is generally required to acquire spatiotemporal reasoning to control a single hazard; thus previous efforts in vision intelligence applications to improve safety monitoring are still limited to specific hazards. Also, in regard to detecting unsafe situations based on extracted correlations from safety rules, to date, previous studies have devoted little attention to this domain. To address these issues, this study proposes a correlation-based approach for mobile scaffold safety monitoring and detecting worker's unsafe behaviors. A deep neural network, Mask R-CNN, was used as classification and segmentation of worker's tasks combined with object correlation detection (OCD) module to identify worker's unsafe behaviors. The approach divides the overall construction worker's safety into two subsets, classification of worker and detection of safe (class-1) and unsafe (class-2) behavior using OCD block. The overall performance was evaluated on set of real scenarios with test results showing 85 % and 97 % precision and recall for class-1 (safe behavior) and 91 % and 65 % precision and recall for class-2 (unsafe behavior). The overall accuracy of 86 % confirms the Mask R-CNN-based OCD module's applicability for detecting worker's unsafe behavior effectively in a construction environment. © 2021 Elsevier B.V.",Computers in Industry,10.1016/j.compind.2021.103448,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103595848&doi=10.1016%2fj.compind.2021.103448&partnerID=40&md5=2d1b3c41db597855f705847a29ab44e6,2021,2021-07-20 15:50:02,2021-07-20 15:50:02
LHF37I6T,journalArticle,2019,"Kyza, E.A.; Georgiou, Y.","Scaffolding augmented reality inquiry learning: the design and investigation of the TraceReaders location-based, augmented reality platform","While learning can happen anywhere and everywhere, most educational practices in K-12 are confined within the walls of a classroom and the school; such practices narrowly define learning and exclude the opportunities that an expanded and digitally mediated definition of learning can offer. Augmented reality (AR) technologies offer exciting new opportunities for supporting ubiquitous learning, by superimposing layers of digital information on the real world. The digital augmentation can provide enriched learning experiences, through situating the learning content in authentic contexts and fostering inquiry-based learning. Nonetheless, learning can often be sidestepped as the use of AR technologies becomes a mere fun activity, akin to a treasure hunt. Such challenges indicate the need to provide scaffolded AR environments to support deep learning. These ideas are reflected in the design of the TraceReaders, a platform for enabling location-based mobile learning using augmented reality (AR) technologies. TraceReaders supports the authoring of inquiry-based AR apps, to engage students in evidence-driven reflective inquiry in situ. This paper first describes the theoretical commitments which guided the development of the TraceReaders platform, followed by a description of its design rationale. Two case studies of informal inquiry learning using TraceReaders are then presented: the first one reports on the use of the “Young Archaeologists” TraceReaders app to support primary school students’ historical reasoning, while the second one reports on the “Mystery at the Lake” app to support high school students’ environmental science inquiry. These cases offer the opportunity to discuss the affordances and challenges in using such a scaffolded tool to support location-based AR learning in situ. The discussion concludes with lessons learned from empirical studies about the design and effectiveness of tools like the TraceReaders platform and future steps. © 2018, © 2018 Informa UK Limited, trading as Taylor & Francis Group.",Interactive Learning Environments,10.1080/10494820.2018.1458039,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045275484&doi=10.1080%2f10494820.2018.1458039&partnerID=40&md5=fe6f599d9bb2ca3d2173229087c9d0d9,2019,2021-07-20 15:50:02,2021-07-20 15:50:02
4XRBGPGW,journalArticle,2021,"Tissot, H.C.; Pedebos, L.A.",Improving Risk Assessment of Miscarriage During Pregnancy with Knowledge Graph Embeddings,"Miscarriages are the most common type of pregnancy loss, mostly occurring in the first 12 weeks of pregnancy. Pregnancy risk assessment aims to quantify evidence to reduce such maternal morbidities, and personalized decision support systems are the cornerstone of high-quality, patient-centered care to improve diagnosis, treatment selection, and risk assessment. However, data sparsity and the increasing number of patient-level observations require more effective forms of representing clinical knowledge to encode known information that enables performing inference and reasoning. Whereas knowledge embedding representation has been widely explored in the open domain data, there are few efforts for its application in the clinical domain. In this study, we contrast differences among multiple embedding strategies, and we demonstrate how these methods can assist in performing risk assessment of miscarriage before and during pregnancy. Our experiments show that simple knowledge embedding approaches that utilize domain-specific metadata perform better than complex embedding strategies, although both can improve results comparatively to a population probabilistic baseline in both AUPRC, F1-score, and a proposed normalized version of these evaluation metrics that better reflects accuracy for unbalanced datasets. Finally, embedding approaches provide evidence about each individual, supporting explainability for its model predictions in such a way that humans understand. © 2021, The Author(s), under exclusive licence to Springer Nature Switzerland AG.",Journal of Healthcare Informatics Research,10.1007/s41666-021-00096-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105396216&doi=10.1007%2fs41666-021-00096-6&partnerID=40&md5=d1a69582fa5922c32266109db6d19acb,2021,2021-07-20 15:50:02,2021-07-20 15:50:02
K9VULEKE,journalArticle,2016,"Orphanou, K.; Stassopoulou, A.; Keravnou, E.",DBN-extended: A dynamic Bayesian network model extended with temporal abstractions for coronary heart disease prognosis,"Dynamic Bayesian networks (DBNs) are temporal probabilistic graphical models that model temporal events and their causal and temporal dependencies. Temporal abstraction (TA) is a knowledge-based process that abstracts raw temporal data into higher level interval-based concepts. In this paper, we present an extended DBN model that integrates TA methods with DBNs applied for prognosis of the risk for coronary heart disease. More specifically, we demonstrate the derivation of TAs from data, which are used for building the network structure. We use machine learning algorithms to learn the parameters of the model through data. We apply the extended model to a longitudinal medical dataset and compare its performance to the performance of a DBN implemented without TAs. The results we obtain demonstrate the predictive accuracy of our model and the effectiveness of our proposed approach. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2015.2420534,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969921744&doi=10.1109%2fJBHI.2015.2420534&partnerID=40&md5=11d84615f34562efecbd0b16ebf8d63c,2016,2021-07-20 15:50:02,2021-07-20 15:50:02
DJ9DG2PT,journalArticle,2016,"Ishak, M.B.; Leray, P.; Amor, N.B.",Probabilistic relational model benchmark generation: Principle and application,"The validation of any database mining methodology goes through an evaluation process where benchmarks availability is essential. In this paper, we aim to randomly generate relational database benchmarks that allow to check probabilistic dependencies among the attributes.We are particularly interested in Probabilistic relational models (PRMs). These latter extend Bayesian networks (BNs) to a relational data mining context that enable effective and robust reasoning about relational data structures. Even though a panoply of works have focused, separately, on Bayesian networks and relational databases random generation, no work has been identified for PRMs on that track. This paper provides an algorithmic approach allowing to generate random PRMs from scratch to cover the absence of generation process. The proposed method allows to generate PRMs as well as synthetic relational data from a randomly generated relational schema and a random set of probabilistic dependencies. This can be of interest for machine learning researchers to evaluate their proposals in a common framework, as for databases designers to evaluate the effectiveness of the components of a database management system. © 2016 - IOS Press and the authors. All rights reserved.",Intelligent Data Analysis,10.3233/IDA-160823,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969793367&doi=10.3233%2fIDA-160823&partnerID=40&md5=c71f0d95028358b652567a22fcfba1d4,2016,2021-07-20 15:50:02,2021-07-20 15:50:02
5NMRCHXT,journalArticle,2011,"Gao, X.; Wang, X.; Li, X.; Tao, D.",Transfer latent variable model based on divergence analysis,"Latent variable models are powerful dimensionality reduction approaches in machine learning and pattern recognition. However, this kind of methods only works well under a necessary and strict assumption that the training samples and testing samples are independent and identically distributed. When the samples come from different domains, the distribution of the testing dataset will not be identical with the training dataset. Therefore, the performance of latent variable models will be degraded for the reason that the parameters of the training model do not suit for the testing dataset. This case limits the generalization and application of the traditional latent variable models. To handle this issue, a transfer learning framework for latent variable model is proposed which can utilize the distance (or divergence) of the two datasets to modify the parameters of the obtained latent variable model. So we do not need to rebuild the model and only adjust the parameters according to the divergence, which will adopt different datasets. Experimental results on several real datasets demonstrate the advantages of the proposed framework. © 2010 Elsevier Ltd. All rights reserved.",Pattern Recognition,10.1016/j.patcog.2010.06.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958858308&doi=10.1016%2fj.patcog.2010.06.013&partnerID=40&md5=2d163c1ed401101e163ea730749f9e42,2011,2021-07-20 15:50:02,2021-07-20 15:50:02
J433GRQH,journalArticle,2020,"Chang, L.; Fu, C.; Wu, Z.; Liu, W.; Yang, S.",Data-driven analysis of radiologists' behavior for diagnosing thyroid nodules,"Thyroid nodule has been a common and serious threaten to human health. With the identification and diagnosis of thyroid nodules in the general population, large volumes of examination reports in clinical practice have been accumulated. They provide data basics of analyzing radiologists' behavior of diagnosing thyroid nodules. To conduct data-driven analysis of radiologists' behavior, an experimental framework is designed based on belief rule base, which is essentially a white box for knowledge representation and uncertain reasoning. Under the framework, with 2744 examination reports of thyroid nodules in the period from January 2012 to February 2019 that have been collected from a tertiary hospital located in Hefei, Anhui, China, experimental results are obtained from conducting missing validation, self-validation, and mutual validation. Three principles are then concluded from the results and corresponding analysis. The first is that missing features on some criteria are considered as benign ones by default, the second is that there is generally inconsistency between the recorded features on criteria and the overall diagnosis, and the third is that different radiologists have different diagnostic preferences. These three principles reflect three diagnostic behavioral characteristics of radiologists, namely reliability, inconsistency, and independence. Based on the three principles and radiologists' behavioral characteristics, managerial insights in a general case are concluded to make the findings in this study available in other situations. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2020.2969322,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095799034&doi=10.1109%2fJBHI.2020.2969322&partnerID=40&md5=80d5cea47d715a0378138c226b9563ba,2020,2021-07-20 15:50:02,2021-07-20 15:50:02
X5MI5WMJ,journalArticle,2018,"Dario, G.-G.; Parés, F.; Vilalta, A.; Moreno, J.; Ayguade, E.; Labarta, J.; Cortés, U.; Suzumura, T.",On the behavior of convolutional nets for feature extraction,"Deep neural networks are representation learning techniques. During training, a deep net is capable of generating a descriptive language of unprecedented size and detail in machine learning. Extracting the descriptive language coded within a trained CNN model (in the case of image data), and reusing it for other purposes is a field of interest, as it provides access to the visual descriptors previously learnt by the CNN after processing millions of images, without requiring an expensive training phase. Contributions to this field (commonly known as feature representation transfer or transfer learning) have been purely empirical so far, extracting all CNN features from a single layer close to the output and testing their performance by feeding them to a classifier. This approach has provided consistent results, although its relevance is limited to classification tasks. In a completely different approach, in this paper we statistically measure the discriminative power of every single feature found within a deep CNN, when used for characterizing every class of 11 datasets. We seek to provide new insights into the behavior of CNN features, particularly the ones from convolutional layers, as this can be relevant for their application to knowledge representation and reasoning. Our results confirm that low and middle level features may behave differently to high level features, but only under certain conditions. We find that all CNN features can be used for knowledge representation purposes both by their presence or by their absence, doubling the information a single CNN feature may provide. We also study how much noise these features may include, and propose a thresholding approach to discard most of it. All these insights have a direct application to the generation of CNN embedding spaces. © 2018 AI Access Foundation. All rights reserved.",Journal of Artificial Intelligence Research,10.1613/jair.5756,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047837205&doi=10.1613%2fjair.5756&partnerID=40&md5=1f5658c804fe8ffef4bb7861e7d89e9b,2018,2021-07-20 15:50:02,2021-07-20 15:50:02
SWCSDNHQ,journalArticle,2017,"Freitas, F.A.; Peres, S.M.; Lima, C.A.M.; Barbosa, F.V.",Grammatical facial expression recognition in sign language discourse: a study at the syntax level,"Facial Expression Recognition is an already well-developed research area, mainly due to its applicability in the construction of different system types. Facial expressions are especially important in the area which relates to the construction of discourses through sign language. Sign languages are visual-spatial languages that are not assisted by voice intonation. Therefore, they use facial expressions to support the manifestation of prosody aspects and some grammatical constructions. Such expressions are called Grammatical Facial Expressions (GFEs) and they are present at sign language morphological and syntactic levels. GFEs stand out in automated recognition processes for sign languages, as they help removing ambiguity among signals, and they also contribute to compose the semantic meaning of discourse. This paper aims to present a study which applies inductive reasoning to recognize patterns, as a way to study the problem involving the automated recognition of GFEs at the discourse syntactic level in the Libras Sign Language (Brazilian Sign Language). In this study, sensor Microsoft Kinect was used to capture three-dimensional points in the faces of subjects who were fluent in sign language, generating a corpus of Libras phrases, which comprised different syntactic constructions. This corpus was analyzed through classifiers that were implemented through neural network Multilayer Perceptron, and then a series of experiments was conducted. The experiments allowed investigating: the recognition complexity that is inherent to each of the GFEs that are present in the corpus; the use suitability of different vector representations, considering descriptive characteristics that are based on coordinates of points in three dimensions, distances and angles therefrom; the need for using time data regarding the execution of expressions during speech; and particularities that are connected to data labeling and the evaluation of classifying models in the context of a sign language. © 2017, Springer Science+Business Media New York.",Information Systems Frontiers,10.1007/s10796-017-9765-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85019691334&doi=10.1007%2fs10796-017-9765-z&partnerID=40&md5=56657af8847f29056c39692912b60049,2017,2021-07-20 15:50:02,2021-07-20 15:50:02
NSMF6RCH,journalArticle,2017,"Galitsky, B.",Improving relevance in a content pipeline via syntactic generalization,"This is a report from the field on a linguistic-based relevance technology based on learning of parse trees for processing, classification and delivery of a stream of texts. We describe the content pipeline for eBay entertainment domain which employs this technology, and show that text processing relevance is the main bottleneck for its performance. A number of components of the content pipeline such as content mining, aggregation, deduplication, opinion mining, integrity enforcing need to rely on domain-independent efficient text classification, entity extraction and relevance assessment operations. Text relevance assessment is based on the operation of syntactic generalization (SG) which finds a maximum common sub-tree for a pair of parse trees for sentences. Relevance of two portions of texts is then defined as a cardinality of this sub-tree. SG is intended to substitute keyword-based analysis for more accurate assessment of relevance which takes phrase-level and sentence-level information into account. In the partial case where short expression are commonly used terms such as Facebook likes, SG ascends to the level of categories and a reasoning technique is required to map these categories in the course of relevance assessment. A number of content pipeline components employ web mining which needs SG to compare web search results. We describe how SG works in a number of components in the content pipeline including personalization and recommendation, and provide the evaluation results for eBay deployment. Content pipeline support is implemented as an open source contribution OpenNLP.Similarity and is available at https://github.com/bgalitsky/relevance-based-on-pars-trees. © 2016 Elsevier Ltd",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2016.11.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006868883&doi=10.1016%2fj.engappai.2016.11.001&partnerID=40&md5=57418c10b16696a7cc4b829b6d24aa2f,2017,2021-07-20 15:50:02,2021-07-20 15:50:02
RDRU26SJ,journalArticle,2020,"Rubio-Solis, A.; Panoutsos, G.; Beltran-Perez, C.; Martinez-Hernandez, U.",A Multilayer Interval Type-2 Fuzzy Extreme Learning Machine for the recognition of walking activities and gait events using wearable sensors,"In this paper, a novel Multilayer Interval Type-2 Fuzzy Extreme Learning Machine (ML-IT2-FELM) for the recognition of walking activities and Gait events is presented. The ML-IT2-FELM uses a hierarchical learning scheme that consists of multiple layers of IT2 Fuzzy Autoencoders (FAEs), followed by a final classification layer based on an IT2-FELM architecture. The core building block in the ML-IT2-FELM is an IT2-FELM, which is a generalised model of the Interval Type-2 Radial Basis Function Neural Network (IT2-RBFNN) and that is functionally equivalent to a class of simplified IT2 Fuzzy Logic Systems (FLSs). Each FAE in the ML-IT2-FELM employs an output layer with a direct-defuzzification process based on the Nie-Tan algorithm, while the IT2-FELM classifier includes a Karnik-Mendel type-reduction method (KM). Real data was collected using three inertial measurements units attached to the thigh, shank and foot of twelve healthy participants. The validation of the ML-IT2-FELM method is performed with two different experiments. The first experiment involves the recognition of three different walking activities: Level-Ground Walking (LGW), Ramp Ascent (RA) and Ramp Descent (RD). The second experiment consists of the recognition of stance and swing phases during the gait cycle. In addition, to compare the efficiency of the ML-IT2-FELM with other ML fuzzy methodologies, a kernel-based ML-IT2-FELM that is inspired by kernel learning and called KML-IT2-FELM is also implemented. The results from the recognition of walking activities and gait events achieved an average accuracy of 99.98% and 99.84% with a decision time of 290.4ms and 105ms, respectively, by the ML-IT2-FELM, while the KML-IT2-FELM achieved an average accuracy of 99.98% and 99.93% with a decision time of 191.9ms and 94ms. The experiments demonstrate that the ML-IT2-FELM is not only an effective Fuzzy Logic-based approach in the presence of sensor noise, but also a fast extreme learning machine for the recognition of different walking activities. © 2020",Neurocomputing,10.1016/j.neucom.2019.11.105,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078495810&doi=10.1016%2fj.neucom.2019.11.105&partnerID=40&md5=c48c9f37f7183db46df4f9b1e75e5de2,2020,2021-07-20 15:50:02,2021-07-20 15:50:02
P79BNGRG,journalArticle,2021,"Ye, F.; Bors, A.G.",Deep Mixture Generative Autoencoders,"Variational autoencoders (VAEs) are one of the most popular unsupervised generative models that rely on learning latent representations of data. In this article, we extend the classical concept of Gaussian mixtures into the deep variational framework by proposing a mixture of VAEs (MVAE). Each component in the MVAE model is implemented by a variational encoder and has an associated subdecoder. The separation between the latent spaces modeled by different encoders is enforced using the d-variable Hilbert-Schmidt independence criterion (dHSIC). Each component would capture different data variational features. We also propose a mechanism for finding the appropriate number of VAE components for a given task, leading to an optimal architecture. The differentiable categorical Gumbel-softmax distribution is used in order to generate dropout masking parameters within the end-to-end backpropagation training framework. Extensive experiments show that the proposed MVAE model can learn a rich latent data representation and is able to discover additional underlying data representation factors. IEEE",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2021.3071401,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85104680096&doi=10.1109%2fTNNLS.2021.3071401&partnerID=40&md5=f9714253f959d0c2af45e272f7ba0fe5,2021,2021-07-20 15:50:02,2021-07-20 15:50:02
BTVA6GG8,journalArticle,2020,"Strauss, R.R.; Ramanujan, R.; Becker, A.; Peck, T.C.",A Steering Algorithm for Redirected Walking Using Reinforcement Learning,"Redirected Walking (RDW) steering algorithms have traditionally relied on human-engineered logic. However, recent advances in reinforcement learning (RL) have produced systems that surpass human performance on a variety of control tasks. This paper investigates the potential of using RL to develop a novel reactive steering algorithm for RDW. Our approach uses RL to train a deep neural network that directly prescribes the rotation, translation, and curvature gains to transform a virtual environment given a user's position and orientation in the tracked space. We compare our learned algorithm to steer-to-center using simulated and real paths. We found that our algorithm outperforms steer-to-center on simulated paths, and found no significant difference on distance traveled on real paths. We demonstrate that when modeled as a continuous control problem, RDW is a suitable domain for RL, and moving forward, our general framework provides a promising path towards an optimal RDW steering algorithm. © 2020 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2020.2973060,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079623190&doi=10.1109%2fTVCG.2020.2973060&partnerID=40&md5=a83e442a5d214465c58aa0a40d60309b,2020,2021-07-20 15:50:02,2021-07-20 15:50:02
KV443I8L,journalArticle,2020,"Haider, S.; Akhunzada, A.; Mustafa, I.; Patel, T.B.; Fernandez, A.; Choo, K.-K.R.; Iqbal, J.",A Deep CNN Ensemble Framework for Efficient DDoS Attack Detection in Software Defined Networks,"As novel technologies continue to reshape the digital era, cyberattacks are also increasingly becoming more commonplace and sophisticated. Distributed denial of service (DDoS) attacks are, perhaps, the most prevalent and exponentially-growing attack, targeting the varied and emerging computational network infrastructures across the globe. This necessitates the design of an efficient and early detection of large-scale sophisticated DDoS attacks. Software defined networks (SDN) point to a promising solution, as a network paradigm which decouples the centralized control intelligence from the forwarding logic. In this work, a deep convolutional neural network (CNN) ensemble framework for efficient DDoS attack detection in SDNs is proposed. The proposed framework is evaluated on a current state-of-the-art Flow-based dataset under established benchmarks. Improved accuracy is demonstrated against existing related detection approaches. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.2976908,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082629665&doi=10.1109%2fACCESS.2020.2976908&partnerID=40&md5=a2090a26e9e3630c8b709c658136cb80,2020,2021-07-20 15:50:02,2021-07-20 15:50:02
75JFPQQ4,journalArticle,2018,"Wang, Y.; Dai, B.; Hua, G.; Aston, J.; Wipf, D.",Recurrent Variational Autoencoders for Learning Nonlinear Generative Models in the Presence of Outliers,"This paper explores two useful modifications of the recent variational autoencoder (VAE), a popular deep generative modeling framework that dresses traditional autoencoders with probabilistic attire. The first involves a specially-tailored form of conditioning that allows us to simplify the VAE decoder structure while simultaneously introducing robustness to outliers. In a related vein, a second, complementary alteration is proposed to further build invariance to contaminated or dirty samples via a data augmentation process that amounts to recycling. In brief, to the extent that the VAE is legitimately a representative generative model, then each output from the decoder should closely resemble an authentic sample, which can then be resubmitted as a novel input ad infinitum. Moreover, this can be accomplished via special recurrent connections without the need for additional parameters to be trained. We evaluate these proposals on multiple practical outlier-removal and generative modeling tasks involving nonlinear low-dimensional manifolds, demonstrating considerable improvements over existing algorithms. © 2018 IEEE.",IEEE Journal on Selected Topics in Signal Processing,10.1109/JSTSP.2018.2876995,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055166578&doi=10.1109%2fJSTSP.2018.2876995&partnerID=40&md5=77c0022a2ed419e1606abc75359c8374,2018,2021-07-20 15:50:02,2021-07-20 15:50:02
9PNLNLQ9,journalArticle,2018,"Wang, Z.; Lin, J.; Wang, Z.",Hardware-Oriented Compression of Long Short-Term Memory for Efficient Inference,"Long short-term memory (LSTM) and its variants have been widely adopted in processing sequential data. However, the intrinsic large memory requirement and high computational complexity make it hard to be employed in embedded systems. This incurs the need of model compression and dedicated hardware accelerator for LSTM. In this letter, efficient clipped gating and top-k pruning schemes are introduced to convert the dense matrix computations in LSTM into structured sparse-matrixsparse- vector multiplications. Then, mixed quantization schemes are developed to eliminate most of the multiplications in LSTM. The proposed compression scheme is well suited for efficient hardware implementations. Experimental results show that the model size and the number of matrix operations can be reduced by 32× and 18.5×, respectively, at a cost of less than 1% accuracy loss on a word-level language modeling task. © 2018 IEEE.",IEEE Signal Processing Letters,10.1109/LSP.2018.2834872,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046771245&doi=10.1109%2fLSP.2018.2834872&partnerID=40&md5=19e8a8a330c044b602fd083fe9ebf1f5,2018,2021-07-20 15:50:02,2021-07-20 15:50:02
XKHS5MVI,journalArticle,2021,"Chen, S.; Dong, J.; Ha, P.; Li, Y.; Labi, S.",Graph neural network and reinforcement learning for multi-agent cooperative control of connected autonomous vehicles,"A connected autonomous vehicle (CAV) network can be defined as a set of connected vehicles including CAVs that operate on a specific spatial scope that may be a road network, corridor, or segment. The spatial scope constitutes an environment where traffic information is shared and instructions are issued for controlling the CAVs movements. Within such a spatial scope, high-level cooperation among CAVs fostered by joint planning and control of their movements can greatly enhance the safety and mobility performance of their operations. Unfortunately, the highly combinatory and volatile nature of CAV networks due to the dynamic number of agents (vehicles) and the fast-growing joint action space associated with multi-agent driving tasks pose difficultly in achieving cooperative control. The problem is NP-hard and cannot be efficiently resolved using rule-based control techniques. Also, there is a great deal of information in the literature regarding sensing technologies and control logic in CAV operations but relatively little information on the integration of information from collaborative sensing and connectivity sources. Therefore, we present a novel deep reinforcement learning-based algorithm that combines graphic convolution neural network with deep Q-network to form an innovative graphic convolution Q network that serves as the information fusion module and decision processor. In this study, the spatial scope we consider for the CAV network is a multi-lane road corridor. We demonstrate the proposed control algorithm using the application context of freeway lane-changing at the approaches to an exit ramp. For purposes of comparison, the proposed model is evaluated vis-à-vis traditional rule-based and long short-term memory-based fusion models. The results suggest that the proposed model is capable of aggregating information received from sensing and connectivity sources and prescribing efficient operative lane-change decisions for multiple CAVs, in a manner that enhances safety and mobility. That way, the operational intentions of individual CAVs can be fulfilled even in partially observed and highly dynamic mixed traffic streams. The paper presents experimental evidence to demonstrate that the proposed algorithm can significantly enhance CAV operations. The proposed algorithm can be deployed at roadside units or cloud platforms or other centralized control facilities. © 2021 Computer-Aided Civil and Infrastructure Engineering",Computer-Aided Civil and Infrastructure Engineering,10.1111/mice.12702,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107485926&doi=10.1111%2fmice.12702&partnerID=40&md5=0f692dc7cac34bea77f09ba056b29b9c,2021,2021-07-20 15:50:03,2021-07-20 15:50:03
KJV8M3SN,journalArticle,2018,"Chouhan, S.S.; Kaul, A.; Singh, U.P.",Soft computing approaches for image segmentation: a survey,"Image segmentation is the method of partitioning an image into a group of pixels that are homogenous in some manner. The homogeneity dependents on some attributes like intensity, color etc. Segmentation being a pre-processing step in image processing have been used in the number of applications like identification of objects to medical images, satellite images and much more. The taxonomy of an image segmentation methods collectively can be divided among two categories Traditional methods and Soft Computing (SC) methods. Unlike Traditional methods, SC methods have the ability to simulate human thinking and are flexible to work with their ownership function, have been predominantly applied to the task of image segmentation. SC techniques are tolerant of partial truth, imprecision, uncertainty, and approximations. Soft Computing approaches also having advantages of providing cost-effective, high performance and steadfast solutions. In this survey paper, our emphasis is on core SC approaches like Fuzzy logic, Artificial Neural Network, and Genetic Algorithm used for image segmentation. The contribution lies in the fact to present this paper to the researchers that explore state-of-the-art elaboration of almost all dimensions associated with the image segmentation. The idea is to encapsulate various aspects like emerging topics, methods, evaluation parameters, the problem associated with different type of images, databases, segmentation applications, and other resources so that, it could be advantageous for researchers to make effort in developing new methods for segmentation. The paper accomplishes with findings and concluding remarks. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-018-6005-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054176865&doi=10.1007%2fs11042-018-6005-6&partnerID=40&md5=df10ab96d90d4ab44a0573de78375fd5,2018,2021-07-20 15:50:03,2021-07-20 15:50:03
H9A5Z4EC,journalArticle,2017,"Pota, M.; Scalco, E.; Sanguineti, G.; Farneti, A.; Cattaneo, G.M.; Rizzo, G.; Esposito, M.",Early prediction of radiotherapy-induced parotid shrinkage and toxicity based on CT radiomics and fuzzy classification,"Motivation Patients under radiotherapy for head-and-neck cancer often suffer of long-term xerostomia, and/or consistent shrinkage of parotid glands. In order to avoid these drawbacks, adaptive therapy can be planned for patients at risk, if the prediction is obtained timely, before or during the early phase of treatment. Artificial intelligence can address the problem, by learning from examples and building classification models. In particular, fuzzy logic has shown its suitability for medical applications, in order to manage uncertain data, and to build transparent rule-based classifiers. In previous works, clinical, dosimetric and image-based features were considered separately, to find different possible predictors of parotid shrinkage. On the other hand, a few works reported possible image-based predictors of xerostomia, while the combination of different types of features has been little addressed. Objective This paper proposes the application of a novel machine learning approach, based on both statistics and fuzzy logic, aimed at the classification of patients at risk of i) parotid gland shrinkage and ii) 12-months xerostomia. Both problems are addressed with the aim of individuating predictors and models to classify respective outcomes. Methods Knowledge is extracted from a real dataset of radiotherapy patients, by means of a recently developed method named Likelihood-Fuzzy Analysis, based on the representation of statistical information by fuzzy rule-based models. This method enables to manage heterogeneous variables and missing data, and to obtain interpretable fuzzy models presenting good generalization power (thus high performance), and to measure classification confidence. Numerous features are extracted to characterize patients, coming from different sources, i.e. clinical features, dosimetric parameters, and radiomics-based measures obtained by texture analysis of Computed Tomography images. A learning approach based on the composition of simple models in a more complicated one allows to consider the features separately, in order to identify predictors and models to use when only some data source is available, and obtaining more accurate results when more information can be combined. Results Regarding parotid shrinkage, a number of good predictors is detected, some already known and confirmed here, and some others found here, in particular among radiomics-based features. A number of models are also designed, some using single features and others involving models composition to improve classification accuracy. In particular, the best model to be used at the initial treatment stage, and another one applicable at the half treatment stage are identified. Regarding 12-months toxicity, some possible predictors are detected, in particular among radiomics-based features. Moreover, the relation between final parotid shrinkage rate and 12-months xerostomia is evaluated. The method is compared to the naïve Bayes classifier, which reveals similar results in terms of classification accuracy and best predictors. The interpretable fuzzy rule-based models are explicitly presented, and the dependence between predictors and outcome is explained, thus furnishing in some cases helpful insights about the considered problems. Conclusion Thanks to the performance and interpretability of the fuzzy classification method employed, predictors of both parotid shrinkage and xerostomia are detected, and their influence on each outcome is revealed. Moreover, models for predicting parotid shrinkage at initial and half radiotherapy stages are found. © 2017 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2017.03.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015690972&doi=10.1016%2fj.artmed.2017.03.004&partnerID=40&md5=a8dd4e70e38b495050baac1ad4154c79,2017,2021-07-20 15:50:03,2021-07-20 15:50:03
XNFK9RRA,journalArticle,2021,"Molina, R.; Loor, F.; Gil-Costa, V.; Nardini, F.M.; Perego, R.; Trani, S.",Efficient traversal of decision tree ensembles with FPGAs,"System-on-Chip (SoC) based Field Programmable Gate Arrays (FPGAs) provide a hardware acceleration technology that can be rapidly deployed and tuned, thus providing a flexible solution adaptable to specific design requirements and to changing demands. In this paper, we present three SoC architecture designs for speeding-up inference tasks based on machine learned ensembles of decision trees. We focus on QUICKSCORER, the state-of-the-art algorithm for the efficient traversal of tree ensembles and present the issues and the advantages related to its deployment on two SoC devices with different capacities. The results of the experiments conducted using publicly available datasets show that the solution proposed is very efficient and scalable. More importantly, it provides almost constant inference times, independently of the number of trees in the model and the number of instances to score. This allows the SoC solution deployed to be fine tuned on the basis of the accuracy and latency constraints of the application scenario considered. © 2021 Elsevier Inc.",Journal of Parallel and Distributed Computing,10.1016/j.jpdc.2021.04.008,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105559601&doi=10.1016%2fj.jpdc.2021.04.008&partnerID=40&md5=192ad20a85853308abc8b636a44c5cbd,2021,2021-07-20 15:50:03,2021-07-20 15:50:03
7LWH7SLB,journalArticle,2021,"Benkabou, S.; Benabdeslem, K.; Kraus, V.; Bourhis, K.; Canitia, B.",Local Anomaly Detection for Multivariate Time Series by Temporal Dependency Based on Poisson Model,"Multivariate time series data are invasive in different domains, ranging from data center supervision and e-commerce data to financial transactions. This kind of data presents an important challenge for anomaly detection due to the temporal dependency aspect of its observations. In this article, we investigate the problem of unsupervised local anomaly detection in multivariate time series data from temporal modeling and residual analysis perspectives. The residual analysis has been shown to be effective in classical anomaly detection problems. However, it is a nontrivial task in multivariate time series as the temporal dependency between the time series observations complicates the residual modeling process. Methodologically, we propose a unified learning framework to characterize the residuals and their coherence with the temporal aspect of the whole multivariate time series. Experiments on real-world datasets are provided showing the effectiveness of the proposed algorithm. IEEE",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2021.3083183,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107355776&doi=10.1109%2fTNNLS.2021.3083183&partnerID=40&md5=4d3c5a5d87728b5d5228b023eb58c383,2021,2021-07-20 15:50:03,2021-07-20 15:50:03
VS2C8A4C,journalArticle,2021,"Newlands, G.",Lifting the curtain: Strategic visibility of human labour in AI-as-a-Service,"Artificial Intelligence-as-a-Service (AIaaS) empowers individuals and organisations to access AI on-demand, in either tailored or ‘off-the-shelf’ forms. However, institutional separation between development, training and deployment can lead to critical opacities, such as obscuring the level of human effort necessary to produce and train AI services. Information about how, where, and for whom AI services have been produced are valuable secrets, which vendors strategically disclose to clients depending on commercial interests. This article provides a critical analysis of how AIaaS vendors manipulate the visibility of human labour in AI production based on whether the vendor relies on paid or unpaid labour to fill interstitial gaps. Where vendors are able to occlude human labour in the organisational ‘backstage,’ such as in data preparation, validation or impersonation, they do so regularly, further contributing to ongoing techno-utopian narratives of AI hype. Yet, when vendors must co-produce the AI service with the client, such as through localised AI training, they must ‘lift the curtain’, resulting in a paradoxical situation of needing to both perpetuate dominant AI hype narratives while emphasising AI’s mundane limitations. © The Author(s) 2021.",Big Data and Society,10.1177/20539517211016026,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106001650&doi=10.1177%2f20539517211016026&partnerID=40&md5=4981b8de39e8243441159ac82b74b520,2021,2021-07-20 15:50:03,2021-07-20 15:50:03
63X8BU4U,journalArticle,2021,"Sanchez-Cauce, R.; Paris, I.; Diez Vegas, F.J.",Sum-product networks: A survey,"A sum-product network (SPN) is a probabilistic model, based on a rooted acyclic directed graph, in which terminal nodes represent probability distributions and non-terminal nodes represent convex sums (weighted averages) and products of probability functions. They are closely related to probabilistic graphical models, in particular to Bayesian networks with multiple context-specific independencies. Their main advantage is the possibility of building tractable models from data, i.e., models that can perform several inference tasks in time proportional to the number of edges in the graph. They are somewhat similar to neural networks and can address the same kinds of problems, such as image processing and natural language understanding. This paper offers a survey of SPNs, including their definition, the main algorithms for inference and learning from data, several applications, a brief review of software libraries, and a comparison with related models. IEEE",IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2021.3061898,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101841246&doi=10.1109%2fTPAMI.2021.3061898&partnerID=40&md5=ce6216d85f968c46283f8bb0ff020bf2,2021,2021-07-20 15:50:03,2021-07-20 15:50:03
YVMXGV6N,journalArticle,2020,"Madani, Y.; Ezzikouri, H.; Erritali, M.; Hssina, B.",Finding optimal pedagogical content in an adaptive e-learning platform using a new recommendation approach and reinforcement learning,"In the learning process, learners have different skills and each one has his own knowledge and his own ability to learn. The adaptive e-learning platforms try to find optimal courses for learners based on their knowledge and skills. Learning online using e-learning platforms becomes indispensable in the teaching process. Companies and scientific researchers try to find new optimal methods and approaches that can improve education online. In this paper, we propose a new recommendation approach for recommending relevant courses to learners. The proposed method is based on social filtering(using the notions of sentiment analysis) and collaborative filtering for defining the best way in which the learner must learn, and recommend courses that better much the learner’s profile and social content. Our work consists also in proposing a new reinforcement learning approach which helps a learner to find the optimal learning path that can improve the quality of learning. © 2019, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-019-01627-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076616117&doi=10.1007%2fs12652-019-01627-1&partnerID=40&md5=3ad7e6d547522221ea15120366360335,2020,2021-07-20 15:50:03,2021-07-20 15:50:03
XK562PTN,journalArticle,2020,"Singh, O.P.; Singh, A.K.; Srivastava, G.; Kumar, N.",Image watermarking using soft computing techniques: A comprehensive survey,"Image watermarking techniques are used to provide copyright protection and verify ownership of media/entities. This technique refers to the concept of embedding of secret data/information of an owner in a given media/entity for determining any ownership conflicts that can arise. Many watermarking approaches have been offered by various authors in the last few years. However, there are not enough studies and comparisons of watermarking techniques in soft computing environments. Nowadays, soft computing techniques are used to improve the performance of watermarking algorithms. This paper surveys soft computing-based image watermarking for several applications. We first elaborate on novel applications, watermark characteristics and different kinds of watermarking systems. Then, soft computing based watermarking approaches providing robustness, imperceptibility and good embedding capacity are compared systematically. Furthermore, major issues and potential solutions for soft computing-based watermarking are also discussed to encourage further research in this area. Thus, this survey paper will be helpful for researchers to implement an optimized watermarking scheme for several applications. © 2020, Springer Science+Business Media, LLC, part of Springer Nature.",Multimedia Tools and Applications,10.1007/s11042-020-09606-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089903815&doi=10.1007%2fs11042-020-09606-x&partnerID=40&md5=30229c0386855e787b9a9a1eea1ee4d9,2020,2021-07-20 15:50:03,2021-07-20 15:50:03
WD62T662,journalArticle,2019,"Hemsley, J.",Followers Retweet! The Influence of Middle-Level Gatekeepers on the Spread of Political Information on Twitter,"Twitter allows political candidates to broadcast messages directly to the public, some of which spread virally, potentially reaching new supportive audiences. During the 2014 U.S. gubernatorial election, 74 candidates for State Governor posted 20,580 tweets, of which 10,946 were retweeted almost 140,000 times. By analyzing a collection of tweets posted by gubernatorial candidates that were classified by machine learning into categories of message types, we find that while candidates tend to post tweets that advocate for themselves the most, the public is more likely to retweet attack messages and messages labeled as call-to-action. As measured by number of retweets, call-to-action tweets tend to reach the broadest audience. We also find that middle-level gatekeepers, those with between 1,800 and 26,000 followers, tend to have the most influence over the flow of political information. Since retweets tend to bring new followers, these findings suggest that politicians wishing to grow their audience may benefit from posting more call-to-action and attack messages, and that candidates may wish to find ways to actively enlist the support of middle-level gatekeepers. © 2019 Policy Studies Organization",Policy and Internet,10.1002/poi3.202,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062800864&doi=10.1002%2fpoi3.202&partnerID=40&md5=8b90d951d30070bd1c92d82289702818,2019,2021-07-20 15:50:03,2021-07-20 15:50:03
AGZRAZUM,journalArticle,2019,"Zhang, C.; Butepage, J.; Kjellstrom, H.; Mandt, S.",Advances in Variational Inference,"Many modern unsupervised or semi-supervised machine learning algorithms rely on Bayesian probabilistic models. These models are usually intractable and thus require approximate inference. Variational inference (VI) lets us approximate a high-dimensional Bayesian posterior with a simpler variational distribution by solving an optimization problem. This approach has been successfully applied to various models and large-scale applications. In this review, we give an overview of recent trends in variational inference. We first introduce standard mean field variational inference, then review recent advances focusing on the following aspects: (a) scalable VI, which includes stochastic approximations, (b) generic VI, which extends the applicability of VI to a large class of otherwise intractable models, such as non-conjugate models, (c) accurate VI, which includes variational models beyond the mean field approximation or with atypical divergences, and (d) amortized VI, which implements the inference over local latent variables with inference networks. Finally, we provide a summary of promising future research directions. © 1979-2012 IEEE.",IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2018.2889774,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059288228&doi=10.1109%2fTPAMI.2018.2889774&partnerID=40&md5=34893a13e59df395933a50622397dbc9,2019,2021-07-20 15:50:03,2021-07-20 15:50:03
FVLKKST2,journalArticle,2019,"Patil, A.D.; Manipatruni, S.; Nikonov, D.E.; Young, I.A.; Shanbhag, N.R.",Boosted Spin Channel Networks for Energy-Efficient Inference,"Computational scaling beyond silicon electronics based on Moore's law requires the adoption of alternate state variables such as electronic spin. Multiple research efforts are underway exploring both Boolean and non-Boolean design space using spin devices in order to make their energy and delay benefits competitive to CMOS. In this paper, we propose spin channel networks (SCN), where the exponential decay property of spin current along the spin channel is exploited to achieve energy-efficient dot product implementation for inference applications. As the use of exponentially decaying spin current for analog computation enforces severe locality constraints, we employ adaptive boosting to design an ensemble of tiny SCNs that work in unison to solve any binary classification task. Such boosted SCNs achieve up to 112 × and × higher energy efficiency over conventional all-spin-logic-based and 20-nm CMOS designs, respectively. © 2014 IEEE.",IEEE Journal on Exploratory Solid-State Computational Devices and Circuits,10.1109/JXCDC.2019.2895641,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067140362&doi=10.1109%2fJXCDC.2019.2895641&partnerID=40&md5=8d55b8a5f95b339f02e5a3b50f5289db,2019,2021-07-20 15:50:03,2021-07-20 15:50:03
3Z7R7TAJ,journalArticle,2018,"Liu, Y.; Xie, Y.; Bao, C.; Srivastava, A.",A combined optimization-theoretic and side-channel approach for attacking strong physical unclonable functions,"The promise of strong physical unclonable functions (PUF) is to utilize the manufacturing variations of circuit elements to produce an independent and unpredictable response to any input challenge vector. Attacks on PUFs that predict the responses to input challenge vectors offer an interesting research problem. An attacking approach based on the optimization theory and side-channel information is proposed where we estimate the manufacturing variations of the circuit elements and predict the PUF's responses to challenge vectors whose actual responses are not known. We apply this attacking approach on some popular PUF designs, including the Arbiter PUFs, the Memristor Crossbar PUFs, and the XOR Arbiter PUFs. Simulations show a substantial reduction in attack complexity compared with previously proposed machine-learning (ML)-based attacks: we achieve an average reduction of 66% in attack time compared with the ML approach. Despite some overhead, our approach is also applicable when the PUF responses are noisy. © 2017 IEEE.",IEEE Transactions on Very Large Scale Integration (VLSI) Systems,10.1109/TVLSI.2017.2759731,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032278617&doi=10.1109%2fTVLSI.2017.2759731&partnerID=40&md5=e12c38618e2a5fb4758644c6eebf402f,2018,2021-07-20 15:50:03,2021-07-20 15:50:03
E4Z8UQKL,journalArticle,2017,"Venturini, L.; Baralis, E.; Garza, P.",Scaling associative classification for very large datasets,"Supervised learning algorithms are nowadays successfully scaling up to datasets that are very large in volume, leveraging the potential of in-memory cluster-computing Big Data frameworks. Still, massive datasets with a number of large-domain categorical features are a difficult challenge for any classifier. Most off-the-shelf solutions cannot cope with this problem. In this work we introduce DAC, a Distributed Associative Classifier. DAC exploits ensemble learning to distribute the training of an associative classifier among parallel workers and improve the final quality of the model. Furthermore, it adopts several novel techniques to reach high scalability without sacrificing quality, among which a preventive pruning of classification rules in the extraction phase based on Gini impurity. We ran experiments on Apache Spark, on a real large-scale dataset with more than 4 billion records and 800 million distinct categories. The results showed that DAC improves on a state-of-the-art solution in both prediction quality and execution time. Since the generated model is human-readable, it can not only classify new records, but also allow understanding both the logic behind the prediction and the properties of the model, becoming a useful aid for decision makers. © 2017, The Author(s).",Journal of Big Data,10.1186/s40537-017-0107-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037613389&doi=10.1186%2fs40537-017-0107-2&partnerID=40&md5=dc97e68b5a99b1e8d53892569696ddc7,2017,2021-07-20 15:50:03,2021-07-20 15:50:03
PXJX5EHS,journalArticle,2017,"Kitani, A.; Kimura, T.; Nakatani, T.",Toward the reduction of incorrect drawn ink retrieval,"As tablet devices become popular, various handwriting applications are used. Some of applications incorporate a specific function, which is generally called palm rejection. Palm rejection enables application users to put the palm of a writing hand onto a touch display. It classifies intended touches and unintended touches so that it prevents accidental inking, which has been known to occur under the writing hand. Though some of palm rejections can remove accidental inking afterward, this function occasionally does not execute correctly as it may remove rather correct ink strokes as well. We call this interaction Incorrect Drawn Ink Retrieval (IDIR). In this paper, we propose a software algorithm that is a combination of two palm rejection logics that reduces IDIR with precision and without latency. That algorithm does not depend on specific hardware, such as an active stylus pen. Our data provides 98.98% correctness and the algorithm takes less than 10 ms for the distinction. We confirm that our experimental application reduced the occurrences of IDIR throughout an experiment. © 2017, The Author(s).",Human-centric Computing and Information Sciences,10.1186/s13673-017-0099-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020890788&doi=10.1186%2fs13673-017-0099-0&partnerID=40&md5=6509889b92e786dc3eb6b21496f4e180,2017,2021-07-20 15:50:04,2021-07-20 15:50:04
ERQDT459,journalArticle,2016,"Du, J.; Li, W.; Lu, K.; Xiao, B.",An overview of multi-modal medical image fusion,"Multi-modal medical image fusion is the process of merging multiple images from single or multiple imaging modalities to improve the imaging quality with preserving the specific features. Medical image fusion covers a broad number of hot topic areas, including image processing, computer vision, pattern recognition, machine learning and artificial intelligence. And medical image fusion has been widely used in clinical for physicians to comprehend the lesion by the fusion of different modalities medical images. In this review, methods in the field of medical image fusion are characterized by (1) image decomposition and image reconstruction, (2) image fusion rules, (3) image quality assessments, and (4) experiments on the benchmark dataset. In addition, this review provides a factual listing of scientific challenges faced in the field of multi-modal medical image fusion. © 2016 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2015.07.160,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84977557211&doi=10.1016%2fj.neucom.2015.07.160&partnerID=40&md5=5244e66c1e1c8f5b4c60056563dc0943,2016,2021-07-20 15:50:04,2021-07-20 15:50:04
PTGQDANU,journalArticle,2016,"Walkinshaw, N.; Taylor, R.; Derrick, J.",Inferring extended finite state machine models from software executions,"The ability to reverse-engineer models of software behaviour is valuable for a wide range of software maintenance, validation and verification tasks. Current reverse-engineering techniques focus either on control-specific behaviour (e.g., in the form of Finite State Machines), or data-specific behaviour (e.g., as pre / post-conditions or invariants). However, typical software behaviour is usually a product of the two; models must combine both aspects to fully represent the software’s operation. Extended Finite State Machines (EFSMs) provide such a model. Although attempts have been made to infer EFSMs, these have been problematic. The models inferred by these techniques can be non-deterministic, the inference algorithms can be inflexible, and only applicable to traces with specific characteristics. This paper presents a novel EFSM inference technique that addresses the problems of inflexibility and non-determinism. It also adapts an experimental technique from the field of Machine Learning to evaluate EFSM inference techniques, and applies it to three diverse software systems. © 2015, Springer Science+Business Media New York.",Empirical Software Engineering,10.1007/s10664-015-9367-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925003128&doi=10.1007%2fs10664-015-9367-7&partnerID=40&md5=01180e7c588db960227191adb7901c19,2016,2021-07-20 15:50:04,2021-07-20 15:50:04
CMMGKIEG,journalArticle,2015,"Kao, Y.; Shi, L.; Xie, J.; Karimi, H.R.",Global exponential stability of delayed Markovian jump fuzzy cellular neural networks with generally incomplete transition probability,"The problem of global exponential stability in mean square of delayed Markovian jump fuzzy cellular neural networks (DMJFCNNs) with generally uncertain transition rates (GUTRs) is investigated in this paper. In this GUTR neural network model, each transition rate can be completely unknown or only its estimate value is known. This new uncertain model is more general than the existing ones. By constructing suitable Lyapunov functionals, several sufficient conditions on the exponential stability in mean square of its equilibrium solution are derived in terms of linear matrix inequalities (LMIs). Finally, a numerical example is presented to illustrate the effectiveness and efficiency of our results. © 2014 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2014.10.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84912006746&doi=10.1016%2fj.neunet.2014.10.009&partnerID=40&md5=597cf9e4dd2c1669658bdc9c371e53d2,2015,2021-07-20 15:50:04,2021-07-20 15:50:04
CR4W65MX,journalArticle,2015,"Chen, R.; Herskovits, E.H.",Examining the multifactorial nature of a cognitive process using Bayesian brain-behavior modeling,"Establishing relationships among brain structures and cognitive functions is a central task in cognitive neuroscience. Existing methods to establish associations among a set of function variables and a set of brain regions, such as dissociation logic and conjunction analysis, are hypothesis-driven. We propose a new data-driven approach to structure-function association analysis. We validated it by analyzing a simulated atrophy study. We applied the proposed method to a study of aging and dementia. We found that the most significant age-related and dementia-related volume reductions were in the hippocampal formation and the supramarginal gyrus, respectively. These findings suggest a multi-component brain-aging model. © 2014 Elsevier Ltd.",Computerized Medical Imaging and Graphics,10.1016/j.compmedimag.2014.05.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923357859&doi=10.1016%2fj.compmedimag.2014.05.001&partnerID=40&md5=eb1953fd7a95aee6f84ae259e7df9317,2015,2021-07-20 15:50:04,2021-07-20 15:50:04
6WKZGVAC,journalArticle,2013,"Kampakis, S.",Investigating the computational power of spiking neurons with non-standard behaviors,"Spiking neural networks have been called the third generation of neural networks. Their main difference with respect to the previous two generations is the use of realistic neuron models. Their computational power has been well studied with respect to threshold gates and sigmoidal neurons. However, biologically realistic models of spiking neurons can produce behaviors that can be computationally relevant, but their power has not been assessed in the same way. This paper studies the computational power of neurons with different behaviors based on the previous analyses conducted by Maass and Schmitt. The studied behaviors are rebound spiking, resonance and bursting. The results of the analysis are presented. A theoretical motivation for this study is presented and a discussion is done on the possible implications of the findings for using networks of spiking neurons for performing computations. © 2013 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2013.01.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875257281&doi=10.1016%2fj.neunet.2013.01.011&partnerID=40&md5=034b9d42346c5fb5138726dbb50b8d61,2013,2021-07-20 15:50:04,2021-07-20 15:50:04
BETN98IM,journalArticle,2013,"Wen, S.; Bao, G.; Zeng, Z.; Chen, Y.; Huang, T.",Global exponential synchronization of memristor-based recurrent neural networks with time-varying delays,"This paper deals with the problem of global exponential synchronization of a class of memristor-based recurrent neural networks with time-varying delays based on the fuzzy theory and Lyapunov method. First, a memristor-based recurrent neural network is designed. Then, considering the state-dependent properties of the memristor, a new fuzzy model employing parallel distributed compensation (PDC) gives a new way to analyze the complicated memristor-based neural networks with only two subsystems. Comparisons between results in this paper and in the previous ones have been made. They show that the results in this paper improve and generalized the results derived in the previous literature. An example is also given to illustrate the effectiveness of the results. © 2013 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2013.10.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887376596&doi=10.1016%2fj.neunet.2013.10.001&partnerID=40&md5=73b1414a9225ce73ae659d14f0e07a92,2013,2021-07-20 15:50:04,2021-07-20 15:50:04
7IZLQU6Z,journalArticle,2012,"Vatankhah, R.; Etemadi, S.; Alasty, A.; Vossoughi, G.",Adaptive critic-based neuro-fuzzy controller in multi-agents: Distributed behavioral control and path tracking,"In this paper, we follow two control tasks in a leader following frame with undirected network and local communications. As the first goal, distributed behavioral imitation, which is necessary to fit agents with complicated motion equations in kinematic frames, is discussed. Providing real agents with behavioral controller makes them capable to act as a kinematic particle. The second goal is to design an active leading strategy for the LA to move the group on a predefined path. Both problems can be mathematically modeled in an affine form, which is the reason behind using a unique adaptive controller to solve them. The controller is based on a neuro-fuzzy structure with critic-based leaning structure. It is shown that the proposed controller can successfully handle both tasks. © 2012 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2011.08.031,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860235311&doi=10.1016%2fj.neucom.2011.08.031&partnerID=40&md5=37b84cb11ffbf7968ca5e8e9be473338,2012,2021-07-20 15:50:04,2021-07-20 15:50:04
GLLGN4KS,journalArticle,2017,"Khozeimeh, F.; Alizadehsani, R.; Roshanzamir, M.; Khosravi, A.; Layegh, P.; Nahavandi, S.",An expert system for selecting wart treatment method,"As benign tumors, warts are made through the mediation of Human Papillomavirus (HPV) and may grow on all parts of body, especially hands and feet. There are several treatment methods for this illness. However, none of them can heal all patients. Consequently, physicians are looking for more effective and customized treatments for each patient. They are endeavoring to discover which treatments have better impacts on a particular patient. The aim of this study is to identify the appropriate treatment for two common types of warts (plantar and common) and to predict the responses of two of the best methods (immunotherapy and cryotherapy) to the treatment. As an original work, the study was conducted on 180 patients, with plantar and common warts, who had referred to the dermatology clinic of Ghaem Hospital, Mashhad, Iran. In this study, 90 patients were treated by cryotherapy method with liquid nitrogen and 90 patients with immunotherapy method. The selection of the treatment method was made randomly. A fuzzy logic rule-based system was proposed and implemented to predict the responses to the treatment method. It was observed that the prediction accuracy of immunotherapy and cryotherapy methods was 83.33% and 80.7%, respectively. According to the results obtained, the benefits of this expert system are multifold: assisting physicians in selecting the best treatment method, saving time for patients, reducing the treatment cost, and improving the quality of treatment. © 2017 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2017.01.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008932868&doi=10.1016%2fj.compbiomed.2017.01.001&partnerID=40&md5=a46d6c59f9c4234405c0c5fdbf5b8d3c,2017,2021-07-20 15:50:04,2021-07-20 15:50:04
WPIX5YSI,journalArticle,2016,"Ye, F.; Firouzi, F.; Yang, Y.; Chakrabarty, K.; Tahoori, M.B.",On-chip droop-induced circuit delay prediction based on support-vector machines,"Voltage droop is a major reliability concern in nano-scale very large-scale integration designs. Undesirable voltage droop is often a result of excessive IR drop. On the other hand, Ldi/dt-induced droop occurs when logic gates in the circuit draw high-switching current from the on-chip power supply network, and this problem is exacerbated at high-clock frequencies and smaller technology nodes. A consequence of voltage droop is usually an increase in path delays and the occurrence of intermittent faults during circuit operation. The addition of conservative timing margins, also known as guardbands, is a common practice to tackle the problem of voltage droop. However, such static and pessimistic guardbands, which are calculated at design time based on worst-case conditions, lead to significant performance loss. Dynamic frequency scaling is an alternative approach that enables the dynamic adjustment of clock frequency based on the actual voltage droop seen during runtime. For dynamic voltage-frequency to be effective, accurate and real-time prediction of voltage droop is essential. We propose a support-vector machine (SVM)-based regression method to predict voltage droop due to pattern-dependent IR drop based on inputs to the chip at runtime. Moreover, we reduce the amount of data needed for accurate prediction by using correlation-based feature selection. Several benchmarks from ITC'99 and International Work on Logic and Synthesis'05 highlight the effectiveness of the proposed method in terms of delay-prediction accuracy. Since real-time droop prediction requires hardware implementation of the predictor, we present the hardware design and synthesis results to demonstrate that the hardware overhead for the SVM predictor is negligible for large circuits. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2015.2474392,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84963732405&doi=10.1109%2fTCAD.2015.2474392&partnerID=40&md5=3a8f787ca2ceb60f296013ed8b8e3cf4,2016,2021-07-20 15:50:04,2021-07-20 15:50:04
D7TZ4627,journalArticle,2015,"Wang, G.; Chu, H.C.E.; Zhang, Y.; Chen, H.; Hu, W.; Li, Y.; Peng, X.J.",Multiple parameter control for ant colony optimization applied to feature selection problem,"The ant colony optimization algorithm (ACO) was initially developed to be a metaheuristic for combinatorial optimization problem. In scores of experiments, it is confirmed that the parameter settings in ACO have direct effects on the performance of the algorithm. However, few studies have specially reported the parameter control for ACO. The aim of this paper was to put forward some strategies to adaptively adjust the parameter in ACO and further provide a deeper understanding of ACO parameter control, including static and dynamic parameters. We choose well-known ant system (AS) and ant colony system (ACS) to be controlled by our proposed strategies. The parameters in AS and ACS include β, pheromone evaporation rate (ρ), exploration probability factor (q0) and number of ants (m). We have proposed three adaptive parameter control strategies (SI, SII and SIII) based on fuzzy logic control which adjusts ρ, q0 and m, respectively. The feature selection problem is considered for evaluating the parameter control strategies. In addition, because AS and ACS are not intrinsically fit for feature selection problem, we have modified the AS and ACS, which are named as fuzzy adaptive ant system (FAAS) and fuzzy adaptive ant colony system (FAACS), to make them more suitable for feature selection problem. Because only one parameter is allowed to be dynamically adjusted in FAAS or FAACS, the remaining parameters should be statically specified. Thus, we have developed parametric guidelines for proper combination of static parameter settings. The performance of FAAS and FAACS is compared with that of the AS-based, ACS-based, particle swarm optimization-based and genetic algorithm-based methods on a comprehensive set of 10 benchmark data sets, which are taken from UCI machine learning and StatLog databases. The numerical results and statistical analysis show that the proposed algorithms outperform significantly than other methods in terms of prediction accuracy with smaller subset of features. © 2015, The Natural Computing Applications Forum.",Neural Computing and Applications,10.1007/s00521-015-1829-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939572033&doi=10.1007%2fs00521-015-1829-8&partnerID=40&md5=5c5d26dfed5900febadcc0ca61cfaf37,2015,2021-07-20 15:50:04,2021-07-20 15:50:04
DV74LI85,journalArticle,2021,"Malik, H.; Almutairi, A.",Modified Fuzzy-Q-Learning (MFQL)-Based Mechanical Fault Diagnosis for Direct-Drive Wind Turbines Using Electrical Signals,"In this paper, a self-learning multi-class intelligent model for wind turbine fault diagnosis is proposed by using MFQL (Modified-Fuzzy-Q-Learning) technique. The MFQL is adaptive in nature and extension of fuzzy-Q-learning method where look-up table of Q-learning is conquered by fuzzy based approximation strategy to reduce the curse of dimensionality of the Q-learning. The proposed MFQL classifier diagnoses the mechanical and imbalance faults without using mechanical sensors. Proposed methodology is addressed with relying on PMSG (Permanent Magnet Synchronous Generator) stator current signals, which is already being used by protection system of wind turbines. According to the aforementioned description, non-stationary current signals of PMSG have been pre-processed to extract the input features by empirical mode decomposition followed with J48 algorithm based most relevant input feature selection. For the one-step ahead performance demonstration of the proposed MFQL approach, results have been compared with neural network, support vector machines, fuzzy logic, and conventional Fuzzy-Q-Learning techniques. Demonstrated results outperform the capability of proposed MFQL approach. Moreover, MFQL is developed first time to implement in the area of WTGS fault diagnosis in the literature. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2021.3070483,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103796004&doi=10.1109%2fACCESS.2021.3070483&partnerID=40&md5=e9f0db7ae370bd25e8aa0407c5e1085a,2021,2021-07-20 15:50:04,2021-07-20 15:50:04
7JBII49C,journalArticle,2020,"Alomar, M.L.; Skibinsky-Gitlin, E.S.; Frasser, C.F.; Canals, V.; Isern, E.; Roca, M.; Rosselló, J.L.",Efficient parallel implementation of reservoir computing systems,"Reservoir computing (RC) is a powerful machine learning methodology well suited for time-series processing. The hardware implementation of RC systems (HRC) may extend the utility of this neural approach to solve real-life problems for which software solutions are not satisfactory. Nevertheless, the implementation of massive parallel-connected reservoir networks is costly in terms of circuit area and power, mainly due to the requirement of implementing synapse multipliers that increase gate count to prohibitive values. Most HRC systems present in the literature solve this area problem by sequencializing the processes, thus loosing the expected fault-tolerance and low latency of fully parallel-connected HRCs. Therefore, the development of new methodologies to implement fully parallel HRC systems is of high interest to many computational intelligence applications requiring quick responses. In this article, we propose a compact hardware implementation for Echo-State Networks (an specific type of reservoir) that reduces the area cost by simplifying the synapses and using linear piece-wise activation functions for neurons. The proposed design is synthesized in a Field-Programmable Gate Array and evaluated for different time-series prediction tasks. Without compromising the overall accuracy, the proposed approach achieves a significant saving in terms of power and hardware when compared with recently published implementations. This technique pave the way for the low-power implementation of fully parallel reservoir networks containing thousands of neurons in a single integrated circuit. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-018-3912-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058015689&doi=10.1007%2fs00521-018-3912-4&partnerID=40&md5=e396ed7707cdc4caf8443f7052a73bf5,2020,2021-07-20 15:50:04,2021-07-20 15:50:04
MQ5QVYIL,journalArticle,2019,"Dong, C.; Zhang, F.; Liu, X.; Huang, X.; Guo, W.; Yang, Y.",A Locating Method for Multi-Purposes HTs Based on the Boundary Network,"Recently, there are various methods for detecting the hardware trojans (HTs) in the integrated circuits (ICs). The circuit's logic representations of different types, structures, and functional characteristics should be different. Each type of circuit has its' own performance characteristics according to its' purpose. However, the traditional HTs detection methods adopt the same approach to deal with the multi-purpose hardware trojan. At the same time, as the scale of integrated circuits growing, the structures are more complex, and the functions are more refined. The current situation makes the traditional HTs detection methods weaker and even unfeasible. Therefore, we propose an HTs classified locating method based on machine learning, named ML-HTCL, which belongs to the static detection and locating method. In ML-HTCL, different purpose HTs were represented by different features. Due to the different features, the ML-HTCL employs the multi-layer BP neural network for the control signal type HTs and the one-class SVM for the information leakage HTs, respectively. To deal with the HTs completely, the boundary nets are considered for all data set, while those were ignored by the most existing methods due to the high detection error rate. After detection, the HTs' precise locations were achieved. To evaluate the ML-HTCL, 17 gate-level netlist benchmarks are used for training and testing by leave-one-out cross-validation. From the results, the ML-HTCL reaches 85.05% of TPR and 73.91% of TNR for all kinds of HTs, which performs better than the most existing methods. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2019.2932478,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083164920&doi=10.1109%2fACCESS.2019.2932478&partnerID=40&md5=f425ff35a1ea6c5dc60be4ee15536385,2019,2021-07-20 15:50:04,2021-07-20 15:50:04
RH2AS9F6,journalArticle,2018,"Krestinskaya, O.; Ibrayev, T.; James, A.P.",Hierarchical Temporal Memory Features with Memristor Logic Circuits for Pattern Recognition,"Hierarchical temporal memory (HTM) is a machine learning algorithm inspired by the information processing mechanisms of the human neocortex and consists of a spatial pooler (SP) and temporal memory (TM). In this paper, we develop circuits and systems to achieve the optimized design of an HTM SP, an HTM TM, and a memristive analog pattern matcher for pattern recognition applications. The HTM SP realizes an optimized hardware design through the introduction of mean overlap calculations and by replacing the threshold determination in the inhibition stage with a weighted summation operator over the neighborhood of the pixel under consideration. HTM TM is based on discrete analog memristive memory arrays and a weight update procedure. The operation of the proposed system is demonstrated for a face recognition problem, using the standard AR, ORL, and Yale databases, and for speech recognition, using the TIMIT database, with achieved accuracies of 87.21% and approximately 90%, respectively, given an SNR of 10 dB. Visual data processing using binary HTM SP features requires less storage and processing memory than required by the traditional processing methods, with the area and power requirements for its implementation being 0.096 mm2 and 1756 mW, respectively. The design of the TM circuit for a single pixel requires 23.85 μm2 of area and 442.26 μW of power. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2017.2748024,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029180302&doi=10.1109%2fTCAD.2017.2748024&partnerID=40&md5=7e569656039dab59aac9d481b2cf3e17,2018,2021-07-20 15:50:04,2021-07-20 15:50:04
4UPD55Z5,journalArticle,2018,"Das, M.; Ghosh, S.K.",Data-driven approaches for meteorological time series prediction: A comparative study of the state-of-the-art computational intelligence techniques,"With the proliferation of sensor generated weather data, the data-driven modeling for prediction of meteorological time series has gained increasing research interest in current years. The recent advancement in machine learning and artificial intelligence paradigm has made such data analysis process more effective, flexible and sound. This paper attempts to provide a comparative study of the state-of-the art computational intelligence (CI) techniques, which have been successfully applied for meteorological time series prediction purpose. The study has been carried out considering eleven distinct variants of CI techniques, especially based on artificial neural network (ANN), fuzzy logic, Bayesian network (BN) and other probabilistic models. Further, one more hybrid CI technique (SpaFBN), derived from the existing approaches, has been proposed in the present work. All these CI techniques have been empirically studied with respect to a multivariate meteorological time series prediction problem, in comparison with three benchmark statistical approaches. Overall, the experimental results demonstrate the superiority of the BN-based models in meteorological prediction. The presently proposed spatial fuzzy Bayesian network (SpaFBN) is also found to be an effective tool, especially for predicting humidity and precipitation rate time series. Moreover, the proposed SpaFBN is a generic CI technique which can be applied for predicting spatial time series from the domains other than meteorology as well. © 2017",Pattern Recognition Letters,10.1016/j.patrec.2017.08.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040330840&doi=10.1016%2fj.patrec.2017.08.009&partnerID=40&md5=c65ff9eb82ad21763b6325d25300e32c,2018,2021-07-20 15:50:04,2021-07-20 15:50:04
4HSZGNWK,journalArticle,2015,"Reynolds, C.R.; Muggleton, S.H.; Sternberg, M.J.E.",Incorporating Virtual Reactions into a Logic-based Ligand-based Virtual Screening Method to Discover New Leads,"The use of virtual screening has become increasingly central to the drug development pipeline, with ligand-based virtual screening used to screen databases of compounds to predict their bioactivity against a target. These databases can only represent a small fraction of chemical space, and this paper describes a method of exploring synthetic space by applying virtual reactions to promising compounds within a database, and generating focussed libraries of predicted derivatives. A ligand-based virtual screening tool Investigational Novel Drug Discovery by Example (INDDEx) is used as the basis for a system of virtual reactions. The use of virtual reactions is estimated to open up a potential space of 1.21×1012 potential molecules. A de novo design algorithm known as Partial Logical-Rule Reactant Selection (PLoRRS) is introduced and incorporated into the INDDEx methodology. PLoRRS uses logical rules from the INDDEx model to select reactants for the de novo generation of potentially active products. The PLoRRS method is found to increase significantly the likelihood of retrieving molecules similar to known actives with a p-value of 0.016. Case studies demonstrate that the virtual reactions produce molecules highly similar to known actives, including known blockbuster drugs. © 2015 WILEY-VCH Verlag GmbH & Co. KGaA, Weinheim.",Molecular Informatics,10.1002/minf.201400162,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942194863&doi=10.1002%2fminf.201400162&partnerID=40&md5=5c14f6c5a641d5261e1c7edcdaada75f,2015,2021-07-20 15:50:04,2021-07-20 15:50:04
U7TPBL9U,journalArticle,2021,"Wu, X.; Weng, J.",Learning to recognize while learning to speak: Self-supervision and developing a speaking motor,"Traditionally, learning speech synthesis and speech recognition were investigated as two separate tasks. This separation hinders incremental development for concurrent synthesis and recognition, where partially-learned synthesis and partially-learned recognition must help each other throughout lifelong learning. This work is a paradigm shift—we treat synthesis and recognition as two intertwined aspects of a lifelong learning agent. Furthermore, in contrast to existing recognition or synthesis systems, babies do not need their mothers to directly supervise their vocal tracts at every moment during the learning. We argue that self-generated non-symbolic states/actions at fine-grained time level help such a learner as necessary temporal contexts. Here, we approach a new and challenging problem—how to enable an autonomous learning system to develop an artificial speaking motor for generating temporally-dense (e.g., frame-wise) actions on the fly without human handcrafting a set of symbolic states. The self-generated states/actions are Muscles-like, High-dimensional, Temporally-dense and Globally-smooth (MHTG), so that these states/actions are directly attended for concurrent synthesis and recognition for each time frame. Human teachers are relieved from supervising learner's motor ends. The Candid Covariance-free Incremental (CCI) Principal Component Analysis (PCA) is applied to develop such an artificial speaking motor where PCA features drive the motor. Since each life must develop normally, each Developmental Network-2 (DN-2) reaches the same network (maximum likelihood, ML) regardless of randomly initialized weights, where ML is not just for a function approximator but rather an emergent Turing Machine. The machine-synthesized sounds are evaluated by both the neural network and humans with recognition experiments. Our experimental results showed learning-to-synthesize and learning-to-recognize-through-synthesis for phonemes. This work corresponds to a key step toward our goal to close a great gap toward fully autonomous machine learning directly from the physical world. © 2021 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2021.05.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107089244&doi=10.1016%2fj.neunet.2021.05.006&partnerID=40&md5=37890b4bebb572f940d7d99c694e2353,2021,2021-07-20 15:50:05,2021-07-20 15:50:05
9R7UVBWC,journalArticle,2021,"Wiese, E.S.; Linn, M.C.","""It Must Include Rules”: Middle School Students’ Computational Thinking with Computer Models in Science","When middle school students encounter computer models of science phenomenon in science class, how do they think those computer models work? Computer models operationalize real-world behaviors of selected variables, and can simulate interactions between the modeled elements through programmed instructions. This study explores how middle school students think about the high-level semantic meaning of those instructions, which we term rules. To investigate this aspect of students' computational thinking, we developed the Computational Modeling Inventory and administered it to 253 7th grade students. The Inventory included three computer models that students interacted with during the assessment. In our sample, 99% of students identified at least one key rule underlying a model, but only 14% identified all key rules; 65% believed that model rules can contradict; and 98% could not distinguish between emergent patterns and behaviors that directly resulted from model rules. Despite these misconceptions, compared to the ""typical""questions about the science content alone, questions about model rules elicited deeper science thinking, with 2 - 10 times more responses including reasoning about scientific mechanisms. These results suggest that incorporating computational thinking instruction into middle school science courses might yield deeper learning and more precise assessments around scientific models. © 2021 ACM.",ACM Transactions on Computer-Human Interaction,10.1145/3415582,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105461139&doi=10.1145%2f3415582&partnerID=40&md5=160904482464d448bb6c8dfa563f2540,2021,2021-07-20 15:50:05,2021-07-20 15:50:05
BPLNMM8J,journalArticle,2020,"Wang, M.; Shu, X.; Feng, J.; Wang, X.; Tang, J.",Deep multi-person kinship matching and recognition for family photos,"In this paper, we propose a novel Deep Kinship Matching and Recognition (DKMR) framework for multi-person kinship matching and recognition, which is a complicated and challenging task with little previous literature. Compared with most existing kinship understanding methods that mainly work on matching kinship in pairwise face images, we target at recognizing the exact kinship in nuclear family photos consisting of multiple persons. The proposed DKMR framework contains three modules. Firstly, we design a deep kinship matching model (termed DKM-TRL) to predict kin-or-not scores by integrating the triple ranking loss into a Siamese CNN model. Secondly, we develop a deep kinship recognition model (named DKR-GA) to predict the exact kinship categories, in which gender and relative age attributes are utilized to learn more discriminative representations. Thirdly, based on the outputs of DKM-TRL and DKR-GA, we propose a reasoning conditional random field (R-CRF) model to infer the corresponding optimal family tree by exploiting the common kinship knowledge of a nuclear family. To evaluate the effectiveness of our DKMR framework, we conduct extensive experiments and the results show that it can gain superior performance on Group-Face dataset, TSKinFace dataset and FIW dataset over state-of-the-arts. © 2020 Elsevier Ltd",Pattern Recognition,10.1016/j.patcog.2020.107342,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082879651&doi=10.1016%2fj.patcog.2020.107342&partnerID=40&md5=b72606513a58309de103ee4c20d463fb,2020,2021-07-20 15:50:05,2021-07-20 15:50:05
6BM6PQF7,journalArticle,2020,"Han, W.; Peng, M.; Xie, Q.; Hu, G.; Gao, W.; Wang, H.; Zhang, Y.; Liu, Z.",DTC: Transfer learning for commonsense machine comprehension,"Commonsense Machine Comprehension (CMC) is a popular natural language understanding task. CMC enables computers to learn about causal and temporal reasoning by exploiting implicit commonsense knowledge and can be applied to Question Answering, Search Engine and Dialogue System. Previous methods for CMC limit the vision on CMC task, neglecting that Recognizing Textual Entailment(RTE) task has much similarities with CMC task. In this paper, we propose a transfer learning model, which can take advantage of commonsense knowledge in RTE task by mapping CMC examples and RTE examples to a shared feature space and comprehending in this feature space. Specifically, we first establish a transfer learning framework which has three components: (1) source and target mappings, (2) domain regularization, and (3) CMC score function. Then we make selection for each component in our transfer learning framework and propose the Domain Transfer Comprehension(DTC) model. Experiments on Story Cloze Test show that our model outperforms most previous approaches and provides competitive results with state-of-art methods. We also show each components of our model have positive effect on performance. © 2019",Neurocomputing,10.1016/j.neucom.2019.07.110,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079892028&doi=10.1016%2fj.neucom.2019.07.110&partnerID=40&md5=38b1fe46c685cd5e7a9cab303eaeb8d7,2020,2021-07-20 15:50:05,2021-07-20 15:50:05
9XW6K5WN,journalArticle,2017,"Neverova, N.; Wolf, C.; Nebout, F.; Taylor, G.W.",Hand pose estimation through semi-supervised and weakly-supervised learning,"We propose a method for hand pose estimation based on a deep regressor trained on two different kinds of input. Raw depth data is fused with an intermediate representation in the form of a segmentation of the hand into parts. This intermediate representation contains important topological information and provides useful cues for reasoning about joint locations. The mapping from raw depth to segmentation maps is learned in a semi- and weakly-supervised way from two different datasets: (i) a synthetic dataset created through a rendering pipeline including densely labeled ground truth (pixelwise segmentations); and (ii) a dataset with real images for which ground truth joint positions are available, but not dense segmentations. Loss for training on real images is generated from a patch-wise restoration process, which aligns tentative segmentation maps with a large dictionary of synthetic poses. The underlying premise is that the domain shift between synthetic and real data is smaller in the intermediate representation, where labels carry geometric and topological meaning, than in the raw input domain. Experiments on the NYU dataset (Tompson et al., 2014b) show that the proposed training method decreases error on joints over direct regression of joints from depth data by 15.7%. © 2017 Elsevier Inc.",Computer Vision and Image Understanding,10.1016/j.cviu.2017.10.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032208535&doi=10.1016%2fj.cviu.2017.10.006&partnerID=40&md5=d29dc5d9148a8af1b9b9c5ac99f26cf0,2017,2021-07-20 15:50:05,2021-07-20 15:50:05
D2U5IN9F,journalArticle,2020,"Le Merrer, E.; Trédan, G.",Remote explainability faces the bouncer problem,"The concept of explainability is envisioned to satisfy society’s demands for transparency about machine learning decisions. The concept is simple: like humans, algorithms should explain the rationale behind their decisions so that their fairness can be assessed. Although this approach is promising in a local context (for example, the model creator explains it during debugging at the time of training), we argue that this reasoning cannot simply be transposed to a remote context, where a model trained by a service provider is only accessible to a user through a network and its application programming interface. This is problematic, as it constitutes precisely the target use case requiring transparency from a societal perspective. Through an analogy with a club bouncer (who may provide untruthful explanations upon customer rejection), we show that providing explanations cannot prevent a remote service from lying about the true reasons leading to its decisions. More precisely, we observe the impossibility of remote explainability for single explanations by constructing an attack on explanations that hides discriminatory features from the querying user. We provide an example implementation of this attack. We then show that the probability that an observer spots the attack, using several explanations for attempting to find incoherences, is low in practical settings. This undermines the very concept of remote explainability in general. © 2020, The Author(s), under exclusive licence to Springer Nature Limited.",Nature Machine Intelligence,10.1038/s42256-020-0216-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089737905&doi=10.1038%2fs42256-020-0216-z&partnerID=40&md5=1e8549e760a96ca24717e238147bf527,2020,2021-07-20 15:50:05,2021-07-20 15:50:05
YZD2NP68,journalArticle,2017,"Boudellioua, I.; Mahamad Razali, R.B.; Kulmanov, M.; Hashish, Y.; Bajic, V.B.; Goncalves-Serra, E.; Schoenmakers, N.; Gkoutos, G.V.; Schofield, P.N.; Hoehndorf, R.",Semantic prioritization of novel causative genomic variants,"Discriminating the causative disease variant(s) for individuals with inherited or de novo mutations presents one of the main challenges faced by the clinical genetics community today. Computational approaches for variant prioritization include machine learning methods utilizing a large number of features, including molecular information, interaction networks, or phenotypes. Here, we demonstrate the PhenomeNET Variant Predictor (PVP) system that exploits semantic technologies and automated reasoning over genotype-phenotype relations to filter and prioritize variants in whole exome and whole genome sequencing datasets. We demonstrate the performance of PVP in identifying causative variants on a large number of synthetic whole exome and whole genome sequences, covering a wide range of diseases and syndromes. In a retrospective study, we further illustrate the application of PVP for the interpretation of whole exome sequencing data in patients suffering from congenital hypothyroidism. We find that PVP accurately identifies causative variants in whole exome and whole genome sequencing datasets and provides a powerful resource for the discovery of causal variants. © 2017 Boudellioua et al.",PLoS Computational Biology,10.1371/journal.pcbi.1005500,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018348604&doi=10.1371%2fjournal.pcbi.1005500&partnerID=40&md5=5d5c886962abbf1f41b3cc4931b14f4a,2017,2021-07-20 15:50:05,2021-07-20 15:50:05
QUTS25F6,journalArticle,2015,"Riveret, R.; Korkinof, D.; Draief, M.; Pitt, J.",Probabilistic abstract argumentation: An investigation with Boltzmann machines,"Probabilistic argumentation and neuro-argumentative systems offer new computational perspectives for the theory and applications of argumentation, but their principled construction involves two entangled problems. On the one hand, probabilistic argumentation aims at combining the quantitative uncertainty addressed by probability theory with the qualitative uncertainty of argumentation, but probabilistic dependences amongst arguments as well as learning are usually neglected. On the other hand, neuro-argumentative systems offer the opportunity to couple the computational advantages of learning and massive parallel computation from neural networks with argumentative reasoning and explanatory abilities, but the relation of probabilistic argumentation frameworks with these systems has been ignored so far. Towards the construction of neuro-argumentative systems based on probabilistic argumentation, we associate a model of abstract argumentation and the graphical model of Boltzmann machines (BMs) in order to (i) account for probabilistic abstract argumentation with possible and unknown probabilistic dependences amongst arguments, (ii) learn distributions of labellings from a set of cases and (iii) sample labellings according to the learned distribution. Experiments on domain independent artificial datasets show that argumentative BMs can be trained with conventional training procedures and compare well with conventional machines for generating labellings of arguments, with the assurance of generating grounded labellings - on demand. © 2015 Taylor & Francis.",Argument and Computation,10.1080/19462166.2015.1107134,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951179979&doi=10.1080%2f19462166.2015.1107134&partnerID=40&md5=d3ae958bfa5a8d467cd3c1b838ac30c2,2015,2021-07-20 15:50:05,2021-07-20 15:50:05
8NM7C7KQ,journalArticle,2011,"Galitsky, B.; De La Rosa, J.-L.; Kovalerchuk, B.",Discovering common outcomes of agents' communicative actions in various domains,"We explore the common patterns of human behavior, expressed via communicative actions, and displayed in various domains of human activities associated with conflicts. We build the generic methodology based on machine learning and reasoning to predict specific communicative actions of human agents, given previous sequence of communicative actions of themselves and their opponents. This methodology is applied to textual as well as structured data on inter-human conflicts of diverse modalities. Scenarios are represented by directed graphs with labeled vertices (for communicative actions) and arcs (for temporal and causal relationships between subjects of these actions). Scenario representation and learning techniques are firstly developed in the domain of textual customer complaints, and then applied to such problems as predicting an outcome of international conflicts, assessment of an attitude of a security clearance candidate, mining emails for suspicious emotional profiles, and recognizing suspicious behavior of cell phone users. We present an evaluation of the proposed methodology in the domain of customer complaint and conduct some comparative evaluation in the other domains mentioned above. Successful use of the proposed methodology in rather distinct domains shows its adequacy for mining human attitude-related data in a wide range of applications. © 2010 Elsevier B.V. All rights reserved.",Knowledge-Based Systems,10.1016/j.knosys.2010.06.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-78650814122&doi=10.1016%2fj.knosys.2010.06.004&partnerID=40&md5=9a42d5751472ab8e1e680c83532e351d,2011,2021-07-20 15:50:05,2021-07-20 15:50:05
TH7FWCMJ,journalArticle,2020,"Li, Z.",E-commerce consumer behaviour perception based on FPGA and machine learning,"E-Commerce Consumer an extraordinary ground in the most recent couple of many years, on account of the beginning of innovation in every cross-segment. Individuals have begun tolerating any mechanical development at a lot quicker rate than any time in recent memory. Web invaded individuals' lives in various circles and even made changes in the manner in which the customer purchases the item. Electronic Commerce (internet business) can be one of the significant financial upsets in this century. More online clients currently become the 'purchaser' instead of just a 'window customer'. Field Programmable Gate Array (FPGA) first came into the picture at the beginning of PLD. This was a created rendition. After PLD'S is modifying and fabricating, it has restricted programmability. Field programmable doors can be actualized in any advanced circuit. This gives a comprehensive exhibit of engineers at the most reduced expense to make a coherent structure online security and absence of customer assurance are the significant worries for E-business food. As we move a more generous amount of our carries on with on the web, misrepresentation dangers follow intently behind. The clients are ignorant of the best approach to battle digital extortion. It is necessary to develop a model for defending the clients from the outcomes. A two-factor validation cycle might be utilized to make sure about the online exchange. It is where more than one secret word is needed to confirm the user. The onus is on the advertiser to grasp best practices and the most recent advances to keep one stride ahead to assemble purchaser certainty. Utilization of cutting edge validation measures or blended models like on the web and disconnected conveyance, installment through banks may help organizations cause the client to feel safe while purchasing items on the web. The business which can make a useful model to address this hazy situation will have a feasible development. © 2020",Microprocessors and Microsystems,10.1016/j.micpro.2020.103513,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85096873766&doi=10.1016%2fj.micpro.2020.103513&partnerID=40&md5=4076f86b52c58d4320054a939eb9956e,2020,2021-07-20 15:50:05,2021-07-20 15:50:05
G9TQ93MT,journalArticle,2020,"Liu, Q.; Ma, G.; Cheng, C.",Data Fusion Generative Adversarial Network for Multi-Class Imbalanced Fault Diagnosis of Rotating Machinery,"For the fault diagnosis problems of rotating machinery in the real industrial practice, measurement data with imbalanced class distributions negatively affect the diagnostic performance of most conventional machine learning classification algorithms since equal cost weights are assigned to different fault classes. Meanwhile, the widely used traditional data generation methods for the imbalanced data problem are limited by data dependencies over time continuity. To fill this research gap, this paper develops a new diagnostic framework based on the adversarial neural networks (GAN) and multi-sensor data fusion technique to generate new synthetic data for data compensation purpose. Two different practice modes are designed based on this framework according to the position logic of the data fusion, namely a Pre-fusion GAN mode and a Post-fusion GAN mode. More concretely, without data pre-processing, the designed generator generates synthetic data to puzzle the discriminator and the synthetic data that out-trick the discriminator can be used to compensate the minor class. To avoid data dependency and to ensure the generality of the proposed framework, the network modelling are trained with a more practical approach where the training and test data are obtained under different rotating speeds. Two imbalanced data sets on the rotating machinery, one benchmark public rolling bearing data set and another gear box data set acquired in our lab, are used to validate the proposed method. The performance is examined through a wide range of data imbalanced ratios (as high as 30:1), and compared with other state-of-the-art methods. The experiment results conclude that the proposed Pre-fusion GAN and Post-fusion GAN frameworks both have good performance on the imbalanced fault diagnosis of rotating machinery. © 2020 IEEE.",IEEE Access,10.1109/ACCESS.2020.2986356,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083992678&doi=10.1109%2fACCESS.2020.2986356&partnerID=40&md5=cccb6f6a4050493f21d862973071acc7,2020,2021-07-20 15:50:05,2021-07-20 15:50:05
AZAN8LE2,journalArticle,2018,"Gu, X.; Yang, H.; Tang, J.; Zhang, J.; Zhang, F.; Liu, D.; Hall, W.; Fu, X.",Profiling Web users using big data,"Profiling Web users is a fundamental issue for Web mining and social network analysis. Its basic tasks include extracting basic information, mining user preferences, and inferring user demographics (Tang et al. in ACM Trans Knowl Discov Data 5(1):2:1–2:44, 2010). Although methodologies for handling the three tasks are different, they all usually contain two stages: first identify relevant pages (data) of a user and then use machine learning models (e.g., SVM, CRFs, or DL) to extract/mine/infer profile attributes from each page. The methods were successful in the traditional Web, but are facing more and more challenges with the rapid evolution of the Web each persons information is distributed over the Web and is changing dynamically. As a result, available data for a user on the Web is redundant, and some sources may be out-of-date or incorrect. The traditional two-stage method suffers from data inconsistency and error propagation between the two stages. In this paper, we revisit the problem of Web user profiling in the big data era and propose a simple but very effective approach, referred to as MagicFG, for profiling Web users by leveraging the power of big data. To avoid error propagation, the approach processes all the extracting/mining/inferring subtasks in one unified framework. To improve the profiling performance, we present the concept of contextual credibility. The proposed framework also supports the incorporation of human knowledge. It defines human knowledge as Markov logics statements and formalizes them into a factor graph model. The MagicFG method has been deployed in an online system AMiner.org for profiling millions of researchers—e.g., extracting E-mail, inferring Gender, and mining research interests. Our empirical study in the real system shows that the proposed method offers significantly improved (+ 4–6%; p≪ 0.01 , t test) profiling performance in comparison with several baseline methods using rules, classification, and sequential labeling. © 2018, Springer-Verlag GmbH Austria, part of Springer Nature.",Social Network Analysis and Mining,10.1007/s13278-018-0495-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044269142&doi=10.1007%2fs13278-018-0495-0&partnerID=40&md5=1dc52c711c015a8e5616e2ebc71fc111,2018,2021-07-20 15:50:05,2021-07-20 15:50:05
GPRAUVD6,journalArticle,2016,"Adnan, M.N.; Islam, M.Z.",Optimizing the number of trees in a decision forest to discover a subforest with high ensemble accuracy using a genetic algorithm,"A decision forest is an ensemble of decision trees, and it is often built to discover more patterns (i.e. logic rules) and predict/classify class values more accurately than a single decision tree. Existing decision forest algorithms are typically used for building huge numbers of decision trees, involving large memory and computational overhead, in order to achieve high accuracy. Generally, many of the trees do not contribute to improving the ensemble accuracy of a forest. As a result, ensemble pruning algorithms aim to get rid of those trees while generating a subforest in order to achieve higher (or comparable) ensemble accuracy than the original forest. The objectives are two fold: select as small number of trees as possible, and maintain the ensemble accuracy of the subforest as high as possible. An optimal subforest can be found by exhaustive search; however it is not practical for any standard-sized forest as the number of candidate subforests grows exponentially. In order to avoid the computational burden of an exhaustive search, many greedy and genetic algorithm-based subforest selection techniques have been proposed in literature. In this paper, we propose a subforest selection technique that achieves small size as well as high accuracy. We use a genetic algorithm where we carefully select high quality individual trees for the initial population of the genetic algorithm in order to improve the final output of the algorithm. Experiments are conducted on 20 data sets from the UCI Machine Learning Repository to compare the proposed technique with several existing state-of-the-art techniques. The results indicate that the proposed technique can select effective subforests which are significantly smaller than original forests while achieving better (or comparable) accuracy than the original forests. © 2016 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2016.07.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84995484917&doi=10.1016%2fj.knosys.2016.07.016&partnerID=40&md5=76e48cf397bac99cae7d425bfaf9a26d,2016,2021-07-20 15:50:05,2021-07-20 15:50:05
5LCSZS5H,journalArticle,2014,"Roychowdhury, S.; Koozekanani, D.D.; Parhi, K.K.",DREAM: Diabetic Retinopathy Analysis Using Machine Learning,"This paper presents a computer-aided screening system (DREAM) that analyzes fundus images with varying illumination and fields of view, and generates a severity grade for diabetic retinopathy (DR) using machine learning. Classifiers such as the Gaussian Mixture model (GMM), k-nearest neighbor (kNN), support vector machine (SVM), and AdaBoost are analyzed for classifying retinopathy lesions from nonlesions. GMM and kNN classifiers are found to be the best classifiers for bright and red lesion classification, respectively. A main contribution of this paper is the reduction in the number of features used for lesion classification by feature ranking using Adaboost where 30 top features are selected out of 78. A novel two-step hierarchical classification approach is proposed where the nonlesions or false positives are rejected in the first step. In the second step, the bright lesions are classified as hard exudates and cotton wool spots, and the red lesions are classified as hemorrhages and micro-aneurysms. This lesion classification problem deals with unbalanced datasets and SVM or combination classifiers derived from SVM using the Dempster-Shafer theory are found to incur more classification error than the GMM and kNN classifiers due to the data imbalance. The DR severity grading system is tested on 1200 images from the publicly available MESSIDOR dataset. The DREAM system achieves 100% sensitivity, 53.16% specificity, and 0.904 AUC, compared to the best reported 96% sensitivity, 51% specificity, and 0.875 AUC, for classifying images as with or without DR. The feature reduction further reduces the average computation time for DR severity per image from 59.54 to 3.46 s. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2013.2294635,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84927171189&doi=10.1109%2fJBHI.2013.2294635&partnerID=40&md5=1bd546d6227a9dfc6487a7b1983e2e26,2014,2021-07-20 15:50:05,2021-07-20 15:50:05
FLDDJWHF,journalArticle,2012,"Nanni, L.; Lumini, A.; Brahnam, S.",A classifier ensemble approach for the missing feature problem,"Objectives: Many classification problems must deal with data that contains missing values. In such cases data imputation is critical. This paper evaluates the performance of several statistical and machine learning imputation methods, including our novel multiple imputation ensemble approach, using different datasets. Materials and methods: Several state-of-the-art approaches are compared using different datasets. Some state-of-the-art classifiers (including support vector machines and input decimated ensembles) are tested with several imputation methods. The novel approach proposed in this work is a multiple imputation method based on random subspace, where each missing value is calculated considering a different cluster of the data. We have used a fuzzy clustering approach for the clustering algorithm. Results: Our experiments have shown that the proposed multiple imputation approach based on clustering and a random subspace classifier outperforms several other state-of-the-art approaches. Using the Wilcoxon signed-rank test (reject the null hypothesis, level of significance 0.05) we have shown that the proposed best approach is outperformed by the classifier trained using the original data (i.e., without missing values) only when >20% of the data are missed. Moreover, we have shown that coupling an imputation method with our cluster based imputation we outperform the base method (level of significance ∼0.05). Conclusion: Starting from the assumptions that the feature set must be partially redundant and that the redundancy is distributed randomly over the feature set, we have proposed a method that works quite well even when a large percentage of the features is missing (≥30%). Our best approach is available (MATLAB code) at bias.csr.unibo.it/nanni/MI.rar. © 2011 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2011.11.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858866778&doi=10.1016%2fj.artmed.2011.11.006&partnerID=40&md5=fa1ad24455614652f846cf9a412b0552,2012,2021-07-20 15:50:05,2021-07-20 15:50:05
EIQ6E9WS,journalArticle,2021,"Nkabiti, K.P.; Chen, Y.",Application of solely self-attention mechanism in CSI-fingerprinting-based indoor localization,"With the growth of IoT services, there has been an increased demand for indoor localization-based services. Wi-Fi access is omnipresent; its high accuracy and ability to use commodity devices makes it suitable to be widely adopted for localization in indoor environments. Recent sequence transduction models such as recurrent neural network (RNN) and long short-term memory (LSTM) mostly rely on recurrent and convolution. Both LSTM and RNN have achieved tremendous results in the localization tasks, but their sequential character prevents them from effectively performing parallel computing, therefore, limiting their performance in processing extremely long sequences. Lately, there have been different models developed for natural language processing transduction tasks that relied solely on attention mechanism, and they performed remarkably well with less computation. This paper is the first to propose the sole utilization of self-attention mechanism for localization time-series modeling. We introduce a self-attention mechanism fingerprinting-based model (SAMFI) which uses positional encoding and masking mechanism. To capture the temporal ordering information, we used the extended symbolic aggregate approximation strategy. Moreover, the proposed model utilizes calibrated channel state information as location fingerprints. SAMFI's pivotal concept is simple and empirically potent. The obtained results significantly minimized the location error on the collected dataset with an accuracy level score of 86.5% outperforming both RNN and LSTM models which scored 82.6 and 67.5%, respectively. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-020-05681-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100007170&doi=10.1007%2fs00521-020-05681-1&partnerID=40&md5=b70fbadf36d0c9fc4228769af0cb04a7,2021,2021-07-20 15:50:06,2021-07-20 15:50:06
BXRGTUWS,journalArticle,2020,"Nakamura, E.; Saito, Y.; Yoshii, K.",Statistical learning and estimation of piano fingering,"Automatic estimation of piano fingering is important for understanding the computational process of music performance and applicable to performance assistance and education systems. While a natural way to formulate the quality of fingerings is to construct models of the constraints/costs of performance, it is generally difficult to find appropriate parameter values for these models. Here we study an alternative data-driven approach based on statistical modeling in which the appropriateness of a given fingering is described by probabilities. Specifically, we construct two types of hidden Markov models (HMMs) and their higher-order extensions. We also study deep neural network (DNN)-based methods for comparison. Using a newly released dataset of fingering annotations, we conduct systematic evaluations of these models as well as a representative constraint-based method. We find that the methods based on high-order HMMs outperform the other methods in terms of estimation accuracies. We also quantitatively study individual difference of fingering and propose evaluation measures that can be used with multiple ground truth data. We conclude that the HMM-based methods are currently state of the art and generate acceptable fingerings in most parts and that they have certain limitations such as ignorance of phrase boundaries and interdependence of the two hands. © 2019 Elsevier Inc.",Information Sciences,10.1016/j.ins.2019.12.068,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85077500909&doi=10.1016%2fj.ins.2019.12.068&partnerID=40&md5=99d3aaa9738096d64c793bd3f33ccee8,2020,2021-07-20 15:50:06,2021-07-20 15:50:06
XHS6DITL,journalArticle,2019,"Zhang, X.; Feng, C.; Li, R.; Lei, J.; Tang, C.",Marking Key Segment of Program Input via Attention Mechanism,"Key segment of a program input is the specific part of the input that has significant affect on the execution of target function. Marking key segment plays an important role in software security analysis. Traditional dynamic analysis methods can not mark the key segments correctly because of control flow dependency problem. The root cause of such problem is that implicit flow analysis method cannot cover all the behavior of the code fragment in a branch, especially when the code snippet contains unexpected jump behavior. The neural network can learn to fit the behavior of the program with proper training data. In this paper, we introduce the attention based neural network to mark the key segments of program input accurately and efficiently. We propose an attention based two-parts network structure and map program inputs into the target code execution by such network. Then we propose a two-step training method to train our network to calculate the importance of each input component on the execution of target function. Finally, we mark the key segments by statistical analysis method. We implement such method and develop a key segment marking tool AttentionMark. Experiments on four real-world software show that AttentionMark outperforms NeuralTaint and traditional dynamic analysis tool in key segment marking. © 2019 IEEE.",IEEE Access,10.1109/ACCESS.2019.2960522,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078020676&doi=10.1109%2fACCESS.2019.2960522&partnerID=40&md5=5552b3f5577dc2dafe3745033cc0ae4e,2019,2021-07-20 15:50:06,2021-07-20 15:50:06
X6CLBQQP,journalArticle,2019,"Huang, M.-L.; Chou, Y.-C.","Combining a gravitational search algorithm, particle swarm optimization, and fuzzy rules to improve the classification performance of a feed-forward neural network","Background and objective: A feed-forward neural network (FNN) is a type of artificial neural network that has been widely used in medical diagnosis, data mining, stock market analysis, and other fields. Many studies have used FNN to develop medical decision-making systems to assist doctors in clinical diagnosis. The aim of the learning process in FNN is to find the best combination of connection weights and biases to achieve the minimum error. However, in many cases, FNNs converge to the local optimum but not the global optimum. Using open disease datasets, the purpose of this study was to optimize the connection weights and biases of the FNN to minimize the error and improve the accuracy of disease diagnosis. Method: In this study, the chronic kidney disease (CKD) and mesothelioma (MES) disease datasets from the University of California Irvine (UCI) machine learning repository were used as research objects. This study applied the FNN to learn the features of each datum and used particle swarm optimization (PSO) and a gravitational search algorithm (GSA) to optimize the weights and biases of the FNN classifiers based on the algorithms inspired by the observation of natural phenomena. Moreover, fuzzy rules were used to optimize the parameters of the GSA to improve the performance of the algorithm in the classifier. Results: When applied to the CKD dataset, the accuracies of PSO and GSA were 99%. By using fuzzy rules to optimize the GSA parameter, the accuracy of fuzzy–GSA was 99.25%. The accuracies of the combined algorithms PSO–GSA and fuzzy–PSO–GSA reached 100%. In the MES disease dataset, all methods exhibited good performance with 100% accuracy. Conclusions: This study used PSO, GSA, fuzzy–GSA, PSO–GSA, and fuzzy–PSO–GSA on CKD and MES disease datasets to identify the disease, and the performance of different algorithms was explored. Compared with other methods in the literature, our proposed method achieved higher accuracy © 2019",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2019.105016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070874970&doi=10.1016%2fj.cmpb.2019.105016&partnerID=40&md5=7689551dd14832ba798c1a236be00daa,2019,2021-07-20 15:50:06,2021-07-20 15:50:06
P987FQWL,journalArticle,2021,"Wei, Q.; Ma, H.; Chen, C.; Dong, D.",Deep Reinforcement Learning With Quantum-Inspired Experience Replay,"In this article, a novel training paradigm inspired by quantum computation is proposed for deep reinforcement learning (DRL) with experience replay. In contrast to the traditional experience replay mechanism in DRL, the proposed DRL with quantum-inspired experience replay (DRL-QER) adaptively chooses experiences from the replay buffer according to the complexity and the replayed times of each experience (also called transition), to achieve a balance between exploration and exploitation. In DRL-QER, transitions are first formulated in quantum representations and then the preparation operation and depreciation operation are performed on the transitions. In this process, the preparation operation reflects the relationship between the temporal-difference errors (TD-errors) and the importance of the experiences, while the depreciation operation is taken into account to ensure the diversity of the transitions. The experimental results on Atari 2600 games show that DRL-QER outperforms state-of-the-art algorithms, such as DRL-PER and DCRL on most of these games with improved training efficiency and is also applicable to such memory-based DRL approaches as double network and dueling network. IEEE",IEEE Transactions on Cybernetics,10.1109/TCYB.2021.3053414,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101773065&doi=10.1109%2fTCYB.2021.3053414&partnerID=40&md5=bae3bc66ef2b92d3e5fbc23c815f763d,2021,2021-07-20 15:50:06,2021-07-20 15:50:06
T7JCAMNY,journalArticle,2019,"Mohammadzadeh, A.; Kayacan, E.",A non-singleton type-2 fuzzy neural network with adaptive secondary membership for high dimensional applications,"This paper develops a non-singleton type-2 fuzzy neural network (NT2FNN) with type-2 3-dimensional membership functions (MFs) and adaptive secondary membership. A new approach based on the square-root cubature quadrature Kalman filter (SR-CQKF) is proposed for the training the level of the secondary membership and the centers of membership functions. The consequent parameters are learned by using rule-ordered extended Kalman filter (EKF). To show the applicability and effectiveness of proposed NT2FNN in high dimensional problems, four real-world datasets with 4, 7, 13 and 32 input variables are considered. Additionally, the performance of NT2FNN with the proposed learning algorithm is compared with other well-known neural networks and learning algorithms. The simulations demonstrate that the developed method results in high performance in contrast to the other methods. © 2019 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2019.01.095,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85061443724&doi=10.1016%2fj.neucom.2019.01.095&partnerID=40&md5=8029dccce34588954f95fac7f433ffbc,2019,2021-07-20 15:50:06,2021-07-20 15:50:06
QCEHCMKX,journalArticle,2018,"Li, J.; Liang, X.; Shen, S.; Xu, T.; Feng, J.; Yan, S.",Scale-Aware Fast R-CNN for Pedestrian Detection,"In this paper, we consider the problem of pedestrian detection in natural scenes. Intuitively, instances of pedestrians with different spatial scales may exhibit dramatically different features. Thus, large variance in instance scales, which results in undesirable large intracategory variance in features, may severely hurt the performance of modern object instance detection methods. We argue that this issue can be substantially alleviated by the divide-and-conquer philosophy. Taking pedestrian detection as an example, we illustrate how we can leverage this philosophy to develop a Scale-Aware Fast R-CNN (SAF R-CNN) framework. The model introduces multiple built-in subnetworks which detect pedestrians with scales from disjoint ranges. Outputs from all of the subnetworks are then adaptively combined to generate the final detection results that are shown to be robust to large variance in instance scales, via a gate function defined over the sizes of object proposals. Extensive evaluations on several challenging pedestrian detection datasets well demonstrate the effectiveness of the proposed SAF R-CNN. Particularly, our method achieves state-of-the-art performance on Caltech, and obtains competitive results on INRIA, ETH, and KITTI. © 2017 IEEE.",IEEE Transactions on Multimedia,10.1109/TMM.2017.2759508,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031821695&doi=10.1109%2fTMM.2017.2759508&partnerID=40&md5=dafd6cf243ac72ab229b750911be87cd,2018,2021-07-20 15:50:06,2021-07-20 15:50:06
GU6BT98Z,journalArticle,2020,"Owen, C.A.; Dick, G.; Whigham, P.A.",Characterizing Genetic Programming Error through Extended Bias and Variance Decomposition,"An error function can be used to select between candidate models but it does not provide a thorough understanding of the behavior of a model. A greater understanding of an algorithm can be obtained by performing a bias-variance decomposition. Splitting the error into bias and variance is effective for understanding a deterministic algorithm such as k -nearest neighbor, which provides the same predictions when performed multiple times using the same data. However, simply splitting the error into bias and variance is not sufficient for nondeterministic algorithms, such as genetic programming (GP), which potentially produces a different model each time it is run, even when using the same data. This article presents an extended bias-variance decomposition that decomposes error into bias, external variance (error attributable to limited sampling of the problem), and internal variance (error due to random actions performed in the algorithm itself). This decomposition is applied to GP to expose the three components of error, providing a unique insight into the role of maximum tree depth, number of generations, size/complexity of function set, and data standardization in influencing predictive performance. The proposed tool can be used to inform targeted improvements for reducing specific components of model error. © 1997-2012 IEEE.",IEEE Transactions on Evolutionary Computation,10.1109/TEVC.2020.2990626,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084222766&doi=10.1109%2fTEVC.2020.2990626&partnerID=40&md5=17b9ea6c7555daa5826d3246009045d8,2020,2021-07-20 15:50:06,2021-07-20 15:50:06
WSE3XXVP,journalArticle,2020,"Al-Roomi, A.R.; El-Hawary, M.E.",Universal Functions Originator,"Nowadays, couples of computing systems have been introduced to perform many applications, such as function approximation, pattern classification, categorization/clustering, forecasting/prediction, control, and optimization. Linear regression (LR) is commonly used for simple data where the relation between its coefficients is linear, while nonlinear regression (NLR) is used when that relation is nonlinear. Artificial neural networks (ANNs) and support vector machines (SVMs) are more efficient and they can be used for complex applications. However, each one of these approaches has its own strengths and weaknesses. This study introduces a new computing system called “universal functions originator (UFO)”. This system is a new symbolic regression (SR) technique that can generate mathematical models universally through two independent optimization algorithms. Different arithmetic operators can be entered into the search pool. Also, any analytic function can be dragged into that pool. UFO has been mathematically designed and practically tested with function approximation problems. However, UFO can also be used for the applications listed above, including anomaly detection, function complication, function simplification, dimension expansion, dimension reduction, and high-dimensional function visualization. This novel computing system shows an impressive performance with many promising uses and distinct capabilities. This study reveals the mechanism of UFO and solves some numerical problems via an advanced graphical user interface (GUI) designed just to validate the process of this computing system. © 2020 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2020.106417,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085638651&doi=10.1016%2fj.asoc.2020.106417&partnerID=40&md5=70faf69ea24539d0556c267f803961ff,2020,2021-07-20 15:50:06,2021-07-20 15:50:06
WYIKUZZA,journalArticle,2021,"Gonzalez Fabre, R.; Camacho Ibáñez, J.; Tejedor Escobar, P.",Moral control and ownership in AI systems,"AI systems are bringing an augmentation of human capabilities to shape the world. They may also drag a replacement of human conscience in large chunks of life. AI systems can be designed to leave moral control in human hands, to obstruct or diminish that moral control, or even to prevent it, replacing human morality with pre-packaged or developed ‘solutions’ by the ‘intelligent’ machine itself. Artificial Intelligent systems (AIS) are increasingly being used in multiple applications and receiving more attention from the public and private organisations. The purpose of this article is to offer a mapping of the technological architectures that support AIS, under the specific focus of the moral agency. Through a literature research and reflection process, the following areas are covered: a brief introduction and review of the literature on the topic of moral agency; an analysis using the BDI logic model (Bratman 1987); an elemental review of artificial ‘reasoning’ architectures in AIS; the influence of the data input and the data quality; AI systems’ positioning in decision support and decision making scenarios; and finally, some conclusions are offered about regarding the potential loss of moral control by humans due to AIS. This article contributes to the field of Ethics and Artificial Intelligence by providing a discussion for developers and researchers to understand how and under what circumstances the ‘human subject’ may, totally or partially, lose moral control and ownership over AI technologies. The topic is relevant because AIS often are not single machines but complex networks of machines that feed information and decisions into each other and to human operators. The detailed traceability of input-process-output at each node of the network is essential for it to remain within the field of moral agency. Moral agency is then at the basis of our system of legal responsibility, and social approval is unlikely to be obtained for entrusting important functions to complex systems under which no moral agency can be identified. © 2020, Springer-Verlag London Ltd., part of Springer Nature.",AI and Society,10.1007/s00146-020-01020-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088369854&doi=10.1007%2fs00146-020-01020-z&partnerID=40&md5=4a8bbb520786e63b85284f6509966331,2021,2021-07-20 15:50:06,2021-07-20 15:50:06
44ZGXW2N,journalArticle,2014,"Nilashi, M.; Ibrahim, O.B.; Ithnin, N.",Multi-criteria collaborative filtering with high accuracy using higher order singular value decomposition and Neuro-Fuzzy system,"Collaborative Filtering (CF) is the most widely used prediction technique in recommender systems. It makes recommendations based on ratings that users have assigned to items. Most of the current CF recommender systems maintain only single user ratings inside the user-item ratings matrix. Multi-criteria based CF presents a possibility of providing accurate recommendations by considering the user preferences in multi aspects of items. However, in the multi-criteria CF, the user behavior about items' features is frequently subjective, imprecise and vague. These in turn induce uncertainty in reasoning and representation of items' features that exactly cannot be solved using crisp machine learning techniques. In contrast, approaches such as fuzzy methods instead of crisp methods can better solve the issue of uncertainty. In addition, fuzzy methods can predict the users' preference more accurately and even better alleviate the sparsity problem in overall rating by considering user perception about items' features. Apart from this, in the multi-criteria CF, users provide the ratings on different aspects (criteria) of an item in new dimensions; thereby, increasing the scalability problem. Appropriate dimensionality reduction techniques are thus needed to capture the high dimensions all together without reducing them into lower dimensions to reveal the latent associations among the components. This study presents a new model for multi-criteria CF using Adaptive Neuro-Fuzzy Inference System (ANFIS) combined with subtractive clustering and Higher Order Singular Value Decomposition (HOSVD). HOSVD is used for dimensionality reduction for improving the scalability problem and ANFIS is used for extracting fuzzy rules from the experimental dataset, alleviating the sparsity problems in overall ratings and representing and reasoning the users' behavior on items' features. Experimental results on real-world dataset show that combination of two techniques remarkably improves the predictive accuracy and recommendation quality of multi-criteria CF. © 2014 Elsevier B.V. All rights reserved.",Knowledge-Based Systems,10.1016/j.knosys.2014.01.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896388776&doi=10.1016%2fj.knosys.2014.01.006&partnerID=40&md5=2b127c76c46f9598f57a110fee13ff32,2014,2021-07-20 15:50:06,2021-07-20 15:50:06
VDPJ7TY5,journalArticle,2021,"Jiang, S.; Zhang, Z.; Zhao, H.; Li, J.; Yang, Y.; Lu, B.; Xia, N.","When SMILES smiles, Practicality Judgment and Yield Prediction of Chemical Reaction via Deep Chemical Language Processing","Simplified Molecular Input Line Entry System (SMILES) provides a text-based encoding method to describe the structure of chemical species and formulize general chemical reactions. Considering that chemical reactions have been represented in a language form, we present a symbol only model to generally predict the yield of organic synthesis reaction without considering complex quantum physical modeling or chemistry knowledge. Our model is the first deep neural network application that treats chemical reaction text segments as embedding representation to the most recent deep natural language processing. Experimental results show our model can effectively predict chemical reactions, which achieves a high accuracy of 99.76% on practicality judgment and the Root Mean Square Error (RMSE) is around 0.2 for yield prediction. Our work shows the great potential for automatic yield prediction for organic reactions under general conditions and further applications in synthesis path prediction with the least modeling cost. CCBY",IEEE Access,10.1109/ACCESS.2021.3083838,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107224551&doi=10.1109%2fACCESS.2021.3083838&partnerID=40&md5=c0a7b6e59cb35edf1c9a50e7bf9b62bd,2021,2021-07-20 15:50:06,2021-07-20 15:50:06
NGH8N47M,journalArticle,2021,"Ho, S.Y.; Tan, S.; Sze, C.C.; Wong, L.; Goh, W.W.B.",What can Venn diagrams teach us about doing data science better?,"Data science is about deriving insight, learning and understanding from data. This process may be automated via the use of advanced algorithms or scaffolded cognitively via the use of graphs. While much emphasis is currently placed on machine learning, there is still much to learn about the role of the data scientist, in particular the thinking process by which he reaches conclusions. The thinking process of the data scientist needs to be scaffolded as the human brain is easily overwhelmed by many variables. Graphs are a form of data abstraction and constitute an essential part of the data scientist’s toolkit. Graphs are also a viable scaffold on which the data scientist may gain familiarity with data. But the process of extracting insight from graphs is not always a trivial or straightforward process; it requires interpretative logic as well. Generalizing from the example of a simple graph type, the Venn diagram, we discuss various logical fallacies that can be committed when interpreting a Venn diagram. Amidst various considerations that dictate how a graph should be tackled, we explain why context is most important, and should form the first guiding principle during data analysis. © 2020, Springer Nature Switzerland AG.",International Journal of Data Science and Analytics,10.1007/s41060-020-00230-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088866741&doi=10.1007%2fs41060-020-00230-4&partnerID=40&md5=a4d1fcae52689a3d7d3bd11539db7cb7,2021,2021-07-20 15:50:06,2021-07-20 15:50:06
GFGY62WW,journalArticle,2020,"Mustafa, H.I.; Tantawy, O.A.",A new approach of attribute reduction of rough sets based on soft metric,"Attribute reduction is considered as an important processing step for pattern recognition, machine learning and data mining. In this paper, we combine soft set and rough set to use them in applications. We generalize rough set model and introduce a soft metric rough set model to deal with the problem of heterogeneous numerical feature subset selection. We construct a soft metric on the family of knowledge structures based on the soft distance between attributes. The proposed model will degrade to the classical one if we specify a zero soft real number. We also provide a systematic study of attribute reduction of rough sets based on soft metric. Based on the constructed metric, we define co-information systems and consistent co-decision systems, and we provide a new method of attribute reductions of each system. Furthermore, we present a judgement theorem and discernibility matrix associated with attribute of each type of system. As an application, we present a case study from Zoo data set to verify our theoretical results. © 2020 - IOS Press and the authors. All rights reserved.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-200457,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093356941&doi=10.3233%2fJIFS-200457&partnerID=40&md5=4e1f6ef9dcc6cf9fb04f7dd7b38be8e7,2020,2021-07-20 15:50:06,2021-07-20 15:50:06
BZSNFMDB,journalArticle,2020,"Al-Khater, W.A.; Al-Maadeed, S.; Ahmed, A.A.; Sadiq, A.S.; Khan, M.K.",Comprehensive review of cybercrime detection techniques,"Cybercrimes are cases of indictable offences and misdemeanors that involve computers or communication tools as targets and commission instruments or are associated with the prevalence of computer technology. Common forms of cybercrimes are child pornography, cyberstalking, identity theft, cyber laundering, credit card theft, cyber terrorism, drug sale, data leakage, sexually explicit content, phishing, and other forms of cyber hacking. They mostly lead to a privacy breach, security violation, business loss, financial fraud, or damage in public and government properties. Thus, this study intensively reviews cybercrime detection and prevention techniques. It first explores the different types of cybercrimes and discusses their threats against privacy and security in computer systems. Then, it describes the strategies that cybercriminals may utilize in committing these crimes against individuals, organizations, and societies. It also reviews the existing techniques of cybercrime detection and prevention. It objectively discusses the strengths and critically analyzes the vulnerabilities of each technique. Finally, it provides recommendations for the development of a cybercrime detection model that can detect cybercrimes effectively compared with the existing techniques. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2020.3011259,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090389950&doi=10.1109%2fACCESS.2020.3011259&partnerID=40&md5=baee0293a44845c181af3943eec815aa,2020,2021-07-20 15:50:06,2021-07-20 15:50:06
D7DSECJI,journalArticle,2020,"Hackman, K.O.; Li, X.; Asenso-Gyambibi, D.; Asamoah, E.A.; Nelson, I.D.",Analysis of geo-spatiotemporal data using machine learning algorithms and reliability enhancement for urbanization decision support,"We present systematic analyses of the temporal dynamics of the growth of Kumasi, the fastest growing city in Ghana using 20-year Landsat time-series data from 2000 to 2020 (with 1986 Landsat image as a baseline). Two classification algorithms–random forest (RF) and support vector machines (SVM)–were used to produce binary (built-up / non-built up) maps for all years within the temporal span. We further implemented an anomaly detection and temporal consistency algorithm followed by a changing logic to correct the classification anomalies due to image contamination from the cloud and other sources. The mean overall accuracies obtained for RF and SVM were 94.9% (kappa = 0.90) and 95.5% (kappa = 0.91), respectively. Our results reveal that the mean built-up area percentages of the metropolis are approximately 74, 65, 47, and 23 for the years 2020, 2010, 2000, and 1986, respectively, representing a mean annual change of 3.5% over the 34 years. With the present lack of labeled data in Ghana for in-depth analyses of the evolution of land use, we believe that this study serves as an initial attempt to a better understanding of the effects of increasing anthropogenic activities due to urbanization, on human and environment health. © 2020 Informa UK Limited, trading as Taylor & Francis Group.",International Journal of Digital Earth,10.1080/17538947.2020.1805036,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089257769&doi=10.1080%2f17538947.2020.1805036&partnerID=40&md5=c0c8c3b67dfd4ab2d2ccb0d8caee35e6,2020,2021-07-20 15:50:06,2021-07-20 15:50:06
L38UB3AT,journalArticle,2019,"Kim, M.S.; Barrio, A.A.D.; Oliveira, L.T.; Hermida, R.; Bagherzadeh, N.",Efficient mitchell's approximate log multipliers for convolutional neural networks,"This paper proposes energy-efficient approximate multipliers based on the Mitchell's log multiplication, optimized for performing inferences on convolutional neural networks (CNN). Various design techniques are applied to the log multiplier, including a fully-parallel LOD, efficient shift amount calculation, and exact zero computation. Additionally, the truncation of the operands is studied to create the customizable log multiplier that further reduces energy consumption. The paper also proposes using the one's complements to handle negative numbers, as an approximation of the two's complements that had been used in the prior works. The viability of the proposed designs is supported by the detailed formal analysis as well as the experimental results on CNNs. The experiments also provide insights into the effect of approximate multiplication in CNNs, identifying the importance of minimizing the range of error.The proposed customizable design at ww = 8 saves up to 88 percent energy compared to the exact fixed-point multiplier at 32 bits with just a performance degradation of 0.2 percent for the ImageNet ILSVRC2012 dataset. © 1968-2012 IEEE.",IEEE Transactions on Computers,10.1109/TC.2018.2880742,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056341338&doi=10.1109%2fTC.2018.2880742&partnerID=40&md5=cd4427017a46408071b5fa5993503891,2019,2021-07-20 15:50:07,2021-07-20 15:50:07
TERZI49U,journalArticle,2018,"Cui, Q.; El-Arroudi, K.; Joos, G.",Islanding detection of hybrid distributed generation under reduced non-detection zone,"Future distribution systems are faced with more challenges on islanding detection due to the increasing penetration level of inverter-based distributed generators (DGs). Different DG technologies, inverter control as well as other advanced inverter functions, such as fault ride through are challenging the capability of islanding detection schemes. On the other hand, for multiple feeders network, topological change of feeders and islanding at adjacent feeder increase the vulnerability of islanding detection devices. Furthermore, available islanding detection schemes are suffering from notable non-detection zones (NDZs) under reduced power mismatches. Therefore, to mitigate these issues, this paper proposes an effective methodology for building decision trees-based intelligent relay (IR). This methodology utilizes the NDZ boundaries of existing standard relays and applies a comprehensive training/testing strategy, which effectively reduces the NDZ while maintaining a superior dependability and security performance. To validate the applicability of the proposed methodology, the hardware-in-the-loop simulations are realized by programming the generated IR logics into a real commercial relay. © 2010-2012 IEEE.",IEEE Transactions on Smart Grid,10.1109/TSG.2017.2679101,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052731934&doi=10.1109%2fTSG.2017.2679101&partnerID=40&md5=b126d524eca0bbc59c1deffa66c0f5fc,2018,2021-07-20 15:50:07,2021-07-20 15:50:07
2EL8RYZ6,journalArticle,2018,"Lakshmi, C.; Thenmozhi, K.; Rayappan, J.B.B.; Amirtharajan, R.",Encryption and watermark-treated medical image against hacking disease—An immune convention in spatial and frequency domains,"Digital Imaging and Communications in Medicine (DICOM) is one among the significant formats used worldwide for the representation of medical images. Undoubtedly, medical-image security plays a crucial role in telemedicine applications. Merging encryption and watermarking in medical-image protection paves the way for enhancing the authentication and safer transmission over open channels. In this context, the present work on DICOM image encryption has employed a fuzzy chaotic map for encryption and the Discrete Wavelet Transform (DWT) for watermarking. The proposed approach overcomes the limitation of the Arnold transform—one of the most utilised confusion mechanisms in image ciphering. Various metrics have substantiated the effectiveness of the proposed medical-image encryption algorithm. © 2018 Elsevier B.V.",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2018.02.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042686523&doi=10.1016%2fj.cmpb.2018.02.021&partnerID=40&md5=86221b6643b6b8a4e0ff455e7a0355b2,2018,2021-07-20 15:50:07,2021-07-20 15:50:07
4368KZP9,journalArticle,2018,"Srivastava, A.K.; Kumar, S.",An effective computational technique for taxonomic position of security vulnerability in software development,"An increasing demand of security standards in open networks and distributed computing environment has become a critical issue for automation of the business process workflow. At automation level, it is a challenging task to methodically analyze the security constraint during the composition of business process component. For the complete automation of business process, one must scrutinize the flow of security patterns, which consist of the bit value of the respective parameter, which is the key entity for identifying the security vulnerability. Various phase-wise security patterns have been used to identify the security vulnerabilities during the black/white box testing phase of the service development. In respect of automation in business logic, this article introduces a machine learning computational technique that classifies the possible types of phase-wise class categories of security vulnerability. The performance matrix along with comparative analysis suggests that the proposed approach proficiently matches the attack pattern to respective security pattern, which can classify phase-wise class categories of security vulnerability in software component development. © 2017 Elsevier B.V.",Journal of Computational Science,10.1016/j.jocs.2017.08.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028365324&doi=10.1016%2fj.jocs.2017.08.003&partnerID=40&md5=0ca73298ed459b3ef9f79324a421c674,2018,2021-07-20 15:50:07,2021-07-20 15:50:07
4UDT99UA,journalArticle,2016,"Bollig, B.",On the Minimization of (Complete) Ordered Binary Decision Diagrams,"Ordered binary decision diagrams (OBDDs) are a popular data structure for Boolean functions. Some applications work with a restricted variant called complete OBDDs which is strongly related to nonuniform deterministic finite automata. One of its complexity measures is the width which has been investigated in several areas in computer science like machine learning, property testing, and the design and analysis of implicit graph algorithms. For a given function the size and the width of a (complete) OBDD is very sensitive to the choice of the variable ordering but the computation of an optimal variable ordering for the OBDD size is known to be NP-hard. Since optimal variable orderings with respect to the OBDD size are not necessarily optimal for the complete model or the OBDD width and hardly anything about the relation between optimal variable orderings with respect to the size and the width is known, this relationship is investigated. Here, using a new reduction idea it is shown that the size minimization problem for complete OBDDs and the width minimization problem are NP-hard. © 2015, Springer Science+Business Media New York.",Theory of Computing Systems,10.1007/s00224-015-9657-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944585797&doi=10.1007%2fs00224-015-9657-x&partnerID=40&md5=bae02b19f29f0234d6566731e4af0e65,2016,2021-07-20 15:50:07,2021-07-20 15:50:07
5QEJVCUA,journalArticle,2016,"Karakuzu, C.; Karakaya, F.; Çavuşlu, M.A.",FPGA implementation of neuro-fuzzy system with improved PSO learning,"This paper presents the first hardware implementation of neuro-fuzzy system (NFS) with its metaheuristic learning ability on field programmable gate array (FPGA). Metaheuristic learning of NFS for all of its parameters is accomplished by using the improved particle swarm optimization (iPSO). As a second novelty, a new functional approach, which does not require any memory and multiplier usage, is proposed for the Gaussian membership functions of NFS. NFS and its learning using iPSO are implemented on Xilinx Virtex5 xc5vlx110-3ff1153 and efficiency of the proposed implementation tested on two dynamic system identification problems and licence plate detection problem as a practical application. Results indicate that proposed NFS implementation and membership function approximation is as effective as the other approaches available in the literature but requires less hardware resources. © 2016 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2016.02.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84964483794&doi=10.1016%2fj.neunet.2016.02.004&partnerID=40&md5=d95d66bbbf8c95ea464639f171a70a2b,2016,2021-07-20 15:50:07,2021-07-20 15:50:07
9SUFPKUR,journalArticle,2016,"Zhou, C.; Lu, X.; Huang, M.",Dempster-Shafer theory-based robust least squares support vector machine for stochastic modelling,"Noise can be produced from various types of sources with different spectral distributions. This often causes the least squares support vector machine (LS-SVM) to be less effective since the LS-SVM is sensitive to noisy data. In this work, a Dempster-Shafer (D-S) theory-based robust LS-SVM is proposed, which has a more reliable modelling performance under various noise regimes. A distributed LS-SVM is first developed to construct the evidence data set. Fuzzy clustering is then used to construct an evidence base from the data. D-S theory is further used to fuse different pieces of evidence to derive the parameters for the construction of a robust LS-SVM. This robust model can represent the original system well even in the presence of different types of random noise. Case studies are presented to demonstrate the effectiveness of the proposed LS-SVM approach. © 2015 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2015.11.081,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84951121554&doi=10.1016%2fj.neucom.2015.11.081&partnerID=40&md5=f5d00ad3a5817e1aeef8fd319ef4c18c,2016,2021-07-20 15:50:07,2021-07-20 15:50:07
7WPFXH8D,journalArticle,2015,"Mohammed, M.F.; Lim, C.P.",An enhanced fuzzy min-max neural network for pattern classification,"An enhanced fuzzy min-max (EFMM) network is proposed for pattern classification in this paper. The aim is to overcome a number of limitations of the original fuzzy min-max (FMM) network and improve its classification performance. The key contributions are three heuristic rules to enhance the learning algorithm of FMM. First, a new hyperbox expansion rule to eliminate the overlapping problem during the hyperbox expansion process is suggested. Second, the existing hyperbox overlap test rule is extended to discover other possible overlapping cases. Third, a new hyperbox contraction rule to resolve possible overlapping cases is provided. Efficacy of EFMM is evaluated using benchmark data sets and a real medical diagnosis task. The results are better than those from various FMM-based models, support vector machine-based, Bayesian-based, decision tree-based, fuzzy-based, and neural-based classifiers. The empirical findings show that the newly introduced rules are able to realize EFMM as a useful model for undertaking pattern classification problems. © 2012 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2014.2315214,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84923924247&doi=10.1109%2fTNNLS.2014.2315214&partnerID=40&md5=235edc45c1cdcbee94ad27ed9972c96a,2015,2021-07-20 15:50:07,2021-07-20 15:50:07
KEGFWJNU,journalArticle,2013,"Yoshikawa, M.; Asai, T.; Shiozaki, M.; Fujino, T.",Physical unclonable function with multiplexing units and its evaluation,"Recently, semiconductor counterfeiting has become an increasingly serious problem. Therefore, techniques to prevent counterfeiting by using random characteristic patterns that are difficult to control artificially have attracted attention. The physical unclonable function (PUF) is one of these techniques. It is a method of deriving ID information peculiar to a device by detecting random physical features that cannot be controlled during the device's manufacture. Because information such as the ID information is difficult to replicate, PUF is used as a technique to prevent counterfeiting. Several studies have been reported on PUF. The arbiter PUF, which utilizes the difference in signal propagation delay between selectors, is a typical method of composing PUF using delay characteristics. This paper proposes a new PUF which is based on the arbiter PUF. The proposed PUF introduces new multiplexing selector units. It attempts to generate an effective response using the orders of three signal arrivals. Experiments using FPGAs verify the validity of the proposed PUF. Although Uniqueness is degraded, Correctness, Steadiness, Randomness, and Resistance to machine learning attacks are improved in comparison with the conventional approach. © 2013 Wiley Periodicals, Inc.",Electronics and Communications in Japan,10.1002/ecj.11458,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877688016&doi=10.1002%2fecj.11458&partnerID=40&md5=858c095b3c6e5cd368373d35ad659f3b,2013,2021-07-20 15:50:07,2021-07-20 15:50:07
NG83FZRE,journalArticle,2012,"Moral, S.",Imprecise probabilities for representing ignorance about a parameter,"This paper distinguishes between objective probability - or chance - and subjective probability. Most statistical methods in machine learning are based on the hypothesis that there is a random experiment from which we get a set of observations. This random experiment could be identified with a chance or objective probability, but these probabilities depend on some unknown parameters. Our knowledge of these parameters is not objective and in order to learn about them, we must assess some epistemic probabilities about their values. In some cases, our objective knowledge about these parameters is vacuous, so the question is: What epistemic probabilities should be assumed? In this paper we argue for the assumption of non-vacuous (a proper subset of [0, 1]) interval probabilities. There are several reasons for this; some are based on the betting interpretation of epistemic probabilities while others are based on the learning capabilities under the vacuous representation. The implications of the selection of epistemic probabilities in different concepts as conditioning and learning are studied. It is shown that in order to maintain some reasonable learning capabilities we have to assume more informative prior models than those frequently used in the literature, such as the imprecise Dirichlet model. © 2010 Elsevier Inc. All right reserved.",International Journal of Approximate Reasoning,10.1016/j.ijar.2010.12.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858448141&doi=10.1016%2fj.ijar.2010.12.001&partnerID=40&md5=c37009f1aa2f79b045db503b52799f57,2012,2021-07-20 15:50:07,2021-07-20 15:50:07
XRT64DVB,journalArticle,2011,"Li, C.; Wu, T.",Adaptive fuzzy approach to function approximation with PSO and RLSE,"A new adaptive fuzzy approach to function approximation is proposed in the paper. A Takagi-Sugeno (T-S) type fuzzy system is used as the function approximator in the study. The proposed approach uses a hybrid learning method to train the T-S fuzzy system to achieve high accuracy in function approximation. The hybrid learning method combines both the particle swarm optimization (PSO) and the recursive least squares estimator (RLSE) to update the parameters of the fuzzy approximator. The PSO is used to update the premise part of the fuzzy system while the consequent part is updated by the RLSE. The PSO-RLSE learning method is very efficient in learning convergence. The proposed approach is compared to other methods. Three benchmark functions are used for the performance comparison. The proposed approach shows superior performance to compared approaches, in terms of approximation accuracy and learning convergence. © 2010 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2011.04.145,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958019819&doi=10.1016%2fj.eswa.2011.04.145&partnerID=40&md5=67c4abb597ef59be2a8b0cbae53432d6,2011,2021-07-20 15:50:07,2021-07-20 15:50:07
WVTGRJ3I,journalArticle,2020,"Ghosh, L.; Konar, A.; Rakshit, P.; Nagar, A.K.",Mimicking short-Term memory in shape-reconstruction task using an eeg-induced type-2 fuzzy deep brain learning network,"The paper attempts to model short-Term memory (STM) for shape-reconstruction tasks by employing a 4-stage deep brain leaning network (DBLN), where the first two stages are built with Hebbian learning and the last two stages with Type-2 Fuzzy logic. The model is trained stage-wise independently with visual stimulus of the object-geometry as the input of the first stage, EEG acquired from different cortical regions as input and output of respective intermediate stages, and recalled object-geometry as the output of the last stage. Two error feedback loops are employed to train the proposed DBLN. The inner loop adapts the weights of the STM based on a measure of error in model-predicted response with respect to the object-shape recalled by the subject. The outer loop adapts the weights of the iconic (visual) memory based on a measure of error of the model predicted response with respect to the desired object-shape. In the test phase, the DBLN model reproduces the recalled object shape from the given input object geometry. The motivation of the paper is to test the consistency in STM encoding (in terms of similarity in network weights) for repeated visual stimulation with the same geometric object. Experiments undertaken on healthy subjects, yield high similarity in network weights, whereas patients with pre-frontal lobe Amnesia yield significant discrepancy in the trained weights for any two trials with the same training object. This justifies the importance of the proposed DBLN model in automated diagnosis of patients with learning difficulty. The novelty of the paper lies in the overall design of the DBLN model with special emphasis to the last two stages of the network, built with vertical slice based type-2 fuzzy logic, to handle uncertainty in function approximation (with noisy EEG data). The proposed technique outperforms the state-of-The-Art functional mapping algorithms with respect to the (pre-defined outer loop) error metric, computational complexity and runtime. © 2017 IEEE.",IEEE Transactions on Emerging Topics in Computational Intelligence,10.1109/TETCI.2019.2937566,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091672553&doi=10.1109%2fTETCI.2019.2937566&partnerID=40&md5=9ea59e6a3e5beb82e82f0ee251174cb3,2020,2021-07-20 15:50:07,2021-07-20 15:50:07
B9NRQ8K7,journalArticle,2020,"Liu, Y.; Fan, Z.; Qi, H.",Dynamic statistical evaluation of safety emergency management in coal enterprises based on neural network algorithms,"By establishing the evaluation system of emergency management capability for coal mine enterprises, we can identify the problems and shortcomings in coal mine emergency management, improve and improve its emergency management capability for coal mine emergencies. In this paper, the authors analyze the dynamic statistical evaluation of safety emergency management in coal enterprises based on neural network algorithms. Neural networks can form any form of topological structure through neurons, so they can directly simulate fuzzy reasoning in structure, that is to say, the equivalent structure of neural networks and fuzzy systems can be formed. This paper constructs the index system based on accident causes, and verifies the scientific rationality of the system. On this basis, according to the specific situation of coal mine emergency management, we design the evaluation criteria of coal mine emergency management capability evaluation index. Because coal mine accidents have the characteristics of complexity, variability and sudden dynamic, it is necessary to adjust and improve the accidents dynamically at any time. The model combines qualitative and quantitative indicators, and can make an overall evaluation of coal mine emergency management capability. It has the characteristics of clear results and strong fitting of simulation results. © 2020-IOS Press and the authors.",Journal of Intelligent and Fuzzy Systems,10.3233/JIFS-189034,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85094890486&doi=10.3233%2fJIFS-189034&partnerID=40&md5=67775bb2b722715413b31b6c2bca665f,2020,2021-07-20 15:50:07,2021-07-20 15:50:07
6C568K4L,journalArticle,2018,"Li, Z.; Xu, Z.; Ramamoorthi, R.; Sunkavalli, K.; Chandraker, M.",Learning to reconstruct shape and spatially-varying reflectance from a single image,"Reconstructing shape and reflectance properties from images is a highly under-constrained problem, and has previously been addressed by using specialized hardware to capture calibrated data or by assuming known (or highly constrained) shape or reflectance. In contrast, we demonstrate that we can recover non-Lambertian, spatially-varying BRDFs and complex geometry belonging to any arbitrary shape class, from a single RGB image captured under a combination of unknown environment illumination and flash lighting. We achieve this by training a deep neural network to regress shape and reflectance from the image. Our network is able to address this problem because of three novel contributions: first, we build a large-scale dataset of procedurally generated shapes and real-world complex SVBRDFs that approximate real world appearance well. Second, single image inverse rendering requires reasoning at multiple scales, and we propose a cascade network structure that allows this in a tractable manner. Finally, we incorporate an in-network rendering layer that aids the reconstruction task by handling global illumination effects that are important for real-world scenes. Together, these contributions allow us to tackle the entire inverse rendering problem in a holistic manner and produce state-of-the-art results on both synthetic and real data. © 2018 Association for Computing Machinery.",ACM Transactions on Graphics,10.1145/3272127.3275055,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063573822&doi=10.1145%2f3272127.3275055&partnerID=40&md5=2be0dbcfb8360a05d6bd361df9043179,2018,2021-07-20 15:50:07,2021-07-20 15:50:07
42M5E9UC,journalArticle,2021,"Nicolas, S.; Agnieszka, W.",The personality of anthropomorphism: How the need for cognition and the need for closure define attitudes and anthropomorphic attributions toward robots,"Anthropomorphism describes the tendency to attribute human characteristics to non-human agents such as robots. These attributions could depend on some dispositional factors such as the individuals' will to engage in reflective processes (need for cognition), to predict their environment (need for closure). Indeed, these traits may moderate how we explain artificial agents’ behavior that is our motivation to use cognitively more effortful reasoning about artificial agents against easily accessible anthropomorphic explanations. In the present study, we measured participants (n = 1141) need for cognition and closure in order to predict attitudes toward robots and anthropomorphic attributions on various robots which differed in terms of human-like characteristics. We found that both dimensions of personality influenced attitudes and anthropomorphism. In addition, these effects were emphasized by the human-like characteristics of the robots. In addition, we used a novel approach of clustering and machine-learning validation to delineate personality profiles based on the need for cognition and need for closure as most predictive variables to provide a first predictive categorization of tendency to anthropomorphize and attitudes toward robots. Our results argue for the importance of considering the interaction between the dispositional traits on the one hand and the design of robots on the other for anthropomorphism. © 2021",Computers in Human Behavior,10.1016/j.chb.2021.106841,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105694480&doi=10.1016%2fj.chb.2021.106841&partnerID=40&md5=44624f3ad1475c3745d4127b7933bf19,2021,2021-07-20 15:50:07,2021-07-20 15:50:07
SV8PUFAQ,journalArticle,2021,"Dhayne, H.; Kilany, R.; Haque, R.; Taher, Y.",EMR2vec: Bridging the gap between patient data and clinical trial,"The human suffering from diseases caused by life-threatening viruses such as SARS, Ebola, and COVID-19 motivated many of us to study and discover the best means to harness the potential of data integration to assist clinical researchers to curb these viruses. Integrating patients data with clinical trials data is enormously promising as it provides a comprehensive knowledge base that accelerates the clinical research response-ability to tackle emerging infectious disease outbreaks. This work introduces EMR2vec, a platform that customises advanced NLP, machine learning and semantic web techniques to link potential patients to suitable clinical trials. Linking these two different but complementary datasets allows clinicians and researchers to compare patients to clinical research opportunities or to automatically select patients for personalized clinical care. The platform derives a ’bag of medical terms’ (BoMT) from eligibility criteria by normalizing extracted entities through SNOMED-CT ontology. With the usage of BoMT, an ontological reasoning method is proposed to represent EMR and clinical trials in a vector space model. The platform presents a matching process that reduces vector dimensionality using a neural network, then applies orthogonality projection to measure the similarity between vectors. Finally, the proposed EMR2vec platform is evaluated with an extendable prototype based on Big data tools. © 2021 Elsevier Ltd",Computers and Industrial Engineering,10.1016/j.cie.2021.107236,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103965868&doi=10.1016%2fj.cie.2021.107236&partnerID=40&md5=330e8bec15258af2d2daf53740cc7f1e,2021,2021-07-20 15:50:07,2021-07-20 15:50:07
C7QNH485,journalArticle,2021,"Feng, F.; He, X.; Zhang, H.; Chua, T.",Cross-GCN: Enhancing Graph Convolutional Network with k-Order Feature Interactions,"Graph Convolutional Network (GCN) is an emerging technique that performs learning and reasoning on graph data. It operates feature learning on the graph structure, through aggregating the features of the neighbor nodes to obtain the embedding of each target node. Owing to the strong representation power, recent research shows that GCN achieves state-of-the-art performance on several tasks such as recommendation and linked document classification. Despite its effectiveness, we argue that existing designs of GCN forgo modeling cross features, making GCN less effective for tasks or data where cross features are important. Although neural network can approximate any continuous function, including the multiplication operator for modeling feature crosses, it can be rather inefficient to do so (i.e., wasting many parameters at the risk of overfitting) if there is no explicit design. To this end, we design a new operator named Cross-feature Graph Convolution, which explicitly models the arbitrary-order cross features with complexity linear to feature dimension and order size. We term our proposed architecture as Cross-GCN, and conduct experiments on three graphs to validate its effectiveness. Extensive analysis validates the utility of explicitly modeling cross features in GCN, especially for feature learning at lower layers. IEEE",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2021.3077524,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105865328&doi=10.1109%2fTKDE.2021.3077524&partnerID=40&md5=f282b2c53dc20733011dc9d517a65ab8,2021,2021-07-20 15:50:07,2021-07-20 15:50:07
AUDI3BYI,journalArticle,2021,"Yang, L.-H.; Liu, J.; Wang, Y.-M.; Martinez, L.",A Micro-Extended Belief Rule-Based System for Big Data Multiclass Classification Problems,"Big data classification problems have drawn great attention from diverse fields, and many classifiers have been developed. Among those classifiers, the extended belief rule-based system (EBRBS) has shown its potential in both big data and multiclass situations, while the time complexity and computing efficiency are two challenging issues to be handled in EBRBS. As such, three improvements of EBRBS are proposed first in this paper to decrease the time complexity and computing efficiency of EBRBS for multiclass classification under the assumption of large amount of data, including the strategy to skip rule weight calculation, a simplified evidential reasoning algorithm, and the domain division-based rule reduction method. This turns out to be a micro version of the EBRBS, called Micro-EBRBS. Moreover, one of commonly used cluster computing, named Apache Spark, is then applied to implement the parallel rule generation and inference schemes of the Micro-EBRBS for big data multiclass classification problems. The comparative analyses of experimental studies demonstrate that the Micro-EBRBS not only can obtain a desired accuracy but also has the comparatively better time complexity and computing efficiency than some popular classifiers, especially for multiclass classification problems. © 2013 IEEE.","IEEE Transactions on Systems, Man, and Cybernetics: Systems",10.1109/TSMC.2018.2872843,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055686291&doi=10.1109%2fTSMC.2018.2872843&partnerID=40&md5=39e1442d19b648131d8050f6f3cb9459,2021,2021-07-20 15:50:07,2021-07-20 15:50:07
XDGGJT6N,journalArticle,2020,"Gubert, P.H.; Costa, M.H.; Silva, C.D.; Trofino-Neto, A.",The performance impact of data augmentation in CSP-based motor-imagery systems for BCI applications,"Objective: This work investigates the performance impact of time-delay data-augmentation in motor-imagery classifiers for brain-computer-interface (BCI) applications. Methods: The considered strategy is an extension of the Common Spectral Spatial Patterns (CSSP) method, which consists of accommodating available additional information about intra- and inter-electrode correlation into the information matrix employed by the conventional Common Spatial Patterns (CSP) method. Results: Simulation with electroencephalographic signals obtained from two public motor-imagery datasets, in a context of differentiation between binary (left and right-hand) movements, result in an overall classification improvement of 5%, and up to 31.7% for a single individual, when compared to the use of the conventional correlation matrix in CSP-based methods. These results are supported through statistical inference and the analysis of the Receiving Operating Characteristic curve. Conclusion: The results obtained indicate that intra and inter-electrode correlation have relevant information, which have been underestimated by the literature in motor-imagery based BCI applications. Significance: The analyzed time-delay data-augmentation method improves the motor-imagery BCI classification accuracy with a foreseeable increase in the computation complexity. © 2020 Elsevier Ltd",Biomedical Signal Processing and Control,10.1016/j.bspc.2020.102152,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089822551&doi=10.1016%2fj.bspc.2020.102152&partnerID=40&md5=ec12f1bfb77b574951d60f4765dc2556,2020,2021-07-20 15:50:08,2021-07-20 15:50:08
BV8957S4,journalArticle,2020,"Nazari, S.; Amiri, M.; Faez, K.; Van Hulle, M.M.",Information Transmitted from Bioinspired Neuron-Astrocyte Network Improves Cortical Spiking Network's Pattern Recognition Performance,"We trained two spiking neural networks (SNNs), the cortical spiking network (CSN) and the cortical neuron-astrocyte network (CNAN), using a spike-based unsupervised method, on the MNIST and alpha-digit data sets and achieve an accuracy of 96.1% and 77.35%, respectively. We then connected CNAN to CSN by preserving maximum synchronization between them thanks to the concept of prolate spheroidal wave functions (PSWF). As a result, CSN receives additional information from CNAN without retraining. The important outcome is that CSN reaches 70.57% correct classification rate on capital letters without being trained on them. The overall contribution of transfer is 87.47%. We observed that for CSN the classifying neurons that relate to digits 0-9 of the alpha-digit data set are completely supported by the ones that relate to digits 0-9 of the MNIST data set. This means that CSN recognizes the similarity between the digits of the MNIST and alpha-digit data sets and classifies each digit of both data sets in the same class. © 2012 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2019.2905003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079085935&doi=10.1109%2fTNNLS.2019.2905003&partnerID=40&md5=98da7b0a7bd3f244aee37c5e6f22a7df,2020,2021-07-20 15:50:08,2021-07-20 15:50:08
CSK85HXC,journalArticle,2018,"Wosiak, A.; Glinka, K.; Zakrzewska, D.",Multi-label classification methods for improving comorbidities identification,"The medical diagnostic process may be supported by computational classification techniques. In many cases, patients are affected by multiple illnesses, and more than one classification label is required to improve medical decision-making. In this paper, we consider a multi-perspective classification problem for medical diagnostics, where cases are described by labels from separate sets. We attempt to improve the identification of comorbidities using multi-label classification techniques. Several investigated methods, which provide label dependencies, are analysed and evaluated. The methods’ performances are verified by experiments conducted on four sets of medical data from subject patients. The results were evaluated using several metrics and were statistically verified. We compare the effects of the techniques that do and do not consider label correlations. We demonstrate that multi-label classification methods from the first group outperform the techniques from the second one. © 2017 Elsevier Ltd",Computers in Biology and Medicine,10.1016/j.compbiomed.2017.07.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021880567&doi=10.1016%2fj.compbiomed.2017.07.006&partnerID=40&md5=2e04a64ceac37dbfd62987e76e74e807,2018,2021-07-20 15:50:08,2021-07-20 15:50:08
EBHVP7TQ,journalArticle,2016,"Damianou, A.C.; Titsias, M.K.; Lawrence, N.D.",Variational inference for latent variables and uncertain inputs in Gaussian processes,"The Gaussian process latent variable model (GP-LVM) provides a flexible approach for non-linear dimensionality reduction that has been widely applied. However, the current approach for training GP-LVMs is based on maximum likelihood, where the latent projection variables are maximised over rather than integrated out. In this paper we present a Bayesian method for training GP-LVMs by introducing a non-standard variational inference framework that allows to approximately integrate out the latent variables and subsequently train a GP-LVM by maximising an analytic lower bound on the exact marginal likelihood. We apply this method for learning a GP-LVM from i.i.d. observations and for learning non-linear dynamical systems where the observations are temporally correlated. We show that a benefit of the variational Bayesian procedure is its robustness to overfitting and its ability to automatically select the dimensionality of the non-linear latent space. The resulting framework is generic, flexible and easy to extend for other purposes, such as Gaussian process regression with uncertain or partially missing inputs. We demonstrate our method on synthetic data and standard machine learning benchmarks, as well as challenging real world datasets, including high resolution video data. ©2016 Andreas Damianou, Michalis Titsias and Neil Lawrence.",Journal of Machine Learning Research,,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979912168&partnerID=40&md5=bd351ee410b1585099eef15e0612ab09,2016,2021-07-20 15:50:08,2021-07-20 15:50:08
XXAJIZDK,journalArticle,2015,"Rios-Alvarado, A.B.; Lopez-Arevalo, I.; Tello-Leal, E.; Sosa-Sosa, V.J.",An Approach for Learning Expressive Ontologies in Medical Domain,"The access to medical information (journals, blogs, web-pages, dictionaries, and texts) has been increased due to availability of many digital media. In particular, finding an appropriate structure that represents the information contained in texts is not a trivial task. One of the structures for modeling the knowledge are ontologies. An ontology refers to a conceptualization of a specific domain of knowledge. Ontologies are especially useful because they support the exchange and sharing of information as well as reasoning tasks. The usage of ontologies in medicine is mainly focussed in the representation and organization of medical terminologies. Ontology learning techniques have emerged as a set of techniques to get ontologies from unstructured information. This paper describes a new ontology learning approach that consists of a method for the acquisition of concepts and its corresponding taxonomic relations, where also axioms disjointWith and equivalentClass are learned from text without human intervention. The source of knowledge involves files about medical domain. Our approach is divided into two stages, the first part corresponds to discover hierarchical relations and the second part to the axiom extraction. Our automatic ontology learning approach shows better results compared against previous work, giving rise to more expressive ontologies. © 2015, Springer Science+Business Media New York.",Journal of Medical Systems,10.1007/s10916-015-0261-z,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84931292077&doi=10.1007%2fs10916-015-0261-z&partnerID=40&md5=d2ba85907f524a22f4e08ebb8545d18d,2015,2021-07-20 15:50:08,2021-07-20 15:50:08
8SMKLDUD,journalArticle,2015,"Patnaik, D.",Theorizing change in artificial intelligence: inductivising philosophy from economic cognition processes,"Economic value additions to knowledge and demand provide practical, embedded and extensible meaning to philosophizing cognitive systems. Evaluation of a cognitive system is an empirical matter. Thinking of science in terms of distributed cognition (interactionism) enlarges the domain of cognition. Anything that actually contributes to the specific quality of output of a cognitive system is part of the system in time and/or space. Cognitive science studies behaviour and knowledge structures of experts and categorized structures based on underlying structures. Knowledge representation through understanding of ‘epistemic cultures’ is an evolutionary stage. But cognition goes beyond knowledge representation. Notwithstanding the importance of epistemology of phenomena, the practicability cum philosophical aspects of machine learning needs to be seen in dynamic behaviour in socio-economic-technical value additions if human machine interaction processes that are context specific are incorporated into strong artificial intelligent systems. Cognitive Science is also studied from both computational and biological angles. Evolution of interactive forms of reasoning through understanding of meta-language of computations or biological learning processes is possible. But the limitation of historical cultures predefines the role of interactive processes in user-networks beyond technology networks. Despite this limitation, inclusive development notions of a heterogeneous national society such as India or Europe can be tested and incorporated. © 2013, Springer-Verlag London.",AI and Society,10.1007/s00146-013-0524-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957961976&doi=10.1007%2fs00146-013-0524-5&partnerID=40&md5=bfb9b613ade4603f273591826e4bf671,2015,2021-07-20 15:50:08,2021-07-20 15:50:08
KZ4NC6V6,journalArticle,2013,"Esfahani, N.; Elkhodary, A.; Malek, S.",A learning-based framework for engineering feature-oriented self-adaptive software systems,"Self-adaptive software systems are capable of adjusting their behavior at runtime to achieve certain functional or quality-of-service goals. Often a representation that reflects the internal structure of the managed system is used to reason about its characteristics and make the appropriate adaptation decisions. However, runtime conditions can radically change the internal structure in ways that were not accounted for during their design. As a result, unanticipated changes at runtime that violate the assumptions made about the internal structure of the system could degrade the accuracy of the adaptation decisions. We present an approach for engineering self-adaptive software systems that brings about two innovations: 1) a feature-oriented approach for representing engineers' knowledge of adaptation choices that are deemed practical, and 2) an online learning-based approach for assessing and reasoning about adaptation decisions that does not require an explicit representation of the internal structure of the managed software system. Engineers' knowledge, represented in feature-models, adds structure to learning, which in turn makes online learning feasible. We present an empirical evaluation of the framework using a real-world self-adaptive software system. Results demonstrate the framework's ability to accurately learn the changing dynamics of the system while achieving efficient analysis and adaptation. © 1976-2012 IEEE.",IEEE Transactions on Software Engineering,10.1109/TSE.2013.37,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887877241&doi=10.1109%2fTSE.2013.37&partnerID=40&md5=4db6d5240b9aba7f072c906e69a1473f,2013,2021-07-20 15:50:08,2021-07-20 15:50:08
JCRYGN3Z,journalArticle,2020,"Bonnard, J.; Abdelouahab, K.; Pelcat, M.; Berry, F.",On building a CNN-based multi-view smart camera for real-time object detection,"Multi-view image sensing is currently gaining momentum, fostered by new applications such as autonomous vehicles and self-propelled robots. In this paper, we prototype and evaluate a multi-view smart vision system for object recognition. The system exploits an optimized Multi-View Convolutional Neural Network (MVCNN) in which the processing is distributed among several sensors (heads) and a camera body. The main challenge for designing such a system comes from the computationally expensive workload of real-time MVCNNs which is difficult to support with embedded processing and high frame rates. This paper focuses on the decisions to be taken for distributing an MVCNN on the camera heads, each camera head embedding a Field-Programmable Gate Array (FPGA) for processing images on the stream. In particular, we show that the first layer of the AlexNet network can be processed at the nearest of the sensors, by performing a Direct Hardware Mapping (DHM) using a dataflow model of computation. The feature maps produced by the first layers are merged and processed by a camera central processing node that executes the remaining layers. The proposed system exploits state-of-the-art deep learning optimization methods, such as parameter removing and data quantization. We demonstrate that accuracy drops caused by these optimizations can be compensated by the multi-view nature of the captured information. Experimental results conducted with the AlexNet CNN show that the proposed partitioning and resulting optimizations can fit the first layer of the multi-view network in low-end FPGAs. Among all the tested configurations, we propose 2 setups with an equivalent accuracy compared to the original network on the ModelNet40 dataset. The first one is composed of 4 cameras based on a Cyclone III E120 FPGA to embed the least expensive version in terms of logic resources while the second version requires 2 cameras based on a Cyclone 10 GX220 FPGA. This distributed computing with workload reduction is demonstrated to be a practical solution when building a real-time multi-view smart camera processing several frames per second. © 2020",Microprocessors and Microsystems,10.1016/j.micpro.2020.103177,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087330509&doi=10.1016%2fj.micpro.2020.103177&partnerID=40&md5=10403d57faa5e6e502cfccb052fd28ff,2020,2021-07-20 15:50:08,2021-07-20 15:50:08
89ZRPBE8,journalArticle,2013,"Mozaffari, A.; Gorji-Bandpy, M.; Samadian, P.; Rastgar, R.; Rezania Kolaei, A.",Comprehensive preference optimization of an irreversible thermal engine using pareto based mutable smart bee algorithm and generalized regression neural network,"Optimizing and controlling of complex engineering systems is a phenomenon that has attracted an incremental interest of numerous scientists. Until now, a variety of intelligent optimizing and controlling techniques such as neural networks, fuzzy logic, game theory, support vector machines and stochastic algorithms were proposed to facilitate controlling of the engineering systems. In this study, an extended version of mutable smart bee algorithm (MSBA) called Pareto based mutable smart bee (PBMSB) is inspired to cope with multi-objective problems. Besides, a set of benchmark problems and four well-known Pareto based optimizing algorithms i.e. multi-objective bee algorithm (MOBA), multi-objective particle swarm optimization (MOPSO) algorithm, non-dominated sorting genetic algorithm (NSGA-II), and strength Pareto evolutionary algorithm (SPEA 2) are utilized to confirm the acceptable performance of the proposed method. In order to find the maximum exploration potentials, these techniques are equipped with an external archive. These archives aid the methods to record all of the non-dominated solutions. Eventually, the proposed method and generalized regression neural network (GRNN) are simultaneously used to optimize the major parameters of an irreversible thermal engine. In order to direct the PBMSB to explore deliberate spaces within the solution domain, a reference point obtained from finite time thermodynamic (FTT) approach, is utilized in the optimization. The outcome results show the acceptable performance of the proposed method to optimize complex real-life engineering systems. © 2012 Elsevier B.V. All rights reserved.",Swarm and Evolutionary Computation,10.1016/j.swevo.2012.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875210352&doi=10.1016%2fj.swevo.2012.11.004&partnerID=40&md5=6d9c1d6a63f85dc07bf68d865405e4b8,2013,2021-07-20 15:50:08,2021-07-20 15:50:08
E9RPCKQE,journalArticle,2019,"McLean, G.; Osei-Frimpong, K.",Hey Alexa … examine the variables influencing the use of artificial intelligent in-home voice assistants,"Artificial Intelligent (AI) In-home Voice Assistants have seen unprecedented growth. However, we have little understanding on the factors motivating individuals to use such devices. Given the unique characteristics of the technology, in the main hands free, controlled by voice, and the presentation of a voice user interface, the current technology adoption models are not comprehensive enough to explain the adoption of this new technology. Focusing on voice interactions, this research combines the theoretical foundations of U&GT with technology theories to gain a clearer understanding on the motivations for adopting and using in-home voice assistants. This research presents a conceptual model on the use of voice controlled technology and an empirical validation of the model through the use of Structural Equation Modelling with a sample of 724 in-home voice assistant users. The findings illustrate that individuals are motivated by the (1) utilitarian benefits, (2) symbolic benefits and (3) social benefits provided by voice assistants, the results found that hedonic benefits only motivate the use of in-home voice assistants in smaller households. Additionally, the research establishes a moderating role of perceived privacy risks in dampening and negatively influencing the use of in-home voice assistants. © 2019 Elsevier Ltd",Computers in Human Behavior,10.1016/j.chb.2019.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066087041&doi=10.1016%2fj.chb.2019.05.009&partnerID=40&md5=93de6e5428540128a50b24f70fd235ed,2019,2021-07-20 15:50:08,2021-07-20 15:50:08
853WD4TS,journalArticle,2019,"Zheng, Z.; Wu, X.; Weng, J.",Emergent neural turing machine and its visual navigation,"Traditional Turing Machines (TMs) are symbolic whose hand-crafted representations are static and limited. Developmental Network 1 (DN-1) uses emergent representation to perform Turing Computation. But DN-1 lacks hierarchy in its internal representations, and is hard to handle the complex visual navigation tasks. In this paper, we improve DN-1 with several new mechanisms and presenta new emergent neural Turing Machine — Developmental Network 2 (DN-2). By neural, we mean that the control of the TM has neurons as basic computing elements. The major novelty of DN-2 over DN-1 is that the representational hierarchy inside DN-2 is emergent and fluid. DN-2 grows complex hierarchies by dynamically allowing initialization of neurons with different ranges of connection. We present a complex task — vision guided navigation in simulated and natural worlds using DN-2. A major function that has not been demonstrated before is that the hierarchy enables attention that disregards distracting features based on the navigation context. In simulated navigation experiments, DN-2 can perform with no errors, and in real-world navigation experiments, the error rate is only 0.78%. These experimental results showed that DN-2 successfully learned rules of navigation with image inputs. © 2018 Elsevier Ltd",Neural Networks,10.1016/j.neunet.2018.11.004,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058003612&doi=10.1016%2fj.neunet.2018.11.004&partnerID=40&md5=ab4aa75dbabcac001969d294ed5a3b9f,2019,2021-07-20 15:50:08,2021-07-20 15:50:08
6MCWSWP8,journalArticle,2015,"Zhang, Y.; Wang, J.Z.; Li, J.",Parallel massive clustering of discrete distributions,"The trend of analyzing big data in artificial intelligence demands highly-scalable machine learning algorithms, among which clustering is a fundamental and arguably the most widely applied method. To extend the applications of regular vector-based clustering algorithms, the Discrete Distribution (D2) clustering algorithm has been developed, aiming at clustering data represented by bags of weighted vectors which are well adopted data signatures in many emerging information retrieval and multimedia learning applications. However, the high computational complexity of D2-clustering limits its impact in solving massive learning problems. Here we present the parallel D2-clustering (PD2-clustering) algorithm with substantially improved scalability. We developed a hierarchical multipass algorithm structure for parallel computing in order to achieve a balance between the individual-node computation and the integration process of the algorithm. Experiments and extensive comparisons between PD2-clustering and other clustering algorithms are conducted on synthetic datasets. The results show that the proposed parallel algorithm achieves significant speed-up with minor accuracy loss. We apply PD2-clustering to image concept learning. In addition, by extending D2-clustering to symbolic data, we apply PD2-clustering to protein sequence clustering. For both applications, we demonstrate the high competitiveness of our new algorithm in comparison with other state-of-the-art methods. © 2015 ACM.","ACM Transactions on Multimedia Computing, Communications and Applications",10.1145/2700293,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930858981&doi=10.1145%2f2700293&partnerID=40&md5=9c673d3e064d4e26cccefef5c2d01137,2015,2021-07-20 15:50:08,2021-07-20 15:50:08
7FWC9JJ7,journalArticle,2011,"Plewczynski, D.",Cognitive architectures in multi-agent systems,"We propose the basic computational framework, which can be used for modeling of hierarchical cognitive architectures. The system is build from at least two layers. The first is used as the semantic network for storing semiotic triads, i.e. objects, categories and symbols, which represent them. The network is constructed from an ensemble of machine learning, or clustering methods, each trained on a set of prototypes for a given category. Those semiotic triads are linked with weights representing the cognitive similarity between represented symbols. The number of symbols that can be stored in such network scales linearly with the number of stored datasets of prototypical objects, as it is assumed in symbol grounding paradigm. The second layer is the communication, or interaction network, which allow agents to communicate with each other, namely to speak about symbols and/or underlying objects (categories). Agent can pass an object (perceptual item), and its name to other agent, in order to modify the categorical system of the second agent, or to communicate the symbolic name for a given category. Both cognitive layers are simulated within Agent-Based Modeling (ABM) approach. Therefore, typical cognitive task can be implemented here as the dynamical process of equilibration for a set of semiotic networks in the population of interacting agents with language. © 2011 The authors and IOS Press. All rights reserved.",Frontiers in Artificial Intelligence and Applications,10.3233/978-1-60750-762-8-237,https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960143185&doi=10.3233%2f978-1-60750-762-8-237&partnerID=40&md5=158950ae175644fb547939d786d4d2f4,2011,2021-07-20 15:50:08,2021-07-20 15:50:08
B5I73P8Z,journalArticle,2020,"Naeem, F.; Srivastava, G.; Tariq, M.",A Software Defined Network Based Fuzzy Normalized Neural Adaptive Multipath Congestion Control for the Internet of Things,"Multipath Transmission Control Protocol (MPTCP) enables multi-homed devices to establish multiple simultaneous routes for data transmission. Congestion Control (CC) is a fundamental mechanism for implementing and designing MPTCP. The Internet of Things (IoT) networks generate a massive volume of heterogeneous traffic with high dimensional states and diverse QoS characteristics. The existing MPTCP CC algorithms are unable to perform efficiently under highly mobile and dynamic IoT environments. We propose a novel model-free SDN-based adaptive actor-critic deep reinforcement learning framework based on a fuzzy normalized neural network to address the issue of CC for MPTCP in the IoT networks. In the proposed method, an agent can learn efficiently and better approximate the state-action value function of the actor and the action function of the critic to adjust the sub-flows congestion windows size adaptively according to the dynamic condition of a network. Simulation results show that the proposed scheme outperforms the state of the art schemes in terms of the goodput under highly-dynamic IoT environments. © 2013 IEEE.",IEEE Transactions on Network Science and Engineering,10.1109/TNSE.2020.2991106,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093655502&doi=10.1109%2fTNSE.2020.2991106&partnerID=40&md5=c082b4abc2652e7832dfae8fc0377a4d,2020,2021-07-20 15:50:08,2021-07-20 15:50:08
PZNU3QTX,journalArticle,2020,"Zhang, H.; Chen, B.; Cong, Y.; Guo, D.; Liu, H.; Zhou, M.",Deep Autoencoding Topic Model with Scalable Hybrid Bayesian Inference,"To build a flexible and interpretable model for document analysis, we develop deep autoencoding topic model(DATM) that uses a hierarchy of gamma distributions to construct its multi-stochastic-layer generative network. In order to provide scalable posterior inference for the parameters of the generative network, we develop topic-layer-adaptive stochastic gradient Riemannian MCMC that jointly learns simplex constrained global parameters across all layers and topics, with topic and layer specific learning rates. Given a posterior sample of the global parameters, in order to efficiently infer the local latent representations of a document under DATM across all stochastic layers, we propose a Weibull upward-downward variational encoder that deterministically propagates information upward via a deep neural network, followed by a Weibull distribution based stochastic downward generative model. To jointly model documents and their associated labels, we further propose supervised DATM that enhances the discriminative power of its latent representations. The efficacy and scalability of our models are demonstrated on both unsupervised and supervised learning tasks on big corpora. IEEE",IEEE Transactions on Pattern Analysis and Machine Intelligence,10.1109/TPAMI.2020.3003660,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099115032&doi=10.1109%2fTPAMI.2020.3003660&partnerID=40&md5=489dbfbfeae5ebecf87812845004de68,2020,2021-07-20 15:50:09,2021-07-20 15:50:09
7BFIIYA9,journalArticle,2019,"Yin, S.; Seo, J.-S.; Kim, Y.; Han, X.; Barnaby, H.; Yu, S.; Luo, Y.; He, W.; Sun, X.; Kim, J.-J.",Monolithically Integrated RRAM- And CMOS-Based In-Memory Computing Optimizations for Efficient Deep Learning,"Resistive RAM (RRAM) has been presented as a promising memory technology toward deep neural network (DNN) hardware design, with nonvolatility, high density, high ON/OFF ratio, and compatibility with logic process. However, prior RRAM works for DNNs have shown limitations on parallelism for in-memory computing, array efficiency with large peripheral circuits, multilevel analog operation, and demonstration of monolithic integration. In this article, we propose circuit-/device-level optimizations to improve the energy and density of RRAM-based in-memory computing architectures. We report experimental results based on prototype chip design of 128 × 64 RRAM arrays and CMOS peripheral circuits, where RRAM devices are monolithically integrated in a commercial 90-nm CMOS technology. We demonstrate the CMOS peripheral circuit optimization using input-splitting scheme and investigate the implication of higher low resistance state on energy efficiency and robustness. Employing the proposed techniques, we demonstrate RRAM-based in-memory computing with up to 116.0 TOPS/W energy efficiency and 84.2% CIFAR-10 accuracy. Furthermore, we investigate four-level programming with single RRAM device, and report the system-level performance and DNN accuracy results using circuit-level benchmark simulator NeuroSim. © 1981-2012 IEEE.",IEEE Micro,10.1109/MM.2019.2943047,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075013149&doi=10.1109%2fMM.2019.2943047&partnerID=40&md5=3131b9d55cd573e7fa19b36d6eae17d9,2019,2021-07-20 15:50:09,2021-07-20 15:50:09
CHRI68IF,journalArticle,2021,"Campana, M.G.; Delmastro, F.",MyDigitalFootprint: An extensive context dataset for pervasive computing applications at the edge,"The widespread diffusion of connected smart devices has greatly contributed to the rapid expansion and evolution of the Internet at its edge, where personal mobile devices follow the behavior of their human users and interact with other smart objects located in the surroundings. In such a scenario, the user context is represented by a large variety of information that can rapidly change, and the ability of personal mobile devices to locally process this data is fundamental to make the system able to quickly adapt its behavior to the current situation. This ability, in practice, can be represented by a single elaboration process integrated in the final user application, or by a middleware platform aimed at implementing different context processing and reasoning to support third-party applications. However, the lack of public datasets that take into account the complexity of the user context in the mobile environment strongly limits the advance of the research in this field. In this paper, we present MyDigitalFootprint, a novel large-scale dataset composed of smartphone embedded sensors data, physical proximity information, and Online Social Networks interactions aimed at supporting multimodal context-recognition and social relationships modeling. The dataset includes two months of measurements and information collected from the personal mobile devices of 31 volunteer users, in their natural environment, without limiting their usual behavior. Existing public datasets generally consist of a limited set of context data, aimed at optimizing specific application domains (human activity recognition is the most common example). On the contrary, our dataset contains a comprehensive set of information describing the user context in the mobile environment. In order to demonstrate the efficacy of the proposed dataset, we present three context-aware applications based on different machine learning tasks: (i) a social link prediction algorithm based on physical proximity data, (ii) the recognition of daily-life activities based on smartphone-embedded sensors data, and (iii) a pervasive context-aware recommender system. To the best of our knowledge, this is the first large-scale dataset containing such heterogeneity of information, representing an invaluable source of data to validate new research in mobile and edge computing. © 2020",Pervasive and Mobile Computing,10.1016/j.pmcj.2020.101309,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097892515&doi=10.1016%2fj.pmcj.2020.101309&partnerID=40&md5=27499884faacc8bf8993cc1d255d2da0,2021,2021-07-20 15:50:09,2021-07-20 15:50:09
9Q47Z37C,journalArticle,2019,"Tchangani, A.",Bipolar Fuzzy Nominal Classification (BFNC) framework: Application to risk analysis,"Nominal classification (NC) is a subfield of multi-criteria decision making where an object (in a broad sense) characterized by some attributes (with their valuation belonging to an ordered set, numeric in general) must be assigned to one of pre-defined classes or categories; these classes are characterized by some numerical valued features. This is also known as supervised classification as opposed to unsupervised classification in machine learning literature. In many applications such as that of risk analysis, characterization of classes by features may not be precisely defined; they will be rather fuzzily expressed using linguistic appreciation such as high is better, low is more appreciated, medium range is better, etc. leading to what is referred to as fuzzy nominal classification (FNC). On other hand bipolar reasoning is pervasive in classification in the sense that given a couple (feature, class), there will be some values of the feature that lead to automatically assigning (respect. automatically excluding to assign) the considered object into that class leading to what we name bipolar fuzzy nominal classification or BFNC for short; the main purpose of this paper is to develop this BFNC framework with risk analysis as an illustrative application domain. The stepping stones of this framework are two indexes for each couple (class, object) known as classifiability index (that measures the extent to which the considered object can be included into that class) and the rejectability index measuring the extent to which one should avoid including this object into that class. By using two indexes for classification, many classes can be qualified for inclusion of a given object rendering this framework flexible. Analyzing risks for large-scale complex systems requires identifying, assessing, and prioritizing different risk scenarios for their appropriate treatment such as resources allocation for risk mitigation, risk prevention, risk sharing, etc. To this end and given scarcity of resources in general, one must consider first prioritizing, filtering, or scoring risks that return to assigning them to pre-defined classes or categories; that is nominally classifying them. The developed BFNC framework applied to a real world application in the domain of countries' risk classification shows its practical potentialities. © 2019 - IOS Press and the authors. All rights reserved.",Intelligent Decision Technologies,10.3233/IDT-190355,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064390656&doi=10.3233%2fIDT-190355&partnerID=40&md5=eb8e09d2748b628535f09a70cd57d75a,2019,2021-07-20 15:50:09,2021-07-20 15:50:09
VWHRTL8N,journalArticle,2021,"Silva-Ramírez, E.-L.; Cabrera-Sánchez, J.-F.",Co-active neuro-fuzzy inference system model as single imputation approach for non-monotone pattern of missing data,"Data imputation aims to solve missing values problem which is common in nowadays applications. Many techniques have been proposed to solve this problem from statistical methods such as Mean/Mode to machine learning models. In this paper, an approach based on Co-active Neuro-Fuzzy Inference System named CANFIS-ART is proposed to automate data imputation procedure. This model is constructed from the Neural Network adaptative capabilities and fuzzy logic qualitative approach using the Fuzzy-ART algorithm. Performance of CANFIS-ART model is compared to other state-of-the-art imputation techniques such as Multilayer Perceptron or Hot-Deck, among others, using a total of eighteen databases exposed to a perturbation procedure based on the random generation of non-monotone missing values pattern. The data sets cover a wide range of fields, types of variables and sizes. A comparison of databases imputed by these models using a set of three classifiers has been conducted. A statistical analysis of these results employing Wilcoxon signed-ranked test has been included. Experiments show that CANFIS-ART approach not only outperforms these state-of-the-art techniques but also demonstrates a higher level of generalization capability, increasing the data quality contained in databases with missing values. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd. part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-020-05661-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100867195&doi=10.1007%2fs00521-020-05661-5&partnerID=40&md5=a8708c8d4497b154218b806c3531ce1b,2021,2021-07-20 15:50:09,2021-07-20 15:50:09
Q7DDDTU8,journalArticle,2021,"Ferdaus, M.M.; Zaman, F.; Chakrabortty, R.K.",Performance Improvement of a Parsimonious Learning Machine Using Metaheuristic Approaches,"Autonomous learning algorithms operate in an online fashion in dealing with data stream mining, where minimum computational complexity is a desirable feature. For such applications, parsimonious learning machines (PALMs) are suitable candidates due to their structural simplicity. However, these parsimonious algorithms depend upon predefined thresholds to adjust their structures in terms of adding or deleting rules. Besides, another adjustable parameter of PALM is the fuzziness in membership grades. The best set of such hyper parameters is determined by experts' knowledge or by optimization techniques such as greedy algorithms. To mitigate such experts' dependency or usage of computationally expensive greedy algorithms, in this work, a meta heuristic-based optimization technique, called the multimethod-based optimization technique (MOT), is utilized to develop an advanced PALM. The performance has been compared with some popular optimization techniques, namely, the greedy search, local search, genetic algorithm (GA), and particle swarm optimization (PSO). The proposed parsimonious learning algorithm with MOT outperforms the others in most cases. It validates the multioperator-based optimization technique's advantages over the single operator-based variants in selecting the best feasible hyperparameters for the autonomous learning algorithm by maintaining a compact architecture. IEEE",IEEE Transactions on Cybernetics,10.1109/TCYB.2021.3051242,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100853298&doi=10.1109%2fTCYB.2021.3051242&partnerID=40&md5=eb3da64417972b8cd88197109e55b927,2021,2021-07-20 15:50:09,2021-07-20 15:50:09
WCX5RDQN,journalArticle,2021,"Kim, M.S.; Del Barrio Garcia, A.A.; Kim, H.; Bagherzadeh, N.",The Effects of Approximate Multiplication on Convolutional Neural Networks,"This paper analyzes the effects of approximate multiplication when performing inferences on deep convolutional neural networks (CNNs). The approximate multiplication can reduce the cost of the underlying circuits so that CNN inferences can be performed more efficiently in hardware accelerators. The study identifies the critical factors in the convolution, fully-connected, and batch normalization layers that allow more accurate CNN predictions despite the errors from approximate multiplication. The same factors also provide an arithmetic explanation of why bfloat16 multiplication performs well on CNNs. The experiments are performed with recognized network architectures to show that the approximate multipliers can produce predictions that are nearly as accurate as the FP32 references, without additional training. For example, the ResNet and Inception-v4 models with Mitch-w6 multiplication produces Top-5 errors that are within 0.2% compared to the FP32 references. A brief cost comparison of Mitch-w6 against bfloat16 is presented, where a MAC operation saves up to 80% of energy compared to the bfloat16 arithmetic. The most far-reaching contribution of this paper is the analytical justification that multiplications can be approximated while additions need to be exact in CNN MAC operations. IEEE",IEEE Transactions on Emerging Topics in Computing,10.1109/TETC.2021.3050989,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099578814&doi=10.1109%2fTETC.2021.3050989&partnerID=40&md5=fbc1e99f222f56ebbde82359746263a9,2021,2021-07-20 15:50:09,2021-07-20 15:50:09
CZ6Z825N,journalArticle,2020,"Sahani, M.; Dash, P.K.",Automatic Power Quality Events Recognition Using Modes Decomposition Based Online P-Norm Adaptive Extreme Learning Machine,"This article presents an automatic recognition of power quality events (PQEs) by integrating variational mode decomposition (VMD) with Hilbert transform (HT) and the proposed online P-norm adaptive extreme learning machine (OPAELM). The robust parameters estimation capability from the highly nonstationary PQE patterns is presented using VMDHT method and a novel mode selection scheme is introduced based on the correlation coefficient. Three most efficient power quality indices are extracted and fed as an input to train and test the OPAELM classifier with a few existing advanced classifiers. The distinctive modes extraction, low computational burden, robust antinoise performance, short event recognition time, and outstanding recognition capability are the prime superiority expediencies of the VMDHT-OPAELM method. Finally, the proposed method is developed in Xilinx integrated synthesis environment (ISE) Design Suite 14.5 configured with MATLAB/Simulink software environment and implemented in a high-speed field-programmable gate array digital circuitry hardware platform to validate the cogency in real time. © 2005-2012 IEEE.",IEEE Transactions on Industrial Informatics,10.1109/TII.2019.2945822,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082989077&doi=10.1109%2fTII.2019.2945822&partnerID=40&md5=2bead514e281df88d9788e70ea28a7b4,2020,2021-07-20 15:50:09,2021-07-20 15:50:09
5P4DT7LW,journalArticle,2020,"Gomez-Gonzalez, S.; Neumann, G.; Scholkopf, B.; Peters, J.",Adaptation and Robust Learning of Probabilistic Movement Primitives,"Probabilistic representations of movement primitives open important new possibilities for machine learning in robotics. These representations are able to capture the variability of the demonstrations from a teacher as a probability distribution over trajectories, providing a sensible region of exploration and the ability to adapt to changes in the robot environment. However, to be able to capture variability and correlations between different joints, a probabilistic movement primitive requires the estimation of a larger number of parameters compared to their deterministic counterparts, which focus on modeling only the mean behavior. In this article, we make use of prior distributions over the parameters of a probabilistic movement primitive to make robust estimates of the parameters with few training instances. In addition, we introduce general purpose operators to adapt movement primitives in joint and task space. The proposed training method and adaptation operators are tested in a coffee preparation and in robot table tennis task. In the coffee preparation task we evaluate the generalization performance to changes in the location of the coffee grinder and brewing chamber in a target area, achieving the desired behavior after only two demonstrations. In the table tennis task we evaluate the hit and return rates, outperforming previous approaches while using fewer task specific heuristics. © 2004-2012 IEEE.",IEEE Transactions on Robotics,10.1109/TRO.2019.2937010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081390485&doi=10.1109%2fTRO.2019.2937010&partnerID=40&md5=9416d5caa2f184992d9a0bd30ea5bff6,2020,2021-07-20 15:50:09,2021-07-20 15:50:09
CXILJ5AL,journalArticle,2020,"Lunglmayr, M.; Wiesinger, D.; Haselmayr, W.",Design and Analysis of Efficient Maximum/Minimum Circuits for Stochastic Computing,"In stochastic computing (SC), a real-valued number is represented by a stochastic bit stream, encoding its value in the probability of obtaining a one. This leads to a significantly lower hardware effort for various functions and provides a higher tolerance to errors (e.g., bit flips) compared to binary radix representation. The implementation of a stochastic max/min function is important for many areas where SC has been successfully applied, such as image processing or machine learning (e.g., max pooling in neural networks). In this work, we propose a novel shift-register-based architecture for a stochastic max/min function. We show that the proposed circuit has significantly higher accuracy than state-of-the-art architectures for uncorrelated bit streams at comparable hardware costs. Moreover, we analytically proof the correctness of the proposed circuit and provide a new error analysis, based on the individual bits of the stochastic streams. Interestingly, the analysis reveals that for a certain practical bit stream length a finite optimal shift register length exists and it allows to determine the optimal length. © 1968-2012 IEEE.",IEEE Transactions on Computers,10.1109/TC.2019.2949779,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079689262&doi=10.1109%2fTC.2019.2949779&partnerID=40&md5=0c33bc188728d6e18cd2e326a3ab8f87,2020,2021-07-20 15:50:09,2021-07-20 15:50:09
L8FJL2UB,journalArticle,2020,"Verborgh, R.; Vander Sande, M.",The Semantic Web identity crisis: In search of the trivialities that never were,"For a domain with a strong focus on unambiguous identifiers and meaning, the Semantic Web research field itself has a surprisingly ill-defined sense of identity. Started at the end of the 1990s at the intersection of databases, logic, and Web, and influenced along the way by all major tech hypes such as Big Data and machine learning, our research community needs to look in the mirror to understand who we really are. The key question amid all possible directions is pinpointing the important challenges we are uniquely positioned to tackle. In this article, we highlight the community's unconscious bias toward addressing the Paretonian 80% of problems through research-handwavingly assuming that trivial engineering can solve the remaining 20%. In reality, that overlooked 20% could actually require 80% of the total effort and involve significantly more research than we are inclined to think, because our theoretical experimentation environments are vastly different from the open Web. As it turns out, these formerly neglected 'trivialities' might very well harbor those research opportunities that only our community can seize, thereby giving us a clear hint of how we can orient ourselves to maximize our impact on the future. If we are hesitant to step up, more pragmatic minds will gladly reinvent technology for the real world, only covering a fraction of the opportunities we dream of. © 2020-IOS Press and the authors. All rights reserved.",Semantic Web,10.3233/SW-190372,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082401090&doi=10.3233%2fSW-190372&partnerID=40&md5=b2f26352178092566ffd5aecbcffcca8,2020,2021-07-20 15:50:09,2021-07-20 15:50:09
GZPTGTTY,journalArticle,2019,"Wang, L.; Wang, M.; Yue, T.",A fuzzy deterministic policy gradient algorithm for pursuit-evasion differential games,"Fuzzy inference systems with reinforcement learning are currently being used in differential games to train agents with no prior experience. However, the reinforcement learning algorithms based on actor-critic structure have a drawback that the policy is depended on a probability distribution. In this paper, a novel fuzzy deterministic policy gradient algorithm is introduced and applied to classical 1-vs-1 constant-velocity pursuit-evasion differential games. The key goal is to self-learn the optimal strategy in the continuous action domain and obtain a specific physical meaning of the fuzzy rules. The novel proposed algorithm is based on the deterministic policy gradient theorem and the agent learns the near-optimal strategy under the actor-critic structure. The fuzzy inference system is applied as approximators so that the specific physical meaning can be obtained by the linguistic fuzzy rules. Furthermore, the proposed algorithm is applied to solve the decision-making problem of pursuit-evasion differential games. The result is compared with other existing algorithms and it elucidates that the proposed algorithm outperforms the precision and convergence efficiency. © 2019 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2019.07.038,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069657134&doi=10.1016%2fj.neucom.2019.07.038&partnerID=40&md5=47d3d5e7bbdd94d8a5443ad91c90104e,2019,2021-07-20 15:50:09,2021-07-20 15:50:09
PFBYGX9G,journalArticle,2019,"Xue, S.; Li, X.; Wang, X.",Fault diagnosis of multi-state gas monitoring network based on fuzzy Bayesian net,"The monitoring system is a critical function in modern underground mine gas accidents prevention. As a complex dynamic critical system, its fault diagnosis is generally based on the traditional two-state fault tree. In order to solve the problem that the model can only deal with two-state problem, a fuzzy Bayesian network (BN) is used to deal with the polymorphic fault tree. A Bayesian model of the fault of the multi-state gas monitoring system is constructed for machine learning and optimization. On this foundation, the model is validated and applied to a real system. Combined with the fuzzy fault tree importance algorithm, the reliability analysis which has guiding significance for improving the reliability of the gas monitoring system is carried out. The top event, the failure probability of each node, and the fuzzy importance and status importance of each factor are obtained respectively through calculating. Finally, the troubleshooting order of all nodes and key troubleshooting nodes are obtained. When the system is in various states, different parts of the system were diagnosed. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Personal and Ubiquitous Computing,10.1007/s00779-019-01237-w,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067784490&doi=10.1007%2fs00779-019-01237-w&partnerID=40&md5=7bf9bbb167b643afc1b2791f4d304724,2019,2021-07-20 15:50:09,2021-07-20 15:50:09
9R7FKJJ7,journalArticle,2019,"Yang, Z.; Zhang, K.; Lei, L.; Zheng, K.",A novel classifier exploiting mobility behaviors for sybil detection in connected vehicle systems,"A Sybil attacker is able to obtain more than one identities and disguise as multiple vehicles in order to interfere the normal operations of the connected vehicle system (CVS). In this paper, we propose a novel classifier to detect Sybil attackers according to their mobility behaviors. Specifically, three levels of Sybil attackers are first defined according to their attack abilities. Through analyzing the mobility behaviors of vehicles, a learning-based model is used in the central server (CS) to extract mobility features and distinguish Sybil attackers from benign vehicles. Three classification algorithms are tested and compared, i.e., the naive Bayes, decision tree, and support vector machine. Furthermore, location certificates issued by base stations are used to resist location forgery by attackers. Based on the location certificates, the CS is able to evaluate the credibilities of uploaded locations using the subjective logic theory. In addition, we develop an edge betweenness-based community detection algorithm to handle the collusion among multiple Sybil attackers. Simulations are conducted based on a real-world vehicle trajectory dataset, which indicate that the proposed scheme is effective to resist Sybil attackers in CVS. © 2014 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2018.2872456,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054235519&doi=10.1109%2fJIOT.2018.2872456&partnerID=40&md5=c8f4c867357b91f1186ca0242806f7b6,2019,2021-07-20 15:50:09,2021-07-20 15:50:09
A99BZ2KD,journalArticle,2019,"Zhao, H.; Li, H.; Maurer-Stroh, S.; Guo, Y.; Deng, Q.; Cheng, L.",Supervised segmentation of Un-annotated retinal fundus images by synthesis,"We focus on the practical challenge of segmenting new retinal fundus images that are dissimilar to existing well-annotated data sets. It is addressed in this paper by a supervised learning pipeline, with its core being the construction of a synthetic fundus image data set using the proposed R-sGAN technique. The resulting synthetic images are realistic-looking in terms of the query images while maintaining the annotated vessel structures from the existing data set. This helps to bridge the mismatch between the query images and the existing well-annotated data set. As a consequence, any known supervised fundus segmentation technique can be directly utilized on the query images, after training on this synthetic data set. Extensive experiments on different fundus image data sets demonstrate the competitiveness of the proposed approach in dealing with a diverse range of mismatch settings. © 1982-2012 IEEE.",IEEE Transactions on Medical Imaging,10.1109/TMI.2018.2854886,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050596096&doi=10.1109%2fTMI.2018.2854886&partnerID=40&md5=bd040557e024bc1788da2de0d28980e6,2019,2021-07-20 15:50:09,2021-07-20 15:50:09
7VJ38DWQ,journalArticle,2018,"Lin, C.-H.; Tsai, C.-Y.; Lee, K.-C.; Yu, S.-C.; Liau, W.-R.; Hou, A.C.-L.; Chen, Y.-Y.; Kuo, C.-Y.; Lee, J.-N.; Chao, M.C.-T.",A Model-Based-Random-Forest Framework for Predicting Vt Mean and Variance Based on Parallel Id Measurement,"To measure the variation of device Vt requires long test for conventional wafer acceptance test (WAT) test structures. This paper presents a framework that can efficiently and effectively obtain the mean and variance of Vt for a large number of designs under test (DUTs). The proposed framework applies the model-based random forest as its core model-fitting technique to learn a model that can predict the mean and variance of Vt based only on the combined Id measured from parallel connected DUTs. The proposed framework can further minimize the total number of Id measurement required for prediction models while limiting their accuracy loss. The experimental results based on the SPICE simulation of a UMC 28-nm technology demonstrate that the proposed model-fitting framework can achieve a more than 99% R-squared for predicting either Vt mean or Vt variance. Compared to conventional WAT test structures using binary search, our proposed framework can achieve a 120.3 × speedup on overall test time for test structures with 800 DUTs. © 2017 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2017.2783304,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038820028&doi=10.1109%2fTCAD.2017.2783304&partnerID=40&md5=3f856dd88df3879c212c87e28b403f35,2018,2021-07-20 15:50:09,2021-07-20 15:50:09
7Q942VQT,journalArticle,2018,"Qin, G.; Lu, X.",Integration of weighted LS-SVM and manifold learning for fuzzy modeling,"In this paper, a robust fuzzy modeling method is proposed for strongly nonlinear systems in the presence of noise and/or outliers. The proposed method integrates the advantages of the fuzzy structure, the manifold learning, and the weighted least squares support vector machine (LS-SVM). First, the Gustafson–Kessel clustering algorithm (GKCA) is applied to split the training data set into several subsets to determine the fuzzy rules and premise parameters. Then, a new objective function is constructed based on the fuzzy structure, the weighted LS-SVM, and the manifold regularization, which takes into account robustness and the intrinsic geometry of the data. A solving method is further developed, from which the fuzzy model is achieved and can effectively approximate a nonlinear system with various types of random noise. The proposed method is applied to an artificial case as well as a practical hydraulic actuator, demonstrating its effectiveness in modeling of a nonlinear system even under noise. © 2017",Neurocomputing,10.1016/j.neucom.2017.12.019,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039432854&doi=10.1016%2fj.neucom.2017.12.019&partnerID=40&md5=6da384190c9cccfa834fdbe0e45cb1ab,2018,2021-07-20 15:50:10,2021-07-20 15:50:10
YP846REE,journalArticle,2018,"Roig, C.; Tardón, L.J.; Barbancho, I.; Barbancho, A.M.",A non-homogeneous beat-based harmony Markov model,"In this paper, a novel probabilistic model of harmonic progressions and a generation scheme based on such model are presented. On the basis of the large amount of publications that show the stochastic nature of the music and the possibility of modelling it by means of statistical processes, this paper shows how to create a non-homogeneous Markov chain to automatically generate harmonic progressions by building a temporal reference of the internal beat structure of music to guide the progressions. Thus, this new model develops on the classic transition matrix to include a beat-dependent / temporal layer to model the residency time. The method for the automatic creation of harmonic progressions based on the model developed is presented after the model. The harmonic progressions generated by our scheme are coherent with the style of the training data employed and, thanks to the specific temporal layer designed, the musical mid-term and long-term dependencies that lead to a natural and logic cadence are taken into account. The model developed is usable for the automatic generation of harmonic patterns that can be used to enlarge the flexibility and creativity of pattern-based computational music composers. © 2017 Elsevier B.V.",Knowledge-Based Systems,10.1016/j.knosys.2017.11.027,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044872115&doi=10.1016%2fj.knosys.2017.11.027&partnerID=40&md5=754872eb8fb4480904b110e6b2c73b76,2018,2021-07-20 15:50:10,2021-07-20 15:50:10
TZMTKKVF,journalArticle,2018,"Zhang, J.; Kowsari, K.; Harrison, Jr., J.H.; Lobo, J.M.; Barnes, L.E.",Patient2Vec: A Personalized Interpretable Deep Representation of the Longitudinal Electronic Health Record,"The wide implementation of electronic health record (EHR) systems facilitates the collection of large-scale health data from real clinical settings. Despite the significant increase in adoption of EHR systems, these data remain largely unexplored, but present a rich data source for knowledge discovery from patient health histories in tasks, such as understanding disease correlations and predicting health outcomes. However, the heterogeneity, sparsity, noise, and bias in these data present many complex challenges. This complexity makes it difficult to translate potentially relevant information into machine learning algorithms. In this paper, we propose a computational framework, Patient2Vec, to learn an interpretable deep representation of longitudinal EHR data, which is personalized for each patient. To evaluate this approach, we apply it to the prediction of future hospitalizations using real EHR data and compare its predictive performance with baseline methods. Patient2Vec produces a vector space with meaningful structure, and it achieves an area under curve around 0.799, outperforming baseline methods. In the end, the learned feature importance can be visualized and interpreted at both the individual and population levels to bring clinical insights. © 2018 IEEE.",IEEE Access,10.1109/ACCESS.2018.2875677,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055031252&doi=10.1109%2fACCESS.2018.2875677&partnerID=40&md5=db08fe095f9c2a6fd51edd6bc4c189f9,2018,2021-07-20 15:50:10,2021-07-20 15:50:10
JI2ZGIB3,journalArticle,2017,"Milošević, P.; Petrović, B.; Jeremić, V.",IFS-IBA similarity measure in machine learning algorithms,"The purpose of this paper is to introduce a novel similarity measure of intuitionistic fuzzy sets (IFSs). The proposed measure is based on the equivalence relation in the IFS-IBA approach. Due to the logic-based background, this measure compares IFS from a different viewpoint than the standard measures, emphasizing comprehension of intuitionism. The IFS-IBA similarity measure has a solid mathematical background and can be combined with various IF aggregation operators. Additionally, we define IFS-IBA distance function as a complement of IFS-IBA similarity. Both IFS-IBA similarity and distance functions may have different realizations that are easy to interpret. Hence, the measures are offering great descriptive power and the ability to model various problems. The benefits of the proposed measure are illustrated on the problem of pattern recognition and classification within k-NN algorithm. Finally, we show that the proposed measure is appropriate for IF hierarchical clustering on the problem of clustering Serbian medium-sized companies according to their financial ratios. Results obtained using the IFS-IBA measure are clear-cut and more meaningful compared to a standard IF distances regardless of the I-fuzzification method used. © 2017 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2017.07.048,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026648469&doi=10.1016%2fj.eswa.2017.07.048&partnerID=40&md5=1550c5523f7bd9e7d588b0ca2001fd90,2017,2021-07-20 15:50:10,2021-07-20 15:50:10
K4XTMZYM,journalArticle,2017,"Yeh, J.-W.; Su, S.-F.",Efficient Approach for RLS Type Learning in TSK Neural Fuzzy Systems,"This paper presents an efficient approach for the use of recursive least square (RLS) learning algorithm in Takagi-Sugeno-Kang neural fuzzy systems. In the use of RLS, reduced covariance matrix, of which the off-diagonal blocks defining the correlation between rules are set to zeros, may be employed to reduce computational burden. However, as reported in the literature, the performance of such an approach is slightly worse than that of using the full covariance matrix. In this paper, we proposed a so-called enhanced local learning concept in which a threshold is considered to stop learning for those less fired rules. It can be found from our experiments that the proposed approach can have better performances than that of using the full covariance matrix. Enhanced local learning method can be more active on the structure learning phase. Thus, the method not only can stop the update for insufficiently fired rules to reduce disturbances in self-constructing neural fuzzy inference network but also raises the learning speed on structure learning phase by using a large backpropagation learning constant. © 2017 IEEE.",IEEE Transactions on Cybernetics,10.1109/TCYB.2016.2638861,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85008499918&doi=10.1109%2fTCYB.2016.2638861&partnerID=40&md5=6845de787d0a533ad4f66f1bed3dc544,2017,2021-07-20 15:50:10,2021-07-20 15:50:10
IJRUKAQJ,journalArticle,2015,"Nguyen, T.; Khosravi, A.; Creighton, D.; Nahavandi, S.",Fuzzy system with tabu search learning for classification of motor imagery data,"This paper introduces an approach to classify EEG signals using wavelet transform and a fuzzy standard additive model (FSAM) with tabu search learning mechanism. Wavelet coefficients are ranked based on statistics of the Wilcoxon test. The most informative coefficients are assembled to form a feature set that serves as inputs to the tabu-FSAM. Two benchmark datasets, named Ia and Ib, downloaded from the brain-computer interface (BCI) competition II are employed for the experiments. Classification performance is evaluated using accuracy, mutual information, Gini coefficient and F-measure. Widely-used classifiers, including feedforward neural network, support vector machine, k-nearest neighbours, ensemble learning Adaboost and adaptive neuro-fuzzy inference system, are also implemented for comparisons. The proposed tabu-FSAM method considerably dominates the competitive classifiers, and outperforms the best performance on the Ia and Ib datasets reported in the BCI competition II. © 2015 Elsevier Ltd. All rights reserved.",Biomedical Signal Processing and Control,10.1016/j.bspc.2015.04.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929094355&doi=10.1016%2fj.bspc.2015.04.007&partnerID=40&md5=a4676d7547594865a3ce059c7b3906da,2015,2021-07-20 15:50:10,2021-07-20 15:50:10
ZUWRFP3V,journalArticle,2015,"Chung, J.; Kim, J.",Segment delay learning from quantized path delay measurements,"Our understanding on a silicon chip is limited due to low measurement resolution or model-silicon miscorrelation including variations. This paper shows that chips are better understood by combining noisy measurement results and model information through a mathematical algorithm. Our proposed method learns segment delays in logic circuits from quantized path delay measurements using ridge regression. During the learning process, we take advantage of both nominal segment delays and the delay sensitivity with respect to variations. We also interpret the ridge regression in Bayesian context and in doing so, propose an analytic formula to set the regularization parameter of the ridge regression. For the silicon measurement environments where low measurement resolution is the dominant source of measurement noise, this formula allows us to predict post-silicon results more accurately and speed up the algorithm eliminating inefficient and inaccurate cross-validation. We also demonstrate our method in enhancing the resolution of already measured path delays. We learn segment delays from quantized path delay measurements and predict the path delays prior to the quantization. Our simulation results show that the predicted path delays are much closer to actual values than the measured values and the nominal values. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2015.2419631,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930506305&doi=10.1109%2fTCAD.2015.2419631&partnerID=40&md5=ced63359013dfd6ee6e88d01a390c7d7,2015,2021-07-20 15:50:10,2021-07-20 15:50:10
KU7K434X,journalArticle,2014,"Liu, B.; Zhang, Z.; Xu, G.; Fan, H.; Fu, Q.",Energy efficient telemonitoring of physiological signals via compressed sensing: A fast algorithm and power consumption evaluation,"Wireless telemonitoring of physiological signals is an important topic in eHealth. In order to reduce on-chip energy consumption and extend sensor life, recorded signals are usually compressed before transmission. In this paper, we adopt compressed sensing (CS) as a low-power compression framework, and propose a fast block sparse Bayesian learning (BSBL) algorithm to reconstruct original signals. Experiments on real-world fetal ECG signals and epilepsy EEG signals showed that the proposed algorithm has good balance between speed and data reconstruction fidelity when compared to state-of-the-art CS algorithms. Further, we implemented the CS-based compression procedure and a low-power compression procedure based on a wavelet transform in field programmable gate array (FPGA), showing that the CS-based compression can largely save energy and other on-chip computing resources. © 2014 Elsevier Ltd.",Biomedical Signal Processing and Control,10.1016/j.bspc.2014.02.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897092406&doi=10.1016%2fj.bspc.2014.02.010&partnerID=40&md5=fd7d111f4cd374e5dea79df83a1812f5,2014,2021-07-20 15:50:10,2021-07-20 15:50:10
7E3ILDK6,journalArticle,2013,"Han, X.; Quan, L.; Xiong, X.; Wu, B.",Facing the classification of binary problems with a hybrid system based on quantum-inspired binary gravitational search algorithm and K-NN method,"Since given classification data often contains redundant, useless or misleading features, feature selection is an important pre-processing step for solving classification problems. This problem is often solved by applying evolutionary algorithms to decrease the dimensional number of features involved. Removing irrelevant features in the feature space and identifying relevant features correctly is the primary objective, which can increase classification accuracy. In this paper, a novel QBGSA-K-NN hybrid system which hybridizes the quantum-inspired binary gravitational search algorithm (QBGSA) with the K-nearest neighbor (K-NN) method with leave-one-out cross-validation (LOOCV) is proposed. The main aim of this system is to improve classification accuracy with an appropriate feature subset in binary problems. We evaluate the proposed hybrid system on several UCI machine learning benchmark examples. The experimental results show that the proposed method is able to select the discriminating input features correctly and achieve high classification accuracy which is comparable to or better than well-known similar classifier systems. © 2013 Elsevier Ltd. All rights reserved.",Engineering Applications of Artificial Intelligence,10.1016/j.engappai.2013.05.011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887019409&doi=10.1016%2fj.engappai.2013.05.011&partnerID=40&md5=e0f9a59215acf0953ecc268bbe457837,2013,2021-07-20 15:50:10,2021-07-20 15:50:10
JIKDADBI,journalArticle,2012,"Kampakis, S.",Improved Izhikevich neurons for spiking neural networks,"Spiking neural networks constitute a modern neural network paradigm that overlaps machine learning and computational neurosciences. Spiking neural networks use neuron models that possess a great degree of biological realism. The most realistic model of the neuron is the one created by Alan Lloyd Hodgkin and Andrew Huxley. However, the Hodgkin-Huxley model, while accurate, is computationally very inefficient. Eugene Izhikevich created a simplified neuron model based on the Hodgkin-Huxley equations. This model has better computational efficiency than the original proposed by Hodgkin and Huxley, and yet it can successfully reproduce all known firing patterns. However, there are not many articles dealing with implementations of this model for a functional neural network. This study presents a spiking neural network architecture that utilizes improved Izhikevich neurons with the purpose of evaluating its speed and efficiency. Since the field of spiking neural networks has reinvigorated the interest in biological plausibility, biological realism was an additional goal. The network is tested on the correct classification of logic gates (including XOR) and on the iris dataset. Results and possible improvements are also discussed. © 2011 Springer-Verlag.",Soft Computing,10.1007/s00500-011-0793-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861003884&doi=10.1007%2fs00500-011-0793-1&partnerID=40&md5=ae2f225404b7bac26c67a5ac5ff52563,2012,2021-07-20 15:50:10,2021-07-20 15:50:10
6Q97MQ9X,journalArticle,2019,"Causse, M.; Lancelot, F.; Maillant, J.; Behrend, J.; Cousy, M.; Schneider, N.",Encoding decisions and expertise in the operator's eyes: Using eye-tracking as input for system adaptation,"We investigated the possibility of developing a decision support system (DSS) that integrates eye-fixation measurements to better adapt its suggestions. Indeed, eye fixation give insight into human decision-making: Individuals tend to pay more attention to key information in line with their upcoming selection. Thus, eye-fixation measures can help the DSS to better capture the context that determines user decisions. Twenty-two participants performed a simplified Air Traffic Control (ATC) simulation in which they had to decide to accept or to modify route suggestions according to specific parameter values displayed on the screen. Decisions and fixation times on each parameter were recorded. The user fixation times were used by an algorithm to estimate the utility of each parameter for its decision. Immediately after this training phase, the algorithm generated new route suggestions under two conditions: 1) Taking into account the participant's decisions, 2) Taking into account the participant's decisions plus their visual behavior using the measurements of dwell times on displayed parameters. Results showed that system suggestions were more accurate than the base system when taking into account the participant's decisions, and even more accurate using their dwell times. Capturing the crucial information for the decision using the eye tracker accelerated the DSS learning phase, and thus helped to further enhance the accuracy of consecutive suggestions. Moreover, exploratory eye-tracking analysis reflected two different stages of the decision-making process, with longer dwell times on relevant parameters (i.e. involved in a rule) during the entire decision time course, and frequency of fixations on these relevant parameters that increased, especially during the last fixations prior to the decision. Consequently, future DSS integrating eye-tracking data should pay specific care to the final fixations prior to the decision. In general, our results emphasize the potential interest of eye-tracking to enhance and accelerate system adaptation to user preference, knowledge, and expertise. © 2018",International Journal of Human Computer Studies,10.1016/j.ijhcs.2018.12.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059175807&doi=10.1016%2fj.ijhcs.2018.12.010&partnerID=40&md5=4a4464a294ef063ee482813643746cf2,2019,2021-07-20 15:50:10,2021-07-20 15:50:10
9U46ZZIQ,journalArticle,2018,"Neto, H.C.; Julia, R.M.S.","ACE-RL-Checkers: decision-making adaptability through integration of automatic case elicitation, reinforcement learning, and sequential pattern mining","In agents that operate in environments where decision-making needs to take into account, not only the environment, but also the minimizing actions of an opponent (as in games), it is fundamental that the agent is endowed with the ability of progressively tracing the profile of its adversaries, in such a manner that this profile aids in the process of selecting appropriate actions. However, it would be unsuitable to construct an agent with a decision-making system based only on the elaboration of such a profile, as this would prevent the agent from having its “own identity,” which would leave the agent at the mercy of its opponent. Following this direction, this study proposes an automatic Checkers player, called ACE-RL-Checkers, equipped with a dynamic decision-making module, which adapts to the profile of the opponent over the course of the game. In such a system, the action selection process is conducted through a composition of multilayer perceptron neural network and case library. In this case, the neural network represents the “identity” of the agent, i.e., it is an already trained static decision-making module. On the other hand, the case library represents the dynamic decision-making module of the agent, which is generated by the Automatic Case Elicitation technique. This technique has a pseudo-random exploratory behavior, which allows the dynamic decision-making of the agent to be directed either by the opponent’s game profile or randomly. In order to avoid a high occurrence of pseudo-random decision-making in the game initial phases—in which the agent counts on very little information about its opponent—this work proposes a new module based on sequential pattern mining for generating a base of experience rules extracted from human expert’s game records. This module will improve the agent’s move selection in the game initial phases. Experiments carried out in tournaments involving ACE-RL-Checkers and other agents correlated to this work, confirm the superiority of the dynamic architecture proposed herein. © 2018, Springer-Verlag London Ltd., part of Springer Nature.",Knowledge and Information Systems,10.1007/s10115-018-1175-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042529071&doi=10.1007%2fs10115-018-1175-0&partnerID=40&md5=37acf1c66ca96736f9162cfe821e8871,2018,2021-07-20 15:50:10,2021-07-20 15:50:10
IQDXYGXM,journalArticle,2017,"Yang, S.; Wei, R.; Guo, J.; Xu, L.",Semantic Inference on Clinical Documents: Combining Machine Learning Algorithms with an Inference Engine for Effective Clinical Diagnosis and Treatment,"Clinical practice calls for reliable diagnosis and optimized treatment. However, human errors in health care remain a severe issue even in industrialized countries. The application of clinical decision support systems (CDSS) casts light on this problem. However, given the great improvement in CDSS over the past several years, challenges to their wide-scale application are still present, including: 1) decision making of CDSS is complicated by the complexity of the data regarding human physiology and pathology, which could render the whole process more time-consuming by loading big data related to patients; and 2) information incompatibility among different health information systems (HIS) makes CDSS an information island, i.e., additional input work on patient information might be required, which would further increase the burden on clinicians. One popular strategy is the integration of CDSS in HIS to directly read electronic health records (EHRs) for analysis. However, gathering data from EHRs could constitute another problem, because EHR document standards are not unified. In addition, HIS could use different default clinical terminologies to define input data, which could cause additional misinterpretation. Several proposals have been published thus far to allow CDSS access to EHRs via the redefinition of data terminologies according to the standards used by the recipients of the data flow, but they mostly aim at specific versions of CDSS guidelines. This paper views these problems in a different way. Compared with conventional approaches, we suggest more fundamental changes; specifically, uniform and updatable clinical terminology and document syntax should be used by EHRs, HIS, and their integrated CDSS. Facilitated data exchange will increase the overall data loading efficacy, enabling CDSS to read more information for analysis at a given time. Furthermore, a proposed CDSS should be based on self-learning, which dynamically updates a knowledge model according to the data-stream-based upcoming data set. The experiment results show that our system increases the accuracy of the diagnosis and treatment strategy designs. © 2016 IEEE.",IEEE Access,10.1109/ACCESS.2017.2672975,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017653450&doi=10.1109%2fACCESS.2017.2672975&partnerID=40&md5=05437be4d2e747f8ea5fe8459f60c009,2017,2021-07-20 15:50:10,2021-07-20 15:50:10
KTSEP3MD,journalArticle,2016,"Madeo, R.C.B.; Peres, S.M.; Lima, C.A.D.M.",Gesture phase segmentation using support vector machines,"An interaction between humans or between a human and a machine will be more effective if it is supported by gestures. In different levels of complexity, the communication system used in human interaction includes the use of gesture. In natural conversation, for instance, speakers use gestures for both to enhance the expressiveness of their speech and to support their own linguistic reasoning. The audience absorbs the content being transmitted also based on the speakers' gesticulation. Thus, an analysis of gestures should add value to the purpose of the interaction. One of the concerns in the analysis of gestures is the problem arising from the segmentation of phases of a gesture (rest position, preparation, stroke, hold and retraction), which, from the standpoint of Gesture Theory, may reveal information on prosody and semantics of what is being said in a discourse. Finding an automation solution to this problem involves enabling the development of theoretical and application areas that are based on the analysis of human behavior and on the interpretation and generation of natural language. In this study, the problem of gesture phase segmentation is modeled as a problem of classification, and then support vector machine is employed to design a model able to learn the patterns of gesture that are inherent to each phase. This work presents two main highlights. The first is to address the limitations of the segmentation approach through the study of its performance in different scenarios that represent the complexity of analyzing patterns of human behavior. In this study, we reached an F-score around 0.9 for rest position and around 0.8 for stroke and preparation as segmentation results in the best cases. Moreover, it was possible to investigate how classification models are influenced by human behavior. The second highlight refers to the conduction of an analysis by considering the standpoint of specialists concerned with gesture phase segmentation in the area of Linguistics and Psycholinguistics, through which we obtained impressive results. Thus, in regard to the suitability of our approach, it is a feasible means of supporting the development of the Gesture Theory as well as the Computational Linguistics and Human Machine Interaction fields. © 2016 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2016.02.021,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961645015&doi=10.1016%2fj.eswa.2016.02.021&partnerID=40&md5=0c2c263570350ccc8474635de81fa5c5,2016,2021-07-20 15:50:10,2021-07-20 15:50:10
HP5QINKM,journalArticle,2020,"Chung, K.; Jung, H.",Knowledge-based dynamic cluster model for healthcare management using a convolutional neural network,"Due to recent growing interest, the importance of preventive and efficient healthcare using big data scattered throughout various IoT devices is being emphasized in healthcare, as well in the IT field. The analysis of information in healthcare is mainly prediction using a user’s basic information and static data from a knowledge base. In this study, a knowledge-based dynamic cluster model using a convolutional neural network (CNN) is suggested for healthcare recommendations. The suggested method carries out a process to extend static data and a previous knowledge base from an ontology-based ambient-context knowledge base beyond knowledge-based healthcare management, which was the focus of previous study. It is possible to acquire and expand a large amount of high-quality information by reproducing inferred knowledge using a CNN, which is a deep-learning algorithm. A dynamic cluster model is developed, and the accuracy of the predictions is improved in order to enable recommendations on healthcare according to a user environment that changes over time and based on environmental factors as dynamic elements, rather than static elements. Also, the accuracy of the predictions is verified through a performance evaluation between the suggested method and the previous method to validate effectiveness, and an approximate 13% performance improvement was confirmed. Currently, the acquisition of knowledge from unstructured data is in its early stages. It is expected that symbolic knowledge-acquisition technology from unstructured information that is produced and that changes in real time, and the dynamic cluster model method suggested in this study, will become the core technologies that promote the development of healthcare management technology. © 2019, Springer Science+Business Media, LLC, part of Springer Nature.",Information Technology and Management,10.1007/s10799-019-00304-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070884713&doi=10.1007%2fs10799-019-00304-1&partnerID=40&md5=9420682fe9104ce95468d666b8a31a26,2020,2021-07-20 15:50:11,2021-07-20 15:50:11
GIGBKUH9,journalArticle,2019,"Santiso, S.; Pérez, A.; Casillas, A.",Exploring Joint AB-LSTM with Embedded Lemmas for Adverse Drug Reaction Discovery,"This work focuses on the detection of adverse drug reactions (ADRs) in electronic health records (EHRs) written in Spanish. The World Health Organization underlines the importance of reporting ADRs for patients' safety. The fact is that ADRs tend to be under-reported in daily hospital praxis. In this context, automatic solutions based on text mining can help to alleviate the workload of experts. Nevertheless, these solutions pose two challenges: 1) EHRs show high lexical variability, the characterization of the events must be able to deal with unseen words or contexts and 2) ADRs are rare events, hence, the system should be robust against skewed class distribution. To tackle these challenges, deep neural networks seem appropriate because they allow a high-level representation. Specifically, we opted for a joint AB-LSTM network, a sub-class of the bidirectional long short-term memory network. Besides, in an attempt to reinforce lexical variability, we proposed the use of embeddings created using lemmas. We compared this approach with supervised event extraction approaches based on either symbolic or dense representations. Experimental results showed that the joint AB-LSTM approach outperformed previous approaches, achieving an f-measure of 73.3. © 2013 IEEE.",IEEE Journal of Biomedical and Health Informatics,10.1109/JBHI.2018.2879744,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056203488&doi=10.1109%2fJBHI.2018.2879744&partnerID=40&md5=1427bef3cb00ca3ba1c3c2afbbfab443,2019,2021-07-20 15:50:11,2021-07-20 15:50:11
J4NN3VZU,journalArticle,2019,"Ji, X.; Shen, H.-W.; Ritter, A.; MacHiraju, R.; Yen, P.-Y.",Visual Exploration of Neural Document Embedding in Information Retrieval: Semantics and Feature Selection,"Neural embeddings are widely used in language modeling and feature generation with superior computational power. Particularly, neural document embedding-converting texts of variable-length to semantic vector representations-has shown to benefit widespread downstream applications, e.g., information retrieval (IR). However, the black-box nature makes it difficult to understand how the semantics are encoded and employed. We propose visual exploration of neural document embedding to gain insights into the underlying embedding space, and promote the utilization in prevalent IR applications. In this study, we take an IR application-driven view, which is further motivated by biomedical IR in healthcare decision-making, and collaborate with domain experts to design and develop a visual analytics system. This system visualizes neural document embeddings as a configurable document map and enables guidance and reasoning; facilitates to explore the neural embedding space and identify salient neural dimensions (semantic features) per task and domain interest; and supports advisable feature selection (semantic analysis) along with instant visual feedback to promote IR performance. We demonstrate the usefulness and effectiveness of this system and present inspiring findings in use cases. This work will help designers/developers of downstream applications gain insights and confidence in neural document embedding, and exploit that to achieve more favorable performance in application domains. © 1995-2012 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2019.2903946,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065410305&doi=10.1109%2fTVCG.2019.2903946&partnerID=40&md5=df1fbeb94218af7c1a004a35ef4e2b2d,2019,2021-07-20 15:50:11,2021-07-20 15:50:11
GS3TRUYZ,journalArticle,2018,"Hussain, A.; Cambria, E.",Semi-supervised learning for big social data analysis,"In an era of social media and connectivity, web users are becoming increasingly enthusiastic about interacting, sharing, and working together through online collaborative media. More recently, this collective intelligence has spread to many different areas, with a growing impact on everyday life, such as in education, health, commerce and tourism, leading to an exponential growth in the size of the social Web. However, the distillation of knowledge from such unstructured Big data is, an extremely challenging task. Consequently, the semantic and multimodal contents of the Web in this present day are, whilst being well suited for human use, still barely accessible to machines. In this work, we explore the potential of a novel semi-supervised learning model based on the combined use of random projection scaling as part of a vector space model, and support vector machines to perform reasoning on a knowledge base. The latter is developed by merging a graph representation of commonsense with a linguistic resource for the lexical representation of affect. Comparative simulation results show a significant improvement in tasks such as emotion recognition and polarity detection, and pave the way for development of future semi-supervised learning approaches to big social data analytics. © 2017 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2017.10.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032371736&doi=10.1016%2fj.neucom.2017.10.010&partnerID=40&md5=8ba4edf6e121c55005f6c8a0894821a8,2018,2021-07-20 15:50:11,2021-07-20 15:50:11
YQWL34MG,journalArticle,2015,"Fiedler, L.J.; Sucar, L.E.; Morales, E.F.",Transfer learning for temporal nodes Bayesian networks,"Traditional machine learning algorithms depend heavily on the assumption that there is sufficient data to learn a reliable model. This is not always the case, and in situations where data is limited, transfer learning can be applied to compensate for the lack of information by learning from several sources. In this work, we present a novel methodology for inducing a Temporal Nodes Bayesian Network (TNBN) when training data is scarce by applying a transfer learning strategy. A TNBN is a probabilistic graphical model that offers a compact representation for dynamic domains by defining multiple time intervals in which events can occur. Learning a TNBN poses additional challenges to learning traditional Bayesian networks due to the incorporation of time intervals. Our proposal incorporates novel approaches to transfer knowledge from several TNBNs to learn the structure, parameters and intervals of a target TNBN. To evaluate our algorithm, we performed experiments with a synthetic network, where we created auxiliary models by altering the structure, parameters and temporal intervals of the original model. Results show that the proposed algorithm is capable of retrieving a reliable model even when few records are available for the target domain. We also performed experiments with a real-world data set belonging to the medical domain of HIV, where we were able to learn some documented mutational pathways and their temporal relations by applying transfer learning. © 2015, Springer Science+Business Media New York.",Applied Intelligence,10.1007/s10489-015-0662-1,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941873992&doi=10.1007%2fs10489-015-0662-1&partnerID=40&md5=943ef07bd6ac772e4e66e87506172e3c,2015,2021-07-20 15:50:11,2021-07-20 15:50:11
W9TMPWD3,journalArticle,2014,"Trammell, B.; Casas, P.; Rossi, D.; Bär, A.; Houidi, Z.B.; Leontiadis, I.; Szemethy, T.; Mellia, M.",MPlane: An intelligent measurement plane for the internet,"The Internet¿s universality is based on its decentralization and diversity. However, its distributed nature leads to operational brittleness and difficulty in identifying the root causes of performance and availability issues, especially when the involved systems span multiple administrative domains. The first step to address this fragmentation is coordinated measurement: we propose to complement the current Internet¿s data and control planes with a measurement plane, or mPlane for short. mPlane¿s distributed measurement infrastructure collects and analyzes traffic measurements at a wide variety of scales to monitor the network status. Its architecture is centered on a flexible control interface, allowing the incorporation of existing measurement tools through lightweight mPlane proxy components, and offering dynamic support for new capabilities. A focus on automated, iterative measurement makes the platform well-suited to troubleshooting support. This is supported by a reasoning system, which applies machine learning algorithms to learn from success and failure in drilling down to the root cause of a problem. This article describes the mPlane architecture and shows its applicability to several distributed measurement problems involving content delivery networks and Internet service roviders. A first case study presents the tracking and iterative analysis of cache selection policies in Akamai, while a second example focuses on the cooperation between Internet service providers and content delivery networks to better orchestrate their traffic engineering decisions and jointly improve their performance. © 2014 IEEE.",IEEE Communications Magazine,10.1109/MCOM.2014.6815906,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901254721&doi=10.1109%2fMCOM.2014.6815906&partnerID=40&md5=f7173779463aebbcdf2a333e332744eb,2014,2021-07-20 15:50:11,2021-07-20 15:50:11
CTKS4HDY,journalArticle,2013,"Didandeh, A.; Mirbakhsh, N.; Afsharchi, M.",Concept learning games: An ontological study in multi-agent systems,"In this paper, we intend to have a game theoretic study on the concept learning problem in a multi-agent system. Concept learning is a very essential and well-studied domain of machine learning when it is studied under the characteristics of a multi-agent system. The most important reasons are the partiality of the environment perception for any agent and also the communication holdbacks, resulting into a deep need for a collaborative protocol in favor of multi-agent transactions. Here we wish to investigate multi-agent concept learning with the help of its components, thoroughly with a game theoretic taste, esp. on the pre-learning processes. Based on two standard notations, we address the non-unanimity of concepts, classification of objects, voting and communicating protocol, and also the learning itself. In such a game of concept learning, we consider a group of agents, communicating and consulting to upgrade their ontologies based on their conceptualizations of the environment. For this purpose, we investigate the problem in two separate and standard distinctions of game theory study, cooperation and competition. Several solution concepts and innovative ideas from the multi-agent realm are used to produce an approach that contains the reasoning process of the agents in this system. Some experimentations come at the end to show the functionality of our approach. These experimentations come distinctly for both cooperative and competitive views. © 2012 Springer Science+Business Media, LLC.",Information Systems Frontiers,10.1007/s10796-012-9343-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84882450501&doi=10.1007%2fs10796-012-9343-3&partnerID=40&md5=6498e905e35d81b2732a4f69cc44812c,2013,2021-07-20 15:50:11,2021-07-20 15:50:11
YGHML3ZJ,journalArticle,2013,"Dagan, I.; Roth, D.; Sammons, M.; Zanzotto, F.",Recognizing Textual Entailment: Models and Applications,"Download Free Sample In the last few years, a number of NLP researchers have developed and participated in the task of Recognizing Textual Entailment (RTE). This task encapsulates Natural Language Understanding capabilities within a very simple interface: Recognizing when the meaning of a text snippet is contained in the meaning of a second piece of text. This simple abstraction of an exceedingly complex problem has broad appeal partly because it can be conceived also as a component in other NLP applications, from Machine Translation to Semantic Search to Information Extraction. It also avoids commitment to any specific meaning representation and reasoning framework, broadening its appeal within the research community. This level of abstraction also facilitates evaluation, a crucial component of any technological advancement program. This book explains the RTE task formulation adopted by the NLP research community, and gives a clear overview of research in this area. It draws out commonalities in this research, detailing the intuitions behind dominant approaches and their theoretical underpinnings. This book has been written with a wide audience in mind, but is intended to inform all readers about the state of the art in this fascinating field, to give a clear understanding of the principles underlying RTE research to date, and to highlight the short- A nd long-term research goals that will advance this technology. © Morgan and Claypool Publishers. All rights reserved.",Synthesis Lectures on Human Language Technologies,10.2200/S00509ED1V01Y201305HLT023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044727708&doi=10.2200%2fS00509ED1V01Y201305HLT023&partnerID=40&md5=a84d226bcb23ac1ce56a2a02c9430a3f,2013,2021-07-20 15:50:11,2021-07-20 15:50:11
QKLCCZ63,journalArticle,2015,"Cellier, P.; Charnois, T.; Plantevit, M.; Rigotti, C.; Crémilleux, B.; Gandrillon, O.; Kléma, J.; Manguin, J.-L.",Sequential pattern mining for discovering gene interactions and their contextual information from biomedical texts,"Background: Discovering gene interactions and their characterizations from biological text collections is a crucial issue in bioinformatics. Indeed, text collections are large and it is very difficult for biologists to fully take benefit from this amount of knowledge. Natural Language Processing (NLP) methods have been applied to extract background knowledge from biomedical texts. Some of existing NLP approaches are based on handcrafted rules and thus are time consuming and often devoted to a specific corpus. Machine learning based NLP methods, give good results but generate outcomes that are not really understandable by a user. Results: We take advantage of an hybridization of data mining and natural language processing to propose an original symbolic method to automatically produce patterns conveying gene interactions and their characterizations. Therefore, our method not only allows gene interactions but also semantics information on the extracted interactions (e.g., modalities, biological contexts, interaction types) to be detected. Only limited resource is required: the text collection that is used as a training corpus. Our approach gives results comparable to the results given by state-of-the-art methods and is even better for the gene interaction detection in AIMed. Conclusions: Experiments show how our approach enables to discover interactions and their characterizations. To the best of our knowledge, there is few methods that automatically extract the interactions and also associated semantics information. The extracted gene interactions from PubMed are available through a simple web interface at https://bingotexte.greyc.fr/. The software is available at https://bingo2.greyc.fr/?q=node/22. © 2015 Cellier et al.; licensee BioMed Central.",Journal of Biomedical Semantics,10.1186/s13326-015-0023-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938814624&doi=10.1186%2fs13326-015-0023-3&partnerID=40&md5=4618d62e741ca36792c94214962a99a2,2015,2021-07-20 15:50:11,2021-07-20 15:50:11
NPYJ5FID,journalArticle,2021,"Lu, F.; Chen, B.; Zhou, X.-D.; Song, D.",STA-VPR: Spatio-Temporal Alignment for Visual Place Recognition,"Recently, the methods based on Convolutional Neural Networks (CNNs) have gained popularity in the field of visual place recognition (VPR). In particular, the features from the middle layers of CNNs are more robust to drastic appearance changes than handcrafted features and high-layer features. Unfortunately, the holistic mid-layer features lack robustness to large viewpoint changes. Here we split the holistic mid-layer features into local features, and propose an adaptive dynamic time warping (DTW) algorithm to align local features from the spatial domain while measuring the distance between two images. This realizes viewpoint-invariant and condition-invariant place recognition. Meanwhile, a local matching DTW (LM-DTW) algorithm is applied to perform image sequence matching based on temporal alignment, which achieves further improvements and ensures linear time complexity. We perform extensive experiments on five representative VPR datasets. The results show that the proposed method significantly improves the CNN-based methods. Moreover, our method outperforms several state-of-the-art methods while maintaining good run-time performance. This work provides a novel way to boost the performance of CNN methods without any re-training for VPR. The code is available at https://github.com/Lu-Feng/STA-VPR. © 2016 IEEE.",IEEE Robotics and Automation Letters,10.1109/LRA.2021.3067623,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85103275968&doi=10.1109%2fLRA.2021.3067623&partnerID=40&md5=73a7e8e3e98f60d3373592477f61b033,2021,2021-07-20 15:50:11,2021-07-20 15:50:11
5VH3E5M9,journalArticle,2021,"Yao, J.; Ansari, N.",Caching in Dynamic IoT Networks by Deep Reinforcement Learning,"The sensing service of Internet-of-Things (IoT) networks enables IoT sensors to sense the environment information (e.g., temperature and traffic conditions) and send them through the IoT gateway to the users who request those information. The explosive growth of IoT users and sensors injects massive traffic into IoT networks and easily depletes the battery of IoT sensors. Caching at the IoT gateway is hence a promising solution to mitigate this problem by storing popular IoT data at the IoT gateway and sending them directly to the users instead of activating IoT sensors to transmit the data. In our work, we investigate the content placement problem, which determines data to be cached at each time epoch in dynamic IoT networks with the objective to minimize the average data transmission delay constrained by the cache storage capacity and IoT data freshness. We formulate our problem as an integer linear programming (ILP) problem and then model it as a Markov decision process (MDP). A deep reinforcement learning algorithm is proposed to solve this problem and its performances are demonstrated via extensive simulations. © 2014 IEEE.",IEEE Internet of Things Journal,10.1109/JIOT.2020.3004394,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101674073&doi=10.1109%2fJIOT.2020.3004394&partnerID=40&md5=86afd2be7854b1ffd9ae9e7be91e2173,2021,2021-07-20 15:50:11,2021-07-20 15:50:11
YL4ZI2G8,journalArticle,2021,"Liu, Z.; Liu, Q.; Wang, L.; Xu, W.; Zhou, Z.",Task-level decision-making for dynamic and stochastic human-robot collaboration based on dual agents deep reinforcement learning,"Human-robot collaboration as a multidisciplinary research topic is still pursuing the robots’ enhanced intelligence to be more human-compatible and fit the dynamic and stochastic characteristics of human. However, the uncertainties brought by the human partner challenge the task-planning and decision-making of the robot. When aiming at industrial tasks like collaborative assembly, dynamics on temporal dimension and stochasticities on the order of procedures need to be further considered. In this work, we bring a new perspective and solution based on reinforcement learning, where the problem is regarded as training an agent towards tasks in dynamic and stochastic environments. Concretely, an adapted training approach based on the deep Q learning method is proposed. This method regards both the robot and the human as the agents in the interactive training environment for deep reinforcement learning. With the consideration of task-level industrial human-robot collaboration, the training logic and the agent-environment interaction have been proposed. For the human-robot collaborative assembly tasks in the case study, it is illustrated that our method could drive the robot represented by one agent to collaborate with the human partner even the human performs randomly on the task procedures. © 2021, The Author(s), under exclusive licence to Springer-Verlag London Ltd., part of Springer Nature.",International Journal of Advanced Manufacturing Technology,10.1007/s00170-021-07265-2,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85107793442&doi=10.1007%2fs00170-021-07265-2&partnerID=40&md5=807819d100526947ce319ce1c9d37fee,2021,2021-07-20 15:50:11,2021-07-20 15:50:11
5J8RTJVP,journalArticle,2021,"Zhou, B.; Tsai, Y.; Chen, X.; Duncan, J.S.; Liu, C.",MDPET: A Unified Motion Correction and Denoising Adversarial Network for Low-dose Gated PET,"In positron emission tomography (PET), gating is commonly utilized to reduce respiratory motion blurring and to facilitate motion correction methods. In application where low-dose gated PET is useful, reducing injection dose causes increased noise levels in gated images that could corrupt motion estimation and subsequent corrections, leading to inferior image quality. To address these issues, we propose MDPET, a unified motion correction and denoising adversarial network for generating motion-compensated low-noise images from low-dose gated PET data. Specifically, we proposed a Temporal Siamese Pyramid Network (TSP-Net) with basic units made up of 1.) Siamese Pyramid Network (SP-Net), and 2.) a recurrent layer for motion estimation among the gates. The denoising network is unified with our motion estimation network to simultaneously correct the motion and predict a motion-compensated denoised PET reconstruction. The experimental results on human data demonstrated that our MDPET can generate accurate motion estimation directly from low-dose gated images and produce high-quality motion-compensated low-noise reconstructions. Comparative studies with previous methods also show that our MDPET is able to generate superior motion estimation and denoising performance. Our code is available at https://github.com/bbbbbbzhou/MDPET. CCBY",IEEE Transactions on Medical Imaging,10.1109/TMI.2021.3076191,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105114461&doi=10.1109%2fTMI.2021.3076191&partnerID=40&md5=6c4ebf8f597305775fb935f4ca6d0d3e,2021,2021-07-20 15:50:11,2021-07-20 15:50:11
52VZV6RF,journalArticle,2021,"Xu, S.; Guo, C.; Hu, R.Q.; Qian, Y.",BlockChain Inspired Secure Computation Offloading in a Vehicular Cloud Network,"With the emergence of computation intensive vehicular applications, computation offloading based on mobile edge computing (MEC) has become a promising paradigm in resource constrained vehicular cloud networks (VCNs). However, when doing computation offloading in a VCN, malicious service providers can cause serious security concerns on the content offloading. To address that, in this paper a blockchain based secure computation offloading scheduling scheme is proposed. It embraces the blockchain based trust management paradigm and smart contract enabled Deep Reinforcement Learning (DRL) algorithm. As for the trust management, the long-term reputation and short-term trust variability are jointly considered. Specifically, a novel three-valued subjective logic (3VSL) scheme is adopted to obtain a more comprehensive reputation, and the statistics of behavioral transitions can provide a short-term trust variability to timely capture the malicious behaviors. In addition, to securely update, validate, and store the trust information, we propose a hierarchical blockchain framework that comprises vehicular blockchain, RSU blockchain, and cloud blockchain. Furthermore, a smart contract enabled DRL algorithm is proposed to implement the secure and intelligent computation offloading scheduling in a VCN. Simulations are conducted to verify the effectiveness of the proposed scheme. IEEE",IEEE Internet of Things Journal,10.1109/JIOT.2021.3054866,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100505523&doi=10.1109%2fJIOT.2021.3054866&partnerID=40&md5=97b0aa3f960646233b4afde121810c52,2021,2021-07-20 15:50:11,2021-07-20 15:50:11
PTRMN82I,journalArticle,2020,"Longari, S.; Valcarcel, D.H.N.; Zago, M.; Carminati, M.; Zanero, S.",CANnolo: An Anomaly Detection System based on LSTM Autoencoders for Controller Area Network,"Automotive security has gained significant traction in the last decade thanks to the development of new connectivity features that have brought the vehicle from an isolated environment to an externally facing domain. Researchers have shown that modern vehicles are vulnerable to multiple types of attacks leveraging remote, direct and indirect physical access, which allow attackers to gain control and affect safety-critical systems. Conversely, Intrusion Detection Systems (IDSs) have been proposed by both industry and academia to identify attacks and anomalous behaviours. In this paper, we propose CANnolo, an IDS based on Long Short-Term Memory (LSTM)-autoencoders to identify anomalies in Controller Area Networks (CANs). During a training phase, CANnolo automatically analyzes the CAN streams and builds a model of the legitimate data sequences. Then, it detects anomalies by computing the difference between the reconstructed and the respective real sequences. We experimentally evaluated CANnolo on a set of simulated attacks applied over a real-world dataset. We show that our approach outperforms the state-of-the-art model by improving the detection rate and precision. IEEE",IEEE Transactions on Network and Service Management,10.1109/TNSM.2020.3038991,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85097160312&doi=10.1109%2fTNSM.2020.3038991&partnerID=40&md5=1a054c90b979a01a9bfb34989d6b3f69,2020,2021-07-20 15:50:12,2021-07-20 15:50:12
XXWA6IDE,journalArticle,2019,"Li, L.; Sau, C.; Fanni, T.; Li, J.; Viitanen, T.; Christophe, F.; Palumbo, F.; Raffo, L.; Huttunen, H.; Takala, J.; Bhattacharyya, S.S.",An integrated hardware/software design methodology for signal processing systems,"This paper presents a new methodology for design and implementation of signal processing systems on system-on-chip (SoC) platforms. The methodology is centered on the use of lightweight application programming interfaces for applying principles of dataflow design at different layers of abstraction. The development processes integrated in our approach are software implementation, hardware implementation, hardware-software co-design, and optimized application mapping. The proposed methodology facilitates development and integration of signal processing hardware and software modules that involve heterogeneous programming languages and platforms. As a demonstration of the proposed design framework, we present a dataflow-based deep neural network (DNN) implementation for vehicle classification that is streamlined for real-time operation on embedded SoC devices. Using the proposed methodology, we apply and integrate a variety of dataflow graph optimizations that are important for efficient mapping of the DNN system into a resource constrained implementation that involves cooperating multicore CPUs and field-programmable gate array subsystems. Through experiments, we demonstrate the flexibility and effectiveness with which different design transformations can be applied and integrated across multiple scales of the targeted computing system. © 2019 The Authors",Journal of Systems Architecture,10.1016/j.sysarc.2018.12.010,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059818575&doi=10.1016%2fj.sysarc.2018.12.010&partnerID=40&md5=2f3902bc568cc09c8481791002ee85a9,2019,2021-07-20 15:50:12,2021-07-20 15:50:12
P958T24R,journalArticle,2019,"Li, G.; Zhou, H.; Feng, B.; Zhang, Y.; Yu, S.",Efficient Provision of Service Function Chains in Overlay Networks using Reinforcement Learning,"Software-Defined Networking (SDN) and Network Functions Virtualization (NFV) technologies facilitate deploying Service Function Chains (SFCs) at clouds in efficiency and flexibility. However, it is still challenging to efficiently chain Virtualized Network Functions (VNFs) in overlay networks without knowledge of underlying network configurations. Although there are many deterministic approaches for VNF placement and chaining, they have high complexity and depend on state information of substrate networks. Fortunately, Reinforcement Learning (RL) brings opportunities to alleviate this challenge as it can learn to make suitable decisions without prior knowledge. Therefore, in this paper, we propose an RL approach for efficient SFC provision in overlay networks, where the same VNFs provided by multiple vendors are with different performance. Specifically, we first formulate the problem into an Integer Linear Programming (ILP) model for benchmarking. Then, we present the online SFC path selection into a Markov Decision Process (MDP) and propose a corresponding policy-gradient-based solution. Finally, we evaluate our proposed approach with extensive simulations with randomly generated SFC requests and a real-world video streaming dataset, and implement an emulation system for feasibility verification. Related results demonstrate that performance of our approach is close to the ILP-based method and better than deep Q-learning, random, and load-least-greedy methods. IEEE",IEEE Transactions on Cloud Computing,10.1109/TCC.2019.2961093,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081986340&doi=10.1109%2fTCC.2019.2961093&partnerID=40&md5=21f6b0e82a7039e670fcca2f778002e4,2019,2021-07-20 15:50:12,2021-07-20 15:50:12
SZEMRD6T,journalArticle,2018,"Merrikh-Bayat, F.; Guo, X.; Klachko, M.; Prezioso, M.; Likharev, K.K.; Strukov, D.B.",High-Performance Mixed-Signal Neurocomputing with Nanoscale Floating-Gate Memory Cell Arrays,"Potential advantages of analog-and mixed-signal nanoelectronic circuits, based on floating-gate devices with adjustable conductance, for neuromorphic computing had been realized long time ago. However, practical realizations of this approach suffered from using rudimentary floating-gate cells of relatively large area. Here, we report a prototype 28\times28 binary-input, ten-output, three-layer neuromorphic network based on arrays of highly optimized embedded nonvolatile floating-gate cells, redesigned from a commercial 180-nm nor flash memory. All active blocks of the circuit, including 101 780 floating-gate cells, have a total area below 1 mm2. The network has shown a 94.7% classification fidelity on the common Modified National Institute of Standards and Technology benchmark, close to the 96.2% obtained in simulation. The classification of one pattern takes a sub-1-\mu \texts time and a sub-20-nJ energy-both numbers much better than in the best reported digital implementations of the same task. Estimates show that a straightforward optimization of the hardware and its transfer to the already available 55-nm technology may increase this advantage to more than 10^2\times in speed and 10^4\times in energy efficiency. © 2012 IEEE.",IEEE Transactions on Neural Networks and Learning Systems,10.1109/TNNLS.2017.2778940,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039794463&doi=10.1109%2fTNNLS.2017.2778940&partnerID=40&md5=7899d5fa9da65a927732479d2439364b,2018,2021-07-20 15:50:12,2021-07-20 15:50:12
LTG4H2T7,journalArticle,2017,"Ullah, A.; Ahmad, J.; Muhammad, K.; Sajjad, M.; Baik, S.W.",Action Recognition in Video Sequences using Deep Bi-Directional LSTM with CNN Features,"Recurrent neural network (RNN) and long short-Term memory (LSTM) have achieved great success in processing sequential multimedia data and yielded the state-of-The-Art results in speech recognition, digital signal processing, video processing, and text data analysis. In this paper, we propose a novel action recognition method by processing the video data using convolutional neural network (CNN) and deep bidirectional LSTM (DB-LSTM) network. First, deep features are extracted from every sixth frame of the videos, which helps reduce the redundancy and complexity. Next, the sequential information among frame features is learnt using DB-LSTM network, where multiple layers are stacked together in both forward pass and backward pass of DB-LSTM to increase its depth. The proposed method is capable of learning long term sequences and can process lengthy videos by analyzing features for a certain time interval. Experimental results show significant improvements in action recognition using the proposed method on three benchmark data sets including UCF-101, YouTube 11 Actions, and HMDB51 compared with the state-of-The-Art action recognition methods. © 2013 IEEE.",IEEE Access,10.1109/ACCESS.2017.2778011,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037618132&doi=10.1109%2fACCESS.2017.2778011&partnerID=40&md5=38d7c2b98984f96167b86954102307ef,2017,2021-07-20 15:50:12,2021-07-20 15:50:12
6YT3A43E,journalArticle,2019,"Li, Z.; Li, J.; Ren, A.; Cai, R.; Ding, C.; Qian, X.; Draper, J.; Yuan, B.; Tang, J.; Qiu, Q.; Wang, Y.",HEIF: Highly Efficient Stochastic Computing-Based Inference Framework for Deep Neural Networks,"Deep convolutional neural networks (DCNNs) are one of the most promising deep learning techniques and have been recognized as the dominant approach for almost all recognition and detection tasks. The computation of DCNNs is memory intensive due to large feature maps and neuron connections, and the performance highly depends on the capability of hardware resources. With the recent trend of wearable devices and Internet of Things, it becomes desirable to integrate the DCNNs onto embedded and portable devices that require low power and energy consumptions and small hardware footprints. Recently stochastic computing (SC)-DCNN demonstrated that SC as a low-cost substitute to binary-based computing radically simplifies the hardware implementation of arithmetic units and has the potential to satisfy the stringent power requirements in embedded devices. In SC, many arithmetic operations that are resource-consuming in binary designs can be implemented with very simple hardware logic, alleviating the extensive computational complexity. It offers a colossal design space for integration and optimization due to its reduced area and soft error resiliency. In this paper, we present HEIF, a highly efficient SC-based inference framework of the large-scale DCNNs, with broad applications including (but not limited to) LeNet-5 and AlexNet, that achieves high energy efficiency and low area/hardware cost. Compared to SC-DCNN, HEIF features: 1) the first (to the best of our knowledge) SC-based rectified linear unit activation function to catch up with the recent advances in software models and mitigate degradation in application-level accuracy; 2) the redesigned approximate parallel counter and optimized stochastic multiplication using transmission gates and inverse mirror adders; and 3) the new optimization of weight storage using clustering. Most importantly, to achieve maximum energy efficiency while maintaining acceptable accuracy, HEIF considers holistic optimizations on cascade connection of function blocks in DCNN, pipelining technique, and bit-stream length reduction. Experimental results show that in large-scale applications HEIF outperforms previous SC-DCNN by the throughput of 4.1 ×, by area efficiency of up to 6.5 ×, and achieves up to 5.6 × energy improvement. © 1982-2012 IEEE.",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2018.2852752,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049489419&doi=10.1109%2fTCAD.2018.2852752&partnerID=40&md5=9210404cfc6b62237810447577a55094,2019,2021-07-20 15:50:12,2021-07-20 15:50:12
UVB23K7X,journalArticle,2019,"Jovic, A.; Brkic, K.; Krstacic, G.",Detection of congestive heart failure from short-term heart rate variability segments using hybrid feature selection approach,"Objectives: The aim of this work is to investigate the accuracy limits of automated detection of congestive heart failure (CHF) from short-term heart rate variability (HRV) series. Short-term HRV analysis uses 5-minute segments from HRV recordings to diagnose a disorder. This work proposes a hybrid feature selection procedure aimed at finding highly accurate models containing only a few highly informative features, which enables physiological interpretation of the features relevant for the model. Materials and methods: Short-term HRV segments are analyzed for CHF diagnosis. Subjects' records from four public PhysioNet databases are considered (66 healthy subjects and 42 CHF subjects). The problem is approached from a machine learning perspective, by extracting 111 linear time domain, frequency domain, time-frequency, nonlinear and symbolic dynamics HRV features. A multistage hybrid feature selection method is proposed that eventually eliminates most features. The method uses a symmetrical uncertainty filter, Naive Bayes wrapper with best first search, and final greedy iterative feature elimination. For classification purposes, we use rotation forest (RTF), radial based support vector machines (SVM), random forest (RF), multilayer perceptron artificial neural network, and k-nearest neighbors’ classifiers in order to evaluate the feature sets at each step of the process and to obtain as accurate model as possible. Leave-one-subject-out cross-validation evaluation method was used, with two variants: subject-level (coarse-grained) and feature vector-level (fine-grained). Results: The results show that the feature selection method is capable of either improving or retaining the classification accuracy of the full feature set (RTF: subject-level ACC = 88.9%, feature vector-level ACC = 85.6%; SVM: subject-level ACC = 89.8%, feature vector-level ACC = 83.5%; RF: subject-level ACC = 87.0%, feature vector-level ACC = 85.5%), while greatly reducing the number of included features, to only four HRV features for RTF and RF, and only two HRV features for SVM. The resulting best models for subject-level classification achieved are: RTF: ACC = 90.7%, SENS = 78.6%, SPEC = 98.6%, obtained with features: LF/HF ratio, maximum alphabet entropy, alphabet entropy variance, and HaarWaveletSD (scale = 8); SVM: ACC = 88.0%, SENS = 78.6%, SPEC = 93.9%, obtained with features: LF/HF ratio and Rate_U; RF: ACC = 90.7%, SENS = 78.6%, SPEC = 98.6%, obtained with features: LF/HF ratio, maximum alphabet entropy, Rate_U, and Rate_B. Other classifiers provided similar, but somewhat lower results. A comparison of the procedure with the results of individual filter, wrapper, and simple hybrid approaches is provided, which demonstrates the efficiency of the proposed procedure. Conclusions: The results suggest that the method can achieve accurate generalizable models for automated diagnosis of CHF from short-term HRV segments in subjects with very few informative features. The choice of the best features and the classification results are similar between the three best classifiers, so the use of any of them with the proposed method is recommended. Nonlinear and symbolic dynamics features are shown to have an important role in the resulting models. The presented methodology may be useful for first-hand screening for CHF as well as for similar diagnostic or automated detection problems in biomedicine. © 2019 Elsevier Ltd",Biomedical Signal Processing and Control,10.1016/j.bspc.2019.101583,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067361211&doi=10.1016%2fj.bspc.2019.101583&partnerID=40&md5=f27b4ba0876a8dcee33883cbe96a4ef4,2019,2021-07-20 15:50:12,2021-07-20 15:50:12
9VPCE6XF,journalArticle,2021,"Reddy, S.S.; Sethi, N.; Rajender, R.",Mining of multiple ailments correlated to diabetes mellitus,"Efficient and user friendly database technologies have enabled the digitization of information pertaining to the medical domain. This has not only eased the smooth record manipulation but also attracted man a researchers to explore certain challenges to solve through implementation of data mining tools and techniques. Among the nature of ailments, the information related to diabetes mellitus (DM) are found to be the maximally digitized. This has provided a challenging but buzzing platform for the researchers to do in-depth analysis and present modern edge solutions which can lead to early diagnosis of the fatal ailment. There arise numerous side-effects to a human body when it is affected by DM. These multiple ailments attack a human body with the direct or indirect influence of DM and it’s corresponding drug intake. Thus, there has been a demand for a generic scheme which can predict the likeliness of certain multiple ailments that a DM patient is supposed to be attacked by in near future. In this work, a suitable scheme has been proposed in the same direction. This scheme provides a viable platform where the probabilities of multiple ailments for a DM patient can be computed. The proposed scheme also provides the probabilities of occurrence of individual ailment as well as the probabilities of occurrence of certain combination of the ailments. Occurrence of three of the major ailment are being computed in this work. These are retinal disorder, kidney malfunction, and heart disease. A Fuzzy logic strategy has been used for matching several disease constraints and produce a decisive outcome. Certain number of novel heuristic functions are presented which take these outputs and provide a probabilistically accurate prediction of occurrences of the said ailments. Suitable experimental evaluation have been made with proper data inputs. The proposed scheme has also been compared with competent schemes. An overall rates of accuracy of 97% is calculated based on a k-fold cross validation performance metric. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Evolutionary Intelligence,10.1007/s12065-020-00432-6,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086373758&doi=10.1007%2fs12065-020-00432-6&partnerID=40&md5=887f40c784264a912040f6a90ed4ab15,2021,2021-07-20 15:50:12,2021-07-20 15:50:12
UJIATK82,journalArticle,2018,"Lau, H.; Lee, C.K.M.; Nakandala, D.; Shum, P.",An outcome-based process optimization model using fuzzy-based association rules,"Purpose: The purpose of this paper is to propose an outcome-based process optimization model which can be deployed in companies to enhance their business operations, strengthening their competitiveness in the current industrial environment. To validate the approach, a case example has been included to assess the practicality and validity of this approach to be applied in actual environment. Design/methodology/approach: This model embraces two approaches including: fuzzy logic for mimicking the human thinking and decision making mechanism; and data mining association rules approach for optimizing the analyzed knowledge for future decision-making as well as providing a mechanism to apply the obtained knowledge to support the improvement of different types of processes. Findings: The new methodology of the proposed algorithm has been evaluated in a case study and the algorithm shows its potential to determine the primary factors that have a great effect upon the final result of the entire operation comprising a number of processes. In this case example, relevant process parameters have been identified as the important factors causing significant impact on the result of final outcome. Research limitations/implications: The proposed methodology requires the dependence on human knowledge and personal experience to determine the various fuzzy regions of the processes. This can be fairly subjective and even biased. As such, it is advisable that the development of artificial intelligence techniques to support automatic machine learning to derive the fuzzy sets should be promoted to provide more reliable results. Originality/value: Recent study on the relevant topics indicates that an intelligent process optimization approach, which is able to interact seamlessly with the knowledge-based system and extract useful information for process improvement, is still seen as an area that requires more study and investigation. In this research, the process optimization system with an effective process mining algorithm embedded for supporting knowledge discovery is proposed for use to achieve better quality control. © 2018, Emerald Publishing Limited.",Industrial Management and Data Systems,10.1108/IMDS-08-2017-0347,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051821082&doi=10.1108%2fIMDS-08-2017-0347&partnerID=40&md5=95b7289629665d69d8febc07177ea624,2018,2021-07-20 15:50:12,2021-07-20 15:50:12
8UG3LWZ2,journalArticle,2018,"Chebouba, L.; Miannay, B.; Boughaci, D.; Guziolowski, C.",Discriminate the response of Acute Myeloid Leukemia patients to treatment by using proteomics data and Answer Set Programming,"Background: During the last years, several approaches were applied on biomedical data to detect disease specific proteins and genes in order to better target drugs. It was shown that statistical and machine learning based methods use mainly clinical data and improve later their results by adding omics data. This work proposes a new method to discriminate the response of Acute Myeloid Leukemia (AML) patients to treatment. The proposed approach uses proteomics data and prior regulatory knowledge in the form of networks to predict cancer treatment outcomes by finding out the different Boolean networks specific to each type of response to drugs. To show its effectiveness we evaluate our method on a dataset from the DREAM 9 challenge. Results: The results are encouraging and demonstrate the benefit of our approach to distinguish patient groups with different response to treatment. In particular each treatment response group is characterized by a predictive model in the form of a signaling Boolean network. This model describes regulatory mechanisms which are specific to each response group. The proteins in this model were selected from the complete dataset by imposing optimization constraints that maximize the difference in the logical response of the Boolean network associated to each group of patients given the omic dataset. This mechanistic and predictive model also allow us to classify new patients data into the two different patient response groups. Conclusions: We propose a new method to detect the most relevant proteins for understanding different patient responses upon treatments in order to better target drugs using a Prior Knowledge Network and proteomics data. The results are interesting and show the effectiveness of our method. © 2018 The Author(s).",BMC Bioinformatics,10.1186/s12859-018-2034-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043481702&doi=10.1186%2fs12859-018-2034-4&partnerID=40&md5=e44810b09f76264c703acc8337daa6a3,2018,2021-07-20 15:50:12,2021-07-20 15:50:12
JND5WIDK,journalArticle,2017,"Patel, A.; Alhussian, H.; Pedersen, J.M.; Bounabat, B.; Júnior, J.C.; Katsikas, S.",A nifty collaborative intrusion detection and prevention architecture for Smart Grid ecosystems,"Smart Grid (SG) systems are critical, intelligent infrastructure utility services connected through open networks that are potentially susceptible to cyber-attacks with very acute security risks of shutdown, loss of life, and loss of revenue. Traditional intrusion detection systems based on signature and anomaly techniques are no longer sufficient to protect SGs due to their new connectivity and management challenges, the ever-rapidly-evolving masquerades, and cyber criminality levied against them. SGs require cyber-security systems to render them resilient and protected through advanced Intrusion Detection and Prevention System (IDPS) techniques and mechanisms. This paper proposes a smart collaborative advanced IDPS to provide the best possible protection of SGs with a fully distributed management structure that supports the network and host based detections and the prevention of attacks. By facilitating a reliable, scalable, and flexible design, the specific requirements of IDPS for SGs can be more easily met via a fuzzy risk analyzer, an independent and ontology knowledge-based inference engine module. These can work collaboratively by managing functions across multiple IDPS domains. A set of extensive and intensive simulated experiments shows that with its smart advanced components incorporating soft computing machine-learning techniques and a rich ontology knowledge base with fuzzy logic analysis, it detects and prevents intrusions more efficiently. The multi-faceted results of the simulation also show that the proposed Collaborative Smart IDPS (CSIDPS) system increases the intrusion detection accuracy and decreases the false positive alarms when compared to traditional IDPSs. This is epitomized by the skillful use of the confusion matrix technique for organizing classifiers, visualizing their performance, and assessing their overall behavior. In the final analysis, the CSIDPS architecture is designed toward contributing to de facto norms for SG ecosystems. © 2016 Elsevier Ltd",Computers and Security,10.1016/j.cose.2016.07.002,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994874812&doi=10.1016%2fj.cose.2016.07.002&partnerID=40&md5=5f6f90e6647d272108664e9c5f43cf5e,2017,2021-07-20 15:50:12,2021-07-20 15:50:12
GDILT24M,journalArticle,2021,"Neto, M.P.; Paulovich, F.V.",Explainable matrix - Visualization for global and local interpretability of random forest classification ensembles,"Over the past decades, classification models have proven to be essential machine learning tools given their potential and applicability in various domains. In these years, the north of the majority of the researchers had been to improve quantitative metrics, notwithstanding the lack of information about models' decisions such metrics convey. This paradigm has recently shifted, and strategies beyond tables and numbers to assist in interpreting models' decisions are increasing in importance. Part of this trend, visualization techniques have been extensively used to support classification models' interpretability, with a significant focus on rule-based models. Despite the advances, the existing approaches present limitations in terms of visual scalability, and the visualization of large and complex models, such as the ones produced by the Random Forest (RF) technique, remains a challenge. In this paper, we propose Explainable Matrix (ExMatrix), a novel visualization method for RF interpretability that can handle models with massive quantities of rules. It employs a simple yet powerful matrix-like visual metaphor, where rows are rules, columns are features, and cells are rules predicates, enabling the analysis of entire models and auditing classification results. ExMatrix applicability is confirmed via different examples, showing how it can be used in practice to promote RF models interpretability. © 1995-2012 IEEE.",IEEE Transactions on Visualization and Computer Graphics,10.1109/TVCG.2020.3030354,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85099560009&doi=10.1109%2fTVCG.2020.3030354&partnerID=40&md5=0d5cc6152b79d9fdaaea0d1dd6aebf3b,2021,2021-07-20 15:50:12,2021-07-20 15:50:12
XX7WS372,journalArticle,2019,"Li, Z.; Yang, Q.; Chen, S.; Zhou, W.; Chen, L.; Song, L.",A fuzzy recurrent neural network for driver fatigue detection based on steering-wheel angle sensor data,"The study of the robust fatigue feature learning method for the driver’s operational behavior is of great significance for improving the performance of the real-time detection system for driver’s fatigue state. Aiming at how to extract more abstract and deep features in the driver’s direction operation data in the robust feature learning, this article constructs a fuzzy recurrent neural network model, which includes input layer, fuzzy layer, hidden layer, and output layer. The steering-wheel direction sensing time series sends the time series to the input layer through a fixed time window. After the fuzzification process, it is sent to the hidden layer to share the weight of the hidden layer, realize the memorization of the fatigue feature, and improve the feature depth capability of the steering wheel angle time sequence. The experimental results show that the proposed model achieves an average recognition rate of 87.30% in the fatigue sample database of real vehicle conditions, which indicates that the model has strong robustness to different subjects under real driving conditions. The model proposed in this article has important theoretical and engineering significance for studying the prediction of fatigue driving under real driving conditions. © The Author(s) 2019.",International Journal of Distributed Sensor Networks,10.1177/1550147719872452,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073027120&doi=10.1177%2f1550147719872452&partnerID=40&md5=64304901a70ca5093c30a0fbdd2ce371,2019,2021-07-20 15:50:12,2021-07-20 15:50:12
XANCSBUN,journalArticle,2018,"Arokia Jesu Prabhu, L.; Jayachandran, A.",Mixture Model Segmentation System for Parasagittal Meningioma brain Tumor Classification based on Hybrid Feature Vector,"Meningioma is the one of the most common type of brain tumor, it as arises from the meninges and encloses the spine and the brain inside the skull. It accounts for 30% of all types of brain tumor. Meningioma’s can occur in many parts of the brain and accordingly it is named. In this paper, a mixture model based classification of meningioma brain tumor using MRI image is developed. The proposed method consists of four stages. In the first stage, with respect to the cells’ boundary, it is necessary to further processing, which ensures the boundary of some cells is a discrete region. Mathematical Morphology brings a fancy result during the discrete processing. Accurate cancer cell nucleus segmentation is necessary for automated cytological image analysis. Thresholding is a crucial step in segmentation.An adaptive binarization technique is an important step for medical image analysis.Finally, a novel hybrid Fuzzy SVM is designed in the classification stage meningioma brain tumor. The tumor classification results of proposed feature extraction with SVM is 74.24%, MM with FSVM is 82.67% and MM with RBF is 62.71% and our proposed method MM with Hybrid SVM is 91.64%. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Journal of Medical Systems,10.1007/s10916-018-1094-3,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056105883&doi=10.1007%2fs10916-018-1094-3&partnerID=40&md5=d5c7a32e82b1f39618eb8b45235569aa,2018,2021-07-20 15:50:12,2021-07-20 15:50:12
8KIU5K89,journalArticle,2018,"Cao, L.; Wang, Y.; Zhang, B.; Jin, Q.; Vasilakos, A.V.",GCHAR: An efficient Group-based Context—aware human activity recognition on smartphone,"With smartphones increasingly becoming ubiquitous and being equipped with various sensors, nowadays, there is a trend towards implementing HAR (Human Activity Recognition) algorithms and applications on smartphones, including health monitoring, self-managing system and fitness tracking. However, one of the main issues of the existing HAR schemes is that the classification accuracy is relatively low, and in order to improve the accuracy, high computation overhead is needed. In this paper, an efficient Group-based Context-aware classification method for human activity recognition on smartphones, GCHAR is proposed, which exploits hierarchical group-based scheme to improve the classification efficiency, and reduces the classification error through context awareness rather than the intensive computation. Specifically, GCHAR designs the two-level hierarchical classification structure, i.e., inter-group and inner-group, and utilizes the previous state and transition logic (so-called context awareness) to detect the transitions among activity groups. In comparison with other popular classifiers such as RandomTree, Bagging, J48, BayesNet, KNN and Decision Table, thorough experiments on the realistic dataset (UCI HAR repository) demonstrate that GCHAR achieves the best classification accuracy, reaching 94.1636%, and time consumption in training stage of GCHAR is four times shorter than the simple Decision Table and is decreased by 72.21% in classification stage in comparison with BayesNet. © 2017 Elsevier Inc.",Journal of Parallel and Distributed Computing,10.1016/j.jpdc.2017.05.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021154566&doi=10.1016%2fj.jpdc.2017.05.007&partnerID=40&md5=814ab55d969d23f43bfb9bfad7e64617,2018,2021-07-20 15:50:13,2021-07-20 15:50:13
LVJ8NGHX,journalArticle,2017,"Martínez-Plumed, F.; Ferri, C.; Hernández-Orallo, J.; Ramírez-Quintana, M.J.",A computational analysis of general intelligence tests for evaluating cognitive development,"The progression in several cognitive tests for the same subjects at different ages provides valuable information about their cognitive development. One question that has caught recent interest is whether the same approach can be used to assess the cognitive development of artificial systems. In particular, can we assess whether the ‘fluid’ or ‘crystallised’ intelligence of an artificial cognitive system is changing during its cognitive development as a result of acquiring more concepts? In this paper, we address several IQ tests problems (odd-one-out problems, Raven's Progressive Matrices and Thurstone's letter series) with a general learning system that is not particularly designed on purpose to solve intelligence tests. The goal is to better understand the role of the basic cognitive operational constructs (such as identity, difference, order, counting, logic, etc.) that are needed to solve these intelligence test problems and serve as a proof-of-concept for evaluation in other developmental problems. From here, we gain some insights into the characteristics and usefulness of these tests and how careful we need to be when applying human test problems to assess the abilities and cognitive development of robots and other artificial cognitive systems. © 2017 Elsevier B.V.",Cognitive Systems Research,10.1016/j.cogsys.2017.01.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013779387&doi=10.1016%2fj.cogsys.2017.01.006&partnerID=40&md5=ed77f43a824db0a3c0903ba7991c4bad,2017,2021-07-20 15:50:13,2021-07-20 15:50:13
MPD5LQBT,journalArticle,2017,"Berg, J.; Järvisalo, M.",Cost-optimal constrained correlation clustering via weighted partial Maximum Satisfiability,"Integration of the fields of constraint solving and data mining and machine learning has recently been identified within the AI community as an important research direction with high potential. This work contributes to this direction by providing a first study on the applicability of state-of-the-art Boolean optimization procedures to cost-optimal correlation clustering under constraints in a general similarity-based setting. We develop exact formulations of the correlation clustering task as Maximum Satisfiability (MaxSAT), the optimization version of the Boolean satisfiability (SAT) problem. For obtaining cost-optimal clusterings, we apply a state-of-the-art MaxSAT solver for solving the resulting MaxSAT instances optimally, resulting in cost-optimal clusterings. We experimentally evaluate the MaxSAT-based approaches to cost-optimal correlation clustering, both on the scalability of our method and the quality of the clusterings obtained. Furthermore, we show how the approach extends to constrained correlation clustering, where additional user knowledge is imposed as constraints on the optimal clusterings of interest. We show experimentally that added user knowledge allows clustering larger datasets, and at the same time tends to decrease the running time of our approach. We also investigate the effects of MaxSAT-level preprocessing, symmetry breaking, and the choice of the MaxSAT solver on the efficiency of the approach. © 2015 Elsevier B.V.",Artificial Intelligence,10.1016/j.artint.2015.07.001,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937879126&doi=10.1016%2fj.artint.2015.07.001&partnerID=40&md5=33fec67b6a5969100d0b96856c3a9630,2017,2021-07-20 15:50:13,2021-07-20 15:50:13
RF2K8SKR,journalArticle,2016,"Tian, E.; Yue, D.",Decentralized fuzzy H∞ filtering for networked interconnected systems under communication constraints,"This study investigates decentralized fuzzy H∞ filtering for a class of network-based interconnected systems. The considered system has the following main features: (1) it is a nonlinear system approximated by a T-S fuzzy model; (2) subsystems are connected through wired or wireless networks and therefore signal transfer in/among subsystems is subject to communication delay and/or packet loss; and (3) the network transmission capacity is limited. A discrete decentralized event-triggering scheme (DDETS) is proposed to overcome the network bandwidth limitation, which can reduce data transmission in subsystems effectively. By employing the above main features and using the DDETS, a new system model is proposed. Then the Lyapunov functional approach is applied to development of two stability conditions (Theorems 1 and 2), which are used to design filters and triggering matrices of each subsystem simultaneously. Finally, an example is used to demonstrate the application of the proposed method. © 2015 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2015.12.023,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84952359490&doi=10.1016%2fj.neucom.2015.12.023&partnerID=40&md5=c50deb935c6a249c419a36200c780a37,2016,2021-07-20 15:50:13,2021-07-20 15:50:13
W4YGE6EP,journalArticle,2015,"Baldoni, R.; Montanari, L.; Rizzuto, M.",On-line failure prediction in safety-critical systems,"In safety-critical systems such as Air Traffic Control system, SCADA systems, Railways Control Systems, there has been a rapid transition from monolithic systems to highly modular ones, using off-the-shelf hardware and software applications possibly developed by different manufactures. This shift increased the probability that a fault occurring in an application propagates to others with the risk of a failure of the entire safety-critical system. This calls for new tools for the on-line detection of anomalous behaviors of the system, predicting thus a system failure before it happens, allowing the deployment of appropriate mitigation policies. The paper proposes a novel architecture, namely CASPER, for online failure prediction that has the distinctive features to be (i) black-box: no knowledge of applications internals and logic of the system is required (ii) non-intrusive: no status information of the components is used such as CPU or memory usage; The architecture has been implemented to predict failures in a real Air Traffic Control System. CASPER exhibits high degree of accuracy in predicting failures with low false positive rate. The experimental validation shows how operators are provided with predictions issued a few hundred of seconds before the occurrence of the failure. © 2014 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2014.11.015,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84917709364&doi=10.1016%2fj.future.2014.11.015&partnerID=40&md5=acfb1ee8837904e22aac3c2a8c577d28,2015,2021-07-20 15:50:13,2021-07-20 15:50:13
C88SJL9M,journalArticle,2012,"Chikh, M.A.; Saidi, M.; Settouti, N.",Diagnosis of Diabetes Diseases Using An Artificial Immune Recognition System2 (AIRS2) with Fuzzy K-nearest neighbor,"The use of expert systems and artificial intelligence techniques in disease diagnosis has been increasing gradually. Artificial Immune Recognition System (AIRS) is one of the methods used in medical classification problems. AIRS2 is a more efficient version of the AIRS algorithm. In this paper, we used a modified AIRS2 called MAIRS2 where we replace the K- nearest neighbors algorithm with the fuzzy K-nearest neighbors to improve the diagnostic accuracy of diabetes diseases. The diabetes disease dataset used in our work is retrieved from UCI machine learning repository. The performances of the AIRS2 and MAIRS2 are evaluated regarding classification accuracy, sensitivity and specificity values. The highest classification accuracy obtained when applying the AIRS2 and MAIRS2 using 10-fold cross-validation was, respectively 82.69% and 89.10%. © 2011 Springer Science+Business Media, LLC.",Journal of Medical Systems,10.1007/s10916-011-9748-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867298235&doi=10.1007%2fs10916-011-9748-4&partnerID=40&md5=70ec047e59af131e157416e5fc3d48bc,2012,2021-07-20 15:50:13,2021-07-20 15:50:13
64PUCVJG,journalArticle,2011,"Tscherepanow, M.; Kortkamp, M.; Kammer, M.",A hierarchical ART network for the stable incremental learning of topological structures and associations from noisy data,"In this article, a novel unsupervised neural network combining elements from Adaptive Resonance Theory and topology-learning neural networks is presented. It enables stable on-line clustering of stationary and non-stationary input data by learning their inherent topology. Here, two network components representing two different levels of detail are trained simultaneously. By virtue of several filtering mechanisms, the sensitivity to noise is diminished, which renders the proposed network suitable for the application to real-world problems. Furthermore, we demonstrate that this network constitutes an excellent basis to learn and recall associations between real-world associative keys. Its incremental nature ensures that the capacity of the corresponding associative memory fits the amount of knowledge to be learnt. Moreover, the formed clusters efficiently represent the relations between the keys, even if noisy data is used for training. In addition, we present an iterative recall mechanism to retrieve stored information based on one of the associative keys used for training. As different levels of detail are learnt, the recall can be performed with different degrees of accuracy. © 2011 Elsevier Ltd.",Neural Networks,10.1016/j.neunet.2011.05.009,https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051797262&doi=10.1016%2fj.neunet.2011.05.009&partnerID=40&md5=290a338d3926bd13842ca0201db60bd4,2011,2021-07-20 15:50:13,2021-07-20 15:50:13
6E8IIJBD,journalArticle,2018,"Weitschek, E.; Lauro, S.D.; Cappelli, E.; Bertolazzi, P.; Felici, G.",CamurWeb: A classification software and a large knowledge base for gene expression data of cancer,"Background: The high growth of Next Generation Sequencing data currently demands new knowledge extraction methods. In particular, the RNA sequencing gene expression experimental technique stands out for case-control studies on cancer, which can be addressed with supervised machine learning techniques able to extract human interpretable models composed of genes, and their relation to the investigated disease. State of the art rule-based classifiers are designed to extract a single classification model, possibly composed of few relevant genes. Conversely, we aim to create a large knowledge base composed of many rule-based models, and thus determine which genes could be potentially involved in the analyzed tumor. This comprehensive and open access knowledge base is required to disseminate novel insights about cancer. Results: We propose CamurWeb, a new method and web-based software that is able to extract multiple and equivalent classification models in form of logic formulas (""if then"" rules) and to create a knowledge base of these rules that can be queried and analyzed. The method is based on an iterative classification procedure and an adaptive feature elimination technique that enables the computation of many rule-based models related to the cancer under study. Additionally, CamurWeb includes a user friendly interface for running the software, querying the results, and managing the performed experiments. The user can create her profile, upload her gene expression data, run the classification analyses, and interpret the results with predefined queries. In order to validate the software we apply it to all public available RNA sequencing datasets from The Cancer Genome Atlas database obtaining a large open access knowledge base about cancer. CamurWeb is available at http://bioinformatics.iasi.cnr.it/camurweb. Conclusions: The experiments prove the validity of CamurWeb, obtaining many classification models and thus several genes that are associated to 21 different cancer types. Finally, the comprehensive knowledge base about cancer and the software tool are released online; interested researchers have free access to them for further studies and to design biological experiments in cancer research. © 2018 The Author(s).",BMC Bioinformatics,10.1186/s12859-018-2299-7,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054818125&doi=10.1186%2fs12859-018-2299-7&partnerID=40&md5=5ae491efd4c1f19cf43523530ec44345,2018,2021-07-20 15:50:13,2021-07-20 15:50:13
C429YPM9,journalArticle,2015,"Papageorgiou, E.I.; Subramanian, J.; Karmegam, A.; Papandrianos, N.",A risk management model for familial breast cancer: A new application using Fuzzy Cognitive Map method,"Breast cancer is the most deadly disease affecting women and thus it is natural for women aged 40-49 years (who have a family history of breast cancer or other related cancers) to assess their personal risk for developing familial breast cancer (FBC). Besides, as each individual woman possesses different levels of risk of developing breast cancer depending on their family history, genetic predispositions and personal medical history, individualized care setting mechanism needs to be identified so that appropriate risk assessment, counseling, screening, and prevention options can be determined by the health care professionals. The presented work aims at developing a soft computing based medical decision support system using Fuzzy Cognitive Map (FCM) that assists health care professionals in deciding the individualized care setting mechanisms based on the FBC risk level of the given women. The FCM based FBC risk management system uses NHL to learn causal weights from 40 patient records and achieves a 95% diagnostic accuracy. The results obtained from the proposed model are in concurrence with the comprehensive risk evaluation tool based on Tyrer-Cuzick model for 38/40 patient cases (95%). Besides, the proposed model identifies high risk women by calculating higher accuracy of prediction than the standard Gail and NSAPB models. The testing accuracy of the proposed model using 10-fold cross validation technique outperforms other standard machine learning based inference engines as well as previous FCM-based risk prediction methods for BC. © 2015 Elsevier Ireland Ltd.",Computer Methods and Programs in Biomedicine,10.1016/j.cmpb.2015.07.003,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944281066&doi=10.1016%2fj.cmpb.2015.07.003&partnerID=40&md5=58dc08d5377bf5e9a387ef9a64cfc460,2015,2021-07-20 15:50:13,2021-07-20 15:50:13
SBAVWUXC,journalArticle,2021,"Lu, Y.; Chen, Y.; Zhao, D.; Li, D.",MGRL: Graph neural network based inference in a Markov network with reinforcement learning for visual navigation,"Visual navigation is an essential task for indoor robots and usually uses the map as assistance to providing global information for the agent. Because the traditional maps match the environments, the map-based and map-building-based navigation methods are limited in the new environments for obtaining maps. Although the deep reinforcement learning navigation method, utilizing the non-map-based navigation technique, achieves satisfactory performance, it lacks the interpretability and the global view of the environment. Therefore, we propose a novel abstract map for the deep reinforcement learning navigation method with better global relative position information and more reasonable interpretability. The abstract map is modeled as a Markov network which is used for explicitly representing the regularity of objects arrangement, influenced by people activities in different environments. Besides, a knowledge graph is utilized to initialize the structure of the Markov network, as providing the prior structure for the model and reducing the difficulty of model learning. Then, a graph neural network is adopted for probability inference in the Markov network. Furthermore, the update of the abstract map, including the knowledge graph structure and the parameters of the graph neural network, are combined into an end-to-end learning process trained by a reinforcement learning method. Finally, experiments in the AI2THOR framework and the physical environment indicate that our algorithm greatly improves the success rate of navigation in case of new environments, thus confirming the good generalization. © 2020 Elsevier B.V.",Neurocomputing,10.1016/j.neucom.2020.07.091,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85095687056&doi=10.1016%2fj.neucom.2020.07.091&partnerID=40&md5=9de6a13ab2cd29e871157583a9f86dab,2021,2021-07-20 15:50:13,2021-07-20 15:50:13
XX94M9IT,journalArticle,2020,"Hua, X.; Wang, X.; Rui, T.; Zhang, H.; Wang, D.",A fast self-attention cascaded network for object detection in large scene remote sensing images,"Aiming at the real-time detection of multiple objects and micro-objects in large-scene remote sensing images, a cascaded convolutional neural network real-time object-detection framework for remote sensing images is proposed, which integrates visual perception and convolutional memory network reasoning. The detection framework is composed of two fully convolutional networks, namely, the strengthened object self-attention pre-screening fully convolutional network (SOSA-FCN) and the object accurate detection fully convolutional network (AD-FCN). SOSA-FCN introduces a self-attention module to extract attention feature maps and constructs a deep feature pyramid to optimize the attention feature maps by combining convolutional long-term and short-term memory networks. It guides the acquisition of potential sub-regions of the object in the scene, reduces the computational complexity, and enhances the network's ability to extract multi-scale object features. It adapts to the complex background and small object characteristics of a large-scene remote sensing image. In AD-FCN, the object mask and object orientation estimation layer are designed to achieve fine positioning of candidate frames. The performance of the proposed algorithm is compared with that of other advanced methods on NWPU_VHR-10, DOTA, UCAS-AOD, and other open datasets. The experimental results show that the proposed algorithm significantly improves the efficiency of object detection while ensuring detection accuracy and has high adaptability. It has extensive engineering application prospects. © 2020 Elsevier B.V.",Applied Soft Computing Journal,10.1016/j.asoc.2020.106495,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087197858&doi=10.1016%2fj.asoc.2020.106495&partnerID=40&md5=4d5272aec0ce8d6fda1e39d5e79554d4,2020,2021-07-20 15:50:13,2021-07-20 15:50:13
LA3ZCUJQ,journalArticle,2018,"Gao, F.; Yu, J.; Zhu, S.; Huang, Q.; Tian, Q.",Blind image quality prediction by exploiting multi-level deep representations,"Blind image quality assessment (BIQA) aims at precisely estimating human perceived image quality with no access to a reference. Recently, several attempts have been made to develop BIQA methods based on deep neural networks (DNNs). Although these methods obtained promising performance, they have some limitations: (1) their DNN models are actually ”shallow” in term of depth; and (2) these methods typically use the output of the last layer in the DNN model as the feature representation for quality prediction. Since the representation depth has been demonstrated beneficial for various vision tasks, it is significant to explore very deep networks for learning BIQA models. Besides, the information in the last layer may unduly generalize over local artifacts which are highly related to quality degradation. On the contrary, intermediate layers may be sensitive to local degradations but will not capture high-level semantics. Thus, reasoning at multiple levels of representation is necessary in the IQA task. In this paper, we propose to extract multi-level representations from a very deep DNN model for learning an effective BIQA model, and consequently present a simple but extraordinarily effective BIQA framework, codenamed BLINDER (BLind Image quality predictioN via multi-level DEep Representations). Thorough experiments have been conducted on five standard databases, which show that a significant improvement can be achieved by adopting multi-level deep representations. Besides, BLINDER considerably outperforms previous state-of-the-art BIQA methods for authentically distorted images. © 2018 Elsevier Ltd",Pattern Recognition,10.1016/j.patcog.2018.04.016,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046353120&doi=10.1016%2fj.patcog.2018.04.016&partnerID=40&md5=b9841039a38cc15439e76eb82b6f8ff4,2018,2021-07-20 15:50:13,2021-07-20 15:50:13
9P9QE6TC,journalArticle,2019,"Kumuthaveni, R.; Chandra, E.",Iterative Conditional Entropy Kalman filter (ICEKF) for noise reduction and Neuro Optimized Emotional Classifier (NOEC),"Emotion has a most important aspect in terms of interactions among the humans and this would become ideal for human emotions to get mechanically identified by the machines and primarily for enhancing the communication among the human–machine. In the recent work Enhanced Bat algorithm with Simulated Annealing (EBSA) are introduced for solving emotion recognition problem. Here the removal of noises from the speech samples and reduction in the number of speech features becomes very difficult task which reduces the accuracy of the classifier. To solve this problem this research work involves detection of emotions from speech which stimulates machines understanding human behavioral tasks namely reasoning, decision making and interaction. EBSA is used in the previous system to identify the happy, sad and neutral emotions from speech input. The performance of the previous system has been decreased due to recognition accuracy and feature selection. Improved Artificial Bee Colony (IABC) with Neuro Optimized Emotional Classifier (NOEC) solves this issue in the proposed system. The Iterative Conditional Entropy Kalman filtering (ICEKF) is initially processed to effectively filter the noisy features from the inputted speech data. Mel Frequency Cepstrum Coefficient (MFCC), pitch, energy, intensity and formants are extracted as speech features. Every extracted feature is maintained in the database and annotated along with their emotional class label. IABC algorithm chooses the feature optimally, which in turn employs the best fitness function values. From the optimally selected dataset, the NOEC is processed. Emotions can be identified from the Tamil news speech dataset with the help of the supervised machine learning technique, which demands the training set (collection of emotional speech recordings). Every recording or sample in the dataset is named with the emotional class and they are indicated as n-dimensional vector of spectrum coefficients which in turn is extracted from the Tamil news speech dataset. This dataset is collected from real time via using the search engine sites like Google, YouTube, twitter etc. By implementing IABC with NOEC classification process, the work segregates the emotional classes such as happy, sad, anger, fear and neutral emotions perfectly. From the experimental verification, it is confirmed that the proposed method IABC with NOEC gives better performances with respect to accuracy, precision, recall and f-measure values. © 2018, Springer Science+Business Media, LLC, part of Springer Nature.",Cluster Computing,10.1007/s10586-018-2177-0,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042410974&doi=10.1007%2fs10586-018-2177-0&partnerID=40&md5=cf30b1f6be8ca2108a070e993627e515,2019,2021-07-20 15:50:14,2021-07-20 15:50:14
2TMBQQEB,journalArticle,2017,"Oneto, L.; Bisio, F.; Cambria, E.; Anguita, D.",Semi-supervised Learning for Affective Common-Sense Reasoning,"Background: Big social data analysis is the area of research focusing on collecting, examining, and processing large multi-modal and multi-source datasets in order to discover patterns/correlations and extract information from the Social Web. This is usually accomplished through the use of supervised and unsupervised machine learning algorithms that learn from the available data. However, these are usually highly computationally expensive, either in the training or in the prediction phase, as they are often not able to handle current data volumes. Parallel approaches have been proposed in order to boost processing speeds, but this clearly requires technologies that support distributed computations. Methods: Extreme learning machines (ELMs) are an emerging learning paradigm, presenting an efficient unified solution to generalized feed-forward neural networks. ELM offers significant advantages such as fast learning speed, ease of implementation, and minimal human intervention. However, ELM cannot be easily parallelized, due to the presence of a pseudo-inverse calculation. Therefore, this paper aims to find a reliable method to realize a parallel implementation of ELM that can be applied to large datasets typical of Big Data problems with the employment of the most recent technology for parallel in-memory computation, i.e., Spark, designed to efficiently deal with iterative procedures that recursively perform operations over the same data. Moreover, this paper shows how to take advantage of the most recent advances in statistical learning theory (SLT) in order to address the issue of selecting ELM hyperparameters that give the best generalization performance. This involves assessing the performance of such algorithms (i.e., resampling methods and in-sample methods) by exploiting the most recent results in SLT and adapting them to the Big Data framework. The proposed approach has been tested on two affective analogical reasoning datasets. Affective analogical reasoning can be defined as the intrinsically human capacity to interpret the cognitive and affective information associated with natural language. In particular, we employed two benchmarks, each one composed by 21,743 common-sense concepts; each concept is represented according to two models of a semantic network in which common-sense concepts are linked to a hierarchy of affective domain labels. Results: The labeled data have been split into two sets: The first 20,000 samples have been used for building the model with the ELM with the different SLT strategies, while the rest of the labeled samples, numbering 1743, have been kept apart as reference set in order to test the performance of the learned model. The splitting process has been repeated 30 times in order to obtain statistically relevant results. We ran the experiments through the use of the Google Cloud Platform, in particular, the Google Compute Engine. We employed the Google Compute Engine Platform with NM = 4 machines with two cores and 1.8 GB of RAM (machine type n1-highcpu-2) and an HDD of 30 GB equipped with Spark. Results on the affective dataset both show the effectiveness of the proposed parallel approach and underline the most suitable SLT strategies for the specific Big Data problem. Conclusion: In this paper we showed how to build an ELM model with a novel scalable approach and to carefully assess the performance, with the use of the most recent results from SLT, for a sentiment analysis problem. Thanks to recent technologies and methods, the computational requirements of these methods have been improved to allow for the scaling to large datasets, which are typical of Big Data applications. © 2016, Springer Science+Business Media New York.",Cognitive Computation,10.1007/s12559-016-9433-5,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84992202246&doi=10.1007%2fs12559-016-9433-5&partnerID=40&md5=1e61c3c67fd6c2b181a0884674ad9dc7,2017,2021-07-20 15:50:14,2021-07-20 15:50:14
JWQ2MLPN,journalArticle,2020,"Lan, L.T.H.; Tuan, T.M.; Ngan, T.T.; Son, L.H.; Giang, N.L.; Ngoc, V.T.N.; Hai, P.V.",A new complex fuzzy inference system with fuzzy knowledge graph and extensions in decision making,"Context and Background: Complex fuzzy theory has a strong practical implication in many real-world applications. Complex Fuzzy Inference System (CFIS) is a powerful technique to overcome the challenges of uncertain, periodic data. However, a question is raised for CFIS: How can we deduce and predict the result in case there is little knowledge about data information and rule base? This is significance because many real applications do not have enough knowledge of rule base for inference so that the performance of systems may be low. Thus, it is necessary to have an approximate reasoning method to represent and derive final results. Motivation: Recently, the Mamdani Complex Fuzzy Inference System (M-CFIS) has been proposed with a specific inference mechanism according to the Mamdani type. A new improvement so-called the Mamdani Complex Fuzzy Inference System with Rule Reduction (M-CFIS-R) has been designed to utilize granular computing with complex similarity measures to reduce the rule base so as to gain better performance in decision-making problems. However in M-CFIS-R, testing data are checked by matching with each rule in the rule base, which leads to a high cost of computational time. Besides, if the testing data contain records that are not inferred by the rule base, the output cannot be generated. This happens in real commerce systems in which the rule base is small at the time of creation and needs to feed with new rules. Methodology: In order to handle those issues, this article first time proposes the Fuzzy Knowledge Graph to represent the rule base in terms of linguistic labels and their relationships according to the rule set. An adjacent matrix of Fuzzy Knowledge Graph is generated for inference. When a record in the Testing dataset is given, it would be fuzzified and labelled. Each component in the record is checked with the Fuzzy Knowledge Graph by the inference mechanism in approximate reasoning called Fast Inference Search Algorithm. Then, we derive the label of the new record by the Max-Min operator. Besides, we also propose four extensions of Mamdani Complex Fuzzy Inference System Rule Reduction including Sugeno Complex Fuzzy Inference Systems, Tsukamoto Complex Fuzzy Inference Systems, Complex Fuzzy Measures and Complex Fuzzy Integrals in M-CFIS-R. Results: The experiments on the UCI Machine Learning datasets show that the proposed method classifies samples as correctly as M-CFIS-R with very lower run time (6.45 times on average). The experiments are performed through many tests via 2 main scenarios. Conclusion: The proposed system has good performance in reducing the computational time of inference with acceptable accuracy. It has ability to work with systems having limited knowledge and rule base. © 2020 Institute of Electrical and Electronics Engineers Inc.. All rights reserved.",IEEE Access,10.1109/ACCESS.2020.3021097,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102810607&doi=10.1109%2fACCESS.2020.3021097&partnerID=40&md5=193866b4217a6ba6bea49fa915c9976a,2020,2021-07-20 15:50:14,2021-07-20 15:50:14
JWFAF6EL,journalArticle,2021,"Wang, M.; Chen, W.; Wang, S.; Jiang, Y.; Yao, L.; Qi, G.",Efficient search over incomplete knowledge graphs in binarized embedding space,"Knowledge graph (KG) embedding techniques represent entities and relations as low-dimensional and continuous vectors. This enables KG machine learning models to be easily adapted for KG reasoning, completion, and querying tasks. However, learned dense vectors are inefficient for large-scale similarity computations. Learning-to-hash is to a method that learns compact binary codes from high-dimensional input data and provides a promising way to accelerate efficiency by measuring the Hamming distance instead of Euclidean distance. Alternatively, a dot-product is used in a continuous vector space. Unfortunately, most learning-to-hash methods cannot be directly applied to KG structure encoding because they focus on similarity preservation between images. In this paper, we introduce a novel end-to-end learning-to-hash framework for encoding incomplete KGs and graph queries in a Hamming space. To preserve KG structure information, from embeddings to hash codes, and address the ill-posed gradient issue in the optimization, we utilize a continuation method (with convergence guarantees) to jointly encode queries and KG entities using geometric operations. The hashed embedding of a query can be utilized to discover target entities from incomplete KGs whilst the efficiency has been greatly improved. To evaluate the proposed framework, we have compared our model to state-of-the-art methods commonly used in real-world KGs. Extensive experimental results show that our framework not only significantly speeds up the search process, but also provides good results when unanswerable queries are caused by incomplete information.1 © 2021 Elsevier B.V.",Future Generation Computer Systems,10.1016/j.future.2021.04.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105694266&doi=10.1016%2fj.future.2021.04.006&partnerID=40&md5=94031d60b1faab4c6079b05b96d0d98c,2021,2021-07-20 15:50:14,2021-07-20 15:50:14
G53C9EAE,journalArticle,2021,"Weichselbraun, A.; Steixner, J.; Braşoveanu, A.M.P.; Scharl, A.; Göbel, M.; Nixon, L.J.B.",Automatic Expansion of Domain-Specific Affective Models for Web Intelligence Applications,"Sentic computing relies on well-defined affective models of different complexity—polarity to distinguish positive and negative sentiment, for example, or more nuanced models to capture expressions of human emotions. When used to measure communication success, even the most granular affective model combined with sophisticated machine learning approaches may not fully capture an organisation’s strategic positioning goals. Such goals often deviate from the assumptions of standardised affective models. While certain emotions such as Joy and Trust typically represent desirable brand associations, specific communication goals formulated by marketing professionals often go beyond such standard dimensions. For instance, the brand manager of a television show may consider fear or sadness to be desired emotions for its audience. This article introduces expansion techniques for affective models, combining common and commonsense knowledge available in knowledge graphs with language models and affective reasoning, improving coverage and consistency as well as supporting domain-specific interpretations of emotions. An extensive evaluation compares the performance of different expansion techniques: (i) a quantitative evaluation based on the revisited Hourglass of Emotions model to assess performance on complex models that cover multiple affective categories, using manually compiled gold standard data, and (ii) a qualitative evaluation of a domain-specific affective model for television programme brands. The results of these evaluations demonstrate that the introduced techniques support a variety of embeddings and pre-trained models. The paper concludes with a discussion on applying this approach to other scenarios where affective model resources are scarce. © 2021, The Author(s).",Cognitive Computation,10.1007/s12559-021-09839-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85100453488&doi=10.1007%2fs12559-021-09839-4&partnerID=40&md5=c448f2c46bfc8745a7c64132c0d631fd,2021,2021-07-20 15:50:14,2021-07-20 15:50:14
QX9HWF7E,journalArticle,2021,"Liu, R.; Yao, X.; Guo, C.; Wei, X.",Can We Forecast Presidential Election Using Twitter Data? An Integrative Modelling Approach,"Forecasting political elections has attracted a lot of attention. Traditional election forecasting models in political science generally take preference in poll surveys and economic growth at the national level as the predictive factors. However, spatially or temporally dense polling has always been expensive. In the recent decades, the exponential growth of social media has drawn enormous research interests from various disciplines. Existing studies suggest that social media data have the potential to reflect the political landscape. Particularly, Twitter data have been extensively used for sentiment analysis to predict election outcomes around the world. However, previous studies have typically been data-driven and the reasoning process was oversimplified without robust theoretical foundations. Most of the studies correlate twitter sentiment directly and solely with the election results which can hardly be regarded as predictions. To develop a more theoretically plausible approach this study draws on political science prediction models and modifies them in two aspects. First, our approach uses Twitter sentiment to replace polling data. Second, we transform traditional political science models from the national level to the county level, the finest spatial level of voting counts. The proposed model has independent variables of support rate based on Twitter sentiment and variables related to economic growth. The dependent variable is the actual voting result. The 2016 U.S. presidential election data in Georgia is used to train the model. Results show that the proposed modely is effective with the accuracy of 81% and the support rate based on Twitter sentiment ranks the second most important feature. © 2020 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group, on behalf of Nanjing Normal University.",Annals of GIS,10.1080/19475683.2020.1829704,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85093956244&doi=10.1080%2f19475683.2020.1829704&partnerID=40&md5=72f2da09c06c083522682df7dfbbe7be,2021,2021-07-20 15:50:14,2021-07-20 15:50:14
G2LNXX9W,journalArticle,2018,"Tzovara, A.; Korn, C.W.; Bach, D.R.",Human pavlovian fear conditioning conforms to probabilistic learning,"Learning to predict threat from environmental cues is a fundamental skill in changing environments. This aversive learning process is exemplified by Pavlovian threat conditioning. Despite a plethora of studies on the neural mechanisms supporting the formation of associations between neutral and aversive events, our computational understanding of this process is fragmented. Importantly, different computational models give rise to different and partly opposing predictions for the trial-by-trial dynamics of learning, for example expressed in the activity of the autonomic nervous system (ANS). Here, we investigate human ANS responses to conditioned stimuli during Pavlovian fear conditioning. To obtain precise, trial-by-trial, single-subject estimates of ANS responses, we build on a statistical framework for psychophysiological modelling. We then consider previously proposed non-probabilistic models, a simple probabilistic model, and non-learning models, as well as different observation functions to link learning models with ANS activity. Across three experiments, and both for skin conductance (SCR) and pupil size responses (PSR), a probabilistic learning model best explains ANS responses. Notably, SCR and PSR reflect different quantities of the same model: SCR track a mixture of expected outcome and uncertainty, while PSR track expected outcome alone. In summary, by combining psychophysiological modelling with computational learning theory, we provide systematic evidence that the formation and maintenance of Pavlovian threat predictions in humans may rely on probabilistic inference and includes estimation of uncertainty. This could inform theories of neural implementation of aversive learning. © 2018 Tzovara et al. This is an open access article distributed under the terms of the Creative Commons Attribution License, which permits unrestricted use, distribution, and reproduction in any medium, provided the original author and source are credited.",PLoS Computational Biology,10.1371/JOURNAL.PCBI.1006243,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055747746&doi=10.1371%2fJOURNAL.PCBI.1006243&partnerID=40&md5=fa693c7ce5ac67dba6fd604490749084,2018,2021-07-20 15:50:14,2021-07-20 15:50:14
V3NNAZML,journalArticle,2018,"Kumar, A.; Shankar, R.; Thakur, L.S.",A big data driven sustainable manufacturing framework for condition-based maintenance prediction,"Smart manufacturing refers to a future-state of manufacturing and it can lead to remarkable changes in all aspects of operations through minimizing energy and material usage while simultaneously maximizing sustainability enabling a futuristic more digitalized scenario of manufacturing. This research develops a big data analytics framework that optimizes the maintenance schedule through condition-based maintenance (CBM) optimization and also improves the prediction accuracy to quantify the remaining life prediction uncertainty. Through effective utilization of condition monitoring and prediction information, CBM would enhance equipment reliability leading to reduction in maintenance cost. The proposed framework uses a CBM optimization method that utilizes a new linguistic interval-valued fuzzy reasoning method for predicting the information. The proposed big data analytics framework in our study for estimating the uncertainty based on backward feature elimination and fuzzy unordered rule induction algorithm prediction errors, is an innovative contribution to the remaining life prediction field. Our paper elaborates on the basic underlying structure of CBM system that is defined by transaction matrix and the threshold value of failure probability. We developed this framework for analysing the CBM policy cost more accurately and to find the probabilistic threshold values of covariate that corresponds to the lowest price of predictive maintenance cost. The experimental results are performed on a big dataset which is generated from a sophisticated simulator of a gas turbine propulsion plant. A comparative analysis confirms that the method used in the proposed framework outpaces the classical methods in terms of classification accuracy and other statistical performance evaluation metrics. © 2017 Elsevier B.V.",Journal of Computational Science,10.1016/j.jocs.2017.06.006,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024090191&doi=10.1016%2fj.jocs.2017.06.006&partnerID=40&md5=7a650c3570ac36c5733ffb6b880e87e5,2018,2021-07-20 15:50:14,2021-07-20 15:50:14
7YX447XS,journalArticle,2016,"Belkebir, R.; Guessoum, A.",Concept generalization and fusion for abstractive sentence generation,"Text summarization is either extractive or abstractive. Extractive summarization is to select the most salient pieces of information (words, phrases, and/or sentences) from a source document without adding any external information. Abstractive summarization allows an internal representation of the source document so as to produce a faithful summary of the source. In this case, external text can be inserted into the generated summary. Because of the complexity of the abstractive approach, the vast majority of work in text summarization has adopted an extractive approach. In this work, we focus on concepts fusion and generalization, i.e. where different concepts appearing in a sentence can be replaced by one concept which covers the meanings of all of them. This is one operation that can be used as part of an abstractive text summarization system. The main goal of this contribution is to enrich the research efforts on abstractive text summarization with a novel approach that allows the generalization of sentences using semantic resources. This work should be useful in intelligent systems more generally since it introduces a means to shorten sentences by producing more general (hence abstractions of the) sentences. It could be used, for instance, to display shorter texts in applications for mobile devices. It should also improve the quality of the generated text summaries by mentioning key (general) concepts. One can think of using the approach in reasoning systems where different concepts appearing in the same context are related to one another with the aim of finding a more general representation of the concepts. This could be in the context of Goal Formulation, expert systems, scenario recognition, and cognitive reasoning more generally. We present our methodology for the generalization and fusion of concepts that appear in sentences. This is achieved through (1) the detection and extraction of what we define as generalizable sentences and (2) the generation and reduction of the space of generalization versions. We introduce two approaches we have designed to select the best sentences from the space of generalization versions. Using four NLTK1 corpora, the first approach estimates the ""acceptability"" of a given generalization version. The second approach is Machine Learning-based and uses contextual and specific features. The recall, precision and F1-score measures resulting from the evaluation of the concept generalization and fusion approach are presented. © 2016 Elsevier Ltd. All rights reserved.",Expert Systems with Applications,10.1016/j.eswa.2016.01.007,https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957598648&doi=10.1016%2fj.eswa.2016.01.007&partnerID=40&md5=8192da646ba41a5baa360966c2509e34,2016,2021-07-20 15:50:14,2021-07-20 15:50:14
GAAS3CGX,journalArticle,2021,"Vasanthi, R.; Jayavadivel, R.; Prasadh, K.; Vellingiri, J.; Akilarasu, G.; Sudhakar, S.; Balasubramaniam, P.M.",A novel user interaction middleware component system for ubiquitous soft computing environment by using fuzzy agent computing system,"Ubiquitous Computing Environment (UCE) has become increasingly crucial for a component-based Middleware for Ubiquitous Soft Computing Environment by using Fuzzy Agent Computing System (USCE–FACS) that can fully exploit interactive service characteristics to support user-centric services in UCE. It has posed some challenges, such as high Memory Consumption and high Component Building Time to users who are hiring services in a heterogeneous environment. To control the UCE for the user's benefit, FACS works non-rudely in an online deep-rooted learning way to get familiar with the user behavior. The point of incorporation of previously mentioned advances is to make more extensive the connection between individuals and data innovation gear through the use of an undetectable system of UCE devices making dynamic computational-environments equipped for fulfilling the users' prerequisites. Due to a large number of available services without ontology and Metadata Repository (MR), it becomes time-consuming for end-users to find appropriate services to satisfy their various needs. To help end-users obtain their desired services, the USCE–FACS proposed, it reduces MC and CBT in a heterogeneous environment. This paper concentrated on the communication from users to devices to enable a general and quick access to accessible element and administrations gave by the UCE. © 2020, Springer-Verlag GmbH Germany, part of Springer Nature.",Journal of Ambient Intelligence and Humanized Computing,10.1007/s12652-020-01893-4,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083365965&doi=10.1007%2fs12652-020-01893-4&partnerID=40&md5=3475fddfb9cff635ba0843e50d266c2a,2021,2021-07-20 15:50:14,2021-07-20 15:50:14
JFZCSQ6R,journalArticle,2021,"Liu, Y.; Yang, S.; Zhang, Y.; Miao, C.; Nie, Z.; Zhang, J.",Learning Hierarchical Review Graph Representations for Recommendation,"The user review data have been demonstrated to be effective in solving different recommendation problems. Previous review-based recommendation methods usually employ sophisticated compositional models, such as Recurrent Neural Networks (RNN) and Convolutional Neural Networks (CNN), to learn semantic representations from the review data for recommendation. However, these methods mainly capture the local dependency between neighboring words in a word window, and they treat each review equally. Therefore, they may not be effective in capturing the global dependency between words and tend to be easily biased by noise review information. In this paper, we propose a novel review-based recommendation model, named Review Graph Neural Network (RGNN). Specifically, RGNN builds a specific review graph for each individual user/item, which provides a global view about the user/item properties to help weaken the biases caused by noise review information. A type-aware graph attention mechanism is developed to learn semantic embeddings of words. Moreover, a personalized graph pooling operator is proposed to learn hierarchical representations of the review graph to form the semantic representation for each user/item. We compared RGNN with state-of-the-art review-based recommendation approaches on two real-world datasets. The experimental results indicate that RGNN consistently outperforms baseline methods, in terms of Mean Square Error (MSE). IEEE",IEEE Transactions on Knowledge and Data Engineering,10.1109/TKDE.2021.3075052,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85105057167&doi=10.1109%2fTKDE.2021.3075052&partnerID=40&md5=3c9cadd180b3ed11b1efa06d784b57ef,2021,2021-07-20 15:50:14,2021-07-20 15:50:14
8TSFYHPM,journalArticle,2019,"Iraji, M.S.",Prediction of fetal state from the cardiotocogram recordings using neural network models,"The combination of machine vision and soft computing approaches in the clinical decisions, using training data, can improve medical decisions and treatments. The cardiotocography (CTG) monitoring and uterine activity (UA) provides useful information about the condition of the fetus and the cesarean or natural delivery. The visual assessment by the pathologists takes a lot of time and may be incompatible. Therefore, creating a computer intelligent method to assess fetal wellbeing before the mother labour is very important. In this study, many diverse approaches are suggested for predicting fetal state classes based on artificial intelligence. The various topologies of multi-layer architecture of a sub-adaptive neuro fuzzy inference system (MLA-ANFIS) using multiple input features, neural networks (NN), deep stacked sparse auto-encoders (DSSAEs), and deep-ANFIS models are implemented on a CTG data set. Experimental results contributing to DSSAE are more accurate than other suggested techniques to predict fetal state. The proposed method achieved a sensitivity of 99.716, specificity of 97.500 and geometric mean of 98.602 with accuracy of 99.503. © 2019 Elsevier B.V.",Artificial Intelligence in Medicine,10.1016/j.artmed.2019.03.005,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063127048&doi=10.1016%2fj.artmed.2019.03.005&partnerID=40&md5=c17343ce36b487407e354bf5ed8fd797,2019,2021-07-20 15:50:14,2021-07-20 15:50:14
IRZUHU7N,journalArticle,2017,"Parmezan, A.R.S.; Lee, H.D.; Wu, F.C.",Metalearning for choosing feature selection algorithms in data mining: Proposal of a new framework,"In Data Mining, during the preprocessing step, there is a considerable diversity of candidate algorithms to select important features, according to some criteria. This broad availability of algorithms that perform the Feature Selection task gives rise to the difficulty of choosing, a priori, between the algorithms at hand, the most promising one for a particular problem. In this paper, we present the proposal and evaluation of a new architecture for the recommendation of Feature Selection algorithms based on the use of Metalearning. Our framework is very flexible since the user can adapt it to its proper needs. This flexibility is one of the main advantages of our proposal over other approaches in the literature, which involve steps that cannot be adapted to the user's local requirements. Furthermore, it combines several concepts of intelligent systems, including Machine Learning and Data Mining, with topics derived from expert systems, as user and data-driven knowledge, with meta-knowledge. This set of solutions coupled with leading-edge technologies allows our architecture to be integrated into any information system, which impact on the automation of services and in reducing human effort during the process. Regarding the Metalearning process, our framework considers several types of properties inherent to the data sets, as well as, Feature Selection algorithms based on many information, distance, dependence and consistency measures. The quality of the methods for Feature Selection was estimated according to a multicriteria performance measure, which guided the ranking process of these algorithms for the construction of data metabases. Proposed by the authors of this work, this multicriteria performance measure combines any three measurements on a single one, creating an interesting and powerful tool to evaluate not only FS algorithms but also to assess any context where it is necessary a combination to maximize a measure or minimize it. The recommendation models, represented by decision trees and induced from the training metabases, allowed us to see in what circumstances a Feature Selection algorithm outperforms the other and what aspects of the data present greater influence in determining the performance of these algorithms. Nevertheless, if the user wishes, any other learning algorithm may be used to induce the recommendation model. This versatility is another strong point of this proposal. Results show that with the characterization of data, through statistical, information and complexity measures, it is possible to reach an accuracy higher than 90%. Besides yielding recommendation models that are interpretable and robust to overfitting, the developed architecture is less computationally expensive than approaches recently proposed in the literature. © 2017 Elsevier Ltd",Expert Systems with Applications,10.1016/j.eswa.2017.01.013,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010470379&doi=10.1016%2fj.eswa.2017.01.013&partnerID=40&md5=09e2d28062f29966690c14aee9b91e82,2017,2021-07-20 15:50:14,2021-07-20 15:50:14
SGXN3E79,journalArticle,2021,"Wang, G.; Jia, Q.-S.; Zhou, M.C.; Bi, J.; Qiao, J.; Abusorrah, A.",Artificial neural networks for water quality soft-sensing in wastewater treatment: a review,"This paper aims to present a comprehensive survey on water quality soft-sensing of a wastewater treatment process (WWTP) based on artificial neural networks (ANNs). We mainly present problem formulation of water quality soft-sensing, common soft-sensing models, practical soft-sensing examples and discussion on the performance of soft-sensing models. In details, problem formulation includes characteristic analysis and modeling principle of water quality soft-sensing. The common soft-sensing models mainly include a back-propagation neural network, radial basis function neural network, fuzzy neural network (FNN), echo state network (ESN), growing deep belief network and deep belief network with event-triggered learning (DBN-EL). They are compared in terms of accuracy, efficiency and computational complexity with partial-least-square-regression DBN (PLSR-DBN), growing ESN, sparse deep belief FNN, self-organizing DBN, wavelet-ANN and self-organizing cascade neural network (SCNN). In addition, this paper generally discusses and explains what factors affect the accuracy of the ANNs-based soft-sensing models. Finally, this paper points out several challenges in soft-sensing models of WWTP, which may be helpful for researchers and practitioner to explore the future solutions for their particular applications. © 2021, The Author(s), under exclusive licence to Springer Nature B.V.",Artificial Intelligence Review,10.1007/s10462-021-10038-8,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85108813078&doi=10.1007%2fs10462-021-10038-8&partnerID=40&md5=6c24af74d0f3afb3abc0fc3122f11165,2021,2021-07-20 15:50:14,2021-07-20 15:50:14
44UQWM7C,journalArticle,2021,"Wu, P.; Chu, F.; Saidani, N.; Chen, H.; Zhou, M.",Optimizing Locations and Qualities of Multiple Facilities With Competition via Intelligent Search,"We study a new competitive multi-facility location and quality design problem in a continuous space. The facility location and quality design are considered together because of their interdependence. Especially, new entrant facilities compete for customer demands with existing ones and the latter's reactions are taken into account. The goal is to maximize the profit of all new entrant facilities by optimally determining their locations and qualities. For this problem, a probabilistic Huff-like gravity model is adopted to analyze the market share to be captured by new and existing facilities, and then a mathematical programming model is provided based on the market share analysis. Since it is shown to be strongly NP-hard, a new iterative solution framework is first proposed to solve it, where at each iteration, new configurations of facility locations are firstly generated, and then the quality decisions of all facilities are modelled as a competitive decision process by a non-cooperative game. The best qualities for new and existing facilities are determined by their Nash equilibrium. Finally, optimal or near-optimal solutions are calculated. Then based on the proposed solution framework, a particle swarm optimization-based approach is developed. Computational results for randomly generated instances indicate that the devised algorithm is able to find suitable locations and qualities of newly entering facilities in a competitive environment and outperforms favorably a genetic algorithm-based approach. IEEE",IEEE Transactions on Intelligent Transportation Systems,10.1109/TITS.2020.3046885,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85101774542&doi=10.1109%2fTITS.2020.3046885&partnerID=40&md5=18de53347b8c43fe0dd72b7f70354a79,2021,2021-07-20 15:50:14,2021-07-20 15:50:14
28FVD3B5,journalArticle,2020,"Treesatayapun, C.",Knowledge-based reinforcement learning controller with fuzzy-rule network: experimental validation,"A model-free controller for a general class of output feedback nonlinear discrete-time systems is established by action-critic networks and reinforcement learning with human knowledge based on IF–THEN rules. The action network is designed by a single input fuzzy-rules emulated network with the set of IF–THEN rules utilized by the relation between control effort and plant’s output such as IF the output is high THEN the control effort should be reduced. The critic network is constructed by a multi-input FREN (MiFREN) for estimating an unknown long-term cost function. The set of IF–THEN rules for MiFREN is defined by the general knowledge of optimization such that IF the quadratic values of control effort and tracking error are high THEN the cost function should be high. The convergence of tracking error and bounded external signals can be guaranteed by Lyapunov direct method under general assumptions which are reasonable for practical plants. A computer simulation system is firstly provided to demonstrate the design method and the performance of the proposed controller. Furthermore, an experimental system with the prototype of DC-motor current control is conducted to show the effectiveness of the control scheme. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",Neural Computing and Applications,10.1007/s00521-019-04509-x,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074359827&doi=10.1007%2fs00521-019-04509-x&partnerID=40&md5=6ef15504921f22f0bb3b1fbb980d935a,2020,2021-07-20 15:50:14,2021-07-20 15:50:14
C3XUFTES,journalArticle,2020,"Xiao, S.; Liu, W.; Lin, J.; Yu, Z.",A Data-Driven Asynchronous Neural Network Accelerator,"Deep Neural Networks (DNNs) are revolutionizing machine learning, with unprecedented accuracy on many AI tasks. Energy-efficient neural acceleration is crucial in broadening DNN applications in cloud and mobile end devices. However, power-hungry clock networks limit the energy-efficiency of DNN accelerators. In this work, we propose a novel DNN hardware accelerator, called the Asynchronous Neural Network Processor (AsNNP). At the heart of AsNNP is a scalable hierarchy matrix multiply unit, with bit-serial PEs working in parallel. It replaces the global clock networks with asynchronous handshake protocols to realize the synchronization and communication between each part, minimizing the dynamic power. Meanwhile, a fine-grain asynchronous pipeline based on Weak-Conditioned Half-Buffer (WCHB) is introduced to pipe successive computations in a data-driven manner, i.e., once data arrives computation begins, maximizing the throughput. These techniques enable AsNNP to work in a fully data-driven asynchronous communication fashion with optimized energy-efficiency. The proposed accelerator is implemented with Quasi-Delay-Insensitive (QDI) clockless logic family and evaluated in a 65nm process. Compared with the synchronous baseline, simulation results show that AsNNP offers 2.2X higher equivalent frequency and 1.59X lower power. Compared with state-of-the-art DNN accelerators, AsNNP shows 1.17X-4.97X energy-efficiency improvement. IEEE",IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems,10.1109/TCAD.2020.3025508,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091684569&doi=10.1109%2fTCAD.2020.3025508&partnerID=40&md5=236294517bc4e5e50ac9f02fb214b027,2020,2021-07-20 15:50:14,2021-07-20 15:50:14
5HARP4V6,journalArticle,2019,"Qu, Y.; Yue, G.; Shang, C.; Yang, L.; Zwiggelaar, R.; Shen, Q.",Multi-criterion mammographic risk analysis supported with multi-label fuzzy-rough feature selection,"Context and background: Breast cancer is one of the most common diseases threatening the human lives globally, requiring effective and early risk analysis for which learning classifiers supported with automated feature selection offer a potential robust solution. Motivation: Computer aided risk analysis of breast cancer typically works with a set of extracted mammographic features which may contain significant redundancy and noise, thereby requiring technical developments to improve runtime performance in both computational efficiency and classification accuracy. Hypothesis: Use of advanced feature selection methods based on multiple diagnosis criteria may lead to improved results for mammographic risk analysis. Methods: An approach for multi-criterion based mammographic risk analysis is proposed, by adapting the recently developed multi-label fuzzy-rough feature selection mechanism. Results: A system for multi-criterion mammographic risk analysis is implemented with the aid of multi-label fuzzy-rough feature selection and its performance is positively verified experimentally, in comparison with representative popular mechanisms. Conclusions: The novel approach for mammographic risk analysis based on multiple criteria helps improve classification accuracy using selected informative features, without suffering from the redundancy caused by such complex criteria, with the implemented system demonstrating practical efficacy. © 2019 The Authors",Artificial Intelligence in Medicine,10.1016/j.artmed.2019.101722,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072570508&doi=10.1016%2fj.artmed.2019.101722&partnerID=40&md5=f09a34f4591ecc83499b5c7ae5c97fb1,2019,2021-07-20 15:50:15,2021-07-20 15:50:15
NKEYVKMC,journalArticle,2019,"Yang, Q.; Peng, G.; Gasti, P.; Balagani, K.S.; Li, Y.; Zhou, G.",MEG: Memory and energy efficient garbled circuit evaluation on smartphones,"Garbled circuits are general tools that allow two parties to compute any function without disclosing their respective inputs. Applications of this technique vary from distributed privacy-preserving machine learning tasks to secure outsourced authentication. Unfortunately, the energy cost of garbled circuit evaluation protocols is substantial. This limits the applicability of garbled circuits in scenarios that involve battery-operated devices, such as Internet-of-Things (IoT) devices and smartphones. In this paper, we propose MEG, a Memory- and Energy-efficient Garbled circuit evaluation mechanism. MEG utilizes batch data transmission and multi-threading to reduce memory and energy consumption. We implement MEG on an Android smartphone and compare its performance and energy consumption with state-of-the-art techniques using two garbled circuits of widely different sizes (AES-128 and 256-bit edit distance). Our results show that, compared with 'plain' garbled circuit evaluation, MEG decreases memory consumption by more than 90%. When compared with current pipelined garbled circuit evaluation techniques, MEG's energy usage was 42% lower for AES-128 and 23% lower for EDT-256. Furthermore, our multi-thread implementation of MEG decreased circuit evaluation time by up to 56.7% for AES-128, and by up to 13.5% for EDT-256, compared with state-of-the-art pipelining techniques. © 2005-2012 IEEE.",IEEE Transactions on Information Forensics and Security,10.1109/TIFS.2018.2868221,https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052841289&doi=10.1109%2fTIFS.2018.2868221&partnerID=40&md5=76a0b5e129869c98264eefc231b7152f,2019,2021-07-20 15:50:15,2021-07-20 15:50:15
