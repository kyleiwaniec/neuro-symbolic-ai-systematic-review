Doc type,Title,Scopus Hash ID,DOI,Journal/Conference,Year,Use Case,Learning,Application,Key-intake,Contribution,Definition of NeSy,Symbolic terms,Neural terms,NeSy Category,Kautz category,Datasets,Model description,Evaluation Metrics,reported score,Study Quality,Comments,MISC,,,,,,,,,,,,,,,,,,
ar,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier,BAMGPUCX,10.1109/ACCESS.2021.3053917,IEEE Access,2021,Sentiment analysis,supervised,classification,"Data cleaning and preprocessing, as well as the addtion of the fuzzy logic component (MFS) significantly improves performance on sentiment analysis tasks on Twitter datsets compared to SOTA. In addition, parallelization using hadoop map reduce significantly reduces computation time.","we develop a new hybrid fuzzy-deep learning approach, that basically integrates the CNN, FFNN deep learning networks with the MFS fuzzy logic system.",NA,fuzzy logic,"convolutional neural network (CNN),
neural network (NN),",sequential,N/A,"sentiment140, 
COVID-19_Sentiments","TREC,
SST-1,
MR,
SST-2","F1,
Accuracy,
Precision/Recall,
True Positive Rate (TPR), True Negative Rate (TNR) or Specificity, False Positive Rate (FPR), False Negative Rate (FNR), Error Rate (ER), Precision (PR), Classification Rate or Accuracy (AC), Kappa Statistic (KS), F1-score (FS) and Time Consumption (TC)",upwards of 99% F1 and accuracy,77.78%,"A pipeline is proposed for sentence level sentiment analysis on two Twitter datasets. The pipeline comproses of data cleaning and preprocessing (this effort alone reduces the error rate by an order of magnitude), learning embeddings, CNN+FFNN for classification, and a MFS (Mandani Fuzzy System) classification step. The algorithms are implemented on hadoop map reduce. The best models achieve F1 and accuracy scores above the SOTA, as well as improve computational efficiency and cost complexity. For the MFS, the choices of linguistic terms and fuzzy rules are made manually relying on domain expertise. It would be interesting to see how well this system performs on less emotionally charged datasets. A future neural-symbolic direction might be to learn these ""parameters"" automatically with a view to a generalizable system. 
The system doesn't fall into any of the Kautz categories, as it's not strictly speaking an integration of neuro-symnolic techniques, but rather a sequential pipeline.",,,,,,,,,,,,,,,,,,,
ar,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification,JN4IZETG,10.1007/s10994-021-05968-x,Machine Learning,2021,Text classification,supervised,classification,"autoBOT is explainable, efficient, and performs on par with language models like BERT.","a novel approach for text classification with limited data and resources, but comparable in erformance to BERT. Hyperparameters are optimized using an evolutionary algorithm. Novel feature types: document keywords, relational features, and first order features (based on grounded relations from ConceptNet)",NA,"symbolic learner,
symbolic representation,
knowledge graph (KG)","Neural Network Language Model (NNLM),
neural network (NN),
neuroevolution (NE)",,N/A,"semeval2019,
mbti,
Fox,
BBC,
ConceptNet KG","autoBOT (automatic Bags-Of-Tokens),
The proposed approach consists of an evolutionary algorithm that jointly optimizes various sparse representations of a given text (including word, subword, POS tag, keyword-based, knowledge graph-based and relational features) and two types of document embeddings (non-sparse representations). The key idea of autoBOT is that, instead of evolving at the learner level, evolution is conducted at the representation level.
",F1,up to 99% on some tasks,88.89%,"autoBot is a genetic algorithm for text data that learns both the representations and the models for classification jointly. The inclusion of novel symbolic features improves performance over traditional linear models such as SVM and Logistic Regression, and performs on par with large language models such as BERT. The advantage of autoBot is its explainability (feature importances), and efficiency in a low resource setting (small data/feature sparsity, and limited compute power).
The study was conducted using off the shelf implementations and a very basic evolutionary algorithm, thus leaving plenty of room for improvements. Classification is performed at the document level, but it would be worth exploring this pipeline for sentence level and span level classification on for example, the semeval2020 task, which already has a leaderboard with BERT based solutions for comaprison.","Does not fall into any of Kautz's categories. However, the use of linear classifiers (ie symbolic learners) in combination with sub-symbolic representations qualifies for the neuro-symbolic moniker.",,,,,,,,,,,,,,,,,,
ar,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture,KTEHK4MZ,10.1016/j.neucom.2020.12.040,Neurocomputing,2021,KG Completion / link prediction,reinforcement,classification,Two phased approach combining NN and traditional path-finding techniques improves KG reasoning performance.,Deep-IDA* framework that integrates the traditional path searching algorithms IDA* and deep neural networks for KG reasoning (link prediction).,,Markov Decision Process (MDP),reinforcement learning (RL),cooperative,2. Symbolic[Neuro],"NELL-995,
WN18RR,
FB15-237
","fusion of various models that do path-finding and path-reasoning. Pathfinding combines Deepening DFS and A*, the result of which is fed to a GNN for path reasoning. There are many neural modules inside this framework including BERT, LSTM, GRU, and GCN.","Mean Reciprocal Rank (MRR),
Hits@K,
Mean Average Precision (MAP)",Better than all benchmarks (see tables in paper),66.67%,"While the results reported exceed SOTA, the system lacks parsimony. The computation times are not reported.",,,,,,,,,,,,,,,,,,,
ar,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text,RDSQSBN7,10.1109/TASLP.2021.3082295,IEEE/ACM Transactions on Audio Speech and Language Processing,2021,Dialog based relation extraction,supervised,classification,the combination of 4 different modules improves overall performance for this task and supercedes the SOTA,a novel end-to-end model for the novel dialogue-based relation extraction task with code.,NA,graph representation,"attention network,
graph neural network (GNN)",,,DialogRE,,F1,64.89%,11.11%,"attention using BERT followed by graph ""reasoning"" with GNN",,,,,,,,,,,,,,,,,,,
ar,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis,JGU2SECC,10.1016/j.artmed.2019.101772,Artificial Intelligence in Medicine,2020,Decision making,supervised,"classification,
multi-label (371 labels) probalities",Using Huffman trees to build symbols and logic into a Recursive NN which can be optimized with GD.,"Knowledge-based neural network, in which nerve cells in a hidden layer consist of first-order logic.
For neural network parameter learning, a complete theoretical framework based on back-propagation was established to minimize the cross-entropy error.
",,"first order logic (FOL),
Huffman tree","recursive neural knowledge network (RNKN),
recursive neural network (RNN)",cooperative,Other,CEMR,"text -> KG triples -> Huffman tree -> logic functions as nerve cells -> softmax,
optimized using GD/backProp","p@k,
Discounted Cummulative Gain (DCG)",[0.36 - 0.72],72.22%,"The study encodes knowledge in the form of huffman trees made of triples, and logic expressions, in order to jointly learn embeddings and model weights. The first layer consists of entities, the second layer consists of relations (x-> y). Higher layers compute logic rules. The root node is the final embedding representing a document (in this case a single health record). Softmax is used to calculate class probabilities. back propagation is used for optimization. The study doesn't fall into any Kautz categories easily. Sub-symbolic representations are learned from symbolic features and rules iteratively.",interesting approach that would be worth trying for propaganda detection,,,,,,,,,,,,,,,,,,
ar,The CoRg Project: Cognitive Reasoning,7MMJY5BM,10.1007/s13218-019-00601-5,KI - Kunstliche Intelligenz,2019,Question answering,supervised,reasoning,Authors argue that combining logic reasoning with ML improves performance and gerates explainable results. The challenge is in finding an appropriate logics for cognitive (human-like) reasoning.,,,logic programming (LP),recurrent neural network (RNN),sequential,,COPA benchmark set (Choice of Plausible Alternatives),raw text -> FOL (using KNEWS) -> augment with bg knwledge bases -> theorem prover -> NN,,,44.44%,"The study explores using a variety of logic systems for Cognitive Reasoning. The authors posit that combining logic programming with NN should produce not only more accurate results, but also provide expalainbility. It is a work in progress. Results are not reported.",,,,,,,,,,,,,,,,,,,
ar,Learning to activate logic rules for textual reasoning,NB39QA35,10.1016/j.neunet.2018.06.012,Neural Networks,2018,Question answering,reinforcement,reasoning,"Image Scheme (IS) is acquired from early life experience, which is also our hypothesis in this work
our work aims at precise reasoning, good interpretability of models, and perfect performances on small dataset","We propose a novel memory-augmented neural network framework making use of logic rules inducted from existing theories of Image Schema and Cognitive Model.
We redefine textual reasoning tasks as interactive-feedback process with human working memory. Under certain assumptions, we jointly solve two main problems: variable binding and relation activating, via deep reinforcement learning.
Our experimental results show the existence of common properties among real-world relations, and the probability to partially re-construct human logic system to boost performances on language comprehension tasks.
Variable-Relation Reasoning Machine (VRRM)

",NA,first order logic (FOL),"reinforcement learning (RL),
Memory Networks,
neural network (NN)",cooperative,3. Neuro; Symbolic,bAbI-20,"We redefine textual reasoning as a sequential decision-making problem, and solve it with reinforcement learning (RL).",Test error rate,0.70%,66.67%,"This work makes the hypothesis that human reasoning can be modeled by Image Schemas (IS). Schemas are made up of logical rules on (Entity1,Relation,Entitity2) tuples, such as transitivity, or inversion. Input sentences and questions are encoded as the concatenation of individual word embeddings with zero paddings. An MLP module is used to learn a set of tuples by performing simulations and utilizing Reinforcement Learning. This work is limited by the nature of the synthetic data set. It is not shown how well it might generalize to a real world scenario. There is also a limited number of schemas, and as the authors point out, future work should include additional schemas more representative of human reasoning. (See also https://arxiv.org/pdf/1706.01427.pdf from DeepMind 2017)",cognitive linguistics,,,,,,,,,,,,,,,,,,
ar,Semi-supervised learning for big social data analysis,GS3TRUYZ,10.1016/j.neucom.2017.10.010,Neurocomputing,2018,Sentiment analysis,semi-supervised,classification,The addtion of prior knowledge to bias the classification model improves the accuracy of sentiment predictions.,"AffectNet: combined ConceptNet and WNA - commonsense and emotional knowledge are melded together.
AffectNet2: AffectNet with reduced dimentionality.
a flexible framework for semisupervised learning comprising of two parts: clustering and regularized classification.",,"graph represenatation,
commonsense knowledege",SVM,cooperative,5. Neuro_Symbolic,"Pang and Lee,
AffectNet benchmark",clutering -> classification regularized by output of clustering,Accuracy,88.50%,66.67%,"The study embeds prior knowledge in the form of a graph representing concepts and relations from natural language. These are projected into multi-dimentional space and a clustering algorithm is employed to generate the solution space. Subsequently, a regularized classification algorithm is employed where the output of the clustering stage is used in the regularization term. I have classified this as Kautz category 5. However, it differs from the cat5 defition in that the regularization term is not a logic rule, but prior knowledge.",commonsense/affect knowledge base,,,,,,,,,,,,,,,,,,
ar,Concept generalization and fusion for abstractive sentence generation,7YX447XS,10.1016/j.eswa.2016.01.007,Expert Systems with Applications,2016,Text summarization,supervised,generative,"Dependency parsing and wordNet provide input to a ML clasification model, the output of which is use to generate sentences based on rules.",we have addressed the problem of concepts fusion and generalization for abstractive sentence generation.,,dependency parsing,SVM,nested,3. Neuro; Symbolic,NLTK,sentences -> dependency parse -> generate candidates (versions) -> reduce search space using rules -> SVM to select best candidate -> generate sentence using rules,F1,0.808,77.78%,"Uses external data sources to semantically enrich text summarization. It is the application text fusion.The system benefits from an internal machine learnign module. Comparisons with other ML text summarization systems are given, however, these baselines appear to be very weak. There is no comparison with NN, or neural language models. To reasses the approach, more modern ML/DL techniques should be be considered.","semantic resources,",,,,,,,,,,,,,,,,,,
ar,Semantic-based regularization for learning and inference,7Q5JRVK2,10.1016/j.artint.2015.08.011,Artificial Intelligence,2015,Topic modeling / categorization,semi-supervised,classification,adding prior knowledge in the form of grounded knowledge as constraints significantly improves accuracy.,"Semantic Based Regularization (SBR),",,"statistical relational learning (SLR, RML),
fuzzy logic,
first order logic (FOL),
Transductive learning,
grounding,
propositionalization",SVM,cooperative,5. Neuro_Symbolic,CORA,"SBR builds a multi-layer architecture having kernel machines at the input layer. The output of the kernel machines is fed to the higher layers implementing a fuzzy generalization of the FOL knowledge. The resulting model is continuous with respect to the feature values. Therefore, the high-level semantic inference provided by the logic can be back-propagated down to the kernel machines using any gradient-based schema.
","Area Under the Curve (AUC),
F1","0.89, 0.8",77.78%,"Prior knowledge in the form of grounded FOL rules is used as constraints to optimization. To translate logical expressions, t-norm functions are defined for connectives AND and NOT, as well as universal and existential quatifiers. The results of which mimic that of fuzzy logic (ie., continuous in the range [0,1]). While the performance of the proposed architecture is a significant improvement on the chosen benchmarks (SVM, MLN, and variants thereof), the absolute accuracy scores are still relatively low (ie., not too far off random chance). The limitation appears to be the use of SVM (seemingly because it lends itself to a transductive (semi-supervised) setting - ie., where there is little labeled data, transduction is used to assign labels to data points in close promiximty to labeled points, a bit like using clustering). This is a classification task in a setting with little labeled data, but with no reasoning capability. Future experiments could be conducted with more recent NN approaches in place of the SVM. In addition, other NLP architectures may be better benchmarks for this particular task.",,,,,,,,,,,,,,,,,,,
ar,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system,2PNFHS7L,10.1016/j.neucom.2021.02.021,Neurocomputing,2021,Dialog system,supervised,classification,RNN (encoder) + GAT + RNN (decoder),"heterogeneous graph reasoning (HGR)
model to unify the dialogue context understanding and
entity-correlation reasoning.",,"heterogeneous graph reasoning (HGR),
probabilistic graphical model (PGM)",recurrent neural network (RNN),nested,4. Neuro: Symbolic → Neuro,Private,RNN -> GAT -> RNN,"F1, Precision/recall, Bleu score",Upto 44%,77.78%,"Though it is medical application, might be useful. Seems like they have used text data. Heterogeneous graph reasoning
model to efficiently incorporate the contextual information and domain knowledge for entity reasoning in medical dialogue.",,,,,,,,,,,,,,,,,,,
ar,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification,3YFVRRKE,10.1109/ACCESS.2020.2972751,IEEE Access,2020,Text classification,supervised,classification,Text -> Graph -> embedding ; attention mechanism at various levels to capture semantics,"graph-based document modeling, hierarchical transformer encoder architecture for features extraction, and weight-directed loss for label classification.",,graph representation,attention network,,1. symbolic Neuro symbolic,"Reuters Corpus Volume 1, RCV1-2K, AmazonCat-13K",Graph embeddings ; CNN for feature extraction ; Transformer LSTM,"NDCG@K, p@k",Upto 95%,77.78%,combination of graph-based document modeling and the hierarchical transformer.,,,,,,,,,,,,,,,,,,,
ar,Robust reasoning over heterogeneous textual information for fact verification,74QZV8X9,10.1109/ACCESS.2020.3019586,IEEE Access,2020,Fact verification,supervised,classification,KG + GNN,Double-graph-based reasoning approach for claim verification.,,knowledge graph (KG),graph neural network (GNN),nested,2. Symbolic[Neuro],"FEVER, UKP Snopes Corpus","XLNet, Combination of knowledge graph (KG) and graph neural network (GNN), XLNet, Combination of knowledge graph (KG) and graph neural network (GNN) -> GCN + GAT ;","F1, Accuracy",Upto 78%,66.67%,"Fact verification using pretrained embedding models ; CONTEXTUAL SEMANTIC GRAPH CONSTRUCTION G1 using AllenNLP(converts sentences into Tuples) finetuned using Wordnet -> CONTEXTUAL KNOWLEDGE GRAPH CONSTRUCTION G2 using conceptnet -> CONTEXTUAL COLLABORATIVE KNOWLEDGE GRAPH CONSTRUCTION modeled using XLNet, TransR (input - G1 + G2 output - prob )",,,,,,,,,,,,,,,,,,,
ar,Question Answering Systems with Deep Learning-Based Symbolic Processing,EGI547RA,10.1109/ACCESS.2019.2948081,IEEE Access,2019,Text classification,supervised,classification,Question answering = Prolog + NMT ; Prolog for symbolic reasoning ; NMT -> Seq2Seq model and Transformer ;,Prolog-like processing system using deep learning,,first order logic (FOL),neural network (NN),,1. symbolic Neuro symbolic,"Kinsources, Geoquery","Seq2seq, tranformers","Accuracy, Rate",Upto 99%,55.56%,,,,,,,,,,,,,,,,,,,,
ar,"Ontology based E-learning framework: A personalized, adaptive and context aware model",PD2A2ZVV,10.1007/s11042-019-08125-8,Multimedia Tools and Applications,2019,Recommendation model,supervised,recommendation,Recommendation via ANN using CBR,hybrid of two machine learning techniques named CBR and (ANN),,ontology,neural network (NN),,N/A,Private,,Accuracy,Upto 80%,50.00%,,,,,,,,,,,,,,,,,,,,
ar,From symbolic to sub-symbolic information in question classification,2FCUJH2G,10.1007/s10462-010-9188-4,Artificial Intelligence Review,2011,Question answering,supervised,classification,,rule-mbased question classifier that partially founds its performance in the detection of the question headword and in its mapping into the target category through the use of WordNet.,,rule based,SVM,,2. Symbolic[Neuro],"UIUC dataset,",Rule based system + SVM,"Accuracy, Precision/recall",Upto 88%,66.67%,"rules based results as input to SVM. Not sure that qualifies as NeSy, but let's keep for now.",,,,,,,,,,,,,,,,,,,