Doc type,researcher,Title,Scopus Hash ID,DOI,Journal/Conference,Year,Use Case,Learning,Application,Key-intake,Contribution,Definition of NeSy,Symbolic terms,Neural terms,NeSy Category,Kautz category,Datasets,Model description,Evaluation Metrics,reported score,Study Quality,Comments,MISC,,,,,,,,
ar,A,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case,ACDMDN8W,10.1108/IJWIS-11-2020-0067,International Journal of Web Information Systems,2021,Text classification,supervised,classification,Neutrosophic reasoning to analyze and generate weights for terms in natural languages.,"a new term weighting method, which uses the term frequencies as components to define the relevance and the ambiguity of term;",,fuzzy logic,neural network (NN),sequential,1. symbolic Neuro symbolic,"BBC News database, BBC SPORTS database, Kaagle news ","SVM, feedforward network",Area Under the Curve (AUC),Upto 97%,77.78%,"Instead of directly using TF-IDF, a new weighting method is proposed. This combines the advantages of the neutrosophic reasoning and ambiguity membership function to determine the weight for a given term from a document. ",,,,eliminated,,,,,
ar,K,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier,BAMGPUCX,10.1109/ACCESS.2021.3053917,IEEE Access,2021,Sentiment analysis,supervised,classification,"Data cleaning and preprocessing, as well as the addtion of the fuzzy logic component (MFS) significantly improves performance on sentiment analysis tasks on Twitter datsets compared to SOTA. In addition, parallelization using hadoop map reduce significantly reduces computation time.","we develop a new hybrid fuzzy-deep learning approach, that basically integrates the CNN, FFNN deep learning networks with the MFS fuzzy logic system.",NA,"fuzzy logic,
fuzzy rules,
mamdani fuzzy system (MFS)","convolutional neural network (CNN),
neural network (NN),
embedding",sequential,N/A,"sentiment140, sentiment140,
COVID-19_Sentiments","TREC,
SST-1,
MR,
SST-2","F1,
Accuracy,
Precision/Recall,
True Positive Rate (TPR), True Negative Rate (TNR) or Specificity, False Positive Rate (FPR), False Negative Rate (FNR), Error Rate (ER), Precision (PR), Classification Rate or Accuracy (AC), Kappa Statistic (KS), F1-score (FS) and Time Consumption (TC)",upwards of 99% F1 and accuracy,77.78%,"A pipeline is proposed for sentence level sentiment analysis on two Twitter datasets. The pipeline comproses of data cleaning and preprocessing (this effort alone reduces the error rate by an order of magnitude), learning embeddings, CNN+FFNN for classification, and a MFS (Mandani Fuzzy System) classification step. The algorithms are implemented on hadoop map reduce. The best models achieve F1 and accuracy scores above the SOTA, as well as improve computational efficiency and cost complexity. For the MFS, the choices of linguistic terms and fuzzy rules are made manually relying on domain expertise.  It would be interesting to see how well this system performs on less emotionally charged datasets. A future neural-symbolic direction might be to learn these ""parameters"" automatically with a view to a generalizable system. 
The system doesn't fall into any of the Kautz categories, as it's not strictly speaking an integration of neuro-symnolic techniques, but rather a sequential pipeline.",,,,,,,,,
ar,A,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system,2PNFHS7L,10.1016/j.neucom.2021.02.021,Neurocomputing,2021,Dialog system,supervised,classification,RNN (encoder) + GAT + RNN (decoder) ,heterogeneous graph reasoning (HGR) model to unify the dialogue context understanding and entity-correlation reasoning.,,knowledge graph (KG),recurrent neural network (RNN),compiled,4. Neuro: Symbolic â†’ Neuro,Private,RNN -> GAT -> RNN,"F1, Precision/recall, Bleu score",Upto 44%,77.78%,"Though it is medical application, might be useful. Seems like they have used text data. Heterogeneous graph reasoning model to efficiently incorporate the contextual information and domain knowledge for entity reasoning in medical dialogue.",,,,,,,,,
ar,K,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification,JN4IZETG,10.1007/s10994-021-05968-x,Machine Learning,2021,Text classification,supervised,classification,"autoBOT is explainable, efficient, and performs on par with language models like BERT.","a novel approach for text classification with limited data and resources, but comparable in erformance to BERT. Hyperparameters are optimized using an evolutionary algorithm. Novel feature types: document keywords, relational features, and first order features (based on grounded relations from ConceptNet)",NA,"symbolic learner,
symbolic representations,
ConceptNet KG","BERT,
transformer,
neural network (NN),
embedding,
neuroevolution (NE)",,N/A,"semeval2019, semeval2019,
mbti,
Fox,
BBC,
etc..","autoBOT (automatic Bags-Of-Tokens),
The proposed approach consists of an evolutionary algorithm that jointly optimizes various sparse representations of a given text (including word, subword, POS tag, keyword-based, knowledge graph-based and relational features) and two types of document embeddings (non-sparse representations). The key idea of autoBOT is that, instead of evolving at the learner level, evolution is conducted at the representation level.
",F1,up to 99% on some tasks ,88.89%,"autoBot is a genetic algorithm for text data that learns both the representations and the models for classification jointly. The inclusion of novel symbolic features improves performance over traditional linear models such as SVM and Logistic Regression, and performs on par with large language models such as BERT. The advantage of autoBot is its explainability (feature importances), and efficiency in a low resource setting (small data/feature sparsity, and limited compute power).
The study was conducted using off the shelf implementations and a very basic evolutionary algorithm, thus leaving plenty of room for improvements. Classification is performed at the document level, but it would be worth exploring this pipeline for sentence level and span level classification on for example, the semeval2020 task, which already has a leaderboard with BERT based solutions for comaprison.","Does not fall into any of Kautz's categories. However, the use of linear classifiers (ie symbolic learners) in combination with sub-symbolic representations qualifies for the neuro-symbolic moniker.",,,,,,,,
ar,K,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture,KTEHK4MZ,10.1016/j.neucom.2020.12.040,Neurocomputing,2021,KG Completion / link prediction,reinforcement ,classification,Two phased approach combining NN and traditional path-finding techniques improves KG reasoning performance.,Deep-IDA* framework that integrates the traditional path searching algorithms IDA* and deep neural networks for KG reasoning (link prediction).,,markov decision process (MDP),reinforcement learning (RL),cooperative,2. Symbolic[Neuro],"NELL-995,
WN18RR,
FB15-237
","fusion of various models that do path-finding and path-reasoning. Pathfinding combines Deepening DFS and A*, the result of which is fed to a GNN for path reasoning. There are many neural modules inside this framework including BERT, LSTM, GRU, and GCN.  ","Mean Reciprocal Rank (MRR),
Hits@K,
Mean Average Precision (MAP)",Better than all benchmarks (see tables in paper),66.67%,"While the results reported exceed SOTA, the system lacks parsimony. The computation times are not reported.",,,,,,,,,
ar,K,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text,RDSQSBN7,10.1109/TASLP.2021.3082295,IEEE/ACM Transactions on Audio Speech and Language Processing,2021,Relation extraction,supervised,classification,the combination of 4 different modules improves overall performance for this task and supercedes the SOTA,a novel end-to-end model for the novel dialogue-based relation extraction task with code.,NA,graph representation,"attention network (BERT),
graph neural network (GNN)",,,DialogRE,,F1,64.89%,11.11%,"attention using BERT followed by graph ""reasoning"" with GNN",,,,,,,,,
ar,K,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification,3YFVRRKE,10.1109/ACCESS.2020.2972751,IEEE Access,2020,Text classification,supervised,classification,Text -> Graph -> embedding ; attention mechanism at various levels to capture semantics,"graph-based document modeling, hierarchical transformer encoder architecture for features extraction, and weight-directed loss for label classification.",,graph representation,"recurrent neural network (RNN),
attention network (LSTM)",sequential,1. symbolic Neuro symbolic,"Reuters Corpus Volume 1, RCV1-2K, AmazonCat-13K",Graph embeddings ; CNN for feature extraction ; Transformer LSTM,"NDCG@K, p@k",Upto 95%,77.78%,"combination of graph-based document modeling and the hierarchical transformer. By capturing the logical structure in the text, performance is improved.",,,,,,,,,
ar,K,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis,JGU2SECC,10.1016/j.artmed.2019.101772,Artificial Intelligence in Medicine,2020,Decision making,supervised,classification,Using Huffman trees to build symbols and logic into a Recursive NN which can be optimized with GD.,"Knowledge-based neural network, in which nerve cells in a hidden layer consist of first-order logic.
For neural network parameter learning, a complete theoretical framework based on back-propagation was established to minimize the cross-entropy error.
",,"FOL,
Huffman tree","recursive neural knowledge network (RNKN),
recursive neural network (RNN)",cooperative,Other,CEMR,"text -> KG triples -> Huffman tree -> logic functions as nerve cells -> softmax,
optimized using GD/backProp","p@k,
Discounted Cummulative Gain (DCG)",[0.36 - 0.72],72.22%,"The study encodes knowledge in the form of huffman trees made of triples, and logic expressions, in order to jointly learn embeddings and model weights. The first layer consists of entities, the second layer consists of relations (x-> y). Higher layers compute logic rules. The root node is the final embedding representing a document (in this case a single health record). Softmax is used to calculate class probabilities. back propagation is used for optimization. The study doesn't fall into any Kautz categories easily. Sub-symbolic representations are learned from symbolic features and rules iteratively.",interesting approach that would be worth trying for propaganda detection,,,,,,,,
ar,A,Robust reasoning over heterogeneous textual information for fact verification,74QZV8X9,10.1109/ACCESS.2020.3019586,IEEE Access,2020,Fact verification,supervised,classification,KG + GNN ,Double-graph-based reasoning approach for claim verification.,,knowledge graph (KG),graph neural network (GNN),nested,2. Symbolic[Neuro],"FEVER, UKP Snopes Corpus","XLNet, Combination of knowledge graph (KG) and graph neural network (GNN), XLNet, Combination of knowledge graph (KG) and graph neural network (GNN) -> GCN + GAT ; ","F1, Accuracy",Upto 78%,66.67%,"Fact verification using pretrained embedding models ; CONTEXTUAL SEMANTIC GRAPH CONSTRUCTION G1 using AllenNLP(converts sentences into Tuples) finetuned using Wordnet -> CONTEXTUAL KNOWLEDGE GRAPH CONSTRUCTION G2 using conceptnet -> CONTEXTUAL COLLABORATIVE KNOWLEDGE GRAPH CONSTRUCTION modeled using XLNet, TransR (input - G1 + G2 output - prob )",,,,,,,,,
ar,K,The CoRg Project: Cognitive Reasoning,7MMJY5BM,10.1007/s13218-019-00601-5,KI - Kunstliche Intelligenz,2019,Question answering,supervised,reasoning,Authors argue that combining logic reasoning with ML improves performance and gerates explainable results. The challenge is in finding an appropriate logics for cognitive (human-like) reasoning.,,,logic,recurrent neural network (RNN),sequential,,COPA benchmark set (Choice of Plausible Alternatives),raw text -> FOL (using KNEWS) -> augment with bg knwledge bases -> theorem prover -> NN,,,44.44%,"The study explores using a variety of logic systems for Cognitive Reasoning. The authors posit that combining logic programming with NN should produce not only more accurate results, but also provide expalainbility. It is a work in progress. Results are not reported. ",,,,,,,,,
ar,K,Fuzzy commonsense reasoning for multimodal sentiment analysis,B47SSE6P,10.1016/j.patrec.2019.04.024,Pattern Recognition Letters,2019,Sentiment analysis,supervised,classification,multi-modal sentiment analysis. The computational cost is reduced by projecting the feature embeddings onto a 4 dimentional affective space. Accuracy is increased by the use of fuzzy rules.,"Convolutional Fuzzy Sentiment Classifier (CFSN). in this paper we use a low-dimensional RNN to classify concepts learned via deep learning. By projecting temporal features onto AffectiveSpace, we are able to interpret the features learned. Lastly, we take into account that most sentences have mixed emotions such as sarcasm that can only be modeled effectively using fuzzy membership functions. Hence, we predict the final accuracy of the classifier using fuzzy blending over each pair of simple emotions",,"fuzzy logic,
fuzzy neural network (FNN),
logical connectives","convolutional neural network (CNN),
recurrent neural network (RNN),
deep belief network (DBN)",nested,2. Symbolic[Neuro],"multiple, multi-modal",text -> NN -> n-grams -> low-dimenstion ->  RNN -> CDBN (convolutional deep belief network) -> Fuzzy Logic calssification ,Accuracy,10-20% accuracy improvement over baselines,77.78%,"Not strictly text input. Text is only part of the input, and has been converted using pretrained embeddings. sentiment analysis. includes text datasets. This pipeline includes selection of logical connectives at the input phase, and a fuzzy logic classifier at the output phase.",,,,,,,,,
ar,K,Learning to activate logic rules for textual reasoning,NB39QA35,10.1016/j.neunet.2018.06.012,Neural Networks,2018,Question answering,reinforcement ,reasoning,"Image Scheme (IS) is acquired from early life experience, which is also our hypothesis in this work
our work aims at precise reasoning, good interpretability of models, and perfect performances on small dataset","We propose a novel memory-augmented neural network framework making use of logic rules inducted from existing theories of Image Schema and Cognitive Model.
We redefine textual reasoning tasks as interactive-feedback process with human working memory. Under certain assumptions, we jointly solve two main problems: variable binding and relation activating, via deep reinforcement learning.
Our experimental results show the existence of common properties among real-world relations, and the probability to partially re-construct human logic system to boost performances on language comprehension tasks.
Variable-Relation Reasoning Machine (VRRM)

",NA,"Sequencial decision making,
predicate logic,
relational logic,
rule based,
rule induction,
cognitive linguistics","reinforcement learning (RL),
memory networks,
neural network (NN)
",cooperative,3. Neuro; Symbolic,bAbI-20,"We redefine textual reasoning as a sequential decision-making problem, and solve it with reinforcement learning (RL).",Test error rate,0.70%,66.67%,"This work makes the hypothesis that human reasoning can be modeled by Image Schemas (IS). Schemas are made up of logical rules on (Entity1,Relation,Entitity2) tuples, such as transitivity, or inversion. Input sentences and questions are encoded as the concatenation of individual word embeddings with zero paddings. An MLP module is used to learn a set of tuples by performing simulations and utilizing Reinforcement Learning. This work is limited by the nature of the synthetic data set. It is not shown how well it might generalize to a real world scenario. There is also a limited number of schemas, and as the authors point out, future work should include additional schemas more representative of human reasoning. (See also https://arxiv.org/pdf/1706.01427.pdf from DeepMind 2017)",,,,,,,,,
"commonsense
graph representation,
common sense,/ffect knowledge base",K,Semi-supervised learning for big social data analysis,GS3TRUYZ,10.1016/j.neucom.2017.10.010,Neurocomputing,2018,Sentiment analysis,semi-supervised,classification,The addtion of prior knowledge to bias the classification model improves the accuracy of sentiment predictions.,"AffectNet: combined ConceptNet and WNA - commonsense and emotional knowledge are melded together.
AffectNet2: AffectNet with reduced dimentionality.
a flexible framework for semisupervised learning comprising of two parts: clustering and regularized classification.",,"commonsense
graph representation,
affect knowledge base",SVM,cooperative,5. Neuro_Symbolic,"Pang and Lee,
AffectNet benchmark",clutering -> classification regularized by output of clustering,Accuracy,88.50%,66.67%,"The study embeds prior knowledge in the form of a graph representing concepts and relations from natural language. These are projected into multi-dimentional space and a clustering algorithm is employed to generate the solution space. Subsequently, a regularized classification algorithm is employed where the output of the clustering stage is used in the regularization term. I have classified this as Kautz category 5. However, it differs from the cat5 defition in that the regularization term is not a logic rule, but prior knowledge.",,,,,,,,,
ar,K,Concept generalization and fusion for abstractive sentence generation,7YX447XS,10.1016/j.eswa.2016.01.007,Expert Systems with Applications,2016,Text summarization,supervised,generative,"Dependency parsing and wordNet provide input to a ML clasification model, the output of which is use to generate sentences based on rules.",we have addressed the problem of concepts fusion and generalization for abstractive sentence generation.,,"semantic resources,
dependency parsing",SVM,nested,3. Neuro; Symbolic,NLTK,sentences -> dependency parse -> generate candidates (versions) -> reduce search space using rules -> SVM to select best candidate -> generate sentence using rules,F1,0.808,77.78%,"Uses external data sources to semantically enrich text summarization. It is the application text fusion.The system benefits from an internal machine learnign module. Comparisons with other ML text summarization systems are given, however, these baselines appear to be very weak. There is no comparison with NN, or neural language models. To reasses the approach, more modern ML/DL techniques should be be considered.",,,,,,,,,
ar,K,Semantic-based regularization for learning and inference,7Q5JRVK2,10.1016/j.artint.2015.08.011,Artificial Intelligence,2015,Topic modeling / categorization,semi-supervised,classification,adding prior knowledge in the form of grounded knowledge as constraints significantly improves accuracy.,"Semantic Based Regularization (SBR),",,"statistical relational learning (SLR),
fuzzy logic,
FOL,
transductive learning,
grounding,
propositionalization",SVM,cooperative,5. Neuro_Symbolic,CORA,"SBR builds a multi-layer architecture having kernel machines at the input layer. The output of the kernel machines is fed to the higher layers implementing a fuzzy generalization of the FOL knowledge. The resulting model is continuous with respect to the feature values. Therefore, the high-level semantic inference provided by the logic can be back-propagated down to the kernel machines using any gradient-based schema.
","Area Under the Curve (AUC),
F1","0.89, 0.8",77.78%,"Prior knowledge in the form of grounded FOL rules is used as constraints to optimization. To translate logical expressions, t-norm functions are defined for connectives AND and NOT, as well as universal and existential quatifiers. The results of which mimic that of fuzzy logic (ie., continuous in the range [0,1]). While the performance of the proposed architecture is a significant improvement on the chosen benchmarks (SVM, MLN, and variants thereof), the absolute accuracy scores are still relatively low (ie., not too far off random chance). The limitation appears to be the use of SVM (seemingly because it lends itself to a transductive (semi-supervised) setting - ie., where there is little labeled data, transduction is used to assign labels to data points in close promiximty to labeled points, a bit like using clustering). This is a classification task in a setting with little labeled data, but with no reasoning capability. Future experiments could be conducted with more recent NN approaches in place of the SVM. In addition, other NLP architectures may be better benchmarks for this particular task.",,,,,,,,,
ar,A,From symbolic to sub-symbolic information in question classification,2FCUJH2G,10.1007/s10462-010-9188-4,Artificial Intelligence Review,2011,Question answering,supervised,classification,,rule-mbased question classifier that partially founds its performance in the detection of the question headword and in its mapping into the target category through the use of WordNet.,,rule based,"ML,
SVM",,2. Symbolic[Neuro],"UIUC dataset,",Rule based system  + SVM,"Accuracy, Precision/recall",Upto 88%,66.67%,"rules based results as input to SVM. Not sure that qualifies as NeSy, but let's keep for now.",,,,,,,,,
cp,A,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information,2-s2.0-85113718164,10.1007/978-3-030-82147-0_34,"14th International Conference on Knowledge Science, Engineering and Management, KSEM 2021",2021,Text classification,unsupervised,classification,"Rather than using TransE, TransR etc they have proposed a new embedding model using pretrained BERT.","sememes-based framework for knowledge graph to streamline the semantic space of entities. Replace entity descriptions with a finite set of semantics and encode the sememe labels of entities using a pre-trained Bert model, and finally jointly learning the symbolic triples and sememe labels",,sememes,ML,sequential,1. symbolic Neuro symbolic,"FB15K, WN18",logistic regression,"Mean Average Precision, MAP)",Upto 95%,55.56%,Pretrained BERT to classify of grammatical n pragmatic sesemes. They have translated english words back to chinese to compare their models with others.,"A sememe is an indivisible semantic unit for human languages defined bylinguists. I see this as a NN approach enriched by additonal information about each node (in the form of sememes). One could argue that this additional knowledge is symbolic, and conclude that the neuro-symbolic approach outperforms SOTA NN link prediction, such asTRANS-E.",,,,,,,,
cp,K,Web question answering with neurosymbolic program synthesis,2-s2.0-85108908770,10.1145/3453483.3454047,"42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2021",2021,Question answering,semi-supervised,information extraction,leverages the structure of HTML and Large pretrained language models to significantly outperform all the baselines ,new technique for web question answering that is based on optimal neurosymbolic program synthesis. a DSL for web information extraction that combines pre-trained NLP models with traditional language constructs for string manipulation and tree traversal.,,"domain specific language (DSL),
transductive learning,
program synthesis",neural network (NN),nested,2. Symbolic[Neuro],Private,NN -> DSL -> informatin extraction,"F1, Precision/recall",[0.7 - 0.9] - varies per task. much higher than baselines,77.78%,"the sytem is made up of eisting NLP models (like BERT), for tasks such as keyword extractiion and NER, and a custom DSL for traversing the HTML structure of web pages. The DSL is selected via transductive learning on unlabeled web pages.",,,,,,,,,
cp,A,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases,2-s2.0-85096590382,10.1007/978-3-030-61609-0_51,"29th International Conference on Artificial Neural Networks, ICANN 2020",2020,KG Completion / link prediction,unsupervised,classification,"Trains a model to reason directly over graphs. enabling reasoning over
many paths at once.",,,symbolic node embedding,graph neural network (GNN),"sequential, cooperative",3. Neuro; Symbolic,"ClueWeb, Freebase",GNN to learn embeddings followed by MLP for prediction,"Mean Average Precision, MAP)",Upto 92%,77.78%,"Authors have created 3 models, GNN-Relation, GNN-Mean and GNN-sum ; GNN relation considers only relations where as other 2 models considers both entity and relations to store/compute embeddings. ",,,,,,,,,
cp,K,Mapping natural-language problems to formal-language solutions using structured neural representations,2-s2.0-85105153943,,"37th International Conference on Machine Learning, ICML 2020",2020,Textual reasoning,supervised,"reasoning,
generative","Encoder binding, decoder unbinding. State-of-the-art performance on two recently developed N2F tasks shows that the TPN2F model has significant structure learning ability on tasks requiring symbolic reasoning through program synthesis.","a new scheme for neural-symbolic relational representations and a new architecture, TP-N2F, for formal-language generation from natural-language descriptions.",,binding problem,recurrent neural network (RNN),compiled,4. Neuro: Symbolic â†’ Neuro,MathQA,natural language to TPR -> LSTM encoder ( binding) -> LSTM decoder (unbinding) -> formal language,Accuracy,71.89%,72.22%,"Natural- to formal-language generation. Using product tensor representations in an emcoder-decoder architecture.
",,,,,,,,,
cp,K,A Framework for a Comprehensive Conceptualization of Urban Constructs,2-s2.0-85091286980,,"25th International Conference on Computer-Aided Architectural Design Research in Asia, CAADRIA 2020",2020,"KG Completion / link prediction,
Textual reasoning,
Analogical reasoning",supervised,information extraction,"The paper proposes a framework for constructing a knowledge-base of urban constructs that builds on an ontology of urbanism. The system is made of two modules:  concepts are represented as a knowledge graph (KG) named SpatialNet, while the physical features are represented by a deep neural network (DNN) called SpatialFeaturesNet. SpatialNet is constructed using NLP using semantic analyses of nine English lingual corpora and then structured using the urban ontology.

",,,"ontology,
knowledge graph completion,
knowledge graph (KG)","recurrent neural network (RNN),
convolutional neural network (CNN)",sequential,,NOT SPECIFIED,KG + DNN,,,44.44%,"Associating the materialized spatial symbols with their meanings is called symbol grounding, alternatively known as symbol conceptualization. The two components of the system are SPatialFeaturesNet, an application of computer vision, and SpatialNet, an application of natural language processing.
This is a system similar to CBD (case based reasoning), in that it keeps a knowledge base from which the user can draw ideas. It surpasses the traditional CBD because the KG is built from big data, using NLP/DL. The KG is built on an existing ontology of urbanism. The purpose of the system is to aid in the creative process of urban design. Unfortunately the authors do not provide any evaluation of their solution.
",,,,,,,,,
cp,K,Just Add Functions: A Neural-Symbolic Language Model,2-s2.0-85106687657,,"34th AAAI Conference on Artificial Intelligence, AAAI 2020",2020,Textual reasoning,supervised,generative,"adding explicit logic functions to a language model improves performance especially on small datasets. It does this by enhancing the inductive bias of NNLM. In other words, it makes the assumption that ""you are the company you keep"" (refered to as the distributional hypothesis of language) even stronger.","NSLM - neuro symbolic language model. A method for embedding domain specific logic and/or knowledge into a language model, thus enhancing the models ability to predict words in said domain. The method can be extended to any domain as long as the architect can find appropriate logical expressions for her domain. A Neural-Symbolic Language Model (NSLM) is a hierarchical NNLM that incorporates simple functions to enhance inductive bias.",,rule based,recurrent neural network (RNN),nested,4. Neuro: Symbolic â†’ Neuro,Wikitext,"1. Identify a class C, which has an inductive bias that can be encoded as a simple mathematical or logical expression. 
2. generate vocabulary for class C 
3. Add/delete candidate metric functions fc(w(c) t , w(c) tâˆ’1) and PDFs to be considered. 
4. Train a hierarchical NNLM on words and class labels and select a {metric function,PDF} pair.
",perplexity,outperforms NHLM,66.67%,"We seek to establish a general method to tune language models for domain-specific tasks by enhancing the inductive bias with symbolic expressions that encode domain knowledge (Grounding with External Knowledge). 
It's a language model, so it can be used for sentence completion, measuring similarity, and other downstream tasks. ",,,,,,,,,
ar,A,Solving the twitter sentiment analysis problem based on a machine learning-based approach,4EF4BXD9,10.1007/s12065-019-00301-x,Evolutionary Intelligence,2020,Sentiment analysis,supervised,classification,Combined 4 methods of feature selection which is given as input to GA which optimizes the features. Einstien conorm is used to combine values from 4 methods,,,,SVM,sequential,1. symbolic Neuro symbolic,"Stanford Twitter Sentiment (STS), STS-Gold, Strict Obama-McCain Debate, Obama-McCain Debate",feature selection -> SV\M,"Precision/recall, Accuracy, F1",Upto 85%,50.00%,Symbolic term?!,,,,,,,,,
ar,A,Question Answering Systems with Deep Learning-Based Symbolic Processing,EGI547RA,10.1109/ACCESS.2019.2948081,IEEE Access,2019,Text classification,supervised,classification,Question answering = Prolog + NMT ; Prolog for symbolic reasoning ; NMT -> Seq2Seq model and Transformer ; ,Prolog-like processing system using deep learning,,"symbolic processing,
FOL
",neural network (NN),,1. symbolic Neuro symbolic,"Kinsources, Geoquery","Seq2seq, tranformers","Accuracy, Rate",Upto 99%,55.56%,,,,,,,,,,
ar,A,"Ontology based E-learning framework: A personalized, adaptive and context aware model",PD2A2ZVV,10.1007/s11042-019-08125-8,Multimedia Tools and Applications,2019,Recommendation model,supervised,recommendation,Recommendation via ANN using CBR,hybrid of two machine learning techniques named CBR and (ANN),,ontology,ML,,N/A,Private,,Accuracy,Upto 80%,50.00%,,,,,,,,,,
ar,,A modular architecture for transparent computation in recurrent neural networks,7I4CIKII,10.1016/j.neunet.2016.09.001,Neural Networks,2017,,,,,,,,,,,,,,,0.00%,,63 pages. Lots of theory. Leaving for last,,,,,,,,
ar,A,Assessing cognitive alignment in different types of dialog by means of a network model,6DWCY3EC,10.1016/j.neunet.2012.02.013,Neural Networks,2012,Relation extraction,semi-supervised,classification,"dialog alignment. It is important for idioms like (learn by hearts, ready-mades etc). But sometimes speakers create their own linguistic representations. ","whether the class of aligned dialogs can be identified across different types of dialog. A successful classification indicates a structural fingerprint left by alignment, regardless of the underlying dialog type.",,"graph representation,
dialogical alignment",neural network (NN),sequential,1. symbolic Neuro symbolic,"Jigsaw Map Game, Speechand  Gesture  Alignment corpus", priming and spreading activation are modeledi n terms of two layered time aligned networking,F1,Upto 91%,44.44%,"dialog, 
conversation, 
graph representations of communication units as input to machine learning",,,,,,,,,