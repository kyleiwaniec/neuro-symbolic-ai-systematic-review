EID_x,Title_x,References,ref_title,Authors,Title_y,Abstract,DOI,Year,Cited by,Author Keywords,Index Keywords,EID_y
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model,"Ahn, S., Choi, H., Pärnamaa, T., Bengio, Y., (2017) A neural knowledge language model, , CoRR abs/1608.00318",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Bengio, Y., Schwenk, H., Senecal, J.-S., Morin, F., Gauvain, J.-L., (2006) Neural probabilistic language models",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Besold, T. R., d'Avila Garcez, A. S., Bader, S., Bowman, H., Domingos, P. M., Hitzler, P., Kühnberger, K.-U., Zaverucha, G., (2017) Neural-symbolic learning and reasoning: A survey and interpretation, , ArXiv abs/1711.03902",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., One billion word benchmark for measuring progress in statistical language modeling (2013) INTERSPEECH",One billion word benchmark for measuring progress in statistical language modeling,"Chelba C., Mikolov T., Schuster M., Ge Q., Brants T., Koehn P.",One billion word benchmark for measuring progress in statistical language modeling,[No abstract available],,2013.0,5.0,,,2-s2.0-85076501349
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Firth, J. R., (1957) A synopsis of linguistic theory 1930-55, pp. 1-32. , 1952 59",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," (2019) GeoNames, , http://geonames.org",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Grave, E., Joulin, A., Usunier, N., (2017) Improving neural language models with a continuous cache, , CoRR abs/1612.04426",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Hupkes, D., Zuidema, W. H., Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure (extended abstract) (2018) IJCAI",Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure (extended abstract),,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Jean, S., Cho, K., Memisevic, R., Bengio, Y., On using very large target vocabulary for neural machine translation (2015) ACL",On using very large target vocabulary for neural machine translation,"Jean S., Cho K., Memisevic R., Bengio Y.",On using very large target vocabulary for neural machine translation,"Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English!German and English!French translation tasks of WMT'14. © 2015 Association for Computational Linguistics.",10.3115/v1/p15-1001,2015.0,309.0,,Complex networks; Computational linguistics; Computer aided language translation; Decoding; Importance sampling; Natural language processing systems; Baseline models; Decoding complexity; Machine translation models; Machine translations; Phrase-based statistical machine translation; State of the art; Target words; Training complexity; Long short-term memory,2-s2.0-84943744936
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Jozefowicz, R., Vinyals, O., Schuster, M., Shazeer, N., Wu, Y., (2016) Exploring the limits of language modeling, , CoRR abs/1602.02410",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Kordopatis-Zilos, G., Papadopoulos, S., Kompatsiaris, Y., Geotagging text content with language models and feature mining (2017) Proceedings of the IEEE, 105, pp. 1971-1986",Geotagging text content with language models and feature mining,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Krause, B., Kahembwe, E., Murray, I., Renals, S., (2019) Dynamic evaluation of transformer language models, , CoRR abs/1904.08378",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Kushman, N., Zettlemoyer, L. S., Barzilay, R., Artzi, Y., Learning to automatically solve algebra word problems (2014) ACL",Learning to automatically solve algebra word problems,"Kushman N., Zettlemoyer L., Barzilay R., Artzi Y.",Learning to automatically solve algebra word problems,[No abstract available],,2014.0,7.0,,,2-s2.0-85007150187
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Marcus, G., (2018) Deep learning: A critical appraisal, , CoRR abs/1801.00631",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Merity, S., Keskar, N. S., Socher, R., (2017) Regularizing and optimizing lstm language models, , ArXiv abs/1708.02182",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Morin, F., Bengio, Y., Hierarchical probabilistic neural network language model (2005) AISTATS",Hierarchical probabilistic neural network language model,"Morin F., Bengio Y.",Hierarchical probabilistic neural network language model,"In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.",,2005.0,544.0,,Conditional probabilities; Continuous spaces; Hier-archical clustering; Hierarchical decompositions; Importance sampling method; Language model; Language modeling; N-gram models; Prior knowledge; Probabilistic neural networks; Speech recognizer; Statistical language modeling; Training example; WordNet semantics; Network architecture; Neural networks; Semantics; Computational linguistics,2-s2.0-34547997987
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," O'Hare, N., Murdock, V., Modeling locations with social media (2012) Information Retrieval, 16, pp. 30-62",Modeling locations with social media,"O'Hare N., Murdock V.",Modeling locations with social media,"In this paper we focus on the locations explicit and implicit in users descriptions of their surroundings. We propose a statistical language modeling approach to identifying locations in arbitrary text, and investigate several ways to estimate the models, based on the term frequency and the user frequency. The geotagged public photos in Flickr serve as a convenient ground truth. Our results show that we can predict location within a one kilometer by one kilometer cell with 17 % accuracy, and within a three kilometer radius around such a one kilometer cell with 40 % accuracy, using only a photo's tags. This is significantly better than the state of the art. Further we examine several estimation strategies that leverage the physical proximity of places, and show that for sparsely represented locations, smoothing from the immediate neighborhood improves results. We also show that estimation strategies based on user frequency are much more reliable than approaches based on the raw term frequency. © 2012 Springer Science+Business Media, LLC.",10.1007/s10791-012-9195-y,2013.0,33.0,Flickr; Geographic context; Geotagging; Language models; User-generated content,,2-s2.0-84873526618
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Pennington, J., Socher, R., Manning, C. D., Glove: Global vectors for word representation (2014) EMNLP",Glove: Global vectors for word representation,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., (2019) Language models are unsupervised multitask learners",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Roy, S., Roth, D., Solving general arithmetic word problems (2015) EMNLP",Solving general arithmetic word problems,"Roy S., Roth D.",Solving general arithmetic word problems,"This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of quantity schemas that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems. © 2015 Association for Computational Linguistics.",10.18653/v1/d15-1202,2015.0,65.0,,Benchmarking; Forestry; Algorithmic approach; Arithmetic expression; Benchmark datasets; Existing systems; Multiple Classification; State-of-the-art performance; Word problem; World knowledge; Natural language processing systems,2-s2.0-84959934544
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Smart, P. D., Jones, C. B., Twaroch, F. A., Multisource toponym data integration and mediation for a metagazetteer service (2010) GIScience",Multisource toponym data integration and mediation for a metagazetteer service,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Spithourakis, G. P., Riedel, S., (2018) Numeracy for language models: Evaluating and improving their ability to predict numbers, , CoRR abs/1805.08154",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Spithourakis, G. P., Augenstein, I., Riedel, S., Numerically grounded language models for semantic error correction (2016) EMNLP",Numerically grounded language models for semantic error correction,"Spithourakis G.P., Augenstein I., Riedel S.",Numerically grounded language models for semantic error correction,"Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements. © 2016 Association for Computational Linguistics",10.18653/v1/d16-1101,2016.0,4.0,,Computational linguistics; Error correction; Knowledge based systems; Semantics; Back-ground knowledge; Grammatical errors; Knowledge base; Language model; Semantic errors; Natural language processing systems,2-s2.0-85063098121
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Tobler, W. R., (1970) A computer movie simulating urban growth in the detroit region",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Yang, Z., Dai, Z., Salakhutdinov, R. R., Cohen, W. W., (2018) Breaking the softmax bottleneck: A high-rank rnn language model, , CoRR abs/1711.03953",,,,,,,,,,
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model," Zaremba, W., Sutskever, I., Vinyals, O., (2014) Recurrent neural network regularization, , CoRR abs/1409.2329",,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations,"Amini, A., Gabriel, S., Lin, P., Kedziorski, R. K., Choi, Y., Hajishirzi, H., Mathqa: Towards interpretable math word problem solving with operation-based formalisms (2019) NACCL",Mathqa: Towards interpretable math word problem solving with operation-based formalisms,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Bednarek, J., Piaskowski, K., Krawiec, K., (2019) Ain't nobody got time for coding: Structure-aware program synthesis from natural language, , arXiv.org",,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Cai, D., Lam, W., (2019) Core semantic frst: A top-down approach for amr parsing, , arXiv:1909.04303",,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Chen, K., Forbus, K. D., Action recognition from skeleton data via analogical generalization over qualitative representations (2018) Thirty-Second AAAI Conference",Action recognition from skeleton data via analogical generalization over qualitative representations,"Chen K., Forbus K.D.",Action recognition from skeleton data via analogical generalization over qualitative representations,"Human action recognition remains a difficult problem for AI. Traditional machine learning techniques can have high recognition accuracy, but they are typically black boxes whose internal models are not inspectable and whose results are not explainable. This paper describes a new pipeline for recognizing human actions from skeleton data via analogical generalization. Specifically, starting with Kinect data, we segment each human action by temporal regions where the motion is qualitatively uniform, creating a sketch graph that provides a form of qualitative representation of the behavior that is easy to visualize. Models are learned from sketch graphs via analogical generalization, which are then used for classification via analogical retrieval. The retrieval process also produces links between the new example and components of the model that provide explanations. To improve recognition accuracy, we implement dynamic feature selection to pick reasonable relational features. We show the explanation advantage of our approach by example, and results on three public datasets illustrate its utility. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2018.0,3.0,,Learning systems; Musculoskeletal system; Action recognition; Dynamic feature selections; Human-action recognition; Machine learning techniques; Qualitative representation; Recognition accuracy; Relational features; Retrieval process; Artificial intelligence,2-s2.0-85060442252
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Chen, K., Rabkina, I., McLure, M. D., Forbus, K. D., Human-like sketch object recognition via analogical learning (2019) Thirty-Third AAAI Conference, 33, pp. 1336-1343",Human-like sketch object recognition via analogical learning,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Crouse, M., McFate, C., Forbus, K. D., Learning from unannotated qa pairs to analogically disanbiguate and answer questions (2018) Thirty-Second AAAI Conference",Learning from unannotated qa pairs to analogically disanbiguate and answer questions,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Forbus, K., Liang, C., Rabkina, I., Representation and computation in cognitive models (2017) Top Cognitive System",Representation and computation in cognitive models,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Gao, J., Galley, M., Li, L., Neural approaches to conversational ai (2019) Foundations and Trends R in Information Retrieval, 13 (2-3), pp. 127-298",Neural approaches to conversational ai,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Goldin-Meadow, S., Gentner, D., (2003) Language in mind: Advances in the study of language and thought, , MIT Press",,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Huang, Q., Smolensky, P., He, X., Wu, O., Deng, L., Tensor product generation networks for deep nlp modeling (2018) NAACL",Tensor product generation networks for deep nlp modeling,"Huang Q., Smolensky P., He X., Deng L., Wu D.",Tensor product generation networks for deep nlp modeling,"We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture - the Tensor Product Generation Network (TPGN) - is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation. © 2018 The Association for Computational Linguistics.",,2018.0,8.0,,Computational linguistics; Deep learning; Natural language processing systems; Network architecture; Product design; Tensors; Distributed neural networks; Distributed representation; Grammatical category; Image caption; Internal representation; NAtural language processing; New approaches; Tensor products; Long short-term memory,2-s2.0-85066460500
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Huang, Q., Deng, L., Wu, D., Liu, c., He, X., Attentive tensor product learning (2019) Thirty-Third AAAI Conference, 33",Attentive tensor product learning,"Huang Q., Deng L., Wu D., Liu C., He X.",Attentive tensor product learning,"This paper proposes a novel neural architecture - Attentive Tensor Product Learning (ATPL) - to represent grammatical structures of natural language in deep learning models. ATPL exploits Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, to integrate deep learning with explicit natural language structures and rules. The key ideas of ATPL are: 1) unsupervised learning of role-unbinding vectors of words via the TPR-based deep neural network; 2) the use of attention modules to compute TPR; and 3) the integration of TPR with typical deep learning architectures including long short-term memory and feedforward neural networks. The novelty of our approach lies in its ability to extract the grammatical structure of a sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. Our ATPL approach is applied to 1) image captioning, 2) part of speech (POS) tagging, and 3) constituency parsing of a natural language sentence. The experimental results demonstrate the effectiveness of the proposed approach in all these three natural language processing tasks. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2019.0,4.0,,Deep neural networks; Feedforward neural networks; Learning systems; Natural language processing systems; Network architecture; Syntactics; Tensors; Cognitive science; Grammatical structure; Image captioning; Learning architectures; NAtural language processing; Natural languages; Neural architectures; Part of speech tagging; Deep learning,2-s2.0-85080593464
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Kamath, A., Das, R., A survey on semantic parsing (2019) AKBC",A survey on semantic parsing,"Kamath Aishwarya, Das Rajarshi",A survey on semantic parsing,[No abstract available],,2019.0,4.0,,,2-s2.0-85093195060
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Kingma, D. P., Ba, J., (2017) Adam: A method for stochastic optimization, , arXiv preprint arXiv:1412.6980",,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Langley, P., Crafting papers on machine learning (2000) Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 1207-1216. , Langley, P. (ed), Stanford, CA, Morgan Kaufmann",Crafting papers on machine learning,Langley P.,Crafting papers on machine learning,[No abstract available],,2000.0,54.0,,,2-s2.0-18744411494
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Lee, K., Palangi, H., Chen, X., Hu, H., Gao, J., (2019) Learning visual relation priors for image-text matching and image captioning with neural scene graph generators, , http://arxiv.org/abs/1909.09953, abs/1909.09953",,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Lee, M., He, X., Yih, W.-t., Gao, J., Deng, L., Smolensky, P., Reasoning in vector space: An exploratory study of question answering (2016) ICLR",Reasoning in vector space: An exploratory study of question answering,"Lee M., He X., Yih W.-T., Gao J., Deng L., Smolensky P.",Reasoning in vector space: An exploratory study of question answering,"Question answering tasks have shown remarkable progress with distributed vector representation. In this paper, we investigate the recently proposed Facebook bAbI tasks which consist of twenty different categories of questions that require complex reasoning. Because the previous work on bAbI are all end-to-end models, errors could come from either an imperfect understanding of semantics or in certain steps of the reasoning. For clearer analysis, we propose two vector space models inspired by Tensor Product Representation (TPR) to perform knowledge encoding and logical reasoning based on common-sense inference. They together achieve near-perfect accuracy on all categories including positional reasoning and path finding that have proved difficult for most of the previous approaches. We hypothesize that the difficulties in these categories are due to the multi-relations in contrast to uni-relational characteristic of other categories. Our exploration sheds light on designing more sophisticated dataset and moving one step toward integrating transparent and interpretable formalism of TPR into existing learning paradigms. © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.",,2016.0,11.0,,Semantics; End-to-end models; Exploratory studies; Learning paradigms; Logical reasoning; Question Answering; Question Answering Task; Vector representations; Vector space models; Vector spaces,2-s2.0-85083951918
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Liao, Y., Bing, L., Li, P., Shi, S., Lam, W., Zhang, T., Core semantic frst: A top-down approach for amr parsing (2018) EMNLP, pp. 3855-3864",Core semantic frst: A top-down approach for amr parsing,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Luong, M.-T., Pham, H., Manning, C. D., Effective approaches to attention-based neural machine translation (2015) EMNLP, pp. 533-536",Effective approaches to attention-based neural machine translation,"Luong M.-T., Pham H., Manning C.D.",Effective approaches to attention-based neural machine translation,[No abstract available],,2015.0,1257.0,,,2-s2.0-84994358876
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Martin, A. E., A compositional neural architecture for language (2020) Journal of Cognitive Neuroscience",A compositional neural architecture for language,Martin A.E.,A compositional neural architecture for language,"Hierarchical structure and compositionality imbue human language with unparalleled expressive power and set it apart from other perception–action systems. However, neither formal nor neurobiological models account for how these defining computational properties might arise in a physiological system. I attempt to reconcile hierarchy and compositionality with principles from cell assembly computation in neuroscience; the result is an emerging theory of how the brain could convert distributed perceptual representations into hierarchical structures across multiple timescales while representing interpretable incremental stages of (de) compositional meaning. The model’s architecture—a multidimensional coordinate system based on neurophysiological models of sensory processing—proposes that a manifold of neural trajectories encodes sensory, motor, and abstract linguistic states. Gain modulation, including inhibition, tunes the path in the manifold in accordance with behavior and is how latent structure is inferred. As a consequence, predictive information about upcoming sensory input during production and comprehension is available without a separate operation. The proposed processing mechanism is synthesized from current models of neural entrainment to speech, concepts from systems neuroscience and category theory, and a symbolic-connectionist computational model that uses time and rhythm to structure information. I build on evidence from cognitive neuroscience and computational modeling that suggests a formal and mechanistic alignment between structure building and neural oscillations, and moves toward unifying basic insights from linguistics and psycholinguistics with the currency of neural computation. © 2020 Massachusetts Institute of Technology",10.1162/jocn_a_01552,2020.0,19.0,,Computational linguistics; Hierarchical systems; Neurology; Physiological models; Cognitive neurosciences; Computational properties; Hierarchical structures; Neurobiological model; Neurophysiological model; Perceptual representations; Predictive information; Structure information; Computational neuroscience; article; brain; cognitive neuroscience; comprehension; computer model; human; human experiment; oscillation; perception; psycholinguistics; rhythm; sensory stimulation; speech; theoretical study,2-s2.0-85084999818
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Palangi, H., Smolensky, P., He, X., Deng, L., Questionanswering with grammatically-interpretable representations (2018) AAAI",Questionanswering with grammatically-interpretable representations,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Polosukhin, I., Skidanov, A., Neural program search: Solving programming tasks from description and examples (2018) ICLR workshop",Neural program search: Solving programming tasks from description and examples,"Polosukhin I., Skidanov A.",Neural program search: Solving programming tasks from description and examples,"We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline. © 6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings. All rights reserved.",,2018.0,20.0,,Natural language processing systems; Problem oriented languages; Statistical tests; Domain specific languages; Input/output; Natural languages; Program synthesis; Programming tasks; Search Algorithms; Sequence modeling; Test examples; Deep learning,2-s2.0-85083953158
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Roads, B., Love, B., (2019) Learning as the unsupervised alignment of conceptual systems, , arXiv preprint arXiv:1906.09012",,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Rumelhart, D. E., Hinton, G. E., Williams, R. J., Learning internal representations by error propagation (1986) Parallel distributed processing: Explorations in the microstructure of cognition, 1, pp. 318-362. , Rumelhart, D. E., McClelland, J. L., and the PDP Group (eds), MIT press, Cambridge, MA",Learning internal representations by error propagation,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Schlag, I., Schmidhuber, J., Learning to reason with third order tensor products (2018) Neural Information Processing Systems",Learning to reason with third order tensor products,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Smolensky, P., Tensor product variable binding and the representation of symbolic structures in connectionist networks (1990) Artifcial Intelligence, 46, pp. 159-216",Tensor product variable binding and the representation of symbolic structures in connectionist networks,,,,,,,,,
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations," Smolensky, P., Lee, M., He, X., Yih, W.-t., Gao, J., Deng, L., (2016) Basic reasoning with tensor product representations, , arXiv preprint arXiv:1601.02745",,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system,"Dodge, J., Gane, A., Zhang, X., Bordes, A., Chopra, S., Miller, A.H., Szlam, A., Weston, J., Evaluating prerequisite qualities for learning end-to-end dialog systems (2016) 4th International Conference on Learning Representations, ICLR 2016",Evaluating prerequisite qualities for learning end-to-end dialog systems,"Dodge J., Gane A., Zhang X., Bordes A., Chopra S., Miller A.H., Szlam A., Weston J.",Evaluating prerequisite qualities for learning end-to-end dialog systems,"A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering ∼75k movie entities and with ∼3.5M training examples. We present results of various models on these tasks, and evaluate their performance. © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.",,2016.0,38.0,,Conversational agents; End-to-end models; Learning machines; Long-term goals; Personalizations; Reasoning tasks; Synthetic data; Training example; Machine learning,2-s2.0-85083950683
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Bordes, A., Boureau, Y., Weston, J., Learning end-to-end goal-oriented dialog (2017) 5th International Conference on Learning Representations, ICLR",Learning end-to-end goal-oriented dialog,"Bordes A., Lan Boureau Y., Weston J.",Learning end-to-end goal-oriented dialog,"Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols in order to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service. © ICLR 2019 - Conference Track Proceedings. All rights reserved.",,2017.0,159.0,,Break down; Dialog systems; Domain specific; End to end; Goal-oriented; Memory network; Non-trivial; State tracking,2-s2.0-85086405484
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Yan, Z., Duan, N., Chen, P., Zhou, M., Zhou, J., Li, Z., Building task-oriented dialogue systems for online shopping (2017) Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence",Building task-oriented dialogue systems for online shopping,"Yan Z., Duan N., Chen P., Zhou M., Zhou J., Li Z.",Building task-oriented dialogue systems for online shopping,"We present a general solution towards building task-oriented dialogue systems for online shopping, aiming to assist online customers in completing various purchase-related tasks, such as searching products and answering questions, in a natural language conversation manner. As a pioneering work, we show what & how existing natural language processing techniques, data resources, and crowdsourcing can be leveraged to build such task-oriented dialogue systems for E-commerce usage. To demonstrate its effectiveness, we integrate our system into a mobile online shopping application. To the best of our knowledge, this is the first time that an dialogue system in Chinese is practically used in online shopping scenario with millions of real consumers. Interesting and insightful observations are shown in the experimental part, based on the analysis of human-bot conversation log. Several current challenges are also pointed out as our future directions. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2017.0,68.0,,Artificial intelligence; Data handling; Electronic commerce; Online systems; Speech processing; Data resources; Dialogue systems; General solutions; Natural languages; Online customers; Online shopping; Task-oriented; Natural language processing systems,2-s2.0-85030462261
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Tang, K.-F., Kao, H.-C., Chou, C.-N., Chang, E.Y., Inquire and diagnose: Neural symptom checking ensemble using deep reinforcement learning (2016), Proceedings of NIPS Workshop on Deep Reinforcement Learning",Inquire and diagnose: Neural symptom checking ensemble using deep reinforcement learning,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Wei, Z., Liu, Q., Peng, B., Tou, H., Chen, T., Huang, X., Wong, K.-F., Dai, X., Task-oriented dialogue system for automatic diagnosis (2018) Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pp. 201-207",Task-oriented dialogue system for automatic diagnosis,"Liu Q., Wei Z., Peng B., Dai X., Tou H., Chen T., Huang X., Wong K.-F.",Task-oriented dialogue system for automatic diagnosis,"In this paper, we make a move to build a dialogue system for automatic diagnosis. We first build a dataset collected from an online medical forum by extracting symptoms from both patients’ self-reports and conversational data between patients and doctors. Then we propose a task-oriented dialogue system framework to make the diagnosis for patients automatically, which can converse with patients to collect additional symptoms beyond their self-reports. Experimental results on our dataset show that additional symptoms extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these symptoms automatically and make a better diagnosis. © 2018 Association for Computational Linguistics.",,2018.0,31.0,,Computational linguistics; Speech processing; Automatic diagnosis; Dialogue systems; Task-oriented; Diagnosis,2-s2.0-85062544074
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Liao, K., Liu, Q., Wei, Z., Peng, B., Chen, Q., Sun, W., Huang, X., (2004), Task-oriented dialogue system for automatic disease diagnosis via hierarchical reinforcement learning, CoRR abs/2004.14254. arXiv14254",,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Du, N., Wang, M., Tran, L., Lee, G., Shafran, I., Learning to infer entities, properties and their relations from clinical conversations (2019) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4979-4990",properties and their relations from clinical conversations,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Lin, X., He, X., Chen, Q., Tou, H., Wei, Z., Chen, T., Enhancing dialogue symptom diagnosis with global attention and symptom graph (2019) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 5033-5042",Enhancing dialogue symptom diagnosis with global attention and symptom graph,"Chen Q., Lin X., He X., Tou H., Chen T., Wei Z.",Enhancing dialogue symptom diagnosis with global attention and symptom graph,"Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset. © 2019 Association for Computational Linguistics",,2020.0,2.0,,Diagnosis; Large dataset; Medical computing; Attention mechanisms; Benchmark models; Electronic medical record; Large amounts; NAtural language processing; Research focus; State-of-the-art performance; Natural language processing systems,2-s2.0-85084308242
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Xu, L., Zhou, Q., Gong, K., Liang, X., Tang, J., Lin, L., End-to-end knowledge-routed relational dialogue system for automatic diagnosis (2019) Proceedings of the Thirty-Third AAAI Conference on Artificial Intelligence",End-to-end knowledge-routed relational dialogue system for automatic diagnosis,"Xu L., Zhou Q., Gong K., Liang X., Tang J., Lin L.",End-to-end knowledge-routed relational dialogue system for automatic diagnosis,"Beyond current conversational chatbots or task-oriented dialogue systems that have attracted increasing attention, we move forward to develop a dialogue system for automatic medical diagnosis that converses with patients to collect additional symptoms beyond their self-reports and automatically makes a diagnosis. Besides the challenges for conversational dialogue systems (e.g. topic transition coherency and question understanding), automatic medical diagnosis further poses more critical requirements for the dialogue rationality in the context of medical knowledge and symptom-disease relations. Existing dialogue systems (Madotto, Wu, and Fung 2018; Wei et al. 2018; Li et al. 2017) mostly rely on data-driven learning and cannot be able to encode extra expert knowledge graph. In this work, we propose an End-to-End Knowledge-routed Relational Dialogue System (KR-DS) that seamlessly incorporates rich medical knowledge graph into the topic transition in dialogue management, and makes it cooperative with natural language understanding and natural language generation. A novel Knowledge-routed Deep Q-network (KR-DQN) is introduced to manage topic transitions, which integrates a relational refinement branch for encoding relations among different symptoms and symptom-disease pairs, and a knowledge-routed graph branch for topic decision-making. Extensive experiments on a public medical dialogue dataset show our KR-DS significantly beats state-of-the-art methods (by more than 8% in diagnosis accuracy). We further show the superiority of our KR-DS on a newly collected medical dialogue system dataset, which is more challenging retaining original self-reports and conversational data between patients and doctors. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2019.0,11.0,,Behavioral research; Decision making; Encoding (symbols); Knowledge representation; Natural language processing systems; Speech processing; Automatic diagnosis; Dialogue management; Dialogue systems; Expert knowledge; Medical knowledge; Natural language generation; Natural language understanding; State-of-the-art methods; Diagnosis,2-s2.0-85090800757
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Zhang, S., Dinan, E., Urbanek, J., Szlam, A., Kiela, D., Weston, J., Personalizing dialogue agents: I have a dog, do you have pets too? (2018) Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pp. 2204-2213",do you have pets too?,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Moghe, N., Arora, S., Banerjee, S., Khapra, M.M., Towards exploiting background knowledge for building conversation systems (2018) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 2322-2332. , Association for Computational Linguistics",Towards exploiting background knowledge for building conversation systems,"Moghe N., Arora S., Banerjee S., Khapra M.M.",Towards exploiting background knowledge for building conversation systems,"Existing dialog datasets contain a sequence of utterances and responses without any explicit background knowledge associated with them. This has resulted in the development of models which treat conversation as a sequence-to-sequence generation task (i.e., given a sequence of utterances generate the response sequence). This is not only an overly simplistic view of conversation but it is also emphatically different from the way humans converse by heavily relying on their background knowledge about the topic (as opposed to simply relying on the previous sequence of utterances). For example, it is common for humans to (involuntarily) produce utterances which are copied or suitably modified from background articles they have read about the topic. To facilitate the development of such natural conversation models which mimic the human process of conversing, we create a new dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie. We establish baseline results on this dataset (90K utterances from 9K conversations) using three different models: (i) pure generation based models which ignore the background knowledge (ii) generation based models which learn to copy information from the background knowledge when required and (iii) span prediction based models which predict the appropriate response span in the background knowledge. © 2018 Association for Computational Linguistics",,2020.0,37.0,,Back-ground knowledge; Baseline results; Conversation systems; Prediction-based; Sequence generation; Natural language processing systems,2-s2.0-85071146068
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Dinan, E., Roller, S., Shuster, K., Fan, A., Auli, M., Weston, J., Wizard of wikipedia: knowledge-powered conversational agents (2019) 7th International Conference on Learning Representations, ICLR",Wizard of wikipedia: knowledge-powered conversational agents,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Wu, W., Guo, Z., Zhou, X., Wu, H., Zhang, X., Lian, R., Wang, H., Proactive human-machine conversation with explicit conversation goal (2019) Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 3794-3804",Proactive human-machine conversation with explicit conversation goal,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Liu, S., Chen, H., Ren, Z., Feng, Y., Liu, Q., Yin, D., Knowledge diffusion for neural dialogue generation (2018) Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pp. 1489-1498",Knowledge diffusion for neural dialogue generation,"Liu S., Chen H., Ren Z., Feng Y., Liu Q., Yin D.",Knowledge diffusion for neural dialogue generation,"End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion, the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset proves that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly. © 2018 Association for Computational Linguistics",10.18653/v1/p18-1138,2018.0,68.0,,Computational linguistics; Knowledge based systems; Baseline models; Dialogue generations; Divergent thinkings; Empirical studies; Factoid questions; Knowledge base; Knowledge diffusion; Natural response; Diffusion,2-s2.0-85063107708
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Zhou, H., Young, T., Huang, M., Zhao, H., Xu, J., Zhu, X., Commonsense knowledge aware conversation generation with graph attention (2018) Proceedings of the Twenty-Seventh International Joint Conference on Artificial Intelligence, IJCAI, pp. 4623-4629",Commonsense knowledge aware conversation generation with graph attention,"Zhou H., Young T., Huang M., Zhao H., Xu J., Zhu X.",Commonsense knowledge aware conversation generation with graph attention,"Commonsense knowledge is vital to many natural language processing tasks. In this paper, we present a novel open-domain conversation generation model to demonstrate how large-scale commonsense knowledge can facilitate language understanding and generation. Given a user post, the model retrieves relevant knowledge graphs from a knowledge base and then encodes the graphs with a static graph attention mechanism, which augments the semantic information of the post and thus supports better understanding of the post. Then, during word generation, the model attentively reads the retrieved knowledge graphs and the knowledge triples within each graph to facilitate better generation through a dynamic graph attention mechanism. This is the first attempt that uses large-scale commonsense knowledge in conversation generation. Furthermore, unlike existing models that use knowledge triples (entities) separately and independently, our model treats each knowledge graph as a whole, which encodes more structured, connected semantic information in the graphs. Experiments show that the proposed model can generate more appropriate and informative responses than state-of-the-art baselines. © 2018 International Joint Conferences on Artificial Intelligence.All right reserved.",10.24963/ijcai.2018/643,2018.0,163.0,,Artificial intelligence; Encoding (symbols); Knowledge based systems; Natural language processing systems; Semantics; Attention mechanisms; Commonsense knowledge; Knowledge base; Knowledge graphs; Language understanding; NAtural language processing; Semantic information; State of the art; Graphic methods,2-s2.0-85055701005
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Tuan, Y.-L., Chen, Y.-N., Lee, H.-Y., DyKgChat: benchmarking dialogue generation grounding on dynamic knowledge graphs (2019) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 1855-1865",DyKgChat: benchmarking dialogue generation grounding on dynamic knowledge graphs,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Zhang, H., Liu, Z., Xiong, C., Liu, Z., Grounded conversation generation as guided traverses in commonsense knowledge graphs (2020) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 2031-2043",Grounded conversation generation as guided traverses in commonsense knowledge graphs,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Zhou, H., Zheng, C., Huang, K., Huang, M., Zhu, X., KdConv, A Chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation (2020) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics",A Chinese multi-domain dialogue dataset towards multi-turn knowledge-driven conversation,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Ghazvininejad, M., Brockett, C., Chang, M., Dolan, B., Gao, J., Yih, W., Galley, M., A knowledge-grounded neural conversation model (2018) Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence, pp. 5110-5117",A knowledge-grounded neural conversation model,"Ghazvininejad M., Brockett C., Chang M.-W., Dolan B., Gao J., Yih W.-T., Galley M.",A knowledge-grounded neural conversation model,"Neural network models are capable of generating extremely natural sounding conversational interactions. However, these models have been mostly applied to casual scenarios (e.g., as “chatbots”) and have yet to demonstrate they can serve in more useful conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses. We generalize the widely-used Sequence-to-Sequence (SEQ2SEQ) approach by conditioning responses on both conversation history and external “facts”, allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive SEQ2SEQ baseline. Human judges found that our outputs are significantly more informative. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2018.0,156.0,,Chatbots; Conversational interaction; Data driven; Neural network model; Artificial intelligence,2-s2.0-85051552811
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Lian, R., Xie, M., Wang, F., Peng, J., Wu, H., Learning to select knowledge for response generation in dialog systems (2019) Proceedings of the Twenty-Eighth International Joint Conference on Artificial Intelligence, IJCAI, pp. 5081-5087",Learning to select knowledge for response generation in dialog systems,"Lian R., Xie M., Wang F., Peng J., Wu H.",Learning to select knowledge for response generation in dialog systems,"End-to-end neural models for intelligent dialogue systems suffer from the problem of generating uninformative responses. Various methods were proposed to generate more informative responses by leveraging external knowledge. However, few previous work has focused on selecting appropriate knowledge in the learning process. The inappropriate selection of knowledge could prohibit the model from learning to make full use of the knowledge. Motivated by this, we propose an end-to-end neural model which employs a novel knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection. Specifically, a posterior distribution over knowledge is inferred from both utterances and responses, and it ensures the appropriate selection of knowledge during the training process. Meanwhile, a prior distribution, which is inferred from utterances only, is used to approximate the posterior distribution so that appropriate knowledge can be selected even without responses during the inference process. Compared with the previous work, our model can better incorporate appropriate knowledge in response generation. Experiments on both automatic and human evaluation verify the superiority of our model over previous baselines. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.",10.24963/ijcai.2019/706,2019.0,31.0,,,2-s2.0-85074950160
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Kim, B., Ahn, J., Kim, G., Sequential latent knowledge selection for knowledge-grounded dialogue (2020) 8th International Conference on Learning Representations, ICLR",Sequential latent knowledge selection for knowledge-grounded dialogue,"Kim Byeongchang, Ahn Jaewoo, Kim Gunhee",Sequential latent knowledge selection for knowledge-grounded dialogue,[No abstract available],,2020.0,13.0,,,2-s2.0-85094399274
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Du, N., Chen, K., Kannan, A., Tran, L., Chen, Y., Shafran, I., Extracting symptoms and their status from clinical conversations (2019) Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pp. 915-925",Extracting symptoms and their status from clinical conversations,"Du N., Chen K., Kannan A., Tran L., Chen Y., Shafran I.",Extracting symptoms and their status from clinical conversations,"This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models. © 2019 Association for Computational Linguistics",,2020.0,8.0,,Computational linguistics; Comprehensive evaluation; Inter-rater agreements; Learning approach; Medical providers; New applications; Realistic applications; Sequence modeling; Sliding Window; Deep learning,2-s2.0-85084044773
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Shi, X., Hu, H., Che, W., Sun, Z., Liu, T., Huang, J., Understanding medical conversations with scattered keyword attention and weak supervision from responses (2020), pp. 8838-8845. , The Thirty-Fourth AAAI Conference on Artificial Intelligence",Understanding medical conversations with scattered keyword attention and weak supervision from responses,"Shi X., Hu H., Che W., Sun Z., Liu T., Huang J.",Understanding medical conversations with scattered keyword attention and weak supervision from responses,"In this work, we consider the medical slot filling problem, i.e., the problem of converting medical queries into structured representations which is a challenging task. We analyze the effectiveness of two points: scattered keywords in user utterances and weak supervision with responses. We approach the medical slot filling as a multi-label classification problem with label-embedding attentive model to pay more attention to scattered medical keywords and learn the classification models by weak-supervision from responses. To evaluate the approaches, we annotate a medical slot filling data and collect a large scale unlabeled data. The experiments demonstrate that these two points are promising to improve the task. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2020.0,2.0,,Classification (of information); Filling; Classification models; Multi label classification; Two-point; Unlabeled data; Artificial intelligence,2-s2.0-85102974214
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Dhingra, B., Li, L., Li, X., Gao, J., Chen, Y.-N., Ahmed, F., Deng, L., Towards end-to-end reinforcement learning of dialogue agents for information access (2017) Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pp. 484-495",Towards end-to-end reinforcement learning of dialogue agents for information access,"Dhingra B., Li L., Li X., Gao J., Chen Y.-N., Ahmed F., Deng L.",Towards end-to-end reinforcement learning of dialogue agents for information access,"This paper proposes KB-InfoBot1 - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ""soft"" posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. © 2017 Association for Computational Linguistics.",10.18653/v1/P17-1045,2017.0,99.0,,Computational linguistics; Linguistics; Query processing; Differentiability; External database; Information access; ITS applications; Posterior distributions; Retrieval process; Search knowledge; Symbolic operations; Reinforcement learning,2-s2.0-85030087610
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Li, X., Chen, Y.-N., Li, L., Gao, J., Celikyilmaz, A., End-to-end task-completion neural dialogue systems (2017) Proceedings of the Eighth International Joint Conference on Natural Language Processing, pp. 733-743",End-to-end task-completion neural dialogue systems,"Li X., Chen Y.-N., Li L., Gao J., Celikyilmaz A.",End-to-end task-completion neural dialogue systems,[No abstract available],,2017.0,160.0,,,2-s2.0-85036635914
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Peng, B., Li, X., Gao, J., Liu, J., Wong, K.-F., Deep Dyna-Q: integrating planning for task-completion dialogue policy learning (2018) Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, pp. 2182-2192",Deep Dyna-Q: integrating planning for task-completion dialogue policy learning,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Serban, I.V., Sordoni, A., Bengio, Y., Courville, A.C., Pineau, J., Building end-to-end dialogue systems using generative hierarchical neural network models (2016) Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence, pp. 3776-3784",Building end-to-end dialogue systems using generative hierarchical neural network models,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Linmei, H., Yang, T., Shi, C., Ji, H., Li, X., Heterogeneous graph attention networks for semi-supervised short text classification (2019) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pp. 4821-4830",Heterogeneous graph attention networks for semi-supervised short text classification,"Hu L., Yang T., Shi C., Ji H., Li X.",Heterogeneous graph attention networks for semi-supervised short text classification,"Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on long texts and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the graph. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly. © 2019 Association for Computational Linguistics",,2020.0,46.0,,Backpropagation; Information dissemination; Information services; Labeled data; Natural language processing systems; Neural networks; Semantics; Attention mechanisms; Critical applications; Heterogeneous graph; Heterogeneous information; Information propagation; Labeled training data; Short text classifications; State-of-the-art methods; Text processing,2-s2.0-85084289136
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Yu, W., Zhou, J., Yu, W., Liang, X., Xiao, N., Heterogeneous graph learning for visual commonsense reasoning (2019), pp. 2765-2775. , Advances in Neural Information Processing Systems",Heterogeneous graph learning for visual commonsense reasoning,"Yu W., Zhou J., Yu W., Liang X., Xiao N.",Heterogeneous graph learning for visual commonsense reasoning,"Visual commonsense reasoning task aims at leading the research field into solving cognition-level reasoning with the ability of predicting correct answers and meanwhile providing convincing reasoning paths, resulting in three sub-tasks i.e., Q?A, QA?R and Q?AR. It poses great challenges over the proper semantic alignment between vision and linguistic domains and knowledge reasoning to generate persuasive reasoning paths. Existing works either resort to a powerful end-to-end network that cannot produce interpretable reasoning paths or solely explore intra-relationship of visual objects (homogeneous graph) while ignoring the cross-domain semantic alignment among visual concepts and linguistic words. In this paper, we propose a new Heterogeneous Graph Learning (HGL) framework for seamlessly integrating the intra-graph and inter-graph reasoning in order to bridge vision and language domain. Our HGL consists of a primal vision-to-answer heterogeneous graph (VAHG) module and a dual question-to-answer heterogeneous graph (QAHG) module to interactively refine reasoning paths for semantic agreement. Moreover, our HGL integrates a contextual voting module to exploit long-range visual context for better global reasoning. Experiments on the large-scale Visual Commonsense Reasoning benchmark demonstrate the superior performance of our proposed modules on three tasks (improving 5% accuracy on Q?A, 3.5% on QA?R, 5.8% on Q?AR)2,. © 2019 Neural information processing systems foundation. All rights reserved.",,2019.0,3.0,,Alignment; Benchmarking; Semantics; Commonsense reasoning; End-to-end network; Heterogeneous graph; Knowledge reasoning; Research fields; Semantic alignments; Visual concept; Visual context; Graph theory,2-s2.0-85090177668
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Odmaa, B., Yunfei, Y., Zhifang, S., Damai, D., Baobao, C., Sujian, L., Hongying, Z., Preliminary study on the construction of chinese medical knowledge graph (2019) J. Chin. Inf. Process., 33 (10), pp. 1-7",Preliminary study on the construction of chinese medical knowledge graph,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., Bengio, Y., Graph attention networks (2018) 6th International Conference on Learning Representations, ICLR",Graph attention networks,"Veličković P., Cucurull G., Casanova A., Romero A., Liò P., Bengio Y.",Graph attention networks,[No abstract available],,2018.0,2262.0,,,2-s2.0-85055102914
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Velickovic, P., Cucurull, G., Casanova, A., Romero, A., Liò, P., Bengio, Y., Graph attention networks (2018) 6th International Conference on Learning Representations, ICLR",Graph attention networks,"Veličković P., Casanova A., Liò P., Cucurull G., Romero A., Bengio Y.",Graph attention networks,"We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training). © Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved.",,2018.0,417.0,,Graphic methods; Network architecture; Proteins; Statistical tests; Citation networks; Graph neural networks; Graph structured data; Graph structures; Matrix operations; Novel neural network; Protein-protein interactions; State of the art; Multilayer neural networks,2-s2.0-85083953221
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," See, A., Liu, P.J., Manning, C.D., Get to the point: summarization with pointer-generator networks (2017) Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1073-1083",Get to the point: summarization with pointer-generator networks,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Bahdanau, D., Cho, K., Bengio, Y., Neural machine translation by jointly learning to align and translate (2015) 3rd International Conference on Learning Representations, ICLR",Neural machine translation by jointly learning to align and translate,"Bahdanau D., Cho K., Bengio Y.",Neural machine translation by jointly learning to align and translate,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. © 2015 International Conference on Learning Representations, ICLR. All rights reserved.",,2015.0,4477.0,,Computational linguistics; Decoding; Signal encoding; Decoder architecture; Hard segments; Machine translations; New approaches; Qualitative analysis; State of the art; Statistical machine translation; Target words; Computer aided language translation,2-s2.0-85083953689
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Chen, B., Cherry, C., A systematic comparison of smoothing techniques for sentence-level BLEU (2014), pp. 362-367. , Proceedings of the Ninth Workshop on Statistical Machine Translation",A systematic comparison of smoothing techniques for sentence-level BLEU,"Chen B., Cherry C.",A systematic comparison of smoothing techniques for sentence-level BLEU,[No abstract available],,2014.0,78.0,,,2-s2.0-85010213757
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Luo, R., Xu, J., Zhang, Y., Ren, X., Sun, X., Pkuseg: A toolkit for multi-domain chinese word segmentation, Arxiv abs/1906.11455",,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput., 9 (8), pp. 1735-1780",Long short-term memory,,,,,,,,,
2-s2.0-85102968935,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system," Gardner, M., Grus, J., Neumann, M., Tafjord, O., Dasigi, P., Liu, N.F., Peters, M., Zettlemoyer, L., AllenNLP A deep semantic natural language processing platform (2018) Proceedings of Workshop for NLP Open Source Software (NLP-OSS) Melbourne, Australia, pp. 1-6",AllenNLP A deep semantic natural language processing platform,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis,"Agarwal, R., Srikant, R., Fast algorithms for mining association rules (1994) Proc. Of the 20th Vldb Conference., pp. 487-499. , htps://doi.org/10.5555/645920.672836",Fast algorithms for mining association rules,"Agrawal R., Srikant R.",Fast algorithms for mining association rules,[No abstract available],,1994.0,12207.0,,,2-s2.0-0001882616
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Anderson, G., Verma, A., Dillig, I., Chaudhuri, S., Neurosymbolic reinforcement learning with formally verifed exploration (2020) NeurIPS",Neurosymbolic reinforcement learning with formally verifed exploration,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Andreas, J., Rohrbach, M., Darrell, T., Klein, D., Learning to compose neural networks for question answering (2016) Naacl, , htps://doi.org/10.18653/v1/N16-1181",Learning to compose neural networks for question answering,"Andreas J., Rohrbach M., Darrell T., Klein D.",Learning to compose neural networks for question answering,"We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains. ©2016 Association for Computational Linguistics.",10.18653/v1/n16-1181,2016.0,250.0,,Computational linguistics; Natural language processing systems; Assembly parameters; Benchmark datasets; Composable; Model use; Module networks; Natural languages; Question Answering; Structured knowledge; Reinforcement learning,2-s2.0-84993660571
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Andreas, J., Rohrbach, M., Darrell, T., Klein, D., Neural module networks (2016) Proceedings of the Ieee Conference on Computer Vision and Pattern Recognition., pp. 39-48. , htps://doi.org/10.1109/CVPR.2016.12",Neural module networks,"Andreas J., Rohrbach M., Darrell T., Klein D.",Neural module networks,"Visual question answering is fundamentally compositional in nature - a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural 'modules' into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes. © 2016 IEEE.",10.1109/CVPR.2016.12,2016.0,355.0,,Computer vision; Linguistics; Natural language processing systems; Pattern recognition; Complex questions; Linguistic structure; Modular network; Module networks; Natural images; Question Answering; Reusable components; State of the art; Complex networks,2-s2.0-84986272553
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Anton, T., Xpath-wrapper induction by generalizing tree traversal patterns (2005) Lernen, Wissensentdeckung und Adaptivitt (LWA) 2005, Gi Workshops, Saarbrcken., pp. 126-133",Xpath-wrapper induction by generalizing tree traversal patterns,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Baik, C., Jin, Z., Cafarella, M., Jagadish, H.V., Duoquest: A dual-specifcation system for expressive sql queries (2020) Proceedings of the 2020 Acm Sigmod International Conference on Management of Data (SIGMOD '20), pp. 2319-2329. , htps://doi.org/10.1145/3318464.3389776, Association for Computing Machinery, New York, NY, USA",Duoquest: A dual-specifcation system for expressive sql queries,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Baldini Soares, L., FitzGerald, N., Ling, J., Kwiatkowski, T., (2019) Matching the Blanks: Distributional Similarity for Relation Learning., , htps://doi.org/10.18653/v1/P19-1279",,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Barman, S., Chasins, S., Bodik, R., Gulwani, S., Ringer: Web automation by demonstration (2016) Proceedings of the 2016 Acm Sigplan International Conference on Object-Oriented Programming, Systems, Languages, and Applications., pp. 748-764. , htps://doi.org/10.1145/3022671.2984020",Ringer: Web automation by demonstration,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Bornholt, J., Torlak, E., Grossman, D., Ceze, L., (2016) Optimizing Synthesis with Metasketches (POPL), pp. 775-788. , htps://doi.org/10.1145/2914770.2837666",,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Chang, C.-H., Lui, S.-C., Iepad: Information extraction based on pattern discovery (2001) Proceedings of the 10th International Conference on World Wide Web., pp. 681-688. , htps://doi.org/10.1145/371920.372182",Iepad: Information extraction based on pattern discovery,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Chasins, S., Barman, S., Bodik, R., Gulwani, S., Browser record and replay as a building block for end-user web automation tools (2015) Proceedings of the 24th International Conference on World Wide Web., pp. 179-182. , htps://doi.org/10.1145/2740908.2742849",Browser record and replay as a building block for end-user web automation tools,"Chasins S., Barman S., Gulwani S., Bodik R.",Browser record and replay as a building block for end-user web automation tools,"To build a programming by demonstration (PBD) web scraping tool for end users, one needs two central components: a list finder, and a record and replay tool. A list finder extracts logical tables from a webpage. A record and replay (R+R) system records a user's interactions with a webpage, and replays them programmatically. The research community has invested substantial work in list finding - variously called wrapper induction, structured data extraction, and template detection. In contrast, researchers largely considered the browser R+R problem solved until recently, when webpage complexity and interactivity began to rise. We argue that the increase in interactivity necessitates the use of new, more robust R+R approaches, which will facilitate the PBD web tools of the future. Because robust R+R is difficult to build and understand, we argue that tool developers need an R+R layer that they can treat as a black box. We have designed an easy-to-use API that allows programmers to use and even customize R+R, without having to understand R+R internals. We have instantiated our API in Ringer, our robust R+R tool. We use the API to implement WebCombine, a PBD scraping tool. A WebCombine user demonstrates how to collect the first row of a relational dataset, and the tool collects all remaining rows. WebCombine uses the Ringer API to handle navigation between pages, enabling users to scrape from modern, interaction-heavy pages. We demonstrate WebCombine by collecting a 3,787,146 row dataset from Google Scholar that allows us to explore the relationship between researchers' years of experience and their papers' citation counts.",10.1145/2740908.2742849,2015.0,9.0,Automation; End-User Programming; Programming by Demonstration; Record and Replay,Automation; Budget control; Computer programming; Websites; World Wide Web; Building blockes; Central component; End user programming; Programming by demon-stration; Record-and-replay; Research communities; Template detection; Wrapper induction; Human computer interaction,2-s2.0-84968654887
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Chasins, S., Bodik, R., Skip blocks: Reusing execution history to accelerate web scripts (2017) Proceedings of the Acm on Programming Languages 1, Oopsla, pp. 1-28. , htps://doi.org/10.1145/3133875, 2017",Skip blocks: Reusing execution history to accelerate web scripts,"Chasins Sarah, Bodik Rastislav",Skip blocks: Reusing execution history to accelerate web scripts,[No abstract available],,2017.0,8.0,,,2-s2.0-85056824098
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Chasins, S.E., Mueller, M., Bodik, R., Rousillon: Scraping distributed hierarchical web data (2018) Proceedings of the 31st Annual Acm Symposium on User Interface Software and Technology., pp. 963-975",Rousillon: Scraping distributed hierarchical web data,"Chasins S.E., Mueller M., Bodik R.",Rousillon: Scraping distributed hierarchical web data,"Programming by Demonstration (PBD) promises to enable data scientists to collect web data. However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks. The missing piece is the capability to collect hierarchically-structured data from across many different webpages. We present Rousillon, a programming system for writing complex web automation scripts by demonstration. Users demonstrate how to collect the first row of a 'universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows. To offer this new demonstration model, we developed novel relation selection and generalization algorithms. In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.",10.1145/3242587.3242661,2018.0,30.0,,Budget control; Computer programming; Data acquisition; Demonstrations; Human computer interaction; Computer scientists; Demonstration models; Generalization algorithms; Programming by demonstration; Programming system; Real world web; Social scientists; Structured data; User interfaces,2-s2.0-85056827807
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Chen, Q., Wang, X., Ye, X., Durrett, G., Dillig, I., Multi-modal synthesis of regular expressions (2020) Proceedings of the 41st Acm Sigplan Conference on Programming Language Design and Implementation (PLDI 2020), pp. 487-502. , htps://doi.org/10.1145/3385412.3385988, Association for Computing Machinery, New York, NY, USA",Multi-modal synthesis of regular expressions,"Chen Q., Wang X., Ye X., Durrett G., Dillig I.",Multi-modal synthesis of regular expressions,"In this paper, we propose a multi-modal synthesis technique for automatically constructing regular expressions (regexes) from a combination of examples and natural language. Using multiple modalities is useful in this context because natural language alone is often highly ambiguous, whereas examples in isolation are often not sufficient for conveying user intent. Our proposed technique first parses the English description into a so-called hierarchical sketch that guides our programming-by-example (PBE) engine. Since the hierarchical sketch captures crucial hints, the PBE engine can leverage this information to both prioritize the search as well as make useful deductions for pruning the search space. We have implemented the proposed technique in a tool called Regel and evaluate it on over three hundred regexes. Our evaluation shows that Regel achieves 80 % accuracy whereas the NLP-only and PBE-only baselines achieve 43 % and 26 % respectively. We also compare our proposed PBE engine against an adaptation of AlphaRegex, a state-of-the-art regex synthesis tool, and show that our proposed PBE engine is an order of magnitude faster, even if we adapt the search algorithm of AlphaRegex to leverage the sketch. Finally, we conduct a user study involving 20 participants and show that users are twice as likely to successfully come up with the desired regex using Regel compared to without it. © 2020 ACM.",10.1145/3385412.3385988,2020.0,10.0,Program Synthesis; Programming by Example; Programming by Natural Languages; Regular Expression,Engines; Modal analysis; Pattern matching; Search engines; Multiple modalities; Natural languages; Programming by Example; Regular expressions; Search Algorithms; Search spaces; State of the art; Synthesis tool; Computer programming languages,2-s2.0-85086823111
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Chen, W., Wang, H., Chen, J., Zhang, Y., Wang, H., Li, S., Zhou, X., Yang Wang, W., Tabfact: A large-scale dataset for table-based fact verifcation (2020) International Conference on Learning Representations (ICLR), , Addis Ababa, Ethiopia",Tabfact: A large-scale dataset for table-based fact verifcation,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Chen, Y., Martins, R., Feng, Y., Maximal multi-layer specifcation synthesis (2019) Proceedings of the 2019 27th Acm Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering (ESEC/FSE 2019), pp. 602-612. , htps://doi.org/10.1145/3338906.3338951, Association for Computing Machinery, New York, NY, USA",Maximal multi-layer specifcation synthesis,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Choi, E., Kwiatkowski, T., Zettlemoyer, L., Scalable semantic parsing with partial ontologies (2015) Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1311-1320. , htps://doi.org/10.3115/v1/P15-1127, Association for Computational Linguistics, Beijing, China",Scalable semantic parsing with partial ontologies,"Choi E., Kwiatkowski T., Zettlemoyer L.",Scalable semantic parsing with partial ontologies,"We consider the problem of building scalable semantic parsers for Freebase, and present a new approach for learning to do partial analyses that ground as much of the input text as possible without requiring that all content words be mapped to Freebase concepts. We study this problem on two newly introduced large-scale noun phrase datasets, and present a new semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. © 2015 Association for Computational Linguistics.",10.3115/v1/p15-1127,2015.0,20.0,,Computational linguistics; Large dataset; Learning algorithms; Machine learning; Ontology; Semantics; Supervised learning; Entity attribute extractions; New approaches; Noun phrase; Referring expressions; Semantic parsing; Semi- supervised learning; Natural language processing systems,2-s2.0-84943740568
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Crescenzi, V., Mecca, G., Merialdo, P., Roadrunner: Towards automatic data extraction from large web sites (2001) Vldb, 1, pp. 109-118. , htps://doi.org/10.5555/645927.672370",Roadrunner: Towards automatic data extraction from large web sites,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., Bert: Pre-training of deep bidirectional transformers for language understanding (2019) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186. , htps://doi.org/10.18653/v1/N19-1423, Association for Computational Linguistics, Minneapolis, Minnesota",Bert: Pre-training of deep bidirectional transformers for language understanding,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Dietterich, T.G., Ensemble methods in machine learning (2000) International Workshop on Multiple Classifer Systems, pp. 1-15. , htps://doi.org/10.5555/648054.743935, Springer",Ensemble methods in machine learning,Dietterich T.G.,Ensemble methods in machine learning,[No abstract available],,2000.0,391.0,,,2-s2.0-0002900451
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Dua, D., Wang, Y., Dasigi, P., Stanovsky, G., Singh, S., Gardner, M., Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs (2019) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 2368-2378. , htps://doi.org/10.18653/v1/N19-1246, Association for Computational Linguistics, Minneapolis, Minnesota",Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs,"Dua D., Wang Y., Dasigi P., Stanovsky G., Singh S., Gardner M.",Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs,"Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.4%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F. © 2019 Association for Computational Linguistics",,2019.0,83.0,,Computational linguistics; Fracture mechanics; Numerical methods; Semantics; Human performance; Multiple inputs; Numerical reasoning; Reading comprehension; Semantic parsing; State-of-the-art methods; Systems matching; Drops,2-s2.0-85084074742
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Ellis, K., Ritchie, D., Solar-Lezama, A., Tenen-Baum, J., Learning to infer graphics programs from hand-drawn images (2018) Advances in Neural Information Processing Systems., pp. 6059-6068",Learning to infer graphics programs from hand-drawn images,"Ellis K., Solar-Lezama A., Ritchie D., Tenenbaum J.B.",Learning to infer graphics programs from hand-drawn images,"We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of LATEX. The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are a specification (spec) of what the graphics program needs to draw. We learn a model that uses program synthesis techniques to recover a graphics program from that spec. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network and extrapolate drawings. © 2018 Curran Associates Inc.All rights reserved.",,2018.0,51.0,,Deep learning; Iterative methods; Neural networks; Convolutional neural network; Correct error; Graphics programs; Hand-drawn; Program synthesis; Variable binding; Drawing (graphics),2-s2.0-85064845801
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Etzioni, O., Banko, M., Soderland, S., Weld, D.S., Open information extraction from the web (2008) Commun. Acm, 51 (12), pp. 68-74. , htps://doi.org/10.1145/1409360.1409378, Dec. 2008)",Open information extraction from the web,"Etzioni O., Banko M., Soderland S., Weld D.S.",Open information extraction from the web,"Open Information Extraction (IE), where the identities of the relations to be extracted are unknown and the billions of documents found on the web necessitate highly scalable processing, is a reliable way of extracting information from the Internet. The first IE systems relied on some form of pattern-matching rules that were manually crafted for each domain. Modern IE automatically learns an extractor from a training set in which domain-specific examples are tagged. The development of suitable training data for IE requires substantial effort and expertise. The Know-ItAll web IE system automates IE by learning to label its own training examples using only a small set of domain-independent extraction patterns. TextRunner is a fully implemented Open IE system that utilizes the two-phase architecture. It's first phase uses a general model of language, which trains a graphical model called a conditional random field (CRF). Open IE also supports aggregating, fusing information across a large number of web pages.",10.1145/1409360.1409378,2008.0,330.0,,Conditional random fields; Domain specifics; Extracting informations; Extraction patterns; General models; Graphical models; Information extractions; Pattern-matching; Training datum; Training examples; Training sets; Web pages; Graphic methods; Image segmentation; Information analysis; Websites,2-s2.0-59449097589
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Freitag, D., Machine learning for information extraction in informal domains (2000) Mach. Learn., 39 (2-3), pp. 169-202. , May 2000)",Machine learning for information extraction in informal domains,Freitag D.,Machine learning for information extraction in informal domains,"We consider the problem of learning to perform information extraction in domains where linguistic processing is problematic, such as Usenet posts, email, and finger plan files. In place of syntactic and semantic information, other sources of information can be used, such as term frequency, typography, formatting, and mark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rote learner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relational rule learner. Experiments on 14 information extraction problems defined over four diverse document collections demonstrate the effectiveness of these approaches. Finally, we describe a multistrategy approach which combines these learners and yields performance competitive with or better than the best of them. This technique is modular and flexible, and could find application in other machine learning problems.",10.1023/a:1007601113994,2000.0,160.0,,Computational linguistics; Electronic mail; Information retrieval; Learning algorithms; Text processing; Grammatical induction; Information extraction; Multistrategy learning; Naive bayes; Semantic information; Learning systems,2-s2.0-0033907729
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Gaunt, A.L., Brockschmidt, M., Kushman, N., Tarlow, D., Diferentiable programs with neural libraries (2017) International Conference on Machine Learning., pp. 1213-1222",Diferentiable programs with neural libraries,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Gavran, I., Darulova, E., Majumdar, R., Interactive synthesis of temporal specifcations from examples and natural language Proc. Acm Program. Lang. 4, Oopsla, 2020. , htps://doi.org/10.1145/3428269, Nov. 2020",,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Gulhane, P., Madaan, A., Mehta, R., Ra-Mamirtham, J., Rastogi, R., Satpal, S., Sengamedu, S.H., Tiwari, C., Web-scale information extraction with vertex (2011) 2011 Ieee 27th International Conference on Data Engineering, pp. 1209-1220. , htps://doi.org/10.1109/ICDE.2011.5767842",Web-scale information extraction with vertex,"Gulhane P., Madaan A., Mehta R., Ramamirtham J., Rastogi R., Satpal S., Sengamedu S.H., Tengli A., Tiwari C.",Web-scale information extraction with vertex,"Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages. To operate at Web scale, Vertex employs a host of novel algorithms for (1) Grouping similar structured pages in a Web site, (2) Picking the appropriate sample pages for wrapper inference, (3) Learning XPath-based extraction rules that are robust to variations in site structure, (4) Detecting site changes by monitoring sample pages, and (5) Optimizing editorial costs by reusing rules, etc. The system is deployed in production and currently extracts more than 250 million records from more than 200 Web sites. To the best of our knowledge, Vertex is the first system to do high-precision information extraction at Web scale. © 2011 IEEE.",10.1109/ICDE.2011.5767842,2011.0,63.0,,Extraction rule; First systems; High-precision; Information Extraction; Novel algorithm; Template-based; Web page; Wrapper induction; Algorithms; Inference engines; Information analysis; Websites; User interfaces,2-s2.0-79957798352
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Gulwani, S., Automating string processing in spreadsheets using input-output examples (2011) Proceedings of the 38th Annual Acm SIGPLAN-SIGACT Symposium on Principles of Programming Languages (POPL), pp. 317-330. , htps://doi.org/10.1145/1926385.1926423",Automating string processing in spreadsheets using input-output examples,Gulwani S.,Automating string processing in spreadsheets using input-output examples,"We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with.We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations. The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination. Copyright © 2011 ACM.",10.1145/1926385.1926423,2010.0,83.0,Program synthesis; Programming by example (PBE); Spreadsheet programming; String manipulation; User intent; Version space algebra,Program synthesis; Programming by Example; String manipulation; User intent; Version space algebra; Algebra; Algorithms; Object oriented programming; Spreadsheets; Convergence of numerical methods,2-s2.0-79952012950
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Gupta, N., Lin, K., Roth, D., Singh, S., Gardner, M., Neural module networks for reasoning over text (2020) Iclr",Neural module networks for reasoning over text,"Gupta Nitish, Lin Kevin, Roth Dan, Singh Sameer, Gardner Matt",Neural module networks for reasoning over text,[No abstract available],,2020.0,13.0,,,2-s2.0-85094990858
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Han, X., Zhu, H., Yu, P., Wang, Z., Yao, Y., Liu, Z., Sun, M., Fewrel: A large-scale supervised few-shot relation classifcation dataset with state-of-the-Art evaluation (2018) Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 4803-4809. , htps://doi.org/10.18653/v1/D18-1514, Association for Computational Linguistics, Brussels, Belgium",Fewrel: A large-scale supervised few-shot relation classifcation dataset with state-of-the-Art evaluation,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Honnibal, M., Montani, I., Van Landeghem, S., Boyd, A., (2020) SpaCy: Industrial-strength Natural Language Processing in Python., , htps://doi.org/10.5281/zenodo.1212303",,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Hsu, C.-N., Dung, M.-T., Generating fnite-state transducers for semi-structured data extraction from the web (1998) Information Systems, 23 (8), pp. 521-538. , htps://doi.org/10.5555/306766.306775, 1998",Generating fnite-state transducers for semi-structured data extraction from the web,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Hu, Q., D'Antoni, L., Syntax-guided synthesis with quantitative syntactic objectives (2018) International Conference on Computer Aided Verifcation, pp. 386-403. , htps://doi.org/10.1007/978-3-319-96145-3-21, Springer",Syntax-guided synthesis with quantitative syntactic objectives,"Hu Q., D’Antoni L.",Syntax-guided synthesis with quantitative syntactic objectives,"Automatic program synthesis promises to increase the productivity of programmers and end-users of computing devices by automating tedious and error-prone tasks. Despite the practical successes of program synthesis, we still do not have systematic frameworks to synthesize programs that are “good” according to certain metrics—e.g., produce programs of reasonable sizes or with good runtime—and to understand when synthesis can result in such good programs. In this paper, we propose QSyGuS, a unifying framework for describing syntax-guided synthesis problems with quantitative objectives over the syntax of the synthesized programs. QSyGuS builds on weighted (tree) grammars, a clean and foundational formalism that provides flexible support for different quantitative objectives, useful closure properties, and practical decision procedures. We then present an algorithm for solving QSyGuS. Our algorithm leverages closure properties of weighted grammars to generate intermediate problems that can be solved using non-quantitative SyGuS solvers. Finally, we implement our algorithm in a tool, QuaSi, and evaluate it on 26 quantitative extensions of existing SyGuS benchmarks. QuaSi can synthesize optimal solutions in 15/26 benchmarks with times comparable to those needed to find an arbitrary solution. © The Author(s) 2018.",10.1007/978-3-319-96145-3_21,2018.0,6.0,,Computation theory; Computer aided analysis; Syntactics; Algorithm for solving; Automatic programs; Computing devices; Decision procedure; Error prone tasks; Quantitative objectives; Synthesis problems; Systematic framework; Computer circuits,2-s2.0-85051131373
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Inala, J.P., Yang, Y., Paulos, J., Pu, Y., Bastani, O., Kumar, V., Rinard, M., Solar-Lezama, A., Neurosymbolic transformers for multi-Agent communication (2020) NeurIPS",Neurosymbolic transformers for multi-Agent communication,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Iyer, A., Jonnalagedda, M., Parthasarathy, S., Rad-Hakrishna, A., Rajamani, S.K, Synthesis and machine learning for heterogeneous extraction (2019) Proceedings of the 40th Acm Sigplan Conference on Programming Language Design and Implementation., pp. 301-315. , htps://doi.org/10.1145/3314221.3322485",Synthesis and machine learning for heterogeneous extraction,"Iyer A., Jonnalagedda M., Parthasarathy S., Radhakrishna A., Rajamani S.K.",Synthesis and machine learning for heterogeneous extraction,"We present a way to combine techniques from the program synthesis and machine learning communities to extract structured information from heterogeneous data. Such problems arise in several situations such as extracting attributes from web pages, machine-generated emails, or from data obtained from multiple sources. Our goal is to extract a set of structured attributes from such data. We use machine learning models (“ML models”) such as conditional random fields to get an initial labeling of potential attribute values. However, such models are typically not interpretable, and the noise produced by such models is hard to manage or debug. We use (noisy) labels produced by such ML models as inputs to program synthesis, and generate interpretable programs that cover the input space. We also employ type specifications (called “field constraints”) to certify well-formedness of extracted values. Using synthesized programs and field constraints, we re-train the ML models with improved confidence on the labels. We then use these improved labels to re-synthesize a better set of programs. We iterate the process of re-synthesizing the programs and re-training the ML models, and find that such an iterative process improves the quality of the extraction process. This iterative approach, called HDEF, is novel, not only the in way it combines the ML models with program synthesis, but also in the way it adapts program synthesis to deal with noise and heterogeneity. More broadly, our approach points to ways by which machine learning and programming language techniques can be combined to get the best of both worlds - handling noise, transferring signals from one context to another using ML, producing interpretable programs using PL, and minimizing user intervention. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.",10.1145/3314221.3322485,2019.0,5.0,Data extraction; Heterogeneous data; Machine Learning; Program synthesis,Ada (programming language); Extraction; Iterative methods; Learning systems; Machine learning; Websites; Conditional random field; Data extraction; Heterogeneous data; Iterative approach; Machine learning communities; Machine learning models; Program synthesis; Structured information; Data mining,2-s2.0-85067684348
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Kushmerick, N., Weld, D.S., Doorenbos, R., Wrapper induction for information extraction (1997) University of Washington Washington",Wrapper induction for information extraction,Kushmerick N.,Wrapper induction for information extraction,[No abstract available],,1997.0,172.0,,,2-s2.0-0003614560
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Le, V., Gulwani, S., (2014) FlashExtract: A Framework for Data Extraction by Examples (PLDI), pp. 542-553. , htps://doi.org/10.1145/2666356.2594333",,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Yuchen Lin, B., Sheng, Y., Vo, N., Tata, S., Freedom: A transferable neural architecture for structured information extraction on web documents (2020) Proceedings of the 26th Acm Sigkdd International Conference on Knowledge Discovery Data Mining, , htps://doi.org/10.1145/3394486.3403153, Aug 2020",Freedom: A transferable neural architecture for structured information extraction on web documents,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Lockard, C., Shiralkar, P., Luna Dong, X., Hajishirzi, H., Zeroshotceres: Zero-shot relation extraction from semi-structured webpages (2020) Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pp. 8105-8117. , htps://doi.org/10.18653/v1/2020.acl-main.721, Association for Computational Linguistics, Online",Zeroshotceres: Zero-shot relation extraction from semi-structured webpages,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Mao, J., Gan, C., Kohli, P., Tenenbaum, J.B., Wu, J., The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision (2019) Iclr",and sentences from natural supervision,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Mintz, M., Bills, S., Snow, R., Jurafsky, D., Distant supervision for relation extraction without labeled data (2009) Proceedings of the Joint Conference of the 47th Annual Meeting of the Acl and the 4th International Joint Conference on Natural Language Processing of the Afnlp, pp. 1003-1011. , htps://www.aclweb.org/anthology/P09-1113, Association for Computational Linguistics, Suntec, Singapore",Distant supervision for relation extraction without labeled data,"Mintz M., Bills S., Snow R., Jurafsky D.",Distant supervision for relation extraction without labeled data,[No abstract available],,2009.0,1484.0,,,2-s2.0-77957863090
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Muslea, I., Minton, S., Knoblock, C., A hierarchical approach to wrapper induction (1999) Proceedings of the Third Annual Conference on Autonomous Agents., pp. 190-197. , htps://doi.org/10.1145/301136.301191",A hierarchical approach to wrapper induction,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Opitz, D., MacLin, R., Popular ensemble methods: An empirical study (1999) Journal of Artifcial Intelligence Research, 11, pp. 169-198. , htps://doi.org/10.1613/jair.614, 1999",Popular ensemble methods: An empirical study,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Pasupat, P., Liang, P., Zero-shot entity extraction from web pages (2014) Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 391-401. , htps://doi.org/10.3115/v1/P14-1037, Association for Computational Linguistics, Baltimore, Maryland",Zero-shot entity extraction from web pages,"Pasupat P., Liang P.",Zero-shot entity extraction from web pages,"In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction systems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach defines a log-linear model over latent extraction predicates, which select lists of entities from the web page. The main challenge is to define features on widely varying candidate entity lists. We tackle this by abstracting list elements and using aggregate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline. © 2014 Association for Computational Linguistics.",10.3115/v1/p14-1037,2014.0,28.0,,Computational linguistics; Regression analysis; Entity extractions; Fine grained; Information extraction systems; Learning tasks; Loglinear model; Natural language queries; Semi structured data; Websites,2-s2.0-84906921910
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Pasupat, P., Liang, P., Compositional semantic parsing on semi-structured tables (2015) Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1470-1480. , htps://doi.org/10.3115/v1/P15-1142, Association for Computational Linguistics, Beijing, China",Compositional semantic parsing on semi-structured tables,"Pasupat P., Liang P.",Compositional semantic parsing on semi-structured tables,"Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: Answering complex questions on semi-structured tables using question-Answer pairs as supervision. The central challenge arises from two compounding factors: The broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. © 2015 Association for Computational Linguistics.",10.3115/v1/p15-1142,2015.0,162.0,,Computational linguistics; Semantics; Combinatorial explosion; Complex questions; Compositional semantics; Knowledge sources; Parsing algorithm; Question Answering; Question-answer pairs; Semantic parsing; Natural language processing systems,2-s2.0-84943788139
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Patwardhan, S., Rilof, E., Efective information extraction with semantic afnity patterns and relevant regions (2007) Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 717-727. , htps://www.aclweb.org/anthology/D07-1075, Association for Computational Linguistics, Prague, Czech Republic",Efective information extraction with semantic afnity patterns and relevant regions,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Pennington, J., Socher, R., Manning, C., Glove: Global vectors for word representation (2014) Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 1532-1543. , htps://doi.org/10.3115/v1/D14-1162, Association for Computational Linguistics, Doha, Qatar",Glove: Global vectors for word representation,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Qian, Y., Santus, E., Jin, Z., Guo, J., Barzilay, R., Graphie: A graph-based framework for information extraction (2019) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies,Volume 1 (Long and Short Papers), pp. 751-761. , htps://doi.org/10.18653/v1/N19-1082, Association for Computational Linguistics, Minneapolis, Minnesota",Graphie: A graph-based framework for information extraction,"Qian Y., Santus E., Jin Z., Guo J., Barzilay R.",Graphie: A graph-based framework for information extraction,"Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks - namely textual, social media and visual information extraction - shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin. © 2019 Association for Computational Linguistics",,2019.0,19.0,,Computational linguistics; Graphic methods; Information retrieval; Graph-based; Information extraction systems; Nonlocal; Social media; State of the art; Tagging models; Word level; Graph algorithms,2-s2.0-85084071725
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Rajpurkar, P., Zhang, J., Lopyrev, K., Liang, P., Squad: 100,000+ questions for machine comprehension of text (2016) Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pp. 2383-2392. , htps://doi.org/10.18653/v1/D16-1264, Association for Computational Linguistics, Austin, Texas",000+ questions for machine comprehension of text,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Raza, M., Gulwani, S., Web data extraction using hybrid program synthesis: A combination of top-down and bottom-up inference (2020) Proceedings of the 2020 Acm Sigmod International Conference on Management of Data., pp. 1967-1978. , htps://doi.org/10.1145/3318464.3380608",Web data extraction using hybrid program synthesis: A combination of top-down and bottom-up inference,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Raza, M., Gulwani, S., Milic-Frayling, N., Compositional program synthesis from natural language and examples (2015) Proceedings of the 24th International Conference on Artifcial Intelligence (IJCAI'15), pp. 792-800. , htps://doi.org/10.5555/2832249.2832359, AAAI Press",Compositional program synthesis from natural language and examples,"Raza M., Gulwani S., Milic-Frayling N.",Compositional program synthesis from natural language and examples,"Compositionality is a fundamental notion in computation whereby complex abstractions can be constructed from simpler ones, yet this property has so far escaped the paradigm of end-user programming from examples or natural language. Existing approaches restrict end users to only give holistic specifications of tasks, which limits the expressivity and scalability of these approaches to relatively simple programs in very restricted domains. In this paper we propose Compositional Program Synthesis (CPS): an approach in which tasks can be specified in a compositional manner through a combination of natural language and examples. We present a domain-agnostic program synthesis algorithm and demonstrate its application to an expressive string manipulation language. We evaluate our approach on complex tasks from online help forums that are beyond the scope of current state-of-the-art methods.",,2015.0,38.0,,Artificial intelligence; Computational linguistics; Computer programming; Synthesis (chemical); Compositionality; Domain agnostics; End user programming; ITS applications; Natural languages; Program synthesis; Restricted-domain; State-of-the-art methods; Application programs,2-s2.0-84949750713
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Reimers, N., Gurevych, I., Sentence-bert: Sentence embeddings using siamese bert-networks (2019) Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing, , htps://arxiv.org/abs/1908.10084, Association for Computational Linguistics",Sentence-bert: Sentence embeddings using siamese bert-networks,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Shah, A., Zhan, E., Sun, J.J., Verma, A., Yue, Y., Chaudhuri, S., Learning diferentiable programs with admissible neural heuristics (2020) NeurIPS",Learning diferentiable programs with admissible neural heuristics,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Valkov, L., Chaudhari, D., Srivastava, A., Sutton, C., Chaudhuri, S., Houdini: Lifelong learning as program synthesis (2018) Advances in Neural Information Processing Systems., pp. 8687-8698",Houdini: Lifelong learning as program synthesis,"Valkov L., Chaudhari D., Sutton C., Srivastava A., Chaudhuri S.",Houdini: Lifelong learning as program synthesis,"We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks significantly accelerates the search. © 2018 Curran Associates Inc.All rights reserved.",,2018.0,16.0,,Computer software reusability; Stochastic systems; Combinatorial search; Complex procedure; Directed searches; Functional programs; Library functions; Life long learning; Shortest path computations; Stochastic gradient descent; Learning algorithms,2-s2.0-85064803205
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Wang, X., Dillig, I., Singh, R., Program synthesis using abstraction refnement (2017) Proceedings of the Acm on Programming Languages 2, Popl, pp. 1-30. , 2017",Program synthesis using abstraction refnement,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Wang, X., Dillig, I., Singh, R., Synthesis of data completion scripts using fnite tree automata (2017) Proceedings of the Acm on Programming Languages 1, Oopsla, pp. 1-26. , htps://doi.org/10.1145/3133886, 2017",Synthesis of data completion scripts using fnite tree automata,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Wang, X., Gulwani, S., Singh, R., Fidex: Filtering spreadsheet data using examples (2016) Proceedings of the 2016 Acm Sigplan International Conference on Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA), pp. 195-213",Fidex: Filtering spreadsheet data using examples,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Wu, S., Hsiao, L., Cheng, X., Hancock, B., Rekatsi-Nas, T., Levis, P., Ré, C., Fonduer: Knowledge base construction from richly formatted data (2018) Proceedings of the 2018 International Conference on Management of Data (SIGMOD '18), pp. 1301-1316. , Association for Computing Machinery, New York, NY, USA",Fonduer: Knowledge base construction from richly formatted data,"Wu S., Hsiao L., Cheng X., Hancock B., Rekatsinas T., Levis P., Ré C.",Fonduer: Knowledge base construction from richly formatted data,"We focus on knowledge base construction (KBC) from richly formatted data. In contrast to KBC from text or tabular data, KBC from richly formatted data aims to extract relations conveyed jointly via textual, structural, tabular, and visual expressions. We introduce Fonduer, a machine-learning-based KBC system for richly formatted data. Fonduer presents a newdata model that accounts for three challenging characteristics of richly formatted data: (1) prevalent document-level relations, (2) multimodality, and (3) data variety. Fonduer uses a new deep-learning model to automatically capture the representation (i.e., features) needed to learn how to extract relations from richly formatted data. Finally, Fonduer provides a new programming model that enables users to convert domain expertise, based on multiple modalities of information, to meaningful signals of supervision for training a KBC system. Fonduer-based KBC systems are in production for a range of use cases, including at a major online retailer. We compare Fonduer against state-ofthe-art KBC approaches in four different domains. We show that Fonduer achieves an average improvement of 41 F1 points on the quality of the output knowledge base-and in some cases produces up to 1.87× the number of correct entries-compared to expertcurated public knowledge bases. We also conduct a user study to assess the usability of Fonduer's new programming model. We show that after using Fonduer for only 30 minutes, non-domain experts are able to design KBC systems that achieve on average 23 F1 points higher quality than traditional machine-learning-based KBC approaches. © 2018 Association for Computing Machinery.",10.1145/3183713.3183729,2018.0,41.0,,Artificial intelligence; Expert systems; Online systems; Different domains; Domain expertise; Knowledge-base construction; Learning models; Multiple modalities; Online retailers; Programming models; Public knowledge; Deep learning,2-s2.0-85048825604
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Young, H., Bastani, O., Naik, M., (2019) Learning Neu-rosymbolic Generative Models Via Program Synthesis",,,,,,,,,,
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Zanda, M., Brown, G., A study of semi-supervised generative ensembles (2009) Multiple Classifer Systems, pp. 242-251. , htps://doi.org/10.1007/978-3-642-02326-2-25, Jón Atli Benedik-tsson, Josef Kittler, and Fabio Roli (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg",A study of semi-supervised generative ensembles,"Zanda M., Brown G.",A study of semi-supervised generative ensembles,"Machine Learning can be divided into two schools of thought: generative model learning and discriminative model learning. While the MCS community has been focused mainly on the latter, our paper is concerned with questions that arise from ensembles of generative models. Generative models provide us with neat ways of thinking about two interesting learning issues: model selection and semi-supervised learning. Preliminary results show that for semi-supervised low-variance generative models, traditional MCS techniques like Bagging and Random Subspace Method (RSM) do not outperform the single classifier approach. However, RSM introduces diversity between base classifiers. This starting point suggests that diversity between base components has to lie within the structure of the base classifier, and not in the dataset, and it highlights the need for novel generative ensemble learning techniques. © 2009 Springer Berlin Heidelberg.",10.1007/978-3-642-02326-2_25,2009.0,1.0,,Base classifiers; Base components; Data sets; Discriminative models; Ensemble learning; Generative model; Machine-learning; Model Selection; Random subspace method; Semi-supervised; Semi-supervised learning; Ways of thinking; Classifiers; Image retrieval; Knowledge engineering; Learning algorithms; Supervised learning; Education,2-s2.0-70349338926
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis," Zhu, X., (2005) Semi-Supervised Learning Literature Survey, , Technical Report 1530. Computer Sciences, University of Wisconsin-Madison",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification,"Agarwal, B., Mittal, N., Text classification using machine learning methods - A survey (2014) Roceedings of the Second International Conference on Soft Computing for Problem Solving (Socpros 2012), 2012, pp. 701-709. , December 28-30, pringer",Text classification using machine learning methods - A survey,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Belinkov, Y., Glass, J., Analysis methods in neural language processing: A survey (2019) Transactions of the Association for Computational Linguistics, 7, pp. 49-72",Analysis methods in neural language processing: A survey,"Belinkov Y., Glass J.",Analysis methods in neural language processing: A survey,[No abstract available],,2019.0,92.0,,,2-s2.0-85070515558
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Beyer, H.G., Schwefel, H.P., Wegener, I., How to analyse evolutionary algorithms (2002) Theoretical Computer Science, 287 (1), pp. 101-130",How to analyse evolutionary algorithms,"Beyer H.-G., Schwefel H.-P., Wegener I.",How to analyse evolutionary algorithms,"Many variants of evolutionary algorithms have been designed and applied. The experimental knowledge is immense. The rigorous analysis of evolutionary algorithms is difficult, but such a theory can help to understand, design, and teach evolutionary algorithms. In this survey, first the history of attempts to analyse evolutionary algorithms is described and then new methods for continuous as well as discrete search spaces are presented and discussed. © 2002 Elsevier Science B.V. All rights reserved.",10.1016/S0304-3975(02)00137-8,2002.0,91.0,,Computer science; Optimization; Probability; Problem solving; Probabilistic optimization methods; Genetic algorithms,2-s2.0-0037174202
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Bird, S., Klein, E., Loper, E., (2009) Natural language processing with Python: Analyzing text with the natural language toolkit, , O’Reilly Media Inc, California",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Bougouin, A., Boudin, F., Daille, B., TopicRank: Graph-based topic ranking for keyphrase extraction (2013) Proceedings of the Sixth International Joint Conference on Natural Language Processing, pp. 543-551. , Asian Federation of Natural Language Processing, Nagoya, Japan",TopicRank: Graph-based topic ranking for keyphrase extraction,"Bougouin A., Boudin F., Daille B.",TopicRank: Graph-based topic ranking for keyphrase extraction,[No abstract available],,2013.0,162.0,,,2-s2.0-84905122756
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Campos, R., Mangaravite, V., Pasquali, A., Jorge, A.M., Nunes, C., Jatowt, A., A text feature based automatic keyword extraction method for single documents (2018) Advances in Information Retrieval, pp. 684-691. , Pasi G, Piwowarski B, Azzopardi L, Hanbury A, (eds), Springer, Germany",A text feature based automatic keyword extraction method for single documents,"Campos R., Mangaravite V., Pasquali A., Jorge A.M., Nunes C., Jatowt A.",A text feature based automatic keyword extraction method for single documents,"In this work, we propose a lightweight approach for keyword extraction and ranking based on an unsupervised methodology to select the most important keywords of a single document. To understand the merits of our proposal, we compare it against RAKE, TextRank and SingleRank methods (three well-known unsupervised approaches) and the baseline TF.IDF, over four different collections to illustrate the generality of our approach. The experimental results suggest that extracting keywords from documents using our method results in a superior effectiveness when compared to similar approaches. © Springer International Publishing AG, part of Springer Nature 2018.",10.1007/978-3-319-76941-7_63,2018.0,35.0,Feature extraction; Information extraction; Keyword extraction,Artificial intelligence; Computer science; Computers; Feature extraction; Keyword extraction; Text feature; Unsupervised approaches; Information retrieval,2-s2.0-85044470670
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Chambers, L.D., (2000) The Practical Handbook of Genetic Algorithms: Applications, , CRC Press, Florida",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Chang, C.C., Lin, C.J., LIBSVM: A library for support vector machines (2011) ACM Transactions on Intelligent Systems and Technology, 2 (3), pp. 1-27",LIBSVM: A library for support vector machines,"Chang C.-C., Lin C.-J.",LIBSVM: A library for support vector machines,[No abstract available],,2001.0,10537.0,,,2-s2.0-0003710380
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Davis, L., (1991) Handbook of Genetic Algorithms, , (ed), Chapman & Hall, London",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," de Rainville, F.M., Fortin, F.A., Gardner, M.A., Parizeau, M., Gagné, C., Deap: A python framework for evolutionary algorithms (2012) Proceedings of the 14Th Annual Conference Companion on Genetic and Evolutionary Computation, pp. 85-92",Deap: A python framework for evolutionary algorithms,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Deb, K., Jain, H., An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, part I: Solving problems with box constraints (2013) IEEE transactions on evolutionary computation, 18 (4), pp. 577-601",part I: Solving problems with box constraints,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Demšar, J., Statistical comparisons of classifiers over multiple data sets (2006) Journal of Machine Learning Research., 7, pp. 1-30",Statistical comparisons of classifiers over multiple data sets,Demšar J.,Statistical comparisons of classifiers over multiple data sets,"While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.",,2006.0,7434.0,Comparative studies; Friedman test; Multiple comparisons tests; Statistical methods; Wilcoxon signed ranks test,Algorithms; Computation theory; Data processing; Learning algorithms; Learning systems; Set theory; Multiple data sets; Parametric tests; Statistical comparisons; Wilcoxon signed ranks test; Statistical methods,2-s2.0-29644438050
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Denysiuk, R., Gaspar-Cunha, A., Delbem, A.C., Neuroevolution for solving multiobjective knapsack problems (2019) Expert Systems with Applications, 116, pp. 65-77",Neuroevolution for solving multiobjective knapsack problems,"Denysiuk R., Gaspar-Cunha A., Delbem A.C.B.",Neuroevolution for solving multiobjective knapsack problems,"The multiobjective knapsack problem (MOKP) is an important combinatorial problem that arises in various applications, including resource allocation, computer science and finance. When tackling this problem by evolutionary multiobjective optimization algorithms (EMOAs), it has been demonstrated that traditional recombination operators acting on binary solution representations are susceptible to a loss of diversity and poor scalability. To address those issues, we propose to use artificial neural networks for generating solutions by performing a binary classification of items using the information about their profits and weights. As gradient-based learning cannot be used when target values are unknown, neuroevolution is adapted to adjust the neural network parameters. The main contribution of this study resides in developing a solution encoding and genotype-phenotype mapping for EMOAs to solve MOKPs. The proposal is implemented within a state-of-the-art EMOA and benchmarked against traditional variation operators based on binary crossovers. The obtained experimental results indicate a superior performance of the proposed approach. Furthermore, it is advantageous in terms of scalability and can be readily incorporated into different EMOAs. © 2018 Elsevier Ltd",10.1016/j.eswa.2018.09.004,2019.0,9.0,Evolutionary computation; Multiobjective knapsack problem; Neuroevolution,Classification (of information); Combinatorial optimization; Evolutionary algorithms; Multiobjective optimization; Neural networks; Scalability; Combinatorial problem; Evolutionary multiobjective optimization algorithm (EMOAs); Genotype-phenotype mapping; Gradient-based learning; Knapsack problems; Neural network parameters; Neuro evolutions; Recombination operators; Problem solving,2-s2.0-85053206729
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Devlin, J., Chang, M.W., Lee, K., Toutanova, K., BERT: Pre-training of deep bidirectional transformers for language understanding (2019) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186. , Minneapolis, Minnesota, Association for Computational Linguistics",BERT: Pre-training of deep bidirectional transformers for language understanding,"Devlin J., Chang M.-W., Lee K., Toutanova K.",BERT: Pre-training of deep bidirectional transformers for language understanding,[No abstract available],,2018.0,6222.0,,,2-s2.0-85057019815
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Devlin, J., Chang, M.W., Lee, K., Toutanova, K., BERT: Pre-training of deep bidirectional transformers for language understanding (2019) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186. , Minneapolis, Minnesota, Association for Computational Linguistics",BERT: Pre-training of deep bidirectional transformers for language understanding,"Devlin J., Chang M.-W., Lee K., Toutanova K.",BERT: Pre-training of deep bidirectional transformers for language understanding,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). © 2019 Association for Computational Linguistics",,2019.0,6017.0,,Computational linguistics; Language inference; Language understanding; NAtural language processing; Output layer; Pre-training; Question Answering; Representation model; State of the art; Natural language processing systems,2-s2.0-85083815650
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Devlin, J., Chang, M.W., Lee, K., Toutanova, K., BERT: Pre-training of deep bidirectional transformers for language understanding (2019) Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 4171-4186. , Minneapolis, Minnesota, Association for Computational Linguistics",BERT: Pre-training of deep bidirectional transformers for language understanding,"Devlin J., Chang M.-W., Lee K., Toutanova K.",BERT: Pre-training of deep bidirectional transformers for language understanding,[No abstract available],,2018.0,660.0,,,2-s2.0-85063394559
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Dorronsoro, B., Pinel, F., Combining machine learning and genetic algorithms to solve the independent tasks scheduling problem (2017) 2017 3Rd IEEE International Conference on Cybernetics (CYBCONF), pp. 1-8. , IEEE",Combining machine learning and genetic algorithms to solve the independent tasks scheduling problem,"Dorronsoro B., Pinel F.",Combining machine learning and genetic algorithms to solve the independent tasks scheduling problem,"We propose a new accurate and fast memetic parallel optimization algorithm for the independent tasks scheduling problem. The new technique combines the Virtual Savant (VS) with a parallel genetic algorithm (called PA-CGA). VS is an optimization framework based on machine learning that learns from a reference set of (pseudo-)optimal solutions how to solve the problem, providing accurate results in extremely low run times. We propose in this work the use of VS to generate a highly accurate initial population for the PA-CGA. Results show how initializing the population with VS (we test two versions of VS, differing on its training process) significantly increases the accuracy of the PA-CGA, compared to two other population initialization techniques: Random and using a state-of-the-art heuristic. © 2017 IEEE.",10.1109/CYBConf.2017.7985766,2017.0,10.0,,Artificial intelligence; Cybernetics; Genetic algorithms; Learning systems; Optimization; Scheduling; Independent tasks scheduling; Initial population; Optimal solutions; Optimization framework; Parallel genetic algorithms; Parallel optimization; Population initializations; State of the art; Problem solving,2-s2.0-85027860081
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," (2017) D., , http://archive.ics.uci.edu/ml, Graff, C, UCI Machine Learning Repository",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Eiben, A.E., Aarts, E.H., van Hee, K.M., Global convergence of genetic algorithms: A Markov chain analysis (1990) Proceedings of the International Conference on Parallel Problem Solving from Nature, pp. 3-12. , Springer",Global convergence of genetic algorithms: A Markov chain analysis,"Eiben A.E., Aarts E.H.L., Van Hee K.M.",Global convergence of genetic algorithms: A Markov chain analysis,[No abstract available],,1991.0,61.0,,,2-s2.0-0002650735
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," El-Beltagy, S.R., Rafea, A., KP-Miner: A keyphrase extraction system for English and Arabic documents (2009) Information Systems, 34 (1), pp. 132-144",KP-Miner: A keyphrase extraction system for English and Arabic documents,"El-Beltagy S.R., Rafea A.",KP-Miner: A keyphrase extraction system for English and Arabic documents,"Automatic keyphrase extraction has many important applications including but not limited to summarization, cataloging/indexing, feature extraction for clustering and classification, and data mining. This paper presents the KP-Miner system, and demonstrates through experimentation and comparison with widely used systems that it is effective and efficient in extracting keyphrases from both English and Arabic documents of varied length. Unlike other existing keyphrase extraction systems, the KP-Miner system does not need to be trained on a particular document set in order to achieve its task. It also has the advantage of being configurable as the rules and heuristics adopted by the system are related to the general nature of documents and keyphrases. This implies that the users of this system can use their understanding of the document(s) being input into the system to fine-tune it to their particular needs. © 2008 Elsevier B.V. All rights reserved.",10.1016/j.is.2008.05.002,2009.0,127.0,Automatic indexing; Heuristic rules; Keyphrase extraction,Automatic indexing; Decision support systems; Heuristic methods; Information management; Miners; Configurable; Document sets; General natures; Heuristic rules; Keyphrase extraction; Keyphrase extractions; Feature extraction,2-s2.0-55549106775
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," English, T.M., Evaluation of evolutionary and genetic optimizers: No free lunch (1996) Evolutionary Programming, pp. 163-169",Evaluation of evolutionary and genetic optimizers: No free lunch,English T.M.,Evaluation of evolutionary and genetic optimizers: No free lunch,[No abstract available],,1996.0,51.0,,,2-s2.0-4344680742
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Fellbaum, C., WordNet (2012) The Encyclopedia of Applied Linguistics",WordNet,Fellbaum C.,WordNet,[No abstract available],,1998.0,72.0,,,2-s2.0-84943740667
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Feurer, M., Klein, A., Eggensperger, K., Springenberg, J.T., Blum, M., Hutter, F., Auto-sklearn: Efficient and robust automated machine learning (2019) Textitautomated Machine Learning, pp. 113-134. , Springer",Auto-sklearn: Efficient and robust automated machine learning,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Friedman, J., Hastie, T., Tibshirani, R., (2001) The Elements of Statistical Learning, 1. , Springer Series, New York, USA: Statistics",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Gijsbers, P., Vanschoren, J., Gama: Genetic automated machine learning assistant (2019) Journal of Open Source Software, 4 (33), p. 1132",Gama: Genetic automated machine learning assistant,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Greene, D., Cunningham, P., Practical solutions to the problem of diagonal dominance in kernel document clustering (2006) Proceedings of the Twenty-Third International Conference (ICML 2006), pp. 377-384. , . In: W.W. Cohen, A.W. Moore (eds.) Machine Learning, Pittsburgh, Pennsylvania, USA, June 25-29, 2006, ACM International Conference Proceeding Series (pp,). ACM",Practical solutions to the problem of diagonal dominance in kernel document clustering,"Greene D., Cunningham P.",Practical solutions to the problem of diagonal dominance in kernel document clustering,"In supervised kernel methods, it has been observed that the performance of the SVM classifier is poor in cases where the diagonal entries of the Gram matrix are large relative to the off-diagonal entries. This problem, referred to as diagonal dominance, often occurs when certain kernel functions are applied to sparse high-dimensional data, such as text corpora. In this paper we investigate the implications of diagonal dominance for unsupervised kernel methods, specifically in the task of document clustering. We propose a selection of strategies for addressing this issue, and evaluate their effectiveness in producing more accurate and stable clusterings.",10.1145/1143844.1143892,2006.0,153.0,,Classification (of information); Clustering algorithms; Matrix algebra; Problem solving; Diagonal dominance; Document clustering; Gram matrix; High-dimensional data; Kernel functions; SVM classifier; Text corpora; Support vector machines,2-s2.0-34250762673
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Hajj, N., Rizk, Y., Awad, M., A subjectivity classification framework for sports articles using improved cortical algorithms (2019) Neural Computing and Applications, 31 (11), pp. 8069-8085",A subjectivity classification framework for sports articles using improved cortical algorithms,"Hajj N., Rizk Y., Awad M.",A subjectivity classification framework for sports articles using improved cortical algorithms,"The enormous number of articles published daily on the Internet, by a diverse array of authors, often offers misleading or unwanted information, rendering activities such as sports betting riskier. As a result, extracting meaningful and reliable information from these sources becomes a time-consuming and near impossible task. In this context, labeling articles as objective or subjective is not a simple natural language processing task because subjectivity can take several forms. With the rise of online sports betting due to the revolution in Internet and mobile technology, an automated system capable of sifting through all these data and finding relevant sources in a reasonable amount of time presents itself as a desirable and marketable product. In this work, we present a framework for the classification of sports articles composed of three stages: The first stage extracts articles from web pages using text extraction libraries, parses the text and then tags words using Stanford’s parts of speech tagger; the second stage extracts unique syntactic and semantic features, and reduces them using our modified cortical algorithm (CA)—hereafter CA*—while the third stage classifies these texts as objective or subjective. Our framework was tested on a database containing 1000 articles, manually labeled using Amazon’s crowdsourcing tool, Mechanical Turk; and results using CA, CA*, support vector machines and one of its soft computing variants (LMSVM) as classifiers were reported. A testing accuracy of 85.6% was achieved on a fourfold cross-validation with a 40% reduction in features using CA* that was trained using an entropy weight update rule and a cross-entropy cost function. © 2018, The Natural Computing Applications Forum.",10.1007/s00521-018-3549-3,2019.0,6.0,Cortical algorithm; Feature reduction; Natural language processing; Subjectivity analysis; Support vector machines,Automation; Cost functions; Entropy; Semantics; Soft computing; Sports; Support vector machines; Syntactics; Text processing; Websites; Automated systems; Feature reduction; Mechanical turks; Mobile Technology; Semantic features; Subjectivity analysis; Subjectivity classifications; Testing accuracy; Natural language processing systems,2-s2.0-85051077042
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," He, Y., Lin, J., Liu, Z., Wang, H., Li, L.J., Han, S., Amc: Automl for model compression and acceleration on mobile devices (2018) Proceedings of the European Conference on Computer Vision (ECCV), pp. 784-800",Amc: Automl for model compression and acceleration on mobile devices,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Ishibuchi, H., Tsukamoto, N., Nojima, Y., Evolutionary many-objective optimization: A short review (2008) Proceedings of the 2008 IEEE Congress on Evolutionary Computation (IEEE World Congress on Computational Intelligence), pp. 2419-2426. , . In:, (pp,). IEEE",Evolutionary many-objective optimization: A short review,"Ishibuchi H., Tsukamoto N., Nojima Y.",Evolutionary many-objective optimization: A short review,"Whereas evolutionary multiobjective optimization (EMO) algorithms have successfully been used in a wide range of real-world application tasks, difficulties in their scalability to many-objective problems have also been reported. In this paper, first we demonstrate those difficulties through computational experiments. Then we review some approaches proposed in the literature for the scalability improvement of EMO algorithms. Finally we suggest future research directions in evolutionary many-objective optimization. © 2008 IEEE.",10.1109/CEC.2008.4631121,2008.0,742.0,,Optimization; Scalability; Application tasks; Computational experiments; EMO algorithms; Evolutionary multiobjective optimization algorithms; Future research directions; Objective optimizations; To many; Multiobjective optimization,2-s2.0-55749105514
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Jennings, P.C., Lysgaard, S., Hummelshøj, J.S., Vegge, T., Bligaard, T., Genetic algorithms for computational materials discovery accelerated by machine learning (2019) NPJ Computational Materials, 5 (1), pp. 1-6",Genetic algorithms for computational materials discovery accelerated by machine learning,"Jennings P.C., Lysgaard S., Hummelshøj J.S., Vegge T., Bligaard T.",Genetic algorithms for computational materials discovery accelerated by machine learning,"Materials discovery is increasingly being impelled by machine learning methods that rely on pre-existing datasets. Where datasets are lacking, unbiased data generation can be achieved with genetic algorithms. Here a machine learning model is trained on-the-fly as a computationally inexpensive energy predictor before analyzing how to augment convergence in genetic algorithm-based approaches by using the model as a surrogate. This leads to a machine learning accelerated genetic algorithm combining robust qualities of the genetic algorithm with rapid machine learning. The approach is used to search for stable, compositionally variant, geometrically similar nanoparticle alloys to illustrate its capability for accelerated materials discovery, e.g., nanoalloy catalysts. The machine learning accelerated approach, in this case, yields a 50-fold reduction in the number of required energy calculations compared to a traditional “brute force” genetic algorithm. This makes searching through the space of all homotops and compositions of a binary alloy particle in a given structure feasible, using density functional theory calculations. © 2019, The Author(s).",10.1038/s41524-019-0181-4,2019.0,55.0,,Binary alloys; Computation theory; Density functional theory; Genetic algorithms; Learning algorithms; Nanocatalysts; Alloy particles; Computational materials; Energy calculation; Inexpensive energy; Machine learning methods; Machine learning models; Nanoalloy catalysts; Nanoparticle-alloys; Machine learning,2-s2.0-85064263097
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Jing, K., Xu, J., (2019) A Survey on Neural Network Language Models",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Jouppi, N.P., Young, C., Patil, N., Patterson, D., Agrawal, G., Bajwa, R., Bates, S., Borchers, A., In-datacenter performance analysis of a tensor processing unit (2017) Proceedings of the 44Th Annual International Symposium on Computer Architecture, pp. 1-12",In-datacenter performance analysis of a tensor processing unit,"Jouppi N.P., Young C., Patil N., Patterson D., Agrawal G., Bajwa R., Bates S., Bhatia S., Boden N., Borchers A., Boyle R., Cantin P.-L., Chao C., Clark C., Coriell J., Daley M., Dau M., Dean J., Gelb B., Ghaemmaghami T.V., Gottipati R., Gulland W., Hagmann R., Richard Ho C., Hogberg D., Hu J., Hundt R., Hurt D., Ibarz J., Jaffey A., Jaworski A., Kaplan A., Khaitan H., Killebrew D., Koch A., Kumar N., Lacy S., Laudon J., Law J., Le D., Leary C., Liu Z., Lucke K., Lundin A., MacKean G., Maggiore A., Mahony M., Miller K., Nagarajan R., Narayanaswami R., Ni R., Nix K., Norrie T., Omernick M., Penukonda N., Phelps A., Ross J., Ross M., Salek A., Samadiani E., Severn C., Sizikov G., Snelham M., Souter J., Steinberg D., Swing A., Tan M., Thorson G., Tian B., Toma H., Tuttle E., Vasudevan V., Walter R., Wang W., Wilcox E., Yoon D.H.",In-datacenter performance analysis of a tensor processing unit,"Many architects believe that major improvements in cost-energyperformance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile responsetime requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an NVIDIA K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X-30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X-80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU. © 2017 Association for Computing Machinery.",10.1145/3079856.3080246,2017.0,1557.0,Accelerator; CNN; Deep learning; DNN; Domain-specific architecture; GPU; LSTM; MLP; Neural network; RNN; TensorFlow; TPU,Computer hardware; Deep learning; Deep neural networks; Graphics processing unit; Image coding; Memory architecture; Network architecture; Neural networks; Particle accelerators; Program processors; Tensors; Average throughput; Deterministic execution; Domain specific architectures; LSTM; Neural network (nn); Performance analysis; Processing units; TensorFlow; Computer architecture,2-s2.0-85025594365
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Khosrovian, K., Pfahl, D., Garousi, V., Gensim 2.0: A customizable process simulation model for software process evaluation (2008) Proceedings of the International Conference on Software Process, pp. 294-306. , Springer",Gensim 2.0: A customizable process simulation model for software process evaluation,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Kipf, T.N., Welling, M., Semi-supervised classification with graph convolutional networks. In: Proceedings of the 5th International Conference on Learning Representations, ICLR 2017, Toulon, France, April 24-26, 2017, Conference Track Proceedings (2017) Openreview.Net",Conference Track Proceedings,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Komer, B., Bergstra, J., Eliasmith, C., Hyperopt-sklearn: Automatic hyperparameter configuration for scikit-learn (2014) ICML Workshop on Automl, p. 50. , Citeseer",Hyperopt-sklearn: Automatic hyperparameter configuration for scikit-learn,"Komer B., Bergstra J., Eliasmith C.",Hyperopt-sklearn: Automatic hyperparameter configuration for scikit-learn,[No abstract available],,2014.0,117.0,,,2-s2.0-84908279482
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Kotthoff, L., Thornton, C., Hoos, H.H., Hutter, F., Leyton-Brown, K., Auto-WEKA 2.0?: Automatic model selection and hyperparameter optimization in WEKA (2017) Journal of Machine Learning Research, 18 (25), pp. 1-5",Auto-WEKA 2.0?: Automatic model selection and hyperparameter optimization in WEKA,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Kowsari, K., Jafari Meimandi, K., Heidarysafa, M., Mendu, S., Barnes, L., Brown, D., Text classification algorithms: A survey (2019) Information, 10 (4), p. 150",Text classification algorithms: A survey,"Kowsari K., Meimandi K.J., Heidarysafa M., Mendu S., Barnes L., Brown D.",Text classification algorithms: A survey,"In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed. © 2019 by the authors.",10.3390/info10040150,2019.0,287.0,Document classification; Text analysis; Text categorization; Text classification; Text mining; Text representation,Classification (of information); Data mining; Information retrieval systems; Learning algorithms; Machine learning; Natural language processing systems; Document Classification; Text analysis; Text categorization; Text classification; Text mining; Text representation; Text processing,2-s2.0-85065859140
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Lavrač, N., Škrlj, B., Robnik-Šikonja, M., Propositionalization and embeddings: two sides of the same coin (2020) Machine Learning, 109 (7), pp. 1465-1507",Propositionalization and embeddings: two sides of the same coin,"Lavrač N., Škrlj B., Robnik-Šikonja M.",Propositionalization and embeddings: two sides of the same coin,"Data preprocessing is an important component of machine learning pipelines, which requires ample time and resources. An integral part of preprocessing is data transformation into the format required by a given learning algorithm. This paper outlines some of the modern data processing techniques used in relational learning that enable data fusion from different input data types and formats into a single table data representation, focusing on the propositionalization and embedding data transformation approaches. While both approaches aim at transforming data into tabular data format, they use different terminology and task definitions, are perceived to address different goals, and are used in different contexts. This paper contributes a unifying framework that allows for improved understanding of these two data transformation techniques by presenting their unified definitions, and by explaining the similarities and differences between the two approaches as variants of a unified complex data transformation task. In addition to the unifying framework, the novelty of this paper is a unifying methodology combining propositionalization and embeddings, which benefits from the advantages of both in solving complex data transformation and learning tasks. We present two efficient implementations of the unifying methodology: an instance-based PropDRM approach, and a feature-based PropStar approach to data transformation and learning, together with their empirical evaluation on several relational problems. The results show that the new algorithms can outperform existing relational learners and can solve much larger problems. © 2020, The Author(s).",10.1007/s10994-020-05890-8,2020.0,4.0,Embeddings; Inductive logic programming; Knowledge graphs; Propositionalization; Relational learning,Data fusion; Data handling; Embeddings; Learning algorithms; Machine learning; Data preprocessing; Data processing techniques; Data representations; Data transformation; Efficient implementation; Empirical evaluations; Propositionalization; Relational learning; Metadata,2-s2.0-85087028636
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Le, Q.V., Mikolov, T., (2014) Distributed representations of sentences and documents. In: Proceedings of the 31th International Conference on Machine Learning, pp. 1188-1196. , ICML 2014, Beijing, China, 21-26 June 2014, JMLR Workshop and Conference Proceedings vol. 32 (pp,). JMLR.org",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Li, X., Roth, D., Learning question classifiers (2002) Proceedings of the 19Th International Conference on Computational Linguistics (COLING 2002), 1, pp. 1-7",Learning question classifiers,"Li X., Roth D.",Learning question classifiers,[No abstract available],,2002.0,731.0,,,2-s2.0-1542370072
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Liu, Y., Ott, M., Goyal, N., Du, J., Joshi, M., Chen, D., Levy, O., Stoyanov, V., (2019) A Robustly Optimized BERT Pretraining Approach, , RoBERTa",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Madrid, J., (2019) Autotext: Automl for Text Classification, , https://inaoe.repositorioinstitucional.mx/jspui/bitstream/1009/1950/1/MadridPJG.pdf",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Manning, C.D., Raghavan, P., Schütze, H., Scoring, term weighting and the vector space model (2008) Introduction to information retrieval, 100, pp. 2-4",term weighting and the vector space model,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Martinc, M., Škrjanec, I., Zupan, K., Pollak, S., (2017) Pan, p. 2017. , Author profiling - gender and language variety prediction. In, Working Notes Papers of the CLEF",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Mihalcea, R., Tarau, P., TextRank: Bringing order into text (2004) Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, pp. 404-411. , Barcelona, Spain, Association for Computational Linguistics",TextRank: Bringing order into text,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Mirończuk, M.M., Protasiewicz, J., A recent overview of the state-of-the-art elements of text classification (2018) Expert Systems with Applications, 106, pp. 36-54",A recent overview of the state-of-the-art elements of text classification,"Mirończuk M.M., Protasiewicz J.",A recent overview of the state-of-the-art elements of text classification,"The aim of this study is to provide an overview the state-of-the-art elements of text classification. For this purpose, we first select and investigate the primary and recent studies and objectives in this field. Next, we examine the state-of-the-art elements of text classification. In the following steps, we qualitatively and quantitatively analyse the related works. Herein, we describe six baseline elements of text classification including data collection, data analysis for labelling, feature construction and weighing, feature selection and projection, training of a classification model, and solution evaluation. This study will help readers acquire the necessary information about these elements and their associated techniques. Thus, we believe that this study will assist other researchers and professionals to propose new studies in the field of text classification. © 2018 The Authors",10.1016/j.eswa.2018.03.058,2018.0,126.0,Document classification; Document classification overview; Text classification; Text classification overview,Classification (of information); Information retrieval systems; Classification models; Data collection; Document Classification; Feature construction; New study; Related works; State of the art; Text classification; Text processing,2-s2.0-85045127865
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Misra, R., Arora, P., (2019) Sarcasm Detection Using Hybrid Neural Network",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Mitchell, M., (1998) An Introduction to Genetic Algorithms, , MIT Press, Cambridge, MA, USA",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Mohr, F., Wever, M., Hüllermeier, E., Ml-plan: Automated machine learning via hierarchical planning (2018) Machine Learning, 107 (8), pp. 1495-1515",Ml-plan: Automated machine learning via hierarchical planning,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Moradi, M., Dorffner, G., Samwald, M., Deep contextualized embeddings for quantifying the informative content in biomedical text summarization (2020) Computer Methods and Programs in Biomedicine, 184, p. 105117",Deep contextualized embeddings for quantifying the informative content in biomedical text summarization,"Moradi M., Dorffner G., Samwald M.",Deep contextualized embeddings for quantifying the informative content in biomedical text summarization,"Background and Objective: Capturing the context of text is a challenging task in biomedical text summarization. The objective of this research is to show how contextualized embeddings produced by a deep bidirectional language model can be utilized to quantify the informative content of sentences in biomedical text summarization. Methods: We propose a novel summarization method that utilizes contextualized embeddings generated by the Bidirectional Encoder Representations from Transformers (BERT) model, a deep learning model that recently demonstrated state-of-the-art results in several natural language processing tasks. We combine different versions of BERT with a clustering method to identify the most relevant and informative sentences of input documents. Using the ROUGE toolkit, we evaluate the summarizer against several methods previously described in literature. Results: The summarizer obtains state-of-the-art results and significantly improves the performance of biomedical text summarization in comparison to a set of domain-specific and domain-independent methods. The largest language model not specifically pretrained on biomedical text outperformed other models. However, among language models of the same size, the one further pretrained on biomedical text obtained best results. Conclusions: We demonstrate that a hybrid system combining a deep bidirectional language model and a clustering method yields state-of-the-art results without requiring labor-intensive creation of annotated features or knowledge bases or computationally demanding domain-specific pretraining. This study provides a starting point towards investigating deep contextualized language models for biomedical text summarization. © 2019",10.1016/j.cmpb.2019.105117,2020.0,16.0,"Biomedical text mining; Clustering; Contextualized embeddings; Deep learning, domain knowledge; Text summarization",Cluster analysis; Computational linguistics; Deep learning; Embeddings; Hybrid systems; Text processing; Biomedical text minings; Clustering; Clustering methods; Domain independents; Domain knowledge; NAtural language processing; State of the art; Text summarization; Natural language processing systems; article; deep learning; embedding; human; human experiment; knowledge base; mining; natural language processing; algorithm; data mining; medical informatics; natural language processing; procedures; semantics; Unified Medical Language System; Algorithms; Data Mining; Deep Learning; Humans; Medical Informatics; Natural Language Processing; Semantics; Unified Medical Language System,2-s2.0-85073186155
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Myers, I.B., (1962) The Myers-Briggs Type Indicator: Manual, , Consulting Psychologists Press, Germany",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," (2013) Second Joint Conference on Lexical and Computational Semantics (*SEM), pp. 312-320. , Association for Computational Linguistics, Atlanta, Georgia, USA",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Olson, R.S., Moore, J.H., Tpot: A tree-based pipeline optimization tool for automating machine learning (2019) Automated Machine Learning, pp. 151-160. , Springer",Tpot: A tree-based pipeline optimization tool for automating machine learning,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Scikit-learn: Machine learning in Python (2011) Journal of Machine Learning Research, 12, pp. 2825-2830",Scikit-learn: Machine learning in Python,"Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A., Cournapeau D., Brucher M., Perrot M., Duchesnay É.",Scikit-learn: Machine learning in Python,"Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.",,2011.0,27209.0,Model selection; Python; Supervised learning; Unsupervised learning,Commercial settings; Ease of use; Model Selection; Python; Source codes; Learning algorithms; Learning systems; High level languages,2-s2.0-80555140075
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Pilat, M., Křen, T., Neruda, R., Asynchronous evolution of data mining workflow schemes by strongly typed genetic programming (2016) 2016 IEEE 28Th International Conference on Tools with Artificial Intelligence (ICTAI), pp. 577-584. , IEEE",Asynchronous evolution of data mining workflow schemes by strongly typed genetic programming,"Pilát M., Křen T., Neruda R.",Asynchronous evolution of data mining workflow schemes by strongly typed genetic programming,"This paper describes an algorithm for the automated design of whole machine learning workflows, including preprocessing of the data and automatic creation of several types of ensembles. The algorithm is based on strongly typed genetic programming which ensures the validity of the workflows. The evolution of the individuals in the population is asynchronous in order to improve the utilization of computational resources. The approach is validated on four data sets from the UCI machine learning repository. © 2016 IEEE.",10.1109/ICTAI.2016.91,2017.0,6.0,,Artificial intelligence; Data mining; Genetic algorithms; Learning systems; Automated design; Automatic creations; Computational resources; Data mining workflow; Strongly-typed genetic programming; UCI machine learning repository; Whole machine; Work-flows; Genetic programming,2-s2.0-85013657562
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Pollak, S., Coesemans, R., Daelemans, W., Lavrač, N., Detecting contrast patterns in newspaper articles by combining discourse analysis and text mining (2011) Pragmatics, Quarterly Publication of the International Pragmatics Association (IPrA)., 21 (4), pp. 647-683",Detecting contrast patterns in newspaper articles by combining discourse analysis and text mining,"Pollak S., Coesemans R., Daelemans W., Lavrač N.",Detecting contrast patterns in newspaper articles by combining discourse analysis and text mining,"Text mining aims at constructing classification models and finding interesting patterns in large text collections. This paper investigates the utility of applying these techniques to media analysis, more specifically to support discourse analysis of news reports about the 2007 Kenyan elections and postelection crisis in local (Kenyan) and Western (British and US) newspapers. It illustrates how text mining methods can assist discourse analysis by finding contrast patterns which provide evidence for ideological differences between local and international press coverage. Our experiments indicate that most significant differences pertain to the interpretive frame of the news events: whereas the newspapers from the UK and the US focus on ethnicity in their coverage, the Kenyan press concentrateson sociopolitical aspects.",10.1075/prag.21.4.07pol,2011.0,18.0,Discourse analysis; Ideology; Kenyan elections; Pragmatics; Text mining,,2-s2.0-84857871448
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Qian, M., Zhai, C., (2014), Unsupervised feature selection for multi-view clustering on text-image web news data. In: J. Li, X.S. Wang, M.N. Garofalakis, I. Soboroff, T. Suel, M. Wang (eds.) Proceedings of the 23rd ACM International Conference on Conference on Information and Knowledge Management CIKM (pp. 1963–1966). Shanghai, China:ACM",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Rappl, G., On linear convergence of a class of random search algorithms (1989) ZAMM-Journal of Applied Mathematics and Mechanics/Zeitschrift für Angewandte Mathematik und Mechanik, 69 (1), pp. 37-45",On linear convergence of a class of random search algorithms,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Reif, M., Shafait, F., Dengel, A., Meta-learning for evolutionary parameter optimization of classifiers (2012) Machine Learning, 87 (3), pp. 357-380",Meta-learning for evolutionary parameter optimization of classifiers,"Reif M., Shafait F., Dengel A.",Meta-learning for evolutionary parameter optimization of classifiers,"The performance of most of the classification algorithms on a particular dataset is highly dependent on the learning parameters used for training them. Different approaches like grid search or genetic algorithms are frequently employed to find suitable parameter values for a given dataset. Grid search has the advantage of finding more accurate solutions in general at the cost of higher computation time. Genetic algorithms, on the other hand, are able to find good solutions in less time, but the accuracy of these solutions is usually lower than those of grid search. This paper uses ideas from meta-learning and case-based reasoning to provide good starting points to the genetic algorithm. The presented approach reaches the accuracy of grid search at a significantly lower computational cost. We performed extensive experiments for optimizing learning parameters of the Support Vector Machine (SVM) and the Random Forest classifiers on over 100 datasets from UCI and StatLib repositories. For the SVM classifier, grid search achieved an average accuracy of 81 % and took six hours for training, whereas the standard genetic algorithm obtained 74 % accuracy in close to one hour of training. Our method was able to achieve an average accuracy of 81 % in only about 45 minutes. Similar results were achieved for the Random Forest classifier. Besides a standard genetic algorithm, we also compared the presented method with three state-of-the-art optimization algorithms: Generating Set Search, Dividing Rectangles, and the Covariance Matrix Adaptation Evolution Strategy. Experimental results show that our method achieved the highest average accuracy for both classifiers. Our approach can be particularly useful when training classifiers on large datasets where grid search is not feasible. © 2012 The Author(s).",10.1007/s10994-012-5286-7,2012.0,85.0,Feature selection; Genetic algorithm; Meta-learning; Parameter optimization,Classification algorithm; Computation time; Computational costs; Covariance matrix adaptation evolution strategies; Data sets; Dividing rectangles; Evolutionary parameters; Generating set search; Grid search; Large datasets; Learning parameters; Metalearning; Optimization algorithms; Parameter optimization; Parameter values; Random forest classifier; Standard genetic algorithm; SVM classifiers; Classification (of information); Decision trees; Feature extraction; Forestry; Optimization; Support vector machines; Genetic algorithms; Algorithms; Classification; Decision Theory; Optimization; Parameters; Performance,2-s2.0-84862009037
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Rose, S., Engel, D., Cramer, N., Cowley, W., (2010) Automatic keyword extraction from individual documents, pp. 1-20. , Wiley Online Library, New Jersey",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Rudin, C., Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead (2019) Nature Machine Intelligence, 1 (5), pp. 206-215",Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead,Rudin C.,Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead,"Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision. © 2019, Springer Nature Limited.",10.1038/s42256-019-0048-x,2019.0,752.0,,Behavioral research; Crime; Decision making; Health care; Medical computing; Bad practices; Black boxes; Black-box model; Criminal justice; Design models; Machine learning models; Machine learning,2-s2.0-85069492292
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Sennrich, R., Haddow, B., Birch, A., Neural machine translation of rare words with subword units (2016) Proceedings of the 54Th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1715-1725. , Berlin, Germany, Association for Computational Linguistics",Neural machine translation of rare words with subword units,"Sennrich R., Haddow B., Birch A.",Neural machine translation of rare words with subword units,"Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 Bleu, respectively. © 2016 Association for Computational Linguistics.",10.18653/v1/p16-1162,2016.0,1382.0,,Computer aided language translation; Encoding (symbols); Signal encoding; Byte-pair encoding; Compositional translation; Compression algorithms; Effective approaches; Machine translations; Morphological transformations; Out of vocabulary words; Word segmentation; Computational linguistics,2-s2.0-85011827221
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Škrlj, B., Repar, A., Pollak, S., RaKUn: Rank-based keyword extraction via unsupervised learning and meta vertex aggregation (2019) International Conference on Statistical Language and Speech Processing, pp. 311-323. , Springer",RaKUn: Rank-based keyword extraction via unsupervised learning and meta vertex aggregation,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Snoek, J., Larochelle, H., Adams, R.P., (2012), Practical bayesian optimization of machine learning algorithms. In: P.L. Bartlett, F.C.N. Pereira, C.J.C. Burges, L. Bottou, K.Q. Weinberger (eds.) Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held December 3-6, 2012 (pp. 2960–2968), Lake Tahoe, Nevada, United States",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Speer, R., Chin, J., Havasi, C., Conceptnet 5.5: An open multilingual graph of general knowledge (2017) Proceeding of the Thirty-First AAAI Conference on Artificial Intelligence, pp. 4441-4451. , Singh SP, Markovitch S, (eds), AAAI Press, San Fransisco, California, USA",Conceptnet 5.5: An open multilingual graph of general knowledge,"Speer R., Chin J., Havasi C.",Conceptnet 5.5: An open multilingual graph of general knowledge,[No abstract available],,2017.0,493.0,,,2-s2.0-85028996745
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Stanley, K.O., Clune, J., Lehman, J., Miikkulainen, R., Designing neural networks through neuroevolution (2019) Nature Machine Intelligence, 1 (1), pp. 24-35",Designing neural networks through neuroevolution,"Stanley K.O., Clune J., Lehman J., Miikkulainen R.",Designing neural networks through neuroevolution,"Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field’s contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence. © 2019, Springer Nature Limited.",10.1038/s42256-018-0006-z,2019.0,181.0,,Evolutionary algorithms; Gradient methods; Learning algorithms; Machine learning; Network architecture; Reinforcement learning; Stochastic systems; Activation functions; Artificial general intelligences; Evolutionary process; Gradient-based neural networks; Large-scale computing; Learning neural networks; Machine learning communities; Stochastic gradient descent; Deep learning,2-s2.0-85064166007
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Sterckx, L., Demeester, T., Deleu, J., Develder, C., Topical word importance for fast keyphrase extraction (2015) Proceedings of the 24Th International Conference on World Wide Web, pp. 121-122. , New York, ACM",Topical word importance for fast keyphrase extraction,"Sterckx L., Demeester T., Deleu J., Develder C.",Topical word importance for fast keyphrase extraction,"We propose an improvement on a state-of-the-art keyphrase extraction algorithm, Topical PageRank (TPR), incorporating topical information from topic models. While the original algorithm requires a random walk for each topic in the topic model being used, ours is independent of the topic model, computing but a single PageRank for each text regardless of the amount of topics in the model. This increases the speed drastically and enables it for use on large collections of text using vast topic models, while not altering performance of the original algorithm.",10.1145/2740908.2742730,2015.0,28.0,,Algorithms; Extraction; World Wide Web; Keyphrase extraction; Original algorithms; PageRank; Random Walk; State of the art; Topic model; Topic Modeling; Data mining,2-s2.0-84968531517
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A.A., Inception-v4, inception-resnet and the impact of residual connections on learning (2017) Proc of the Thirty-First AAAI Conference on Artificial Intelligence, pp. 4278-4284. , Singh SP, Markovitch S, (eds), AAAI Press, San Francisco, California, USA",inception-resnet and the impact of residual connections on learning,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Thornton, C., Hutter, F., Hoos, H.H., Leyton-Brown, K., Auto-weka: combined selection and hyperparameter optimization of classification algorithms (2013) The 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining KDD 2013, pp. 847-855. , Dhillon IS, Koren Y, Ghani R, Senator TE, Bradley P, Parekh R, He J, Grossman RL, Uthurusamy R, (eds), ACM, Chicago, IL, USA",Auto-weka: combined selection and hyperparameter optimization of classification algorithms,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Vafaie, H., De Jong, K., Feature space transformation using genetic algorithms (1998) IEEE Intelligent Systems and their Applications, 13 (2), pp. 57-65",Feature space transformation using genetic algorithms,"Vafaie H., De Jong K.",Feature space transformation using genetic algorithms,"The authors have developed an autonomous system that transforms feature spaces to improve classification techniques. They apply their method to an eye-detection face recognition system, demonstrating substantially better classification rates than competing systems.",10.1109/5254.671093,1998.0,61.0,,Artificial intelligence; Pattern recognition; Pattern recognition systems; Face recognition; Genetic algorithms,2-s2.0-0032028849
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Virtanen, P., Gommers, R., Oliphant, T.E., Haberland, M., Reddy, T., Cournapeau, D., Burovski, E., Bright, J., Scipy 10 Fundamental algorithms for scientific computing in Python (2020) Nature Methods, 17 (3), pp. 261-272",Scipy 10 Fundamental algorithms for scientific computing in Python,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Wan, X., Xiao, J., Single document keyphrase extraction using neighborhood knowledge (2008) Proceedings of the AAAI Conference, 8, pp. 855-860",Single document keyphrase extraction using neighborhood knowledge,"Wan X., Xiao J.",Single document keyphrase extraction using neighborhood knowledge,"Existing methods for single document keyphrase extraction usually make use of only the information contained in the specified document. This paper proposes to use a small number of nearest neighbor documents to provide more knowledge to improve single document keyphrase extraction. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents. Experimental results demonstrate the good effectiveness and robustness of our proposed approach. Copyright © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2008.0,207.0,,Bionics; Document sets; Existing methods; Global informations; Keyphrase extractions; Local informations; Nearest neighbors; Ranking algorithms; Artificial intelligence,2-s2.0-57749198218
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Transformers: State-of-the-art natural language processing. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38–45). Association for Computational Linguistics, Online (2020) Https://Doi.Org/10.18653/V1/2020.Emnlp-Demos.6, , https://www.aclweb.org/anthology/2020.emnlp-demos.6, Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R., Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., Le Scao, T., Gugger, S., Drame, M., Lhoest, Q., Rush, A",Online,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Wolpert, D.H., Macready, W.G., No free lunch theorems for optimization (1997) IEEE Transactions on Evolutionary Computation, 1 (1), pp. 67-82",No free lunch theorems for optimization,"Wolpert D.H., Macready W.G.",No free lunch theorems for optimization,"A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of ""no free lunch"" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori ""head-to-head"" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms. © 1997 IEEE.",10.1109/4235.585893,1997.0,6418.0,Evolutionary algorithms; Information theory; Optimization,Algorithms; Information theory; Problem solving; Time varying systems; Head to head minimax distinctions; No free lunch (NFL) theorems; Optimization,2-s2.0-0031118203
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Yang, C., Akimoto, Y., Kim, D.W., Udell, M., Oboe (2019) Proceedings of the 25Th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining",Oboe,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Yang, Z., Dai, Z., Yang, Y., Carbonell, J.G., Salakhutdinov, R., Le, Q.V., (2019), Xlnet: Generalized autoregressive pretraining for language understanding. In: H.M. Wallach, H. Larochelle, A. Beygelzimer, F. d’Alché-Buc, E.B. Fox, R. Garnett (eds.) Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019(pp. 5754–5764) Vancouver, BC, Canada: NeurIPS 2019",,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Zimmer, M., Doncieux, S., Bootstrapping q -learning for robotics from neuro-evolution results (2017) IEEE Transactions on Cognitive and Developmental Systems, 10 (1), pp. 102-119",Bootstrapping q -learning for robotics from neuro-evolution results,,,,,,,,,
2-s2.0-85104608374,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification," Zoph, B., Vasudevan, V., Shlens, J., Le, Q.V., Learning transferable architectures for scalable image recognition (2018) 2018 IEEE Conference on Computer Vision and Pattern Recognition CVPR 2018, pp. 8697-8710. , Salt Lake City, UT, USA, IEEE Computer Society",Learning transferable architectures for scalable image recognition,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture,"Hart, P.E., Nilsson, N.J., Raphael, B., A formal basis for the heuristic determination of minimum cost paths (1968) IEEE Trans. Syst. Sci. Cybernet., 4 (2), pp. 100-107",A formal basis for the heuristic determination of minimum cost paths,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2017), Xiong, Wenhan, Thien Hoang, and William Yang Wang. “Deeppath: A RL method for knowledge graph reasoning.” arXiv preprint arXiv:1707.06690",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2018), Chen, Wenhu, et al. “Variational knowledge graph reasoning.” arXiv preprint arXiv:1803.06581",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2019), Wang, Jingyuan, et al. “Empowering A* Search Algorithms with Neural Networks for Personalized Route Recommendation.” Proceedings of the 25th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. ACM",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Wang, Q.I., Hao, Y., ALSTM: An attention-based long short-term memory framework for knowledge base reasoning (2020) Neurocomputing",ALSTM: An attention-based long short-term memory framework for knowledge base reasoning,"Wang Q., Hao Y.",ALSTM: An attention-based long short-term memory framework for knowledge base reasoning,"Knowledge Graphs (KGs) have been applied to various application scenarios including Web searching, Q&A, recommendation system, natural language processing and so on. However, the vast majority of Knowledge Bases (KBs) are incomplete, necessitating a demand for KB completion (KBC). Methods of KBC used in the mainstream current knowledge base include the latent factor model, the random walk model and recent popular methods based on reinforcement learning, which performs well in their respective areas of expertise. Recurrent neural network (RNN) and its variants model temporal data by remembering information for long periods, however, whether they also have the ability to use the information they have already remembered to achieve complex reasoning in the knowledge graph. In this paper, we produce a novel framework (ALSTM) based on the Attention mechanism and Long Short-Term Memory (LSTM), which associates structure learning with parameter learning of first-order logical rules in an end-to-end differentiable neural networks model. In this framework, we designed a memory system and employed a multi-head dot product attention (MHDPA) to interact and update the memories embedded in the memory system for reasoning purposes. This is also consistent with the process of human cognition and reasoning, looking for enlightenment for the future in historical memory. In addition, we explored the use of inductive bias in deep learning to facilitate learning of entities, relations, and rules. Experiments establish the efficiency and effectiveness of our model and show that our method achieves better performance in tasks which include fact prediction and link prediction than baseline models on several benchmark datasets such as WN18RR, FB15K-237 and NELL-995. © 2020 Elsevier Ltd",10.1016/j.neucom.2020.02.065,2020.0,10.0,Attention; Deep learning; Knowledge base; Logical rule; LSTM; Memory,Benchmarking; Brain; Data storage equipment; Deep learning; Formal logic; Knowledge based systems; Learning systems; Natural language processing systems; Reinforcement learning; Attention; Knowledge base; Knowledge basis (KBs); Logical rules; LSTM; NAtural language processing; Neural networks model; Recurrent neural network (RNN); Long short-term memory; article; attention; deep learning; human; human experiment; knowledge base; prediction; reasoning; recurrent neural network; short term memory,2-s2.0-85081931785
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2017), Das, Rajarshi, et al. “Go for a walk and arrive at the answer: Reasoning over paths in knowledge bases using reinforcement learning.” arXiv preprint arXiv:1711.05851",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2015), Vinyals, Oriol, Samy Bengio, and Manjunath Kudlur. “Order matters: Sequence to sequence for sets.” arXiv preprint arXiv:1511.06391",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (1810), Jacob Devlin, Ming-Wei Chang, Kenton Lee, and KristinaToutanova.2018. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv04805",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2016), Seo, Minjoon, et al. “Bidirectional attention flow for machine comprehension.” arXiv preprint arXiv:1611.01603",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Scarselli, F., The graph neural network model (2008) IEEE Trans. Neural Networks, 20 (1), pp. 61-80",The graph neural network model,"Scarselli F., Gori M., Tsoi A.C., Hagenbuchner M., Monfardini G.",The graph neural network model,"Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function τ (G,n) ∈ Rm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities. © 2008 IEEE.",10.1109/TNN.2008.2005605,2009.0,1542.0,Graph neural networks (GNNs); Graph processing; Graphical domains; Recursive neural networks,"Biochemistry; Computer vision; Data processing; Image processing; Information management; Learning algorithms; Learning systems; Molecular biology; Pattern recognition; Recursive functions; Computational costs; Euclidean spaces; Generalization capabilities; Graph neural networks (GNNs); Graph processing; Graphical domains; Network methods; Network modelling; Recursive neural networks; Neural networks; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; comparative study; factual database; Internet; nonlinear system; regression analysis; reproducibility; statistical model; Algorithms; Artificial Intelligence; Databases, Factual; Internet; Linear Models; Neural Networks (Computer); Nonlinear Dynamics; Pattern Recognition, Automated; Regression Analysis; Reproducibility of Results",2-s2.0-58649113008
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2015), Li, Yujia, et al. “Gated graph sequence neural networks.” arXiv preprint arXiv:1511.05493",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2014), Cho, Kyunghyun, et al. “Learning phrase representations using RNN encoder-decoder for statistical machine translation.” arXiv preprint arXiv:1406.1078",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2018), Espeholt, Lasse, et al. “Impala: Scalable distributed deep-rl with importance weighted actor-learner architectures.” arXiv preprint arXiv:1802.01561",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Sutton, R.S., Barto, A.G., Reinforcement, learning: An introduction (2018), MIT press",learning: An introduction,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Wang, Q., Hao, Y., Cao, J., ADRL: An attention based deep reinforcement learning framework for knowledge graph reasoning (2020) Knowledge-Based Syst., 197. , 105910",ADRL: An attention based deep reinforcement learning framework for knowledge graph reasoning,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2019), Wu, Zonghan, et al. “A comprehensive survey on graph neural networks.” arXiv preprint arXiv:1901.00596",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Bordes, A., “Translating embeddings for modeling multi-relational data.” (2013) Adv. Neural Inform. Process. Syst.",“Translating embeddings for modeling multi-relational data.”,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Nickel, M., Tresp, V., Kriegel, H.-P., “A Three-Way Model for Collective Learning on Multi-Relational Data.” (2011) ICML, 11",“A Three-Way Model for Collective Learning on Multi-Relational Data.”,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2014), Yang, Bishan, et al. “Embedding entities and relations for learning and inference in knowledge bases.” arXiv preprint arXiv:1412.6575",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Wang, Q., “Knowledge graph embedding: A survey of approaches and applications.” (2017) IEEE Trans. Knowledge Data Eng., 29 (12), pp. 2724-2743",“Knowledge graph embedding: A survey of approaches and applications.”,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2014), Wang, Zhen, et al. “Knowledge graph embedding by translating on hyperplanes.” Twenty-Eighth AAAI conference on artificial intelligence",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2015), Lin, Yankai, et al. “Learning entity and relation embeddings for knowledge graph completion.” Twenty-ninth AAAI conference on artificial intelligence",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2015), Toutanova, Kristina, et al. “Representing text for joint embedding of text and knowledge bases.” Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2011), Lao, Ni, Tom Mitchell, and William W. Cohen. “Random walk inference and learning in a large scale knowledge base.” Proceedings of the Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2018), Shen, Yelong, et al. “M-walk: Learning to walk over graphs using monte carlo tree search.” Advances in Neural Information Processing Systems",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2015), Neelakantan, Arvind, Benjamin Roth, and Andrew McCallum. “Compositional vector space models for knowledge base inference.”2015 AAAI spring symposium series",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2016), Das, Rajarshi, et al. “Chains of reasoning over entities, relations, and text using recurrent neural networks.” arXiv preprint arXiv:1607.01426",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Williams, R.J., Simple statistical gradient-following algorithms for connectionist reinforcement learning (1992) Mach. Learn., 8 (3-4), pp. 229-256",Simple statistical gradient-following algorithms for connectionist reinforcement learning,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2018), Dettmers, Tim, et al. “Convolutional 2d knowledge graph embeddings.” Thirty-Second AAAI Conference on Artificial Intelligence",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2017), Veličković, Petar, et al. “Graph attention networks.” arXiv preprint arXiv: 1710.10903",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2010), Glorot, Xavier, and Yoshua Bengio. “Understanding the difficulty of training deep feedforward neural networks.” Proceedings of the thirteenth international conference on artificial intelligence and statistics",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2015), Lillicrap, Timothy P., et al. “Continuous control with deep reinforcement learning.” arXiv preprint arXiv:1509.02971",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2015), Mnih, Volodymyr, et al. “Human-level control through deep reinforcement learning.” Nature 518.7540: 529",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," (2017), Silver, David, et al. “Mastering the game of go without human knowledge.” Nature 550.7676: 354",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Ji, G., He, S., Liheng, X.U., Liu, K., Zhao, J., Knowledge graph embedding via dynamic mapping matrix (2015) ACL, 1, pp. 687-696",Knowledge graph embedding via dynamic mapping matrix,"Ji G., He S., Xu L., Liu K., Zhao J.",Knowledge graph embedding via dynamic mapping matrix,"Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-The-Art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms stateof-the-Art methods. © 2015 Association for Computationl Linguisticss.",10.3115/v1/p15-1067,2015.0,649.0,,Classification (of information); Computational linguistics; Mapping; AI applications; Dynamic mapping; Evaluation results; Knowledge graphs; Link prediction; Matrix-vector multiplication operation; State-of-the-art methods; State-of-the-art performance; Natural language processing systems,2-s2.0-84943792156
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2014. Embedding entities and relations for learning and inference in knowledge bases. CoRR, abs/1412.6575",,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," LeCun, Y., Bengio, Y., Hinton, G., “Deep learning.” (2015) Nature, 521 (7553), pp. 436-444",“Deep learning.”,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Džeroski, S., De Raedt, L., Driessens, K., “Relational reinforcement learning.” (2001) Mach. Learn., 43 (1-2), pp. 7-52",“Relational reinforcement learning.”,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Yang, F., Yang, Z., Cohen, W.W., “Differentiable learning of logical rules for knowledge base reasoning.” (2017) Adv. Neural Inform. Process. Syst.",“Differentiable learning of logical rules for knowledge base reasoning.”,,,,,,,,,
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Wang, Q., Ji, Y., Hao, Y., Cao, J., GRL: Knowledge graph completion with GAN-based reinforcement learning (2020) Knowledge-Based Syst., 209",GRL: Knowledge graph completion with GAN-based reinforcement learning,"Wang Q., Ji Y., Hao Y., Cao J.",GRL: Knowledge graph completion with GAN-based reinforcement learning,"Knowledge graph completion intends to infer the entities that need to be queried through the entities and relations known in the knowledge graphs. It is used in many applications, such as question and answer systems, and searching engines. As the completion process can be represented as a Markov process, existing works would solve this problem with reinforcement learning. However, there are three issues blocking them from achieving high accuracy, which are reward sparsity, missing specific domain rules, and ignoring the generation of knowledge graphs. In this paper, we design a generative adversarial net (GAN)-based reinforcement learning model, named GRL, for knowledge graph completion. First, GRL employs the graph convolutional network to embed the knowledge graphs into the low-dimensional space. Second, GRL employs both GAN and long short-term memory (LSTM) to record trajectory sequences obtained by the agent from traversing the knowledge graph and generate new trajectory sequences if needed. At the same time, GRL applies domain-specific rules accordingly. Finally, GRL employs the deep deterministic policy gradient method to optimize both rewards and adversarial loss. The experiments show that GRL is able to both generate better policies and outperform traditional methods for several tasks. © 2020 Elsevier B.V.",10.1016/j.knosys.2020.106421,2020.0,7.0,Deep learning; Knowledge graph; Knowledge graph completion; Reinforcement learning,Convolutional neural networks; Gradient methods; Knowledge representation; Long short-term memory; Markov processes; Convolutional networks; Domain specific; Knowledge graphs; Low-dimensional spaces; Policy gradient methods; Question and answer system; Reinforcement learning models; Searching engine; Reinforcement learning,2-s2.0-85091653381
2-s2.0-85098717859,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture," Lan, Z., Sharma, P., (2020) Albert: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS, 1-17",,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information,"Bloomfield, L., A set of postulates for the science of language (1926) Language, 2 (3), pp. 153-164",A set of postulates for the science of language,Bloomfield L.,A set of postulates for the science of language,[No abstract available],,1926.0,194.0,,,2-s2.0-0040153743
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Bollacker, K., Evans, C., Paritosh, P., Sturge, T., Taylor, J., Freebase: A collaboratively created graph database for structuring human knowledge (2008) Proceedings of the 2008 ACM SIGMOD International Conference on Management of Data, pp. 1247-1250. , pp",Freebase: A collaboratively created graph database for structuring human knowledge,"Bollacker K., Evans C., Paritosh P., Sturge T., Taylor J.",Freebase: A collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Free-base currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",10.1145/1376616.1376746,2008.0,2377.0,Design; Human factors; Languages,Applications.; Data queries; Graph databases; Human factors; Human knowledges; Languages; Manipulation languages; Object-oriented; Database systems; Human engineering; Linguistics; Query languages; Object oriented programming,2-s2.0-57149137628
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Bordes, A., Usunier, N., Garcia-Duran, A., Weston, J., Yakhnenko, O., Translating embeddings for modeling multi-relational data (2013) Advances in Neural Information Processing Systems, pp. 2787-2795. , pp",Translating embeddings for modeling multi-relational data,"Bordes A., Usunier N., Garcia-Durán A., Weston J., Yakhnenko O.",Translating embeddings for modeling multi-relational data,"We consider the problem of embedding entities and relationships of multi relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.",,2013.0,2395.0,,Canonical modeling; Knowledge basis; Large scale data sets; Link prediction; Relational data; State-of-the-art methods; Training sample; Very large database,2-s2.0-84899013802
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Devlin, J., Chang, M.W., Lee, K., Toutanova, K., (2018) Bert: Pre-Training of Deep Bidirectional Transformers for Language Understanding. Arxiv Preprint Arxiv, 1810, p. 04805",,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Dong, Z., Dong, Q., HowNet-a hybrid language and knowledge resource (2003) International Conference on Natural Language Processing and Knowledge Engineering, 2003. Proceedings. 2003, Pp. 820–824. IEEE",HowNet-a hybrid language and knowledge resource,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Glorot, X., Bengio, Y., Understanding the difficulty of training deep feedforward neural networks (2010) Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, pp. 249-256. , pp",Understanding the difficulty of training deep feedforward neural networks,"Glorot X., Bengio Y.",Understanding the difficulty of training deep feedforward neural networks,"Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. Copyright 2010 by the authors.",,2010.0,7005.0,,Faster convergence; Gradient descent; Hidden layers; Jacobians; Mean values; Non-linear activation; Non-Linearity; Singular values; Algorithms; Artificial intelligence; Feedforward neural networks; Multilayer neural networks; Chemical activation,2-s2.0-84862277874
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Han, X., Openke: An open toolkit for knowledge embedding (2018) Proceedings of EMNLP",Openke: An open toolkit for knowledge embedding,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Ji, G., He, S., Xu, L., Liu, K., Zhao, J., Knowledge graph embedding via dynamic mapping matrix (2015) Proceedings of the 53Rd Annual Meeting of the Association for Computational Linguistics and the 7Th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 687-696. , pp",Knowledge graph embedding via dynamic mapping matrix,"Ji G., He S., Xu L., Liu K., Zhao J.",Knowledge graph embedding via dynamic mapping matrix,"Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-The-Art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms stateof-the-Art methods. © 2015 Association for Computationl Linguisticss.",10.3115/v1/p15-1067,2015.0,649.0,,Classification (of information); Computational linguistics; Mapping; AI applications; Dynamic mapping; Evaluation results; Knowledge graphs; Link prediction; Matrix-vector multiplication operation; State-of-the-art methods; State-of-the-art performance; Natural language processing systems,2-s2.0-84943792156
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Jin, H., (2018) Incorporating Chinese Characters of Words for Lexical Sememe Prediction. Arxiv Preprint Arxiv, 1806, p. 06349",,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Lin, Y., Liu, Z., Sun, M., Liu, Y., Zhu, X., Learning entity and relation embeddings for knowledge graph completion (2015) Proceedings of AAAI, pp. 2181-2187. , pp",Learning entity and relation embeddings for knowledge graph completion,"Lin Y., Liu Z., Sun M., Liu Y., Zhu X.",Learning entity and relation embeddings for knowledge graph completion,"Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https://github.com/mrlyk423/relation-extraction. © Copyright 2015, Association for the Advancement of Artificial Intelligence (www.aaa1.org). All rights reserved.",,2015.0,1122.0,,Semantics; Common spaces; Corresponding relations; Fact extraction; Knowledge graphs; Link prediction; Semantic Space; Source codes; State of the art; Artificial intelligence,2-s2.0-84959863917
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Miller, G.A., Wordnet: A lexical database for English (1995) Commun. ACM, 38 (11), pp. 39-41",Wordnet: A lexical database for English,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Qin, Y., Improving sequence modeling ability of recurrent neural networks via sememes (2020) IEEE/ACM Trans. Audio Speech Lang. Process., 28, pp. 2364-2373",Improving sequence modeling ability of recurrent neural networks via sememes,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Wang, Z., Zhang, J., Feng, J., Chen, Z., Knowledge graph embedding by translating on hyperplanes (2014) AAAI, Vol. 14, Pp. 1112–1119. Citeseer",Knowledge graph embedding by translating on hyperplanes,"Wang Z., Zhang J., Feng J., Chen Z.",Knowledge graph embedding by translating on hyperplanes,"We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplanc together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up. Copyright © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2014.0,1050.0,,Knowledge graphs,2-s2.0-84908213178
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Xiao, H., (2018) Bert-As-Service, , https://github.com/hanxiao/bert-as-service",,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Xiao, H., Huang, M., Meng, L., Zhu, X., SSP: Semantic space projection for knowledge graph embedding with text descriptions (2017) Thirty-First AAAI Conference on Artificial Intelligence",SSP: Semantic space projection for knowledge graph embedding with text descriptions,"Xiao H., Huang M., Meng L., Zhu X.",SSP: Semantic space projection for knowledge graph embedding with text descriptions,"Knowledge graph embedding represents entities and relations in knowledge graph as low-dimensional, continuous vectors, and thus enables knowledge graph compatible with machine learning models. Though there have been a variety of models for knowledge graph embedding, most methods merely concentrate on the fact triples, while supplementary textual descriptions of entities and relations have not been fully employed. To this end, this paper proposes the semantic space projection (SSP) model which jointly learns from the symbolic triples and textual descriptions. Our model builds interaction between the two information sources, and employs textual descriptions to discover semantic relevance and offer precise semantic embedding. Extensive experiments show that our method achieves substantial improvements against baselines on the tasks of knowledge graph completion and entity classification. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2017.0,94.0,,Artificial intelligence; Learning systems; Information sources; Knowledge graphs; Low dimensional; Machine learning models; Semantic embedding; Semantic relevance; Semantic Space; Textual description; Semantics,2-s2.0-85030468818
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Xie, R., Liu, Z., Jia, J., Luan, H., Sun, M., Representation learning of knowledge graphs with entity descriptions (2016) Thirtieth AAAI Conference on Artificial Intelligence",Representation learning of knowledge graphs with entity descriptions,"Xie R., Liu Z., Jia J., Luan H., Sun M.",Representation learning of knowledge graphs with entity descriptions,"Representation learning (RL) of knowledge graphs aims to project both entities and relations into a continuous lowdimensional space. Most methods concentrate on learning representations with knowledge triples indicating relations between entities. In fact, in most knowledge graphs there are usually concise descriptions for entities, which cannot be well utilized by existing methods. In this paper, we propose a novel RL method for knowledge graphs taking advantages of entity descriptions. More specifically, we explore two encoders, including continuous bag-of-words and deep convolutional neural models to encode semantics of entity descriptions. We further learn knowledge representations with both triples and descriptions.We evaluate our method on two tasks, including knowledge graph completion and entity classification. Experimental results on real-world datasets show that, our method outperforms other baselines on the two tasks, especially under the zero-shot setting, which indicates that our method is capable of building representations for novel entities according to their descriptions. The source code of this paper can be obtained from https://github.com/xrb92/DKRL. © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2016.0,315.0,,Artificial intelligence; Knowledge representation; Semantics; Bag of words; Knowledge graphs; Low-dimensional spaces; Neural models; Real-world datasets; Source codes; Graphic methods,2-s2.0-85007148114
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Yi-Xin, Z., A study on information-knowledge-intelligence transformation (2004) Acta Electronica Sinica, 32 (4), p. 16",A study on information-knowledge-intelligence transformation,,,,,,,,,
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information," Zhong, Y., Mechanism-based artificial intelligence theory: A universal theory of artificial intelligence (2018) CAAI Trans. Intell. Syst., 13 (1), pp. 2-18",Mechanism-based artificial intelligence theory: A universal theory of artificial intelligence,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text,"Calí, A., Gottlob, G., Pieris, A., Query answering under expressive entity-relationship schemata (2010) Proc. Int. Conf. Conceptual Model., pp. 347-361. , Springer",Query answering under expressive entity-relationship schemata,"Calì A., Gottlob G., Pieris A.",Query answering under expressive entity-relationship schemata,"We address the problem of answering conjunctive queries under constraints representing schemata expressed in an extended version of the Entity- Relationship model. This extended model, called ER+ model, comprises is-a constraints among entities and relationships, plus functional and mandatory participation constraints. In particular, it allows arbitrary permutations of the roles in is-a among relationships. A key notion that ensures high tractability in ER+ schemata is separability, i.e., the absence of interaction between the functional participation constraints and the other constructs of ER+.We provide a precise syntactic characterization of separable ER+ schemata, called ER± schemata, by means of a necessary and sufficient condition. We present a complete complexity analysis of the conjunctive query answering problem under ER± schemata. We show that the addition of so-called negative constraints does not increase the complexity of query answering. With such constraints, our model properly generalizes the most widely-adopted tractable ontology languages. © 2010 Springer-Verlag.",10.1007/978-3-642-16373-9_25,2010.0,9.0,,Arbitrary permutations; Complexity analysis; Conjunctive queries; Entity-relationship model; Entity-Relationship schema; Extended model; Extended versions; Negative constraints; Ontology language; Query answering; Sufficient conditions; Syntactic characterization; Ontology; Query languages,2-s2.0-78649938489
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Shin, J., Wu, S., Wang, F., De Sa, C., Zhang, C., Ré, C., Incremental knowledge base construction using deepdive (2015) Proc. VLDB Endowment Int. Conf. Very Large Data Bases, 8 (11), pp. 1310-1321",Incremental knowledge base construction using deepdive,"Shin J., Wu S., Wang F., De Sa C., Zhang C., Ré C.",Incremental knowledge base construction using deepdive,"Populating a database with unstructured information is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. Recent names used for this problem include dealing with dark data and knowledge base construction (KBC). In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based respectively on sampling and variational techniques. We also study the tradeoff space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate Deep- Dive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality. © 2015 VLDB Endowment 21508097/15/07.",10.14778/2809974.2809991,2015.0,136.0,,Artificial intelligence; Database systems; Knowledge based systems; Learning systems; Variational techniques; Deep dives; Knowledge-base construction; Optimizers; Orders of magnitude; Rule based; Speed up; Standing problems; Iterative methods,2-s2.0-84953875555
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Zhang, C., (2015) Deepdive: A Data Management System for Automatic Knowledge Base Construction, pp. 1-205. , PhD Thesis, University of Wisconsin-Madison, Madison, Wisconsin",,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Graja, M., Jaoua, M., Belguith, L.H., Statistical framework with knowledge base integration for robust speech understanding of the tunisian dialect (2015) IEEE/ACM Trans. Audio, Speech Lang. Proc., 23 (12), pp. 2311-2321. , https://doi.org/10.1109/TASLP.2015.2464687, Dec. [Online]",Statistical framework with knowledge base integration for robust speech understanding of the tunisian dialect,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Lan, Y., Wang, S., Jiang, J., Knowledge base question answering with a matching-aggregation model and question-specific contextual relations (2019) IEEE/ACM Trans. Audio, Speech Lang. Proc., 27 (10), pp. 1629-1638. , https://doi.org/10.1109/TASLP.2019.2926125, Oct. [Online]",Knowledge base question answering with a matching-aggregation model and question-specific contextual relations,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Hendrickx, I., Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals (2010) Proc. 5th Int. Workshop Semantic Eval., pp. 33-38",Semeval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Riedel, S., Yao, L., McCallum, A., Modeling relations and their mentions without labeled text (2010) Proc. Joint Eur. Conf. Mach. Learn. Knowl. Discov. Databases, pp. 148-163. , Springer",Modeling relations and their mentions without labeled text,"Riedel S., Yao L., McCallum A.",Modeling relations and their mentions without labeled text,"Several recent works on relation extraction have been applying the distant supervision paradigm: instead of relying on annotated text to learn how to predict relations, they employ existing knowledge bases (KBs) as source of supervision. Crucially, these approaches are trained based on the assumption that each sentence which mentions the two related entities is an expression of the given relation. Here we argue that this leads to noisy patterns that hurt precision, in particular if the knowledge base is not directly related to the text we are working with. We present a novel approach to distant supervision that can alleviate this problem based on the following two ideas: First, we use a factor graph to explicitly model the decision whether two entities are related, and the decision whether this relation is mentioned in a given sentence; second, we apply constraint-driven semi-supervision to train this model without any knowledge about which sentences express the relations in our training KB. We apply our approach to extract relations from the New York Times corpus and use Freebase as knowledge base. When compared to a state-of-the-art approach for relation extraction under distant supervision, we achieve 31% error reduction. © 2010 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-642-15939-8_10,2010.0,544.0,,Error reduction; Factor graphs; Knowledge base; Knowledge basis; New york time; Noisy patterns; Problem-based; Relation extraction; State-of-the-art approach; Learning systems; Knowledge based systems,2-s2.0-77958036662
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Socher, R., Huval, B., Manning, C.D., Ng, A.Y., Semantic compositionality through recursive matrix-vector spaces (2012) Proc. 2012 Joint Conf. Empirical Methods Natural Lang. Process. Comput. Natural Lang. Learn., pp. 1201-1211",Semantic compositionality through recursive matrix-vector spaces,"Socher R., Huval B., Manning C.D., Ng A.Y.",Semantic compositionality through recursive matrix-vector spaces,"Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. © 2012 Association for Computational Linguistics.",,2012.0,870.0,,Lexical information; Natural languages; Propositional logic; Recursive neural networks; Semantic relationships; State-of-the-art performance; Vector representations; Vector space models; Formal logic; Neural networks; Semantics; Syntactics; Vector spaces; Natural language processing systems,2-s2.0-84870715081
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Liu, Y., Li, S., Wei, F., Ji, H., Relation classification via modeling augmented dependency paths (2016) IEEE/ACM Trans. Audio, Speech Lang. Proc., 24 (9), pp. 1585-1594. , https://doi.org/10.1109/TASLP.2016.2573050, Sep. [Online]",Relation classification via modeling augmented dependency paths,"Liu Y., Li S., Wei F., Ji H.",Relation classification via modeling augmented dependency paths,"Previous research on relation classification has verified the effectiveness of using dependency shortest paths or dependency subtrees. How to efficiently unify these two kinds of dependency information in relation classification is still an open problem. In this paper, we propose a novel structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop the dependency-based neural networks (DepNN) model which combines the advantages of the recursive neural network (RNN) and the convolutional neural network (CNN). In DepNN, RNN is designed to model the dependency subtrees since it is good at capturing the hierarchical structures. Then, the semantic representation in subtrees is passed to the nodes on the shortest path and CNN is used to get the most important features on the ADP. Experiments on the SemEval-2010 dataset show that the ADP structure including both the shortest dependency path and the attached subtrees is helpful to classify the semantic relations between two entities and our proposed method can achieve the state-of-the-art performance. © 2014 IEEE.",10.1109/TASLP.2016.2573050,2016.0,11.0,Convolution neural network; dependency subtree; recursive neural network; relation classification; shortest dependency path,Convolution; Graph theory; Neural networks; Semantics; Convolution neural network; Recursive neural networks; Relation classifications; shortest dependency path; Sub trees; Classification (of information),2-s2.0-84979995722
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Li, L., Wang, J., Li, J., Ma, Q., Wei, J., Relation classification via keyword-attentive sentence mechanism and synthetic stimulation loss (2019) IEEE/ACM Trans. Audio, Speech, Lang. Proc., 27 (9), pp. 1392-1404. , https://doi.org/10.1109/TASLP.2019.2921726, Sep. [Online]",Relation classification via keyword-attentive sentence mechanism and synthetic stimulation loss,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Yao, Y., Docred: A large-scale document-level relation extraction dataset (2019) Proc. 57th Annual Meeting Asso. Comput. Linguist., pp. 764-777",Docred: A large-scale document-level relation extraction dataset,"Yao Y., Ye D., Li P., Han X., Lin Y., Liu Z., Liu Z., Huang L., Zhou J., Sun M.",Docred: A large-scale document-level relation extraction dataset,"Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED. © 2019 Association for Computational Linguistics",,2020.0,36.0,,Computational linguistics; Extraction; Named entities; Plain text; Recent state; Relation extraction; Wikipedia; Large dataset,2-s2.0-85081561313
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Christopoulou, F., Miwa, M., Ananiadou, S., Connecting the dots: Document-level neural relation extraction with edge-oriented graphs (2019) Proc. Conf. Emp. Methods Natu. Lang. Process. 9th Inter. Joint (EMNLPIJCNLP), pp. 4927-4938",Connecting the dots: Document-level neural relation extraction with edge-oriented graphs,"Christopoulou F., Miwa M., Ananiadou S.",Connecting the dots: Document-level neural relation extraction with edge-oriented graphs,"Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.1. © 2019 Association for Computational Linguistics",,2020.0,26.0,,Extraction; Graph theory; Graphic methods; Natural language processing systems; Gene-disease associations; Inference mechanism; Logical inference; Multi-instance learning; Named entities; Neural modeling; Oriented graph; Relation extraction; Graph structures,2-s2.0-85084317655
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Nan, G., Guo, Z., Sekulić, I., Lu, W., Reasoning with latent structure refinement for document-level relation extraction (2020) Proc. 58th Ann. Meeting Asso. Comput. Linguist., pp. 1546-1557",Reasoning with latent structure refinement for document-level relation extraction,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Quirk, C., Poon, H., Distant supervision for relation extraction beyond the sentence boundary (2016) Proc. 15th Conf. Eur. Ch. Asso. Comput. Linguist.: Volume 1, Long Papers, pp. 1171-1182. , 2017",Distant supervision for relation extraction beyond the sentence boundary,"Quirk C., Poon H.",Distant supervision for relation extraction beyond the sentence boundary,"The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to crosssentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach. © 2017 Association for Computational Linguistics.",10.18653/v1/e17-1110,2017.0,70.0,,Computational linguistics; Extraction; Knowledge based systems; Biomedical research; Graph representation; Growing demand; Knowledge base; Modeling relations; Relation extraction; Sentence boundaries; Structured knowledge; Data mining,2-s2.0-85021675745
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Song, L., Zhang, Y., Wang, Z., Gildea, D., N-ary relation extraction using graph-state LSTM (2018) Proc. 2018 Conf. Empirical Methods Natural Lang. Process, pp. 2226-2235. , https://www.aclweb.org/anthology/D18-1246, Brussels, Belgium: Association for Computational Linguistics, Oct.-Nov. [Online]",N-ary relation extraction using graph-state LSTM,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Gupta, P., Rajaram, S., Schütze, H., Runkler, T., Neural relation extraction within and across sentence boundaries (2019) Proc. AAAI Conf. Artif. Intell., 33, pp. 6513-6520",Neural relation extraction within and across sentence boundaries,"Gupta P., Rajaram S., Schütze H., Runkler T.",Neural relation extraction within and across sentence boundaries,"Past work in relation extraction mostly focuses on binary relation between entity pairs within single sentence. Recently, the NLP community has gained interest in relation extraction in entity pairs spanning multiple sentences. In this paper, we propose a novel architecture for this task: inter-sentential dependency-based neural networks (iDepNN). iDepNN models the shortest and augmented dependency paths via recurrent and recursive neural networks to extract relationships within (intra-) and across (inter-) sentence boundaries. Compared to SVM and neural network baselines, iDepNN is more robust to false positives in relationships spanning sentences. We evaluate our models on four datasets from newswire (MUC6) and medical (BioNLP shared task) domains that achieve state-of-the-art performance and show a better balance in precision and recall for inter-sentential relationships. We perform better than 11 teams participating in the BioNLP shared task 2016 and achieve a gain of 5.2% (0.587 vs 0.558) in F1 over the winning team. We also release the cross-sentence annotations for MUC6. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2019.0,30.0,,Extraction; Support vector machines; Binary relation; False positive; Novel architecture; Precision and recall; Recursive neural networks; Relation extraction; Sentence boundaries; State-of-the-art performance; Recurrent neural networks,2-s2.0-85075427540
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Doddington, G., Mitchell, A., Przybocki, M., Ramshaw, L., Strassel, S., Weischedel, R., The automatic content extraction (ACE) program tasks, data, and evaluation (2004) Proc. LREC'04, pp. 837-840",and evaluation,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Mintz, M., Bills, S., Snow, R., Jurafsky, D., Distant supervision for relation extraction without labeled data (2009) Proc. Joint Conf. 47th Annu. Meeting ACL 4th Int. Joint Conf. Natural Lang. Process. AFNLP. Suntec, pp. 1003-1011. , https://www.aclweb.org/anthology/P09-1113, Singapore: Association for Computational Linguistics, Aug. [Online]",Distant supervision for relation extraction without labeled data,"Mintz M., Bills S., Snow R., Jurafsky D.",Distant supervision for relation extraction without labeled data,[No abstract available],,2009.0,1484.0,,,2-s2.0-77957863090
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Yu, D., Sun, K., Cardie, C., Yu, D., Dialogue-based relation extraction (2020) Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, pp. 4927-4940",Dialogue-based relation extraction,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Zeng, D., Liu, K., Lai, S., Zhou, G., Zhao, J., Relation classification via convolutional deep neural network (2014) Proc. 25th Int. Conf. Comput. Linguistics: Tech. Papers, pp. 2335-2344",Relation classification via convolutional deep neural network,"Zeng D., Liu K., Lai S., Zhou G., Zhao J.",Relation classification via convolutional deep neural network,"The state-of-the-art methods used for relation classification are primarily based on statistical machine learning, and their performance strongly depends on the quality of the extracted features. The extracted features are often derived from the output of pre-existing natural language processing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems. In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing. First, the word tokens are transformed to vectors by looking up word embeddings1. Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the features are fed into a softmax classifier to predict the relationship between two marked nouns. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.",,2014.0,917.0,,Artificial intelligence; Classification (of information); Computational linguistics; Convolution; Data mining; Learning algorithms; Learning systems; Linguistics; Deep neural networks; Feature vectors; NAtural language processing; Pre-processing; Relation classifications; Sentence level; State-of-the-art methods; Statistical machine learning; Natural language processing systems,2-s2.0-84959862537
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Xu, Y., Mou, L., Li, G., Chen, Y., Peng, H., Jin, Z., Classifying relations via long short term memory networks along shortest dependency paths (2015) Proc. EMNLP, pp. 1785-1794",Classifying relations via long short term memory networks along shortest dependency paths,"Xu Y., Mou L., Li G., Chen Y., Peng H., Jin Z.",Classifying relations via long short term memory networks along shortest dependency paths,"Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature. © 2015 Association for Computational Linguistics.",10.18653/v1/d15-1206,2015.0,343.0,,Brain; Classification (of information); Natural language processing systems; Heterogeneous information; Heterogeneous sources; Information integration; NAtural language processing; Neural architectures; Novel neural network; Relation classifications; Short term memory; Long short-term memory,2-s2.0-84959865227
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Guo, Z., Zhang, Y., Lu, W., Attention guided graph convolutional networks for relation extraction (2019) Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, pp. 241-251",Attention guided graph convolutional networks for relation extraction,"Guo Z., Zhang Y., Lu W.",Attention guided graph convolutional networks for relation extraction,"Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches. © 2019 Association for Computational Linguistics",,2020.0,77.0,,Computational linguistics; Convolution; Extraction; Forestry; Convolutional networks; Dependency structures; Dependency trees; Optimal results; Pruning strategy; Relation extraction; Research questions; Structural information; Trees (mathematics),2-s2.0-85084054426
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Lee, K., He, L., Lewis, M., Zettlemoyer, L., End-to-end neural coreference resolution (2017) Proc. 2017 Conf. Empirical Methods Natural Lang. Process, pp. 188-197. , https://www.aclweb.org/anthology/D17-1018, Copenhagen, Denmark: Association for Computational Linguistics, Sep. [Online]",End-to-end neural coreference resolution,"Lee K., He L., Lewis M., Zettlemoyer L.",End-to-end neural coreference resolution,"We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources. © 2017 Association for Computational Linguistics.",10.18653/v1/d17-1018,2017.0,231.0,,Benchmarking; Syntactics; Attention mechanisms; Boundary representations; Co-reference resolutions; Context dependent; External resources; Marginal likelihood; State-of-the-art performance; Syntactic parsers; Natural language processing systems,2-s2.0-85073172227
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., BERT: Pre-training of deep bidirectional transformers for language understanding (2019) Proc. Conf. North Amer. Ch. Asso. Comput. Linguist.: Human Lang. Technol., Volume 1 (Long and Short Papers), pp. 4171-4186",BERT: Pre-training of deep bidirectional transformers for language understanding,"Devlin J., Chang M.-W., Lee K., Toutanova K.",BERT: Pre-training of deep bidirectional transformers for language understanding,[No abstract available],,2018.0,6222.0,,,2-s2.0-85057019815
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., BERT: Pre-training of deep bidirectional transformers for language understanding (2019) Proc. Conf. North Amer. Ch. Asso. Comput. Linguist.: Human Lang. Technol., Volume 1 (Long and Short Papers), pp. 4171-4186",BERT: Pre-training of deep bidirectional transformers for language understanding,"Devlin J., Chang M.-W., Lee K., Toutanova K.",BERT: Pre-training of deep bidirectional transformers for language understanding,"We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). © 2019 Association for Computational Linguistics",,2019.0,6017.0,,Computational linguistics; Language inference; Language understanding; NAtural language processing; Output layer; Pre-training; Question Answering; Representation model; State of the art; Natural language processing systems,2-s2.0-85083815650
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., BERT: Pre-training of deep bidirectional transformers for language understanding (2019) Proc. Conf. North Amer. Ch. Asso. Comput. Linguist.: Human Lang. Technol., Volume 1 (Long and Short Papers), pp. 4171-4186",BERT: Pre-training of deep bidirectional transformers for language understanding,"Devlin J., Chang M.-W., Lee K., Toutanova K.",BERT: Pre-training of deep bidirectional transformers for language understanding,[No abstract available],,2018.0,660.0,,,2-s2.0-85063394559
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Vaswani, A., Attention is all you need (2017) Proc. NIPS, pp. 5998-6008",Attention is all you need,"Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser Ł., Polosukhin I.",Attention is all you need,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. © 2017 Neural information processing systems foundation. All rights reserved.",,2017.0,11162.0,,Convolution; Decoding; Network architecture; Program processors; Signal encoding; Attention mechanisms; Best model; Bleu scores; Convolutional neural network; Single models; Training costs; Two machines; Recurrent neural networks,2-s2.0-85043317328
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Vaswani, A., Attention is all you need (2017) Proc. NIPS, pp. 5998-6008",Attention is all you need,"Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I.",Attention is all you need,[No abstract available],,2017.0,3632.0,,,2-s2.0-85038368581
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Doddington, G.R., Mitchell, A., Przybocki, M.A., Ramshaw, L.A., Strassel, S.M., Weischedel, R.M., The automatic content extraction (Ace) program-tasks, data, and evaluation (2004) Proc. Lrec, 2 (1), pp. 837-840. , Lisbon",and evaluation,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Walker, C., Strassel, S., Medero, J., Maeda, K., Ace 2005 multilingual training corpus (2006) Linguistic Data Consortium, 57, p. 45. , https://catalog.ldc.upenn.edu/LDC2006T06, Philadelphia",Ace 2005 multilingual training corpus,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Nguyen, T.H., Grishman, R., Relation extraction: Perspective from convolutional neural networks (2015) Proc. 1st Workshop Vector Space Model. Natural Lang. Process, pp. 39-48",Relation extraction: Perspective from convolutional neural networks,"Nguyen T.H., Grishman R.",Relation extraction: Perspective from convolutional neural networks,[No abstract available],,2015.0,243.0,,,2-s2.0-84991022526
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Miwa, M., Bansal, M., End-to-end relation extraction using lstms on sequences and tree structures (2016) Proc. ACL, pp. 1105-1116. , Berlin, Germany:Association for Computational Linguistics",End-to-end relation extraction using lstms on sequences and tree structures,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Riedel, S., Yao, L., McCallum, A., Modeling relations and their mentions without labeled text (2010) Proc. Mach. Learn. Knowl. Discovery Databases, pp. 148-163. , J. L. Balcázar, F. Bonchi, A. Gionis, and M. Sebag, Eds. Berlin, Heidelberg: Springer Berlin Heidelberg",Modeling relations and their mentions without labeled text,"Riedel S., Yao L., McCallum A.",Modeling relations and their mentions without labeled text,"Several recent works on relation extraction have been applying the distant supervision paradigm: instead of relying on annotated text to learn how to predict relations, they employ existing knowledge bases (KBs) as source of supervision. Crucially, these approaches are trained based on the assumption that each sentence which mentions the two related entities is an expression of the given relation. Here we argue that this leads to noisy patterns that hurt precision, in particular if the knowledge base is not directly related to the text we are working with. We present a novel approach to distant supervision that can alleviate this problem based on the following two ideas: First, we use a factor graph to explicitly model the decision whether two entities are related, and the decision whether this relation is mentioned in a given sentence; second, we apply constraint-driven semi-supervision to train this model without any knowledge about which sentences express the relations in our training KB. We apply our approach to extract relations from the New York Times corpus and use Freebase as knowledge base. When compared to a state-of-the-art approach for relation extraction under distant supervision, we achieve 31% error reduction. © 2010 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-642-15939-8_10,2010.0,544.0,,Error reduction; Factor graphs; Knowledge base; Knowledge basis; New york time; Noisy patterns; Problem-based; Relation extraction; State-of-the-art approach; Learning systems; Knowledge based systems,2-s2.0-77958036662
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Zeng, D., Liu, K., Chen, Y., Zhao, J., Distant supervision for relation extraction via piecewise convolutional neural networks (2015) Proc. Conf. Empirical Methods Natural Lang. Process, pp. 1753-1762",Distant supervision for relation extraction via piecewise convolutional neural networks,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Lin, Y., Shen, S., Liu, Z., Luan, H., Sun, M., Neural relation extraction with selective attention over instances (2016) Proc. 54th Annu. Meeting Assoc. Comput. Linguistics, 1, pp. 2124-2133",Neural relation extraction with selective attention over instances,"Lin Y., Shen S., Liu Z., Luan H., Sun M.",Neural relation extraction with selective attention over instances,"Distant supervised relation extraction has been widely used to find novel relational facts from text. However, distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the performance of relation extraction. To alleviate this issue, we propose a sentence-level attention-based model for relation extraction. In this model, we employ convolutional neural networks to embed the semantics of sentences. Afterwards, we build sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances. Experimental results on real-world datasets show that, our model can make full use of all informative sentences and effectively reduce the influence of wrong labelled instances. Our model achieves significant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https://github.com/thunlp/NRE. © 2016 Association for Computational Linguistics.",10.18653/v1/p16-1200,2016.0,552.0,,Computational linguistics; Neural networks; Semantics; Convolutional neural network; Multiple instances; Noisy data; Real-world datasets; Relation extraction; Selective attention; Sentence level; Source codes; Extraction,2-s2.0-85011945255
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Li, J., Biocreative v CDR task corpus: A resource for chemical disease relation extraction (2016) Database, 1, p. 10",Biocreative v CDR task corpus: A resource for chemical disease relation extraction,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Wu, Y., Luo, R., Leung, H.C., Ting, H.-F., Lam, T.-W., Renet: A deep learning approach for extracting gene-disease associations from literature (2019) Proc. Int. Conf. Res. Comput. Mol. Biol., pp. 272-284. , Springer",Renet: A deep learning approach for extracting gene-disease associations from literature,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Peng, N., Poon, H., Quirk, C., Toutanova, K., Yih, W.-T., Cross-sentence n-ary relation extractionwith graph lstms (2017) Trans. Assoc. Comput. Linguistics, 5, pp. 101-115. , https://www.aclweb.org/anthology/Q17-1008, [Online]",Cross-sentence n-ary relation extractionwith graph lstms,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Sahu, S.K., Christopoulou, F., Miwa, M., Ananiadou, S., Inter-sentence relation extraction with document-level graph convolutional neural network (2019) Proc. 57th Annual Meeting Assoc. Comput. Linguist., pp. 4309-4316",Inter-sentence relation extraction with document-level graph convolutional neural network,"Sahu S.K., Christopoulou F., Miwa M., Ananiadou S.",Inter-sentence relation extraction with document-level graph convolutional neural network,"Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction. © 2019 Association for Computational Linguistics",,2020.0,30.0,,Computational linguistics; Convolution; Extraction; Semantics; Dependency informations; Level graphs; Multi-instance learning; Neural models; Relation extraction; Semantic dependency; Semantic relationships; State of the art; Convolutional neural networks,2-s2.0-85084031927
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Manning, C.D., Surdeanu, M., Bauer, J., Finkel, J., Bethard, S.J., McClosky, D., The stanford corenlp natural language processing toolkit (2014) Proc. 52nd ACL, pp. 55-60. , Sep",The stanford corenlp natural language processing toolkit,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Kipf, T.N., Welling, M., Semi-supervised classification with graph convolutional networks (2017) Proc. ICLR, pp. 1-14. , Toulon, France",Semi-supervised classification with graph convolutional networks,"Kipf T.N., Welling M.",Semi-supervised classification with graph convolutional networks,We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin. © ICLR 2019 - Conference Track Proceedings. All rights reserved.,,2017.0,3007.0,,Convolution; Graphic methods; Machine learning; Neural networks; Citation networks; Convolutional networks; Convolutional neural network; First-order approximations; Graph structured data; Scalable approach; Semi- supervised learning; Semi-supervised classification; Supervised learning,2-s2.0-85086180249
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Kingma, D.P., Ba, J., (2014) Adam: A Method for Stochastic Optimization",,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Wang, H., Focke, C., Sylvester, R., Mishra, N., Wang, W., (2019) Fine-tune Bert for Docred with Two-step Process",,,,,,,,,,
2-s2.0-85107200162,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text," Sukhbaatar, S., Szlam, A., Weston, J., Fergus, R., End-to-end memory networks (2015) Proc. Adv. Neural Inf. Process. Syst., 28, pp. 2440-2448. , C. N. Cortes, D. Lawrence, M. Lee Sugiyama, and R. Garnett, Eds., Curran Associates, Inc",End-to-end memory networks,"Sukhbaatar S., Szlam A., Weston J., Fergus R.",End-to-end memory networks,"We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.",,2015.0,1139.0,,Information science; Natural language processing systems; Attention model; End to end; External memory; Language model; Memory network; Question Answering; Treebanks; Modeling languages,2-s2.0-84965143740
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case,"Aharrane, N., El Moutaouakil, K., Satori, K., (2015) A comparison of supervised classification methods for a statistical set of features: application: amazigh OCR, pp. 1-8. , 2015 Intelligent Systems and Computer Vision (ISCV), IEEE",,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Aizawa, A., An information-theoretic perspective of tf–idf measures (2003) Information Processing and Management, 39 (1), pp. 45-65",An information-theoretic perspective of tf–idf measures,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Akhtar, N., Qureshi, M.N., Ahamad, M.V., An improved clustering method for text documents using neutrosophic logic (2017) Applications of Soft Computing for the Web, pp. 167-179. , Springer, Singapore",An improved clustering method for text documents using neutrosophic logic,"Akhtar N., Qureshi M.N., Ahamad M.V.",An improved clustering method for text documents using neutrosophic logic,[No abstract available],,2017.0,1.0,,,2-s2.0-85103915355
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Ansari, A.Q., Biswas, R., Aggarwal, S., Neutrosophic classifier: an extension of fuzzy classifer (2013) Applied Soft Computing, 13 (1), pp. 563-573",Neutrosophic classifier: an extension of fuzzy classifer,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Bounabi, M., El Moutaouakil, K., Satori, K., A comparison of text classification methods method of weighted terms selected by different stemming techniques (2017) in Proceedings of the 2nd international Conference on Big Data, Cloud and Applications, p. 43. , p., ACM",A comparison of text classification methods method of weighted terms selected by different stemming techniques,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Bounabi, M., El Moutaouakil, K., Satori, K., A probabilistic vector representation and neural network for text classification (2018) International Conference on Big Data, Cloud and Applications, pp. 343-355. , Springer, Cham",A probabilistic vector representation and neural network for text classification,"Bounabi M., Moutaouakil K.E., Satori K.",A probabilistic vector representation and neural network for text classification,"The increasing of the textual databases and its representation in large spaces prevents the automation of the treatment of these great masses and the extraction of knowledge. In order to address the challenges of high dimensionality which using the methods and technics of the text mining. Where the term frequency-inverse document frequency (TF-IDF), weighting method, is the most required approach to represent the document. Unfortunately, TF-IDF produces descriptors of large sizes (generally greater than 1000), which requires models with great complexity. However, the texts classification systems based on these models suffer from the overfitting phenomenon and are very slow. Therefore, to overcome these problems, we use the select attributes methods; by giving the deterministic aspect of this latter, we risk to lose huge information. Thus, to recover from this loss, we propose a probabilistic vector representation of each document, based on the relevant terms selected previously. Then, we associate a set of features to each document composed by local and global probabilistic coefficients basing on the selected terms. More specifically and precisely, the components formulas are composed by the frequency of each descriptor, the length of each document and the size of the corpus. To show the performance of this treatment we propose comparative studies between TF-IDF representation and the new probabilistic representation, to classify the BBCSPORT corpus. Moreover, in the classification phase, we use several versions of Bayesian Network and Multilayer Perceptron. The obtained results are satisfied, where the neural network classifier, multilayer perceptron, gives 100% as a recognition rate, using the new representation and 94.69%, using the simple TF-IDF weighting. © Springer Nature Switzerland AG 2018.",10.1007/978-3-319-96292-4_27,2018.0,8.0,Bayes Net; Feature selection; Multilayer perceptron; Probabilistic representation; Select attributes; Text classification,Bayesian networks; Classification (of information); Data mining; Feature extraction; Inverse problems; Multilayer neural networks; Multilayers; Bayes net; Classification system; Neural network classifier; Probabilistic representation; Probabilistic vector; Select attributes; Term frequencyinverse document frequency (TF-IDF); Text classification; Text processing,2-s2.0-85063738781
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Bounabi, M., El Moutaouakil, K., Satori, K., Association models to select the best rules for fuzzy inference system (2020) Embedded Systems and Artificial Intelligence, pp. 349-357. , Springer, Singapore",Association models to select the best rules for fuzzy inference system,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Bounabi, M., Moutaouakil, K.E., Satori, K., A comparison of text classification methods using different stemming techniques (2019) International Journal of Computer Applications in Technology, 60 (4), pp. 298-306",A comparison of text classification methods using different stemming techniques,"Bounabi M., El Moutaouakil K., Satori K.",A comparison of text classification methods using different stemming techniques,"In the retrieval of information, two factors have an important impact on the performance of systems: the extract features and the matching process. In this work, we compare three well-known stemming techniques: Lovins stemmer, iterated Lovins and snowball stemmer. Concerning the classification phase, we compare, experimentally, six methods: BNET, NBMU, CNB, RF, SLogicF, and SVM. Basing on this comparison, we propose a new retrieval system by calling the voting method, as a matching tool, to improve the performance of the classical systems. In this paper, we use the TF-IDF algorithm to extract features. The envisaged systems are tested on two databases: BBCNEWS and BBCSPORT. The systems based on Lovins stemmers and on the voting technique give the best results. In fact, for the first databases, the best accuracy observed is for the system Lovins + Vote with a recognition rate of 97%. Concerning the second database, the system snowball +Vote gives us 99% as recognition rate. Copyright © 2019 Inderscience Enterprises Ltd.",10.1504/IJCAT.2019.101171,2019.0,10.0,Classification; CNB; NB; NBMU; RF; SLogiF; Stemmer; SVM; Term-weighting; Voting technique,Classification (of information); Database systems; Natural language processing systems; Niobium; Rayon yarn; Support vector machines; Text processing; NBMU; SLogiF; Stemmer; Term weighting; Voting techniques; Search engines,2-s2.0-85069942172
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Broumi, S., Bakali, A., Talea, M., Smarandache, F., Dey, A., Spanning tree problem with neutrosophic edge weights (2018) Procedia Computer Science, 127, pp. 190-199",Spanning tree problem with neutrosophic edge weights,"Broumi S., Bakali A., Talea M., Smarandache F., Dey A., Sonf L.H.",Spanning tree problem with neutrosophic edge weights,"Neutrosophic set and neutrosophic logic theory are renowned theories to deal with complex, not clearly explained and uncertain real life problems, in which classical fuzzy sets/models may fail to model properly. This paper introduces an algorithm for finding minimum spanning tree (MST) of an undirected neutrosophic weighted connected graph (abbr. UNWCG) where the arc/edge lengths are represented by a single valued neutrosophic numbers. To build the MST of UNWCG, a new algorithm based on matrix approach has been introduced. The proposed algorithm is compared to other existing methods and finally a numerical example is provided. © 2018 The Authors. Published by Elsevier B.V.",10.1016/j.procs.2018.01.114,2018.0,16.0,Minimum spanning tree problem; Neutrosophic matrix; Score function; Single valued neutrosophic sets,Clustering algorithms; Computation theory; Data Science; Graph algorithms; Intelligent computing; Matrix algebra; Numerical methods; Connected graph; Minimum Spanning Tree problem; Minimum spanning trees; Neutrosophic logic; Neutrosophic sets; Real-life problems; Score function; Spanning tree problems; Trees (mathematics),2-s2.0-85045676559
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Chen, K., Zhang, Z., Long, J., Zhang, H., Turning from TF-IDF to TF-IGM for term weighting in text classification (2016) Expert Systems with Applications, 66, pp. 245-260",Turning from TF-IDF to TF-IGM for term weighting in text classification,"Chen K., Zhang Z., Long J., Zhang H.",Turning from TF-IDF to TF-IGM for term weighting in text classification,"Massive textual data management and mining usually rely on automatic text classification technology. Term weighting is a basic problem in text classification and directly affects the classification accuracy. Since the traditional TF-IDF (term frequency & inverse document frequency) is not fully effective for text classification, various alternatives have been proposed by researchers. In this paper we make comparative studies on different term weighting schemes and propose a new term weighting scheme, TF-IGM (term frequency & inverse gravity moment), as well as its variants. TF-IGM incorporates a new statistical model to precisely measure the class distinguishing power of a term. Particularly, it makes full use of the fine-grained term distribution across different classes of text. The effectiveness of TF-IGM is validated by extensive experiments of text classification using SVM (support vector machine) and kNN (k nearest neighbors) classifiers on three commonly used corpora. The experimental results show that TF-IGM outperforms the famous TF-IDF and the state-of-the-art supervised term weighting schemes. In addition, some new findings different from previous studies are obtained and analyzed in depth in the paper. © 2016 Elsevier Ltd",10.1016/j.eswa.2016.09.009,2016.0,137.0,Class distinguishing power; Classifier; Inverse gravity moment (IGM); Term weighting; Text classification,Classification (of information); Classifiers; Information management; Information retrieval systems; Nearest neighbor search; Support vector machines; Automatic text classification; Class distinguishing power; Classification accuracy; Inverse Document Frequency; Inverse gravity moment (IGM); SVM(support vector machine); Term weighting; Text classification; Text processing,2-s2.0-84987860660
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Dang, S., Ahmad, P.H., Text mining: Techniques and its application (2014) International Journal of Engineering and Technology Innovations, 1 (4), pp. 866-2348",Text mining: Techniques and its application,"Dang S., Ahmad P.H.",Text mining: Techniques and its application,[No abstract available],,2014.0,23.0,,,2-s2.0-85044921073
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Dunne, R.A., Campbell, N.A., On the pairing of the softmax activation and cross-entropy penalty functions and the derivation of the softmax activation function (1997) in Proc. 8th Aust. Conf. on the Neural Networks, 181, p. 185. , p., Melbourne",On the pairing of the softmax activation and cross-entropy penalty functions and the derivation of the softmax activation function,"Dunne R.A., Campbell N.A.",On the pairing of the softmax activation and cross-entropy penalty functions and the derivation of the softmax activation function,[No abstract available],,1997.0,125.0,,,2-s2.0-27544483995
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Feng, X., Liang, Y., Shi, X., Xu, D., Wang, X., Guan, R., Overfitting reduction of text classification based on AdaBELM (2017) Entropy, 19 (7), p. 330",Overfitting reduction of text classification based on AdaBELM,"Feng X., Liang Y., Shi X., Xu D., Wang X., Guan R.",Overfitting reduction of text classification based on AdaBELM,"Overfitting is an important problem in machine learning. Several algorithms, such as the extreme learning machine (ELM), suffer from this issue when facing high-dimensional sparse data, e.g., in text classification. One common issue is that the extent of overfitting is not well quantified. In this paper, we propose a quantitative measure of overfitting referred to as the rate of overfitting (RO) and a novel model, named AdaBELM, to reduce the overfitting. With RO, the overfitting problem can be quantitatively measured and identified. The newly proposed model can achieve high performance on multi-class text classification. To evaluate the generalizability of the new model, we designed experiments based on three datasets, i.e., the 20 Newsgroups, Reuters-21578, and BioMed corpora, which represent balanced, unbalanced, and real application data, respectively. Experiment results demonstrate that AdaBELM can reduce overfitting and outperform classical ELM, decision tree, random forests, and AdaBoost on all three text-classification datasets; for example, it can achieve 62.2% higher accuracy than ELM. Therefore, the proposed model has a good generalizability. © 2017 by the authors.",10.3390/e19070330,2017.0,11.0,AdaBoost; Extreme learning machine; Feedforward neural network; Machine learning; Overfitting,,2-s2.0-85022182795
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Ge, L., Moh, T.S., Improving text classification with word embedding (2017) in 2017 IEEE International Conference on Big Data (Big Data), pp. 1796-1805. , IEEE",Improving text classification with word embedding,"Ge L., Moh T.-S.",Improving text classification with word embedding,"One challenge in text classification is that it is difficult to make feature reductions based on the definition of the features. An ineffective feature reduction may even worsen the classification accuracy. Word2Vec, a word embedding method, has recently been gaining popularity due to its high precision rate of analyzing the semantic similarity between words at relatively low computational cost. However, there is limited research about feature reduction using Word2Vec. In this project, we developed a method using Word2Vec to reduce the feature size while increasing the classification accuracy. We achieved feature reduction by loosely clustering similar features using graph search techniques. Similarity thresholds above 0.5 were used in our method to pair and cluster the features. Finally, we utilized Multinomial Naïve Bayes classifier, Support Vector Machine, K Nearest Neighbor and Random Forest classifier to evaluate the effect of our method. Four datasets with dimensions up to 100,000 feature size and 400,000 document size were used to evaluate the result of our method. The result showed that around 4-10% feature reduction was achieved with up to 1-4% improvement of classification accuracy in terms of different datasets and classifiers. Meanwhile, we also succeeded in improving feature reduction and classification accuracy by combining our method with other classic feature reduction techniques such as chi-square and mutual information. © 2017 IEEE.",10.1109/BigData.2017.8258123,2017.0,21.0,Feature reduction; KNN; Machine learning; Naïve Bayes; SVM; Word embedding; Word2Vec,Big data; Data reduction; Decision trees; Embeddings; Learning systems; Nearest neighbor search; Reduction; Semantics; Support vector machines; Text processing; Classification accuracy; Computational costs; Feature reduction; Mutual informations; Random forest classifier; Similarity threshold; Word embedding; Word2Vec; Classification (of information),2-s2.0-85047810548
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Greene, D., Cunningham, P., Practical solutions to the problem of diagonal dominance in kernel document clustering (2006) in Proceedings of the 23rd international conference on Machine learning, pp. 377-384",Practical solutions to the problem of diagonal dominance in kernel document clustering,"Greene D., Cunningham P.",Practical solutions to the problem of diagonal dominance in kernel document clustering,"In supervised kernel methods, it has been observed that the performance of the SVM classifier is poor in cases where the diagonal entries of the Gram matrix are large relative to the off-diagonal entries. This problem, referred to as diagonal dominance, often occurs when certain kernel functions are applied to sparse high-dimensional data, such as text corpora. In this paper we investigate the implications of diagonal dominance for unsupervised kernel methods, specifically in the task of document clustering. We propose a selection of strategies for addressing this issue, and evaluate their effectiveness in producing more accurate and stable clusterings.",10.1145/1143844.1143892,2006.0,153.0,,Classification (of information); Clustering algorithms; Matrix algebra; Problem solving; Diagonal dominance; Document clustering; Gram matrix; High-dimensional data; Kernel functions; SVM classifier; Text corpora; Support vector machines,2-s2.0-34250762673
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Gupta, Y., Saini, A., Saxena, A.K., A new fuzzy logic based ranking function for efficient information retrieval system (2015) Expert Systems with Applications, 42 (3), pp. 1223-1234",A new fuzzy logic based ranking function for efficient information retrieval system,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Hamon, D., (2016) System and method providing a binary representation of a web page, , (accessed U.S. Patent, 29 March 2016",,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Jang, J.S., ANFIS: adaptive-network-based fuzzy inference system (1993) IEEE Transactions on Systems, Man, and Cybernetics, 23 (3), pp. 665-685",ANFIS: adaptive-network-based fuzzy inference system,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Jones, K.S., A statistical interpretation of term specificity and its application in retrieval (1972) Journal of Documentation",A statistical interpretation of term specificity and its application in retrieval,Jones K.S.,A statistical interpretation of term specificity and its application in retrieval,"The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure. © 1972, MCB UP Limited",10.1108/eb026526,1972.0,2032.0,,,2-s2.0-84953744816
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Kandasamy, I., Vasantha, W.B., Obbineni, J.M., Smarandache, F., Sentiment analysis of tweets using refined neutrosophic sets (2020) Computers in Industry, 115, p. 103180",Sentiment analysis of tweets using refined neutrosophic sets,"Kandasamy I., Vasantha W.B., Obbineni J.M., Smarandache F.",Sentiment analysis of tweets using refined neutrosophic sets,"In the last decade, opinion mining and sentiment analysis have been the subject of fascinating interdisciplinary research. Alongside the evolution of social media networks, the sheer volume of social media text available for sentiment analysis has increased multi-fold, leading to a formidable corpus. Sentiment analysis of tweets have been carried out to gauge public opinion on breaking news, various policies, legislations, personalities and social movements. Fuzzy logic has been used in the sentiment analysis of twitter data, whereas neutrosophy which factors in the concept of indeterminacy has not been used to analyse tweets. In this paper, the concept of multi refined neutrosophic set (MRNS) with two positive, three indeterminate and two negative memberships is proposed. Single valued neutrosophic set (SVNS), triple refined indeterminate neutrosophic set (TRINS) and MRNS have been used in the sentiment analysis of tweets on ten different topics. Eight of these topics chosen for sentiment analysis are related to Indian scenario and two topics to international scenario. A comparative analysis of the methods show that the approach with MRNS provides better refinement to the indeterminacy present in the data. © 2019 Elsevier B.V.",10.1016/j.compind.2019.103180,2020.0,19.0,Indeterminacy; Multi refined neutrosophic set (MRNS); Neutrosophic logic; Neutrosophy; Opinion mining; Refined neutrosophic sets; Sentiment analysis; Single valued neutrosophic set (SVNS); Social media; Text mining; Triple refined indeterminate neutrosophic set (TRINS); Tweets,Computer circuits; Data mining; Fuzzy logic; Social aspects; Social networking (online); Indeterminacy; Neutrosophic logic; Neutrosophic sets; Neutrosophy; Opinion mining; Social media; Text mining; Tweets; Sentiment analysis,2-s2.0-85076731765
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Ko, Y., A new term‐weighting scheme for text classification using the odds of positive and negative class probabilities (2015) Journal of the Association for Information Science and Technology, 66 (12), pp. 2553-2565",A new term‐weighting scheme for text classification using the odds of positive and negative class probabilities,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Kou, G., Yang, P., Peng, Y., Xiao, F., Chen, Y., Alsaadi, F.E., Evaluation of feature selection methods for text classification with small datasets using multiple criteria decision-making methods (2020) Applied Soft Computing, 86, p. 105836",Evaluation of feature selection methods for text classification with small datasets using multiple criteria decision-making methods,"Kou G., Yang P., Peng Y., Xiao F., Chen Y., Alsaadi F.E.",Evaluation of feature selection methods for text classification with small datasets using multiple criteria decision-making methods,"The evaluation of feature selection methods for text classification with small sample datasets must consider classification performance, stability, and efficiency. It is, thus, a multiple criteria decision-making (MCDM) problem. Yet there has been few research in feature selection evaluation using MCDM methods which considering multiple criteria. Therefore, we use MCDM-based methods for evaluating feature selection methods for text classification with small sample datasets. An experimental study is designed to compare five MCDM methods to validate the proposed approach with 10 feature selection methods, nine evaluation measures for binary classification, seven evaluation measures for multi-class classification, and three classifiers with 10 small datasets. Based on the ranked results of the five MCDM methods, we make recommendations concerning feature selection methods. The results demonstrate the effectiveness of the used MCDM-based method in evaluating feature selection methods. © 2019 The Author(s)",10.1016/j.asoc.2019.105836,2020.0,121.0,Feature selection; MCDM; Small sample dataset; Text classification,Decision making; Feature extraction; Text processing; Classification performance; Feature selection methods; MCDM; Multi-class classification; Multiple criteria decision making; Multiple criteria decision-making problems; Small samples; Text classification; Classification (of information),2-s2.0-85075378577
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Krstajic, D., Buturovic, L.J., Leahy, D.E., Thomas, S., Cross-validation pitfalls when selecting and assessing regression and classification models (2014) Journal of Cheminformatics, 6 (1), pp. 1-15",Cross-validation pitfalls when selecting and assessing regression and classification models,"Krstajic D., Buturovic L.J., Leahy D.E., Thomas S.",Cross-validation pitfalls when selecting and assessing regression and classification models,"Background: We address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches. Methods. We describe in detail an algorithm for repeated grid-search V-fold cross-validation for parameter tuning in classification and regression, and we define a repeated nested cross-validation algorithm for model assessment. As regards variable selection and parameter tuning we define two algorithms (repeated grid-search cross-validation and double cross-validation), and provide arguments for using the repeated grid-search in the general case. Results: We show results of our algorithms on seven QSAR datasets. The variation of the prediction performance, which is the result of choosing different splits of the dataset in V-fold cross-validation, needs to be taken into account when selecting and assessing classification and regression models. Conclusions: We demonstrate the importance of repeating cross-validation when selecting an optimal model, as well as the importance of repeating nested cross-validation when assessing a prediction error. © 2014 Krstajic et al.; licensee Chemistry Central Ltd.",10.1186/1758-2946-6-10,2014.0,300.0,,,2-s2.0-84899084283
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Liang, W.E.I., Exploration on translation of the literary term ambiguity (2017) China Terminology, (4), p. 9",Exploration on translation of the literary term ambiguity,Liang W.E.I.,Exploration on translation of the literary term ambiguity,[No abstract available],,2017.0,1.0,,,2-s2.0-85103886759
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Liu, C.Z., Sheng, Y.X., Wei, Z.Q., Yang, Y.Q., Research of text classification based on improved TF-IDF algorithm (2018) in 2018 IEEE International Conference of Intelligent Robotic and Control Engineering (IRCE), pp. 218-222. , August)., IEEE",Research of text classification based on improved TF-IDF algorithm,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Luhn, H.P., The automatic creation of literature abstracts (1958) IBM Journal of Research and Development, 2 (2), pp. 159-165",The automatic creation of literature abstracts,Luhn H.P.,The automatic creation of literature abstracts,[No abstract available],,1958.0,1619.0,,,2-s2.0-0000880768
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Mullai, M., Broumi, S., Stephen, A., Shortest path problem by minimal spanning tree algorithm using bipolar neutrosophic numbers (2017) International Journal of Mathematic Trends and Technology, 46 (2), pp. 80-87",Shortest path problem by minimal spanning tree algorithm using bipolar neutrosophic numbers,"Mullai M., Broumi S., Stephen A.",Shortest path problem by minimal spanning tree algorithm using bipolar neutrosophic numbers,[No abstract available],,2017.0,12.0,,,2-s2.0-85044095583
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Nancy Garg, H., An improved score function for ranking neutrosophic sets and its application to decision making process (2016) International Journal for Uncertainty Quantification, 6 (5), pp. 377-385",An improved score function for ranking neutrosophic sets and its application to decision making process,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Rajman, M., Besançon, R., Text mining: natural language techniques and text mining applications (1998) Data Mining and Reverse Engineering, pp. 50-64. , Springer, Boston, MA",Text mining: natural language techniques and text mining applications,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Rao, D.H., Saraf, S.S., Study of defuzzification methods of fuzzy logic controller for speed control of a DC motor (1996) in Proceedings of International Conference on Power Electronics, Drives and Energy Systems for Industrial Growth, 2, pp. 782-787. , IEEE",Study of defuzzification methods of fuzzy logic controller for speed control of a DC motor,"Rao D.H., Saraf S.S.",Study of defuzzification methods of fuzzy logic controller for speed control of a DC motor,"A typical Fuzzy Logic Controller (FLC) has the following components: fuzzification, knowledge base, decision making and defuzzification. Various defuzzification techniques have been proposed in the literature. The efficacy of a FLC depends very much on the defuzzification process. This is so because the overall performance of the system under control is determined by the controlling signal (the defuzzified output of the FLC) the system receives. The aim of this paper is to evaluate qualitatively the performance of the different defuzzification techniques as applied to speed control of a DC motor.",,1996.0,23.0,,Control theory; DC motors; Decision making; Error analysis; Fuzzy sets; Knowledge based systems; Performance; Speed control; Defuzzification methods; Fuzzification; Fuzzy control,2-s2.0-0029702850
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Ropero, J., Gómez, A., León, C., Carrasco, A., Term weighting: novel fuzzy logic based method vs classical TF-IDF method for web information extraction (2009) ICEIS, 2, pp. 130-137",Term weighting: novel fuzzy logic based method vs classical TF-IDF method for web information extraction,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Smarandache, F., (2002) Neutrosophy, a new branch of philosophy, , Infinite Study",,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Smarandache, F., Definiton of neutrosophic logic – a generalization of the intuitionistic fuzzy logic (2003) In EUSFLAT Conf, pp. 141-146",Definiton of neutrosophic logic – a generalization of the intuitionistic fuzzy logic,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Smarandache, F., Neutrosophic set–a generalization of the intuitionistic fuzzy set (2010) Journal of Defense Resources Management (JoDRM), 1 (1), pp. 107-116",Neutrosophic set–a generalization of the intuitionistic fuzzy set,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," (2003), Smarandache, F. (Ed.) (, Infinite Study",,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Svozil, D., Kvasnicka, V., Pospichal, J., Introduction to multi-layer feed-forward neural networks (1997) Chemometrics and Intelligent Laboratory Systems, 39 (1), pp. 43-62",Introduction to multi-layer feed-forward neural networks,"Svozil D., Kvasnička V., Pospíchal J.",Introduction to multi-layer feed-forward neural networks,Basic definitions concerning the multi-layer feed-forward neural networks are given. The back-propagation training algorithm is explained. Partial derivatives of the objective function with respect to the weight and threshold coefficients are derived. These derivatives are valuable for an adaptation process of the considered neural network. Training and generalisation of multi-layer feed-forward neural networks are discussed. Improvements of the standard back-propagation algorithm are reviewed. Example of the use of multi-layer feed-forward neural networks for prediction of carbon-13 NMR chemical shifts of alkanes is given. Further applications of neural networks in chemistry are reviewed. Advantages and disadvantages of multilayer feed-forward neural networks are discussed.,10.1016/S0169-7439(97)00061-0,1997.0,726.0,Back-propagation network; Neural networks,algorithm; analytic method; artificial neural network; carbon nuclear magnetic resonance; conference paper; internet; priority journal; protein folding; quantitative structure activity relation; spectroscopy,2-s2.0-0342871690
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Tharwat, A., Parameter investigation of support vector machine classifier with kernel functions (2019) Knowledge and Information Systems, 61 (3), pp. 1269-1302",Parameter investigation of support vector machine classifier with kernel functions,Tharwat A.,Parameter investigation of support vector machine classifier with kernel functions,"Support vector machine (SVM) is one of the well-known learning algorithms for classification and regression problems. SVM parameters such as kernel parameters and penalty parameter have a great influence on the complexity and performance of predicting models. Hence, the model selection in SVM involves the penalty parameter and kernel parameters. However, these parameters are usually selected and used as a black box, without understanding the internal details. In this paper, the behavior of the SVM classifier is analyzed when these parameters take different values. This analysis consists of illustrative examples, visualization, and mathematical and geometrical interpretations with the aim of providing the basics of kernel functions with SVM and to show how it works to serve as a comprehensive source for researchers who are interested in this field. This paper starts by highlighting the definition and underlying principles of SVM in details. Moreover, different kernel functions are introduced and the impact of each parameter in these kernel functions is explained from different perspectives. © 2019, Springer-Verlag London Ltd., part of Springer Nature.",10.1007/s10115-019-01335-4,2019.0,35.0,Gaussian kernel; Kernel functions; Linear kernel; Parameter optimization; Polynomial kernel; Radial basis function; Support vector machine (SVM),Functions; Learning algorithms; Support vector regression; Gaussian kernels; Kernel function; Linear kernel; Parameter optimization; Polynomial kernels; Radial basis functions; Support vector machines,2-s2.0-85060920705
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," WangSmarandacheZhang, H., Sunderraman, R., (2010) Single valued neutrosophic sets. Infinite study",,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Wu, H.C., Luk, R.W.P., Wong, K.F., Kwok, K.L., Interpreting tf-idf term weights as making relevance decisions (2008) ACM Transactions on Information Systems (TOIS), 26 (3), pp. 1-37",Interpreting tf-idf term weights as making relevance decisions,,,,,,,,,
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," Zadeh, L.A., Fuzzy sets (1965) Information and Control, 8 (3), pp. 338-353",Fuzzy sets,Zadeh L.A.,Fuzzy sets,"A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint. © 1965 Academic Press, Inc.",10.1016/S0019-9958(65)90241-X,1965.0,55488.0,,,2-s2.0-34248666540
2-s2.0-85103899477,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case," ZhangWang, H.Y., Chen, X.H., (2014) Interval neutrosophic sets and their application in multicriteria decision making problems, p. 645953. , p., Sci. World J. 2014",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity,"Ezzat, M., A comprehensive proposition of urbanism (2019) New Metropolitan Perspectives, ISHT 2018, Regio Calabria, Italy. Smart Innovation, Systems and Technologies, 100, pp. 433-443. , Springer, Cham",A comprehensive proposition of urbanism,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Ezzat, M., A framework for a comprehensive conceptualization of urban constructs: SpatialNet and SpatialFeaturesNet for computer-aided creative urban design (2020) RE: Anthropocene, Proceedings of the 25th International Conference of the Association for Computer-Aided Architectural Design Research in Asia (CAADRIA)",A framework for a comprehensive conceptualization of urban constructs: SpatialNet and SpatialFeaturesNet for computer-aided creative urban design,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Fishe, S., Philosophy of architecture (2016) The Stanford Encyclopedia of Philosophy, Metaphysics Research Lab, Stanford University",Philosophy of architecture,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Ventura, D., The computational creativity complex (2015) Computational Creativity Research: Towards Creative Machines, , Besold, T., Schorlemmer, M., Smaill, A. (eds) Atlantis Press, Paris",The computational creativity complex,Ventura D.,The computational creativity complex,[No abstract available],,2015.0,3.0,,,2-s2.0-85080690920
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Ezzat, M., A computational tool for mapping the users’ urban cognition-a framework and a representation for the evolutionary optimization of the fuzzy binary relation between the urban conceptions of “us” and “others (2018) The 36th eCAADe Conference",A computational tool for mapping the users’ urban cognition-a framework and a representation for the evolutionary optimization of the fuzzy binary relation between the urban conceptions of “us” and “others,Ezzat M.,A computational tool for mapping the users’ urban cognition-a framework and a representation for the evolutionary optimization of the fuzzy binary relation between the urban conceptions of “us” and “others,[No abstract available],,2018.0,1.0,,,2-s2.0-85091286818
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Tamari, T., The phenomenology of architecture: a short introduction to Juhani Pallasmaa (2017) Body Soc, 23 (1), pp. 91-95",The phenomenology of architecture: a short introduction to Juhani Pallasmaa,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Lefebvre, H., (1974) The Production of Space, , Blackwell, Oxford",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Derrida, J., (1967) Of Grammatology. Les Éditions de Minuit, , Paris",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Derrida, J., (1967) Speech and Phenomena, , Presses Universitaires de France, Paris",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Funke, J., On the psychology of creativity (2009) Milieus of Creativity: An Interdisciplinary Approach to Spatiality of Creativity, , Meusburger, P., Funke, J., Wunder, E. (eds) Springer, Dordrecht",On the psychology of creativity,Funke J.,On the psychology of creativity,[No abstract available],,2009.0,15.0,,,2-s2.0-84952714748
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Brinck, I., The gist of creativity (1997) The Complexity of Creativity, , Andersson, Å.E., Sahlin, N.E. (eds) Springer, Dordrecht",The gist of creativity,Brick I.,The gist of creativity,[No abstract available],,1997.0,3.0,,,2-s2.0-85082895205
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Evans, V., What’s in a concept? Analog versus parametric concepts in LCCM theory (2015) The Conceptual Mind: New Directions in the Study of Concepts, , Margolis, E., Laurence, S. (eds) The MIT Press, Cambridge",What’s in a concept? Analog versus parametric concepts in LCCM theory,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Heath, D., Norton, D., Ventura, D., A conveying semantics through visual metaphor (2014) ACM Trans. Intell. Syst. Technol, 5 (2). , 31:1–31:17",A conveying semantics through visual metaphor,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Heath, D., Norton, D., Ventura, D., Autonomously communicating conceptual knowledge through visual art (2013) ICCC",Autonomously communicating conceptual knowledge through visual art,"Heath D., Norton D., Ventura D.",Autonomously communicating conceptual knowledge through visual art,"In visual art, the communication of meaning or intent is an important part of eliciting an aesthetic experience in the viewer. Building on previous work, we present three additions to DARCI that enhances its ability to communicate concepts through the images it creates. The first addition is a model of semantic memory based on word associations for providing meaning to concepts. The second addition composes universal icons into a single image and renders the image to match an associated adjective. The third addition is a similarity metric that maintains recognizability while allowing for the introduction of artistic elements. We use an online survey to show that the system is successful at creating images that communicate concepts to human viewers. © 2013 Proceedings of the 4th International Conference on Computational Creativity, ICCC 2013. All rights reserved.",,2013.0,3.0,,Artificial intelligence; Arts computing; Conceptual knowledge; Online surveys; Semantic memory; Similarity metrics; Single images; Visual arts; Word association; Image enhancement,2-s2.0-84949423196
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Prinz, J.J., (2002) Furnishing the Mind: Concepts and Their Perceptual Basis, , The MIT Press, Cambridge",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Koolhaas, R., (2018) Harvard Graduate School of Design, Trüby, S., Westcott, J., Petermann, S.: Rem Koolhaas. Elements of Architecture, , Taschen, Cologne",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Goel, A.K., Craw, S., Design, innovation and case-based reasoning (2006) Knowl. Eng. Rev, 20, pp. 271-276",innovation and case-based reasoning,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Richter, K., Donath, D., (2006) eCAADe, , Towards a better understanding of the case-based reasoning paradigm in architectural education and design",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Peckham, A., Schmiedeknecht, T., (2014) The Rationalist Reader, , Routledge, Abingdon",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Stojanovski, T., (2013) City Information Modeling (CIM) and urbanism: blocks, connections, territories, people and situations, , SimAUD",,,,,,,,,,
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Norton, D., Heath, D., Ventura, D., Finding creativity in an artificial artist (2013) J. Creat. Behav, 47 (2), pp. 106-124",Finding creativity in an artificial artist,"Norton D., Heath D., Ventura D.",Finding creativity in an artificial artist,"Creativity is an important component of human intelligence, and imbuing artificially intelligent systems with creativity is an interesting challenge. In particular, it is difficult to quantify (or even qualify) creativity. Recently, it has been suggested that conditions for attributing creativity to a system include: appreciation, imagination, and skill. We demonstrate and describe an original computer system (called DARCI) that is designed to produce images through creative means. We present methods for evaluating DARCI and other artificially creative systems with respect to appreciation, imagination, and skill, and use these methods to show that DARCI is arguably a creative system. © 2013 by the Creative Education Foundation, Inc.",10.1002/jocb.27,2013.0,25.0,Computational creativity; Evaluation of creativity; Evolutionary algorithm; Visual art,,2-s2.0-84880355377
2-s2.0-85091292384,A comprehensive conceptualization of urban constructs as a basis for design creativity: An ontological conception of urbanism for human-computer aided spatial experiential simulation and design creativity," Malt, B.C., Gennari, S.P., Imai, M., Ameel, E., Saji, N., Majid, A., Where are the concepts? What words can and can’t reveal (2015) The Conceptual Mind: New Directions in the Study of Concepts, , Margolis, E., Laurence, S. (eds) The MIT Press, Cambridge",Where are the concepts? What words can and can’t reveal,"Malt B.C., Gennari S.P., Imai M., Ameel E., Saji N., Majid A.",Where are the concepts? What words can and can’t reveal,[No abstract available],,2015.0,26.0,,,2-s2.0-84939237309
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach,"Supriya, B.N., Kallimani, V., Prakash, S., Akki, C.B., Twitter sentiment analysis using binary classification technique (2016) International Conference on Nature of Computation and Communication ICTCC 2016: Nature of Computation and Communication, pp. 91-396",Twitter sentiment analysis using binary classification technique,"Supriya B.N., Kallimani V., Prakash S., Akki C.B.",Twitter sentiment analysis using binary classification technique,[No abstract available],,2016.0,1.0,,,2-s2.0-85088846768
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Haque, M.A., Rahman, T., Sentiment analysis by using fuzzy logic (2014) Int J Comput Sci Eng Inf Technol (IJCSEIT), 4, pp. 33-48",Sentiment analysis by using fuzzy logic,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Shirdastian, H., Laroche, M., Richard, M.-O., Using big data analytics to study brand authenticity sentiments: the case of starbucks on twitter (2019) Int J Inf Manage, 48, pp. 291-307",Using big data analytics to study brand authenticity sentiments: the case of starbucks on twitter,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Mansour, R., Hady, M.F.A., Hosam, E., Amr, H., Ashour, A., Feature selection for twitter sentiment analysis: An experimental study (2015) International Conference on Intelligent Text Processing and Computational Linguistics Cicling Computational Linguistics and Intelligent Text Processing, pp. 92-103",Feature selection for twitter sentiment analysis: An experimental study,"Mansour R., Hady M.F.A., Hosam E., Amr H., Ashour A.",Feature selection for twitter sentiment analysis: An experimental study,"Feature selection is an important problem for any pattern classification task. In this paper, we developed an ensemble of two Maximum Entropy classifiers for Twitter sentiment analysis: one for subjectivity and the other for polarity classification. Our ensemble employs surface-form, semantic and sentiment features. The classification complexity of this ensemble of linear models is linear with respect to the number of features. Our goal is to select a compact feature subset from the exhaustive list of extracted features in order to reduce the computational complexity without scarifying the classification accuracy. We evaluate the performance on two benchmark datasets, CrowdScale and SemEval. Our selected 20K features have shown very similar results in subjectivity classification to the NRC state-of-the-art system with 4 million features that has ranked first in 2013 SemEval competition. Also, our selected features have shown a relative performance gain in the ensemble classification over the baseline of uni-gram and bi-gram features of 9.9% on CrowdScale and 11.9% on SemEval. © Springer International Publishing Switzerland 2015.",10.1007/978-3-319-18117-2_7,2015.0,12.0,,Benchmarking; Classification (of information); Computational linguistics; Data mining; Linguistics; Maximum entropy methods; Semantics; Social networking (online); Text processing; Benchmark datasets; Classification accuracy; Ensemble classification; Polarity classification; Relative performance; Sentiment features; State-of-the-art system; Subjectivity classifications; Feature extraction,2-s2.0-84942508773
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Bao, Y., Quan, C., Wang, L., Ren, F., The role of pre-processing in twitter sentiment analysis (2014) International Conference on Intelligent Computing ICIC: Intelligent Computing Methodologies, pp. 615-624",The role of pre-processing in twitter sentiment analysis,"Bao Y., Quan C., Wang L., Ren F.",The role of pre-processing in twitter sentiment analysis,"Recently, increasing attention has been attracted to Social Networking Sentiment Analysis. Twitter as one of the most fashional social networking platforms has been researched as a hot topic in this domain. Normally, sentiment analysis is regarded as a classification problem. Training a classifier with tweets data, there is a large amount of noise due to tweets' shortness, marks, irregular words etc. In this work we explore the impact pre-processing methods make on twitter sentiment classification. We evaluate the effects of URLs, negation, repeated letters, stemming and lemmatization. Experimental results on the Stanford Twitter Sentiment Dataset show that sentiment classification accuracy rises when URLs features reservation, negation transformation and repeated letters normalization are employed while descends when stemming and lemmatization are applied. Moreover, we get a better result by augmenting the original feature space with bigram and emotions features. Comprehensive application of these measures makes us achieve classification accuracy of 85.5%. © 2014 Springer International Publishing Switzerland.",10.1007/978-3-319-09339-0_62,2014.0,56.0,Pre-processing; Sentiment Analysis; Twitter,Classification (of information); Data mining; Intelligent computing; Social networking (online); Text processing; Classification accuracy; Feature space; Large amounts; Pre-processing; Pre-processing method; Sentiment analysis; Sentiment classification; Twitter; Natural language processing systems,2-s2.0-84958541163
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Keshavarz, H., Abadeh, M.-S., ALGA: adaptive lexicon learning using genetic algorithm for sentiment analysis of microblogs (2017) Knowl-Based Syst, 122, pp. 1-16",ALGA: adaptive lexicon learning using genetic algorithm for sentiment analysis of microblogs,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Ismail, H.-M., Belkhouche, B., Zaki, N., Semantic twitter sentiment analysis based on a fuzzy thesaurus (2018) Soft Comput, 22, pp. 6011-6024",Semantic twitter sentiment analysis based on a fuzzy thesaurus,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Medhat, W., Hassan, A., Korashy, H., Sentiment analysis algorithms and applications: a survey (2014) Ain Shams Eng J, 5, pp. 1093-1113",Sentiment analysis algorithms and applications: a survey,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Asghar, M.-Z., Khan, A., Khan, F., Kundi, F.-M., RIFT: a rule induction framework for twitter sentiment analysis (2018) Arabian J Sci Eng, 43, pp. 857-877",RIFT: a rule induction framework for twitter sentiment analysis,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Le, B., Nguyen, H., Twitter sentiment analysis using machine learning techniques (2015) Advanced Computational Methods for Knowledge Engineering AISC: Advances in Intelligent Systems and Computing, pp. 279-289",Twitter sentiment analysis using machine learning techniques,"Le B., Nguyen H.",Twitter sentiment analysis using machine learning techniques,"Twitter is a microblogging site in which users can post updates (tweets) to friends (followers). It has become an immense dataset of the so-called sentiments. In this paper, we introduce an approach to selection of a new feature set based on Information Gain, Bigram, Objectoriented extraction methods in sentiment analysis on social networking side. In addition, we also proposes a sentiment analysis model based on Naive Bayes and Support Vector Machine. Its purpose is to analyze sentiment more effectively. This model proved to be highly effective and accurate on the analysis of feelings. © Springer International Publishing Switzerland 2015.",10.1007/978-3-319-17996-4_25,2015.0,34.0,Sentiment analysis; Sentiment classification; Twitter,Artificial intelligence; Data mining; Social networking (online); Extraction method; Information gain; Machine learning techniques; Microblogging; Object oriented; Sentiment analysis; Sentiment classification; Twitter; Learning systems,2-s2.0-84942586533
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Pandey, A.-C., Rajpoot, D.-S., Saraswat, M., Twitter sentiment analysis using hybrid cuckoo search method (2017) Inf Process Manage, 53, pp. 764-779",Twitter sentiment analysis using hybrid cuckoo search method,"Chandra Pandey A., Singh Rajpoot D., Saraswat M.",Twitter sentiment analysis using hybrid cuckoo search method,"Sentiment analysis is one of the prominent fields of data mining that deals with the identification and analysis of sentimental contents generally available at social media. Twitter is one of such social medias used by many users about some topics in the form of tweets. These tweets can be analyzed to find the viewpoints and sentiments of the users by using clustering-based methods. However, due to the subjective nature of the Twitter datasets, metaheuristic-based clustering methods outperforms the traditional methods for sentiment analysis. Therefore, this paper proposes a novel metaheuristic method (CSK) which is based on K-means and cuckoo search. The proposed method has been used to find the optimum cluster-heads from the sentimental contents of Twitter dataset. The efficacy of proposed method has been tested on different Twitter datasets and compared with particle swarm optimization, differential evolution, cuckoo search, improved cuckoo search, gauss-based cuckoo search, and two n-grams methods. Experimental results and statistical analysis validate that the proposed method outperforms the existing methods. The proposed method has theoretical implications for the future research to analyze the data generated through social networks/medias. This method has also very generalized practical implications for designing a system that can provide conclusive reviews on any social issues. © 2017 Elsevier Ltd",10.1016/j.ipm.2017.02.004,2017.0,164.0,Cuckoo search; Data preprocessing; K-means; Sentiment analysis; Twitter,Data mining; Evolutionary algorithms; Particle swarm optimization (PSO); Social networking (online); Cuckoo searches; Data preprocessing; K-means; Sentiment analysis; Twitter; Optimization,2-s2.0-85014785827
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Speriosu, M., Sudan, N., Upadhyay, S., Baldridge, J., Twitter polarity classification with label propagation over lexical links and the follower graph (2011) Conference on Empirical Methods in Natural Language Processing, pp. 53-63. , UK",Twitter polarity classification with label propagation over lexical links and the follower graph,"Speriosu M., Sudan N., Upadhyay S., Baldridge J.",Twitter polarity classification with label propagation over lexical links and the follower graph,[No abstract available],,2011.0,233.0,,,2-s2.0-84868586679
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Masud, F., Khan, A., Ahmad, S., Asghar, M.-Z., Lexicon-based sentiment analysis in the social web (2014) J Basic Appl Sci Res, 4 (6), pp. 238-248",Lexicon-based sentiment analysis in the social web,"Kundi F.M., Khan A., Ahmad S., Asghar M.Z.",Lexicon-based sentiment analysis in the social web,[No abstract available],,2014.0,28.0,,,2-s2.0-84979239226
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Asghar, M.-Z., Kundi, F.-M., Ahmad, S., Khan, A., Khan, F., T-SAF: twitter sentiment analysis framework using a hybrid classification scheme (2018) Exp Syst, 35, pp. 1-19",T-SAF: twitter sentiment analysis framework using a hybrid classification scheme,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Saif, H., He, Y., Fernandez, M., Alani, H., Contextual semantics for sentiment analysis of Twitter (2016) Inf Process Manage, 52, pp. 5-19",Contextual semantics for sentiment analysis of Twitter,"Saif H., He Y., Fernandez M., Alani H.",Contextual semantics for sentiment analysis of Twitter,"Sentiment analysis on Twitter has attracted much attention recently due to its wide applications in both, commercial and public sectors. In this paper we present SentiCircles, a lexicon-based approach for sentiment analysis on Twitter. Different from typical lexicon-based approaches, which offer a fixed and static prior sentiment polarities of words regardless of their context, SentiCircles takes into account the co-occurrence patterns of words in different contexts in tweets to capture their semantics and update their pre-assigned strength and polarity in sentiment lexicons accordingly. Our approach allows for the detection of sentiment at both entity-level and tweet-level. We evaluate our proposed approach on three Twitter datasets using three different sentiment lexicons to derive word prior sentiments. Results show that our approach significantly outperforms the baselines in accuracy and F-measure for entity-level subjectivity (neutral vs. polar) and polarity (positive vs. negative) detections. For tweet-level sentiment detection, our approach performs better than the state-of-the-art SentiStrength by 4-5% in accuracy in two datasets, but falls marginally behind by 1% in F-measure in the third dataset. © 2015 Elsevier Ltd. All rights reserved.",10.1016/j.ipm.2015.01.005,2016.0,264.0,Contextual semantics; Sentiment analysis; Twitter,Data mining; Semantics; Co-occurrence pattern; Contextual semantics; Lexicon-based; Public sector; Sentiment analysis; Sentiment lexicons; State of the art; Twitter; Social networking (online),2-s2.0-84924110504
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Khan, F.-H., Qamar, U., Bashir, S., SentiMI: introducing point-wise mutual information with SentiWordNet to improve sentiment polarity detection (2016) Appl Soft Comput, 39, pp. 140-153",SentiMI: introducing point-wise mutual information with SentiWordNet to improve sentiment polarity detection,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Esuli, A., Sebastiani, F., Sentiwordnet: A publicly available lexical resource for opinion mining (2006) Proceedings of the Fifth International Conference on Language Resources and Evaluation, pp. 417-422",Sentiwordnet: A publicly available lexical resource for opinion mining,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Nielsen, F.-A., A new ANEW: Evaluation of a word list for sentiment analysis for microblogs (2011) Proceedings of the ESWC2011 Workshop on ‘Making Sense of Microposts’: Big Things Come in Small Packages, pp. 93-98",A new ANEW: Evaluation of a word list for sentiment analysis for microblogs,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Taboada, M., Brooke, J., Tofiloski, M., Voll, K., Stede, M., Lexicon-based methods for sentiment analysis (2011) Comput Lingust, 37, pp. 267-307",Lexicon-based methods for sentiment analysis,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Paltoglou, G., Thelwall, M., A study of information retrieval weighting schemes for sentiment analysis (2010) Proceedings of the 48Th Annual Meeting of the Association for Computational Linguistics: Association for Computational Linguistics, pp. 1386-1395",A study of information retrieval weighting schemes for sentiment analysis,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Yager, R.R., Kelman, A., Fusion of fuzzy information with considerations for compatibility, partial aggregation, and reinforcement (1996) Int J Appr Reason, 15, pp. 93-122",and reinforcement,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Appel, O., Chiclana, F., Carter, J., Fujita, H., a hybrid approach to the sentiment analysis problem at the sentence level (2016) Knowl-Based Syst, 108, pp. 110-124",a hybrid approach to the sentiment analysis problem at the sentence level,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Gassert, H., (2018) Operators on Fuzzy Sets: Zadeh and Einsteinations on Fuzzy Sets Properties of T-Norms and T-Conorms, , https://pdfs.semanticscholar.org/a045/52b74047208d23d77b8aa9f5f334b59e65ea.pdf, Accessed 8 Dec 2018",,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Goldberg, D.-E., (1989) Genetic algorithms in search optimization and machine learning, , Addition Wesley, Massachusetts",,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Effrosynidis, D., Symeonidis, S., Arampatzis, A., A comparison of pre-processing techniques (2017) International Conference on Theory and Practice of Digital Libraries TPDL: Research and Advanced Technology for Digital Libraries, pp. 394-406",A comparison of pre-processing techniques,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Salton, G., Wong, A., Yang, C.-S., A vector space model for automatic indexing (1975) Commun ACM, 18, pp. 613-620",A vector space model for automatic indexing,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Han, J., Kamber, M., (2006) Data Mining: Concepts and Techniques, 2Nd Edn. University of Illinois at Urbana-Champaign, Printed on Elsevier Inc",,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Vierira, S.-M., Mendonca, L.-F., Farinha, G.-J., Sousa, J.-M.-C., Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients (2013) Appl Soft Comput, 13, pp. 3494-3504",Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients,"Vieira S.M., Mendonça L.F., Farinha G.J., Sousa J.M.C.",Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients,"This paper proposes a modified binary particle swarm optimization (MBPSO) method for feature selection with the simultaneous optimization of SVM kernel parameter setting, applied to mortality prediction in septic patients. An enhanced version of binary particle swarm optimization, designed to cope with premature convergence of the BPSO algorithm is proposed. MBPSO control the swarm variability using the velocity and the similarity between best swarm solutions. This paper uses support vector machines in a wrapper approach, where the kernel parameters are optimized at the same time. The approach is applied to predict the outcome (survived or deceased) of patients with septic shock. Further, MBPSO is tested in several benchmark datasets and is compared with other PSO based algorithms and genetic algorithms (GA). The experimental results showed that the proposed approach can correctly select the discriminating input features and also achieve high classification accuracy, specially when compared to other PSO based algorithms. When compared to GA, MBPSO is similar in terms of accuracy, but the subset solutions have less selected features. © 2013 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2013.03.021,2013.0,197.0,Feature selection; Particle swarm optimization; Premature convergence; Sepsis; Support vector machines; Wrapper methods,Benchmark datasets; Binary particle swarm optimization; Classification accuracy; Kernel parameter; Pre-mature convergences; Sepsis; Simultaneous optimization; Wrapper methods; Feature extraction; Forecasting; Genetic algorithms; Particle swarm optimization (PSO); Support vector machines,2-s2.0-84878154666
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Gen, M., Cheng, R., (1997) Genetic Algorithms and Engineering Design, , printed on Wiley",,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Vapnik, V.-N., (1995) The nature of statistical learning theory, , Springer, New York",,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Saif, H., Fernande, M., Alani, Y.H.H., Evaluation datasets for twitter sentiment analysis: A survey and a new dataset, the STS-Gold (2013) 1St Interantional Workshop on Emotion and Sentiment in Social and Expressive Media: Approaches and Perspectives from AI (ESSEM 2013), pp. 9-21. , Turin, Italy",the STS-Gold,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Go, A., Bhayani, R., Huang, L., (2010) Twitter Sentiment Classification Using Distant Supervision, , Technical report Stanford University",,,,,,,,,,
2-s2.0-85074497303,Solving the twitter sentiment analysis problem based on a machine learning-based approach," Shapiro, S.S., Wilk, M.B., Chen, H.J., A comparative study of various tests for normality (1968) J Am Stat Assoc, 63 (324), pp. 1343-1372",A comparative study of various tests for normality,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis,"Bollacker, K., Evans, C., Paritosh, P., Freebase: a collaboratively created graph database for structuring human knowledge (2008) Proc. SIGMOD Conference, pp. 1247-1250",Freebase: a collaboratively created graph database for structuring human knowledge,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Bordes, A., Glorot, X., Weston, J., A semantic matching energy function for learning with multi-relational data (2014) Mach Learn, 94 (2), pp. 233-259",A semantic matching energy function for learning with multi-relational data,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Bordes, A., Weston, J., Collobert, R., Learning structured embeddings of knowledge bases (2012) AAAI Conference on Artificial Intelligence, AAAI 2011, San Francisco, California, USA, August. DBLP",Learning structured embeddings of knowledge bases,"Bordes A., Weston J., Collobert R., Bengio Y.",Learning structured embeddings of knowledge bases,[No abstract available],,2011.0,162.0,,,2-s2.0-84872513656
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," https://www.nlm.nih.gov/research/umls/, U.S. National Library of Medicine, “Unified Medical Language System.” [Online] Available:",,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Palmerini, T., Benedetto, U., Bacchi-Reggiani, L., Mortality in patients treated with extended duration dual antiplatelet therapy after drug-eluting stent implantation: a pairwise and Bayesian network meta-analysis of randomised trials (2015) Lancet, 385 (9985), pp. 2371-2382",Mortality in patients treated with extended duration dual antiplatelet therapy after drug-eluting stent implantation: a pairwise and Bayesian network meta-analysis of randomised trials,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Snidaro, L., Visentini, I., Bryan, K., Fusing uncertain knowledge and evidence for maritime situational awareness via Markov Logic Networks (2015) Inf Fusion, 21, pp. 159-172",Fusing uncertain knowledge and evidence for maritime situational awareness via Markov Logic Networks,"Snidaro L., Visentini I., Bryan K.",Fusing uncertain knowledge and evidence for maritime situational awareness via Markov Logic Networks,"The concepts of event and anomaly are important building blocks for developing a situational picture of the observed environment. We here relate these concepts to the JDL fusion model and demonstrate the power of Markov Logic Networks (MLNs) for encoding uncertain knowledge and compute inferences according to observed evidence. MLNs combine the expressive power of first-order logic and the probabilistic uncertainty management of Markov networks. Within this framework, different types of knowledge (e.g. a priori, contextual) with associated uncertainty can be fused together for situation assessment by expressing unobservable complex events as a logical combination of simpler evidences. We also develop a mechanism to evaluate the level of completion of complex events and show how, along with event probability, it could provide additional useful information to the operator. Examples are demonstrated on two maritime scenarios of rules for event and anomaly detection. © 2013 NATO Science and Technology Organization, Centre for Maritime Research and Experimentation. Published by Elsevier B.V. All rights reserved.",10.1016/j.inffus.2013.03.004,2015.0,49.0,Context-based fusion; Markov Logic Networks; Situational awareness; Uncertainty management,Markov processes; Context-based; Logical combination; Markov logic networks; Probabilistic uncertainty; Situation assessment; Situational awareness; Uncertain knowledge; Uncertainty management; Complex networks,2-s2.0-84904793445
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Marini, S., Trifoglio, E., Barbarini, N., A dynamic Bayesian network model for long-term simulation of clinical complications in type 1 diabetes (2015) J Biomed Inform, 57 (100), pp. 369-376",A dynamic Bayesian network model for long-term simulation of clinical complications in type 1 diabetes,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Fuster-Parra, P., Tauler, P., Bennasar-Veny, M., Bayesian network modeling: a case study of an epidemiologic system analysis of cardiovascular risk (2016) Comput Methods Programs Biomed, 126, pp. 128-142",Bayesian network modeling: a case study of an epidemiologic system analysis of cardiovascular risk,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Yu, T., Li, J., Yu, Q., Knowledge graph for TCM health preservation (2017) Artif Intell Med, 77, pp. 48-52",Knowledge graph for TCM health preservation,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Rao, D., Mcnamee, P., Dredze, M., Entity linking: finding extracted entities in a knowledge base. (2013) Multi-source, multilingual information extraction and summarization, pp. 93-115. , Springer Berlin Heidelberg",Entity linking: finding extracted entities in a knowledge base.,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Shen, W., Wang, J., Han, J., Entity linking with a knowledge base: issues, techniques, and solutions (2015) Knowl Data Eng IEEE Trans, 27 (2), pp. 443-460",and solutions,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Dredze, M., Mcnamee, P., Rao, D., Entity disambiguation for knowledge base population (2010) International Conference on Computational Linguistics. Association for Computational Linguistics, pp. 277-285",Entity disambiguation for knowledge base population,"Dredze M., Mcnamee P., Rao D., Gerber A., Finin T.",Entity disambiguation for knowledge base population,"The integration of facts derived from information extraction systems into existing knowledge bases requires a system to disambiguate entity mentions in the text. This is challenging due to issues such as non-uniform variations in entity names, mention ambiguity, and entities absent from a knowledge base. We present a state of the art system for entity disambiguation that not only addresses these challenges but also scales to knowledge bases with several million entries using very little resources. Further, our approach achieves performance of up to 95% on entities mentioned from newswire and 80% on a public test set that was designed to include challenging queries.",,2010.0,202.0,,Entity disambiguation; Information extraction systems; Knowledge base; Knowledge basis; State-of-the-art system; Test sets; Computational linguistics; Information retrieval systems; Knowledge based systems,2-s2.0-80053221437
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Duque, A., Stevenson, M., Martinez-Romo, J., Co-occurrence graphs for word sense disambiguation in the biomedical domain (2018) Artif Intell Med, 87, pp. 9-19",Co-occurrence graphs for word sense disambiguation in the biomedical domain,"Duque A., Stevenson M., Martinez-Romo J., Araujo L.",Co-occurrence graphs for word sense disambiguation in the biomedical domain,"Word sense disambiguation is a key step for many natural language processing tasks (e.g. summarization, text classification, relation extraction) and presents a challenge to any system that aims to process documents from the biomedical domain. In this paper, we present a new graph-based unsupervised technique to address this problem. The knowledge base used in this work is a graph built with co-occurrence information from medical concepts found in scientific abstracts, and hence adapted to the specific domain. Unlike other unsupervised approaches based on static graphs such as UMLS, in this work the knowledge base takes the context of the ambiguous terms into account. Abstracts downloaded from PubMed are used for building the graph and disambiguation is performed using the personalized PageRank algorithm. Evaluation is carried out over two test datasets widely explored in the literature. Different parameters of the system are also evaluated to test robustness and scalability. Results show that the system is able to outperform state-of-the-art knowledge-based systems, obtaining more than 10% of accuracy improvement in some cases, while only requiring minimal external resources. © 2018 Elsevier B.V.",10.1016/j.artmed.2018.03.002,2018.0,18.0,Graph-based systems; Information extraction; Natural language processing; Unified medical language system; Unsupervised machine learning; Word sense disambiguation,Abstracting; Classification (of information); Graphic methods; Information retrieval; Information retrieval systems; Knowledge based systems; Learning algorithms; Learning systems; Medical information systems; Text processing; Co-occurrence informations; Graph-based; Personalized PageRank; Unified medical language systems; Unsupervised approaches; Unsupervised machine learning; Unsupervised techniques; Word Sense Disambiguation; Natural language processing systems; accuracy; algorithm; ambiguity; Article; classification; human; information processing; knowledge base; Medline; natural language processing; priority journal; Unified Medical Language System; unsupervised machine learning; word sense disambiguation; information processing; knowledge base; natural language processing; semantics; Algorithms; Datasets as Topic; Knowledge Bases; Natural Language Processing; PubMed; Semantics; Unified Medical Language System,2-s2.0-85044304673
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Yang, B., Yih, W.T., He, X., Embedding entities and relations for learning and inference in knowledge bases (2014) Eprint Arxiv",Embedding entities and relations for learning and inference in knowledge bases,"Yang B., Yih W.-T., He X., Gao J., Deng L.",Embedding entities and relations for learning and inference in knowledge bases,[No abstract available],,2014.0,273.0,,,2-s2.0-84952335118
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Rocktäschel, T., Singh, S., Riedel, S., Injecting logical background knowledge into embeddings for relation extraction (2014) North Am Chap Assoc Comput Linguis, pp. 648-664",Injecting logical background knowledge into embeddings for relation extraction,"Rocktschel T., Singh S., Riedel S.",Injecting logical background knowledge into embeddings for relation extraction,[No abstract available],,2014.0,2.0,,,2-s2.0-85049368596
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Rocktäschel, T., Bošnjak, M., Singh, S., Low-dimensional embeddings of logic (2014) ACL 2014 Workshop on Semantic Parsing, pp. 45-49",Low-dimensional embeddings of logic,"Rocktäschel T., Bosnjak M., Singh S., Riedel S.",Low-dimensional embeddings of logic,[No abstract available],,0.0,38.0,,,2-s2.0-84937214491
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Xiao, H., Huang, M., Meng, L., SSP: semantic space projection for knowledge graph embedding with text descriptions (2017) AAAI, 17, pp. 3104-3110",SSP: semantic space projection for knowledge graph embedding with text descriptions,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Guo, S., Wang, Q., Wang, B., SSE: semantically smooth embedding for knowledge graphs (2017) IEEE Trans Knowl Data Eng, 29 (4), pp. 884-897",SSE: semantically smooth embedding for knowledge graphs,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Zhao, C., Jiang, J., Guan, Y., EMR-based medical knowledge representation and inference via Markov random fields and distributed representation learning (2018) Artif Intell Med, 87, pp. 49-59",EMR-based medical knowledge representation and inference via Markov random fields and distributed representation learning,"Zhao C., Jiang J., Guan Y., Guo X., He B.",EMR-based medical knowledge representation and inference via Markov random fields and distributed representation learning,"Objective: Electronic medical records (EMRs) contain medical knowledge that can be used for clinical decision support (CDS). Our objective is to develop a general system that can extract and represent knowledge contained in EMRs to support three CDS tasks—test recommendation, initial diagnosis, and treatment plan recommendation—given the condition of a patient. Methods: We extracted four kinds of medical entities from records and constructed an EMR-based medical knowledge network (EMKN), in which nodes are entities and edges reflect their co-occurrence in a record. Three bipartite subgraphs (bigraphs) were extracted from the EMKN, one to support each task. One part of the bigraph was the given condition (e.g., symptoms), and the other was the condition to be inferred (e.g., diseases). Each bigraph was regarded as a Markov random field (MRF) to support the inference. We proposed three graph-based energy functions and three likelihood-based energy functions. Two of these functions are based on knowledge representation learning and can provide distributed representations of medical entities. Two EMR datasets and three metrics were utilized to evaluate the performance. Results: As a whole, the evaluation results indicate that the proposed system outperformed the baseline methods. The distributed representation of medical entities does reflect similarity relationships with respect to knowledge level. Conclusion: Combining EMKN and MRF is an effective approach for general medical knowledge representation and inference. Different tasks, however, require individually designed energy functions. © 2018 Elsevier B.V.",10.1016/j.artmed.2018.03.005,2018.0,17.0,Clinical decision support; Distributed representation; Electronic medical record; Markov random field; Medical knowledge network,"Decision support systems; Function evaluation; Graphic methods; Image segmentation; Knowledge representation; Markov processes; Medical computing; Patient treatment; Clinical decision support; Distributed representation; Electronic medical record; Markov Random Fields; Medical knowledge; Diagnosis; Article; clinical decision support system; clinical effectiveness; clinical evaluation; diagnostic test; early diagnosis; electronic medical record; electronic medical record based medical knowledge network; hidden Markov model; human; knowledge discovery; learning algorithm; Markov random field; mathematical analysis; outcome assessment; priority journal; probability; symptom; treatment indication; treatment planning; algorithm; clinical decision support system; electronic health record; machine learning; Markov chain; statistical model; Algorithms; Decision Support Systems, Clinical; Electronic Health Records; Likelihood Functions; Machine Learning; Markov Chains",2-s2.0-85046118718
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Mikolov, T., Chen, K., Corrado, G., Efficient estimation of word representations in vector space (2013) Comput Sci",Efficient estimation of word representations in vector space,"Mikolov T., Chen K., Corrado G., Dean J.",Efficient estimation of word representations in vector space,"We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. © 2013 International Conference on Learning Representations, ICLR. All rights reserved.",,2013.0,12416.0,,Semantics; Vectors; Computational costs; Efficient estimation; Model architecture; State-of-the-art performance; Vector representations; Very large datum; Word representations; Word similarity; Vector spaces,2-s2.0-85083951332
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Mikolov, T., Chen, K., Corrado, G., Efficient estimation of word representations in vector space (2013) Comput Sci",Efficient estimation of word representations in vector space,"Mikolov T., Chen K., Corrado G., Dean J.",Efficient estimation of word representations in vector space,[No abstract available],,2013.0,790.0,,,2-s2.0-84903761492
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Mikolov, T., Sutskever, I., Chen, K., Distributed representations of words and phrases and their compositionality (2013) Adv Neural Inf Process Syst, 26, pp. 3111-3119",Distributed representations of words and phrases and their compositionality,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Goller, C., Kuchler, A., Learning task-dependent distributed representations by backpropagation through structure (2002) IEEE International Conference on Neural Networks. IEEE, 1, pp. 347-352",Learning task-dependent distributed representations by backpropagation through structure,"Goller Christoph, Kuechler Andreas",Learning task-dependent distributed representations by backpropagation through structure,"While neural networks are very successfully applied to the processing of fixed-length vectors and variable-length sequences, the current state of the art does not allow the efficient processing of structured objects of arbitrary shape (like logical terms, trees or graphs). We present a connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size. The most general structures that can be handled are labeled directed acyclic graphs. The major difference of our approach compared to others is that the structure-representations are exclusively tuned for the intended inference task. Our method is applied to tasks consisting in the classification of logical terms. These range from the detection of a certain subterm to the satisfaction of a specific unification pattern. Compared to previously known approaches we got superior results on that domain.",,1996.0,358.0,,Associative storage; Backpropagation; Data structures; Distributed computer systems; Learning algorithms; Pattern recognition; Recursive functions; Complex symbolic structures; Labeled directed acyclic graphs; Recursive autoassociative memory; Supervised learning; Neural networks,2-s2.0-0029727454
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Bordes, A., Usunier, N., Garcia-Duran, A., Translating embeddings for modeling multi-relational data (2013) International Conference on Neural Information Processing Systems. Curran Associates Inc., pp. 2787-2795",Translating embeddings for modeling multi-relational data,"Bordes A., Usunier N., Garcia-Durán A., Weston J., Yakhnenko O.",Translating embeddings for modeling multi-relational data,"We consider the problem of embedding entities and relationships of multi relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.",,2013.0,2395.0,,Canonical modeling; Knowledge basis; Large scale data sets; Link prediction; Relational data; State-of-the-art methods; Training sample; Very large database,2-s2.0-84899013802
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Williams, D., Hinton, G., Learning representations by back-propagating errors (1986) Nature, 323 (6088), pp. 533-538",Learning representations by back-propagating errors,"Rumelhart D.E., Hinton G.E., Williams R.J.",Learning representations by back-propagating errors,"We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. © 1986 Nature Publishing Group.",10.1038/323533a0,1986.0,12560.0,,learning; model; nerve cell network; nervous system; nonbiological model; nonhuman; priority journal,2-s2.0-0022471098
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Mikolov, T., Le, Q.V., Sutskever, I., Exploiting similarities among languages for machine translation (2013) Comput Sci",Exploiting similarities among languages for machine translation,"Mikolov T., Le Q.V., Sutskever I.",Exploiting similarities among languages for machine translation,[No abstract available],,2013.0,97.0,,,2-s2.0-84906927200
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Socher, R., Chen, D., Manning, C.D., Reasoning with neural tensor networks for knowledge base completion (2013) International Conference on Neural Information Processing Systems. Curran Associates Inc., pp. 926-934",Reasoning with neural tensor networks for knowledge base completion,"Socher R., Chen D., Manning C.D., Ng A.Y.",Reasoning with neural tensor networks for knowledge base completion,"Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the ""Sumatran tiger"" and ""Bengal tiger."" Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.",,2013.0,1029.0,,Knowledge based systems; Tensors; Atomic units; Knowledge base; Knowledge basis; Large corpora; Question Answering; Statistical strength; Vector representations; Word vectors; Neural networks,2-s2.0-84898956227
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Chen, D., Socher, R., Manning, C.D., Learning new facts from knowledge bases with neural tensor networks and semantic word vectors (2013) Comput Sci",Learning new facts from knowledge bases with neural tensor networks and semantic word vectors,"Chen D., Socher R., Manning C.D.",Learning new facts from knowledge bases with neural tensor networks and semantic word vectors,[No abstract available],,2013.0,1.0,,,2-s2.0-85078152104
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Lakkaraju, H., Socher, R., Manning, C., Aspect specific sentiment analysis using hierarchical deep learning (2014) NIPS Workshop on Deep Learning and Representation Learning",Aspect specific sentiment analysis using hierarchical deep learning,"Lakkaraju H., Socher R., Manning C.",Aspect specific sentiment analysis using hierarchical deep learning,[No abstract available],,2014.0,57.0,,,2-s2.0-84951860502
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Zou, W.Y., Socher, R., Cer, D., Bilingual word embeddings for phrase-based machine translation (2013) Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1393-1398",Bilingual word embeddings for phrase-based machine translation,"Zou W.Y., Socher R., Cer D., Manning C.D.",Bilingual word embeddings for phrase-based machine translation,"We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task. © 2013 Association for Computational Linguistics.",,2013.0,341.0,,Computer aided language translation; Natural language processing systems; Semantics; Translation (languages); Embeddings; Language model; Machine translations; Phrase-based machine translations; Semantic similarity; Translational equivalence; Word alignment; Computational linguistics,2-s2.0-84926285904
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Pei, W., Ge, T., Chang, B., Max-margin tensor neural network for Chinese word segmentation (2014) Meeting of the Association for Computational Linguistics, pp. 293-303",Max-margin tensor neural network for Chinese word segmentation,"Pei W., Ge T., Chang B.",Max-margin tensor neural network for Chinese word segmentation,"Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering. In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN). By exploiting tag embeddings and tensorbased transformation, MMTNN has the ability to model complicated interactions between tags and context characters. Furthermore, a new tensor factorization approach is proposed to speed up the model and avoid overfitting. Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering. Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks. © 2014 Association for Computational Linguistics.",10.3115/v1/p14-1028,2014.0,123.0,,Benchmarking; Natural language processing systems; Neural networks; Tensors; Benchmark datasets; Chinese word segmentation; Competitive performance; Feature engineerings; NAtural language processing; Neural network model; Novel neural network; Tensor factorization; Computational linguistics,2-s2.0-84906927532
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Socher, R., Huval, B., Manning, C.D., Semantic compositionality through recursive matrix-vector spaces (2012) Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, pp. 1201-1211",Semantic compositionality through recursive matrix-vector spaces,"Socher R., Huval B., Manning C.D., Ng A.Y.",Semantic compositionality through recursive matrix-vector spaces,"Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. © 2012 Association for Computational Linguistics.",,2012.0,870.0,,Lexical information; Natural languages; Propositional logic; Recursive neural networks; Semantic relationships; State-of-the-art performance; Vector representations; Vector space models; Formal logic; Neural networks; Semantics; Syntactics; Vector spaces; Natural language processing systems,2-s2.0-84870715081
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Choi, E., Bahadori, M.T., Song, L., GRAM: graph-based attention model for healthcare representation learning (2016) 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pp. 787-795",GRAM: graph-based attention model for healthcare representation learning,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Lewis, M., Steedman, M., Combined distributional and logical semantics (2013) Trans Assoc Comput Linguist, 1, pp. 179-192",Combined distributional and logical semantics,"Lewis M., Steedman M.",Combined distributional and logical semantics,[No abstract available],,2013.0,69.0,,,2-s2.0-84905695006
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Jiang, J., Zhao, C., Guan, Y., Learning and inference in knowledge-based probabilistic model for medical diagnosis (2017) Knowledge Based Syst",Learning and inference in knowledge-based probabilistic model for medical diagnosis,"Jiang J., Li X., Zhao C., Guan Y., Yu Q.",Learning and inference in knowledge-based probabilistic model for medical diagnosis,"Based on a weighted knowledge graph to represent first-order knowledge and combining it with a probabilistic model, we propose a methodology for creating a medical knowledge network (MKN) in medical diagnosis. When a set of evidence is activated for a specific patient, we can generate a ground medical knowledge network that is composed of evidence nodes and potential disease nodes. By incorporating a Boltzmann machine into the potential function of a Markov network, we investigated the joint probability distribution of the MKN. To consider numerical evidence, a multivariate inference model is presented that uses conditional probability. In addition, the weights for the knowledge graph are efficiently learned from manually annotated Chinese Electronic Medical Records (CEMRs) and Blood Examination Records (BERs). In our experiments, we found numerically that an improved expression of evidence variables is necessary for medical diagnosis. Our experimental results comparing a Markov logic network and six kinds of classic machine learning algorithms on the actual CEMR database and BER database indicate that our method holds promise and that MKN can facilitate studies of intelligent diagnosis. © 2017 Elsevier B.V.",10.1016/j.knosys.2017.09.030,2017.0,13.0,First-order knowledge; Gradient descent; Markov logic network; Markov network; Probabilistic model,Computer circuits; Knowledge based systems; Learning algorithms; Learning systems; Markov processes; Medical computing; Probabilistic logics; Probability distributions; First order; Gradient descent; Markov logic networks; Markov networks; Probabilistic modeling; Diagnosis,2-s2.0-85033491118
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," https://github.com/WILAB-HIT/Resources/, WILAB-HIT, “Resources.” [Online] Available:",,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Jiang, J., Xie, J., Zhao, C., Max-margin weight learning for medical knowledge network (2018) Comput Methods Prog Biomed",Max-margin weight learning for medical knowledge network,"Jiang J., Xie J., Zhao C., Su J., Guan Y., Yu Q.",Max-margin weight learning for medical knowledge network,"Background and objective: The application of medical knowledge strongly affects the performance of intelligent diagnosis, and method of learning the weights of medical knowledge plays a substantial role in probabilistic graphical models (PGMs). The purpose of this study is to investigate a discriminative weight-learning method based on a medical knowledge network (MKN). Methods: We propose a training model called the maximum margin medical knowledge network (M3KN), which is strictly derived for calculating the weight of medical knowledge. Using the definition of a reasonable margin, the weight learning can be transformed into a margin optimization problem. To solve the optimization problem, we adopt a sequential minimal optimization (SMO) algorithm and the clique property of a Markov network. Ultimately, M3KN not only incorporates the inference ability of PGMs but also deals with high-dimensional logic knowledge. Results: The experimental results indicate that M3KN obtains a higher F-measure score than the maximum likelihood learning algorithm of MKN for both Chinese Electronic Medical Records (CEMRs) and Blood Examination Records (BERs). Furthermore, the proposed approach is obviously superior to some classical machine learning algorithms for medical diagnosis. To adequately manifest the importance of domain knowledge, we numerically verify that the diagnostic accuracy of M3KN is gradually improved as the number of learned CEMRs increase, which contain important medical knowledge. Conclusions: Our experimental results show that the proposed method performs reliably for learning the weights of medical knowledge. M3KN outperforms other existing methods by achieving an F-measure of 0.731 for CEMRs and 0.4538 for BERs. This further illustrates that M3KN can facilitate the investigations of intelligent healthcare. © 2018 Elsevier B.V.",10.1016/j.cmpb.2018.01.005,2018.0,4.0,Electronic medical records; Markov logic network; Medical knowledge network; Weight learning,"Computer circuits; Diagnosis; Hospital data processing; Learning systems; Markov processes; Maximum likelihood; Medical computing; Optimization; Speech recognition; Electronic medical record; Intelligent diagnosis; Markov logic networks; Maximum likelihood learning; Medical knowledge; Probabilistic graphical models; Sequential minimal optimization algorithms; Weight learning; Learning algorithms; article; blood examination; diagnosis; diagnostic accuracy; diagnostic test accuracy study; electronic medical record; human; human tissue; learning algorithm; logic; maximum likelihood method; algorithm; artificial neural network; China; computer assisted diagnosis; computer graphics; electronic health record; machine learning; Markov chain; procedures; reproducibility; signal processing; statistical model; theoretical model; Algorithms; China; Computer Graphics; Diagnosis, Computer-Assisted; Electronic Health Records; Humans; Likelihood Functions; Machine Learning; Markov Chains; Models, Statistical; Models, Theoretical; Neural Networks (Computer); Reproducibility of Results; Signal Processing, Computer-Assisted",2-s2.0-85041550100
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Socher, R., Manning, C.D., Ng, A.Y., Learning continuous phrase representations and syntactic parsing with recursive neural networks (2010) NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop",Learning continuous phrase representations and syntactic parsing with recursive neural networks,"Socher R., Manning C.D., Ng A.Y.",Learning continuous phrase representations and syntactic parsing with recursive neural networks,[No abstract available],,2010.0,191.0,,,2-s2.0-80053234189
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Socher, R., Lin, C.Y., Ng, A.Y., Parsing natural scenes and natural language with recursive neural networks (2012) International Conference on International Conference on Machine Learning. Omnipress, pp. 129-136",Parsing natural scenes and natural language with recursive neural networks,"Socher R., Lin C.C.-Y., Ng A.Y., Manning C.D.",Parsing natural scenes and natural language with recursive neural networks,"Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%. Copyright 2011 by the author(s)/owner(s).",,2011.0,819.0,,Alternative approach; Complex scenes; Data sets; Descriptors; Natural languages; Natural scene images; Natural scenes; Parse trees; Recursive neural networks; Recursive structure; Scene classification; Scene segmentation; Stanford; State-of-the-art performance; Structure prediction; Syntactic parsers; Treebanks; Algorithms; Learning systems; Semantics; Neural networks,2-s2.0-80053438267
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Socher, R., Perelygin, A., Wu, J., Recursive deep models for semantic compositionality over a sentiment treebank (2013) Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pp. 1631-1642",Recursive deep models for semantic compositionality over a sentiment treebank,"Socher R., Perelygin A., Wu J.Y., Chuang J., Manning C.D., Ng A.Y., Potts C.",Recursive deep models for semantic compositionality over a sentiment treebank,"Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. © 2013 Association for Computational Linguistics.",,2013.0,2896.0,,Natural language processing systems; Semantics; Bag of features; Compositionality; Fine grained; Parse trees; Positive/negative classifications; State of the art; Supervised trainings; Word spaces; Forestry; Classification; Languages; Word Processing,2-s2.0-84926358845
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," https://www.i2b2.org/, I2B2, “Informatics for integrating biology & the bedside.” [Online] Available:",,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Yilmaz, E., Kanoulas, E., Aslam, J.A., A simple and efficient sampling method for estimating AP and NDCG (2008) International ACM SIGIR Conference on Research and Development in Information Retrieval. ACM, pp. 603-610",A simple and efficient sampling method for estimating AP and NDCG,"Yilmaz E., Kanoulas E., Aslam J.A.",A simple and efficient sampling method for estimating AP and NDCG,"We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were proposed as a solution to the extensive effort required to judge tens of thousands of documents. While the first method proposed by Aslam et al. [1] is quite accurate and efficient, it is overly complex, making it difficult to be used by the community, and while the second method proposed by Yilmaz et al., infAP [14], is relatively simple, it is less efficient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide confidence intervals on the estimated values. The contribution of this paper is threefold: (1) we derive confidence intervals for infAP, (2) we extend infAP to incorporate nonrandom relevance judgments by employing stratified random sampling, hence combining the efficiency of stratification with the simplicity of random sampling, (3) we describe how this approach can be utilized to estimate nDCG from incomplete judgments. We validate the proposed methods using TREC data and demonstrate that these new methods can be used to incorporate nonrandom samples, as were available in TREC Terabyte track '06. Copyright 2008 ACM.",10.1145/1390334.1390437,2008.0,203.0,Average precision; Evaluation; Incomplete judgments; InfAP; nDCG; Sampling,Average precision; Evaluation; Incomplete judgments; InfAP; nDCG; Information retrieval; Information retrieval systems; Information services; Research and development management; Random processes,2-s2.0-57349107098
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Krizhevsky, A., Sutskever, I., Hinton, G., ImageNet classification with deep convolutional neural networks (2012) NIPS",ImageNet classification with deep convolutional neural networks,"Krizhevsky A., Sutskever I., Hinton G.E.",ImageNet classification with deep convolutional neural networks,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",,2012.0,54278.0,,Convolutional neural network; Different class; GPU implementation; High resolution image; Max-pooling; Overfitting; Regularization methods; Test errors; Convolution; Neural networks,2-s2.0-84876231242
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Krizhevsky, A., Sutskever, I., Hinton, G., ImageNet classification with deep convolutional neural networks (2012) NIPS",ImageNet classification with deep convolutional neural networks,"Krizhevsky A., Sutskever I., Hinton G.E.",ImageNet classification with deep convolutional neural networks,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. © 2017 ACM.",10.1145/3065386,2017.0,5124.0,,Convolution; Image classification; Neural networks; Convolutional neural network; Different class; GPU implementation; High resolution image; Max-pooling; Overfitting; Regularization methods; State of the art; Deep neural networks,2-s2.0-85020126914
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Mikolov, T., Karafiát, M., Burget, L., Recurrent neural network based language model (2010) 11st Annual Conference of the International Speech Communication Association",Recurrent neural network based language model,"Mikolov T., Karafiát M., Burget L., Jan C., Khudanpur S.",Recurrent neural network based language model,"A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. © 2010 ISCA.",,2010.0,2847.0,Language modeling; Recurrent neural networks; Speech recognition,Computational linguistics; Modeling languages; Speech communication; Speech recognition; Backoff; Language model; N-grams; State of the art; Wall Street Journal; Word error rate; Recurrent neural networks,2-s2.0-79959829092
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Yang, Z., Yang, D., Dyer, C., Hierarchical attention networks for document classification (2016) Proceedings of the 2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1480-1489",Hierarchical attention networks for document classification,"Yang Z., Yang D., Dyer C., He X., Smola A., Hovy E.",Hierarchical attention networks for document classification,"We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences. ©2016 Association for Computational Linguistics.",10.18653/v1/n16-1174,2016.0,2239.0,,Classification (of information); Computational linguistics; Text processing; Attention mechanisms; Document Classification; Document Representation; Hierarchical structures; Proposed architectures; Sentence level; Text classification; Information retrieval systems,2-s2.0-84994158553
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Richardson, M., Domingos, P., Markov logic networks (2006) Mach Learn, 63 (2). , pp. 207-207",Markov logic networks,"Richardson M., Domingos P.",Markov logic networks,"We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a firstorder knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.",10.1007/s10994-006-5833-1,2006.0,1746.0,First-order logic; Graphical models; Inductive logic programming; Knowledge-based model construction; Log-linear models; Markov chain Monte Carlo; Markov networks; Markov random fields; Pseudo-likelihood; Satisfiability; Statistical relational learning,Information analysis; Learning systems; Markov processes; Monte Carlo methods; Optimization; Relational database systems; First-order logic; Graphical models; Inductive logic programming; Knowledge-based model construction; Log-linear models; Markov logic networks (MLN); Markov random fields; Pseudo likelihood; Satisfiability; Statistical relational learning; Computer graphics,2-s2.0-32044466073
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Richardson, M., Domingos, P., Markov logic networks (2006) Mach Learn, 63 (2). , pp. 207-207",Markov logic networks,"Richardson M., Domingos P.",Markov logic networks,[No abstract available],,2006.0,2.0,,,2-s2.0-34748853447
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," http://i.stanford.edu/hazy/tuffy/, Project Tuffy, “Meet Tuffy.” [Online] Available:",,,,,,,,,,
2-s2.0-85078077985,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis," Maaten, L.V.D., Hinton, G., Visualizing data using t-SNE (2017) J Mach Learn Res, 9 (2605), pp. 2579-2605",Visualizing data using t-SNE,"Van Der Maaten L., Hinton G.",Visualizing data using t-SNE,"We present a new technique called ""t-SNE"" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.",,2008.0,13802.0,Dimensionality reduction; Embedding algorithms; Manifold learning; Multidimensional scaling; Visualization,Learning algorithms; Maps; Numerical analysis; Three dimensional; Visualization; Data sets; Different scales; Dimensional datums; Dimensionality reduction; Embedding algorithms; Locally linear embedding.; Manifold learning; Multidimensional scaling; Multiple classes; Multiple viewpoints; Neighborhood graphs; New techniques; Random walks; Sammon mappings; Stochastic neighbor embedding; Very large datums; Visualization techniques; Data visualization,2-s2.0-57249084011
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases,"Battaglia, P., (2018) Relational Inductive Biases, Deep Learning, and Graph Networks, , arXiv preprint arXiv",,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Bordes, A., Usunier, N., García-Durán, A., Weston, J., Yakhnenko, O., Translating embeddings for modeling multi-relational data (2013) Proceedings of NIPS, pp. 2787-2795",Translating embeddings for modeling multi-relational data,"Bordes A., Usunier N., Garcia-Durán A., Weston J., Yakhnenko O.",Translating embeddings for modeling multi-relational data,"We consider the problem of embedding entities and relationships of multi relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.",,2013.0,2395.0,,Canonical modeling; Knowledge basis; Large scale data sets; Link prediction; Relational data; State-of-the-art methods; Training sample; Very large database,2-s2.0-84899013802
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Brockschmidt, M., Allamanis, M., Gaunt, A.L., Polozov, O., (2018) Generative Code Modeling with Graphs",,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Das, R., Neelakantan, A., Belanger, D., McCallum, A., Chains of reasoning over entities, relations, and text using recurrent neural networks (2017) Proceedings of EACL, pp. 132-141. , 2017",and text using recurrent neural networks,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Evans, R., Grefenstette, E., Learning explanatory rules from noisy data (2018) J. Artif. Intell. Res., 61, pp. 1-64",Learning explanatory rules from noisy data,"Evans R., Grefenstette E.",Learning explanatory rules from noisy data,"Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data—which is not necessarily easily obtained—that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve. © 2018 AI Access Foundation. All rights reserved.",10.1613/jair.5477,2018.0,128.0,,Computer circuits; Logic programming; Neural networks; Ambiguous Data; Data distribution; Efficient process; Inductive logic; Over fitting problem; Powerful functions; Regularisation; Robustness to noise; Inductive logic programming (ILP),2-s2.0-85041111935
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," D’Avila Garcez, A., Gori, M., Lamb, L., Serafini, L., Spranger, M., Tran, S., Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning (2019) FLAP, 6 (4), pp. 611-632",Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning,"Garcez A.D., Gori M., Lamb L.C., Serafini L., Spranger M., Tran S.N.",Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems. © 2019, College Publications. All rights reserved.",,2019.0,25.0,,,2-s2.0-85071341558
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Gilmer, J., Schoenholz, S.S., Riley, P.F., Vinyals, O., Dahl, G.E., Neural message passing for quantum chemistry (2017) Proceeding of ICML, pp. 1263-1272",Neural message passing for quantum chemistry,"Gilmer J., Schoenholz S.S., Riley P.F., Vinyals O., Dahl G.E.",Neural message passing for quantum chemistry,[No abstract available],,2017.0,814.0,,,2-s2.0-85038875959
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Hu, J., Guo, C., Yang, B., Jensen, C.S., Chen, L., (2018) Recurrent Multi-Graph Neural Networks for Travel Cost Prediction",,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Lao, N., Mitchell, T.M., Cohen, W.W., Random walk inference and learning in a large scale knowledge base (2011) In: Proceeding of EMNLP 2011, pp. 529-539",Random walk inference and learning in a large scale knowledge base,"Lao N., Mitchell T., Cohen W.W.",Random walk inference and learning in a large scale knowledge base,"We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL's earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. © 2011 Association for Computational Linguistics.",,2011.0,340.0,,Carlson; Inference methods; Knowledge base; Learning methods; Path ranking; Random Walk; Computational linguistics; Knowledge based systems; Random processes; User interfaces; Natural language processing systems,2-s2.0-80053232762
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Mao, J., Gan, C., Kohli, P., Tenenbaum, J.B., Wu, J., The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision (2019) ICLR 2019",and sentences from natural supervision,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Neelakantan, A., Roth, B., McCallum, A., Compositional vector space models for knowledge base completion (2015) Proceedings of ACL, pp. 156-166. , 2015",Compositional vector space models for knowledge base completion,"Neelakantan A., Roth B., McCallum A.",Compositional vector space models for knowledge base completion,"Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z)!containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-Atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%. © 2015 Association for Computational Linguistics.",10.3115/v1/p15-1016,2015.0,113.0,,Classification (of information); Computational linguistics; Embeddings; Knowledge based systems; Recurrent neural networks; Vector spaces; Binary relation; Compositional modeling; High capacity; Knowledge base; Multihop; Recurrent neural network (RNN); Training time; Vector space models; Natural language processing systems,2-s2.0-84943805009
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Nguyen, D.Q., Sirts, K., Qu, L., Johnson, M., Stranse: A novel embedding model of entities and relationships in knowledge bases (2016) NAACL HLT, 2016, pp. 460-466. , http://aclweb.org/anthology/N/N16/N16-1054.pdf",Stranse: A novel embedding model of entities and relationships in knowledge bases,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Raghavan, S., (2020) AI Predictions from IBM Research (2019)., , https://www.ibm.com/blogs/research/2019/12/2020-ai-predictions/, Accessed 14 Jan 2020",,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Riedel, S., Yao, L., McCallum, A., Marlin, B.M., Relation extraction with matrix factorization and universal schemas (2013) NAACL-HLT, pp. 74-84. , http://aclweb.org/anthology/N/N13/N13-1008.pdf",Relation extraction with matrix factorization and universal schemas,"Riedel S., Yao L., McCallum A., Marlin B.M.",Relation extraction with matrix factorization and universal schemas,"Traditional relation extraction predicts relations within some fixed and finite target schema. Machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. The need for existing datasets can be avoided by using a universal schema: the union of all involved schemas (surface form predicates as in OpenIE, and relations in the schemas of preexisting databases). This schema has an almost unlimited set of relations (due to surface forms), and supports integration with existing structured data (through the relation types of existing databases). To populate a database of such schema we present matrix factorization models that learn latent feature vectors for entity tuples and relations. We show that such latent models achieve substantially higher accuracy than a traditional classification approach. More importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms stateof- the-Art distant supervision. © 2013 Association for Computational Linguistics.",,2013.0,339.0,,Artificial intelligence; Classification (of information); Computational linguistics; Database systems; Extraction; Factorization; Learning systems; Classification approach; Feature vectors; Machine learning approaches; Manual annotation; Matrix factorizations; Relation extraction; State of the art; Structured data; Matrix algebra,2-s2.0-84926180274
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Schlichtkrull, M., Kipf, T.N., Bloem, P., van den Berg, R., Titov, I., Welling, M., Modeling relational data with graph convolutional networks (2018) ESWC 2018. LNCS, 10843, pp. 593-607. , https://doi.org/10.1007/978-3-319-93417-438, Gangemi, A., et al. (eds.) , Springer, Cham",Modeling relational data with graph convolutional networks,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Smolensky, P., (2019) Next-Generation Architectures Bridge Gap between Neural and Symbolic Representations with Neural Symbols, , https://www.microsoft.com/en-us/research/blog/next-generation-architectures-bridge-gap-between-neural-and-symbolic-representations-with-neural-symbols/, Accessed 14 Jan 2020",,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Socher, R., Chen, D., Manning, C.D., Ng, A.Y., Reasoning with neural tensor networks for knowledge base completion (2013) Proceeding of NIPS, pp. 926-934",Reasoning with neural tensor networks for knowledge base completion,"Socher R., Chen D., Manning C.D., Ng A.Y.",Reasoning with neural tensor networks for knowledge base completion,"Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the ""Sumatran tiger"" and ""Bengal tiger."" Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.",,2013.0,1029.0,,Knowledge based systems; Tensors; Atomic units; Knowledge base; Knowledge basis; Large corpora; Question Answering; Statistical strength; Vector representations; Word vectors; Neural networks,2-s2.0-84898956227
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., Yu, P.S., (2019) A Comprehensive Survey on Graph Neural Networks, , CoRR abs/1901.00596",,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Xiong, W., Hoang, T., Wang, W.Y., Deeppath: A reinforcement learning method for knowledge graph reasoning (2017) Proceedings of EMNLP, pp. 564-573. , 2017",Deeppath: A reinforcement learning method for knowledge graph reasoning,,,,,,,,,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Yin, W., Yaghoobzadeh, Y., Schütze, H., Recurrent one-hop predictions for reasoning over knowledge graphs (2018) Proceedings of COLING, 2018, pp. 2369-2378",Recurrent one-hop predictions for reasoning over knowledge graphs,"Yin W., Yaghoobzadeh Y., Schütze H.",Recurrent one-hop predictions for reasoning over knowledge graphs,[No abstract available],,2018.0,4.0,,,2-s2.0-85079053011
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases," Zhang, M., Chen, Y., Link prediction based on graph neural networks (2018) Proceeding of NIPS, pp. 5171-5181",Link prediction based on graph neural networks,"Zhang M., Chen Y.",Link prediction based on graph neural networks,"Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a “heuristic” that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel -decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the -decaying theory, we propose a new method to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems. © 2018 Curran Associates Inc..All rights reserved.",,2018.0,237.0,,Forecasting; Heuristic methods; Function mapping; Graph neural networks; Heuristic learning; Interpretability; Link prediction; Practical use; Score function; Structured data; Graph theory,2-s2.0-85064835122
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification,"Jiang, S., Baumgartner, S., Ittycheriah, A., Yu, C., Factoring factchecks: Structured information extraction from fact-checking articles (2020) Proc. Web Conf., pp. 1592-1603. , Apr",Factoring factchecks: Structured information extraction from fact-checking articles,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Thorne, J., Vlachos, A., Christodoulopoulos, C., Mittal, A., (2018) FEVER: A Large-scale Dataset for Fact Extraction and Verification, , http://arxiv.org/abs/1803.05355",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Thorne, J., Vlachos, A., Cocarascu, O., Christodoulopoulos, C., Mittal, A., (2018) The Fact Extraction and Verification (FEVER) Shared Task, , http://arxiv.org/abs/1811.10971",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Zhou, J., Han, X., Yang, C., Liu, Z., Wang, L., Li, C., Sun, M., (2019) GEAR: Graph-based Evidence Aggregating and Reasoning for Fact Verification, , http://arxiv.org/abs/1908.01843",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Nie, Y., Chen, H., Bansal, M., Combining fact extraction and verification with neural semantic matching networks (2019) Proc. Aaai Conf. Artif. Intell., 33, pp. 6859-6866",Combining fact extraction and verification with neural semantic matching networks,"Nie Y., Chen H., Bansal M.",Combining fact extraction and verification with neural semantic matching networks,"The increasing concern with misinformation has stimulated research efforts on automatic fact checking. The recently-released FEVER dataset introduced a benchmark fact-verification task in which a system is asked to verify a claim using evidential sentences from Wikipedia documents. In this paper, we present a connected system consisting of three homogeneous neural semantic matching models that conduct document retrieval, sentence selection, and claim verification jointly for fact extraction and verification. For evidence retrieval (document retrieval and sentence selection), unlike traditional vector space IR models in which queries and sources are matched in some pre-designed term vector space, we develop neural models to perform deep semantic matching from raw textual input, assuming no intermediate term representation and no access to structured external knowledge bases. We also show that Pageview frequency can also help improve the performance of evidence retrieval results, that later can be matched by using our neural semantic matching network. For claim verification, unlike previous approaches that simply feed upstream retrieved evidence and the claim to a natural language inference (NLI) model, we further enhance the NLI model by providing it with internal semantic relatedness scores (hence integrating it with the evidence retrieval modules) and ontological WordNet features. Experiments on the FEVER dataset indicate that (1) our neural semantic matching method outperforms popular TF-IDF and encoder models, by significant margins on all evidence retrieval metrics, (2) the additional relatedness score and WordNet features improve the NLI model via better semantic awareness, and (3) by formalizing all three subtasks as a similar semantic matching problem and improving on all three stages, the complete model is able to achieve the state-of-the-art results on the FEVER test set (two times greater than baseline results). © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2019.0,52.0,,Extraction; Information retrieval; Natural language processing systems; Ontology; Semantic Web; Semantics; Statistical tests; Vector spaces; Connected systems; Document Retrieval; External knowledge; Semantic matching; Semantic relatedness; Sentence selection; Term representation; Verification task; Artificial intelligence,2-s2.0-85070497878
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Fan, C.-T., Wang, Y.-K., Huang, C.-R., Heterogeneous information fusion and visualization for a large-scale intelligent video surveillance system (2017) Ieee Trans. Syst., Man, Cybern. Syst., 47 (4), pp. 593-604. , Apr",Heterogeneous information fusion and visualization for a large-scale intelligent video surveillance system,"Fan C.-T., Wang Y.-K., Huang C.-R.",Heterogeneous information fusion and visualization for a large-scale intelligent video surveillance system,"Wide-area monitoring for a smart community can be challenging in systems engineering because of its large scale and heterogeneity at the sensor, algorithm, and visualization levels. A smart interface to visualize high-level information fused from a diversity of low-level surveillance data, and to facilitate rapid response of events, is critical for the design of the system. This paper presents an event-driven visualization mechanism fusing multimodal information for a large-scale intelligent video surveillance system. The mechanism proactively helps security personnel intuitively be aware of events through close cooperation among visualization, data fusion, and sensor tasking. The visualization not only displays 2-D, 3-D, and geographical information within a condensed form of interface but also automatically shows the only important video streams corresponding to spontaneous alerts and events by a decision process called display switching arbitration. The display switching arbitration decides the importance of cameras by score ranking that considers event urgency and semantic object features. This system has been successfully deployed in a campus to demonstrate its usability and efficiency for an installation with two camera clusters that include dozens of cameras, and with a lot of video analytics to detect alerts and events. A further simulation comparing the display switching arbitration with similar camera selection methods shows that our method improves the visualization by selecting better representative camera views and reducing redundant switchover among multiview videos. © 2016 IEEE.",10.1109/TSMC.2016.2531671,2017.0,41.0,Display switching arbitration; information fusion; third-generation surveillance system (3GSS); visual surveillance; visualizability; visualization,Cameras; Data fusion; Data visualization; Flow visualization; Information fusion; Monitoring; Semantics; Sensor data fusion; Switching; Three dimensional computer graphics; Video streaming; Visualization; Camera selection methods; Geographical information; Heterogeneous information; Intelligent video surveillance systems; Multi-modal information; Surveillance systems; Visual surveillance; visualizability; Security systems,2-s2.0-85017619710
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Zhang, X., Zhang, Y., Wang, S., Yao, Y., Fang, B., Yu, P.S., Improving stock market prediction via heterogeneous information fusion (2018) Knowl.-Based Syst., 143, pp. 236-247. , Mar",Improving stock market prediction via heterogeneous information fusion,"Zhang X., Zhang Y., Wang S., Yao Y., Fang B., Yu P.S.",Improving stock market prediction via heterogeneous information fusion,"Traditional stock market prediction approaches commonly utilize the historical price-related data of the stocks to forecast their future trends. As the Web information grows, recently some works try to explore financial news to improve the prediction. Effective indicators, e.g., the events related to the stocks and the people's sentiments toward the market and stocks, have been proved to play important roles in the stocks’ volatility, and are extracted to feed into the prediction models for improving the prediction accuracy. However, a major limitation of previous methods is that the indicators are obtained from only a single source whose reliability might be low, or from several data sources but their interactions and correlations among the multi-sourced data are largely ignored. In this work, we extract the events from Web news and the users’ sentiments from social media, and investigate their joint impacts on the stock price movements via a coupled matrix and tensor factorization framework. Specifically, a tensor is firstly constructed to fuse heterogeneous data and capture the intrinsic relations among the events and the investors’ sentiments. Due to the sparsity of the tensor, two auxiliary matrices, the stock quantitative feature matrix and the stock correlation matrix, are constructed and incorporated to assist the tensor decomposition. The intuition behind is that stocks that are highly correlated with each other tend to be affected by the same event. Thus, instead of conducting each stock prediction task separately and independently, we predict multiple correlated stocks simultaneously through their commonalities, which are enabled via sharing the collaboratively factorized low rank matrices between matrices and the tensor. Evaluations on the China A-share stock data and the HK stock data in the year 2015 demonstrate the effectiveness of the proposed model. © 2017",10.1016/j.knosys.2017.12.025,2018.0,68.0,Social media; Stock correlation; Stock prediction; Tensor factorization,Commerce; Factorization; Finance; Financial markets; Forecasting; Investments; Social networking (online); Tensors; 00-01; 99-00; Social media; Stock predictions; Tensor factorization; Matrix algebra,2-s2.0-85039943279
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Wang, F., Hu, L., Zhou, J., Hu, J., Zhao, K., A semantics-based approach to multi-source heterogeneous information fusion in the Internet of Things (2017) Soft Comput., 21 (8), pp. 2005-2013. , Apr",A semantics-based approach to multi-source heterogeneous information fusion in the Internet of Things,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Li, G., Kou, G., Peng, Y., A group decision making model for integrating heterogeneous information (2018) Ieee Trans. Syst., Man, Cybern. Syst., 48 (6), pp. 982-992. , Jun",A group decision making model for integrating heterogeneous information,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Qiu, R.G., Towards ontology-driven knowledge synthesis for heterogeneous information systems (2006) J. Intell. Manuf., 17 (1), pp. 99-109",Towards ontology-driven knowledge synthesis for heterogeneous information systems,Qiu R.G.,Towards ontology-driven knowledge synthesis for heterogeneous information systems,"Information integration enables delivery of the right information to the right user in a timely manner giving manufacturers a competitive edge in today's global manufacturing market. However, as enterprise information is usually aggregated from a variety of heterogeneous information sources, without using an adequate integration framework it is difficult to extract pertinent information and apply current knowledge to assessing production situations and making informed decisions. This paper investigates a method of facilitating knowledge synthesis in a distributed computing environment. A formal model of domain ontology and knowledge base is presented, which aims at providing a vehicle for representing information and knowledge using a common shared semantics in a given application domain. As a result, a common knowledge representation based architecture is proposed, creating a foundation for establishing a systematic approach for ease of knowledge synthesis in a manufacturing environment. © 2006 Springer Science+Business Media, Inc.",10.1007/s10845-005-5515-z,2006.0,19.0,Decision-making; E-manufacturing; Heterogeneity; Information fusion; Knowledge synthesis,Decision making; Electronic commerce; Information science; Manufacturing data processing; Mathematical models; Semantics; E-manufacturing; Heterogeneity; Information fusion; Knowledge synthesis; Shared semantics; Knowledge based systems,2-s2.0-30844471985
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Hu, B., Shi, C., Zhao, W.X., Yang, T., Local and global information fusion for top-N recommendation in heterogeneous information network (2018) Proc. 27th Acm Int. Conf. Inf. Knowl. Manage., pp. 1683-1686. , Oct",Local and global information fusion for top-N recommendation in heterogeneous information network,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Wang, X., Ji, H., Shi, C., Wang, B., Ye, Y., Cui, P., Yu, P.S., Heterogeneous graph attention network (2019) Proc. World Wide Web Conf. (WWW), pp. 2022-2032",Heterogeneous graph attention network,"Wang X., Ji H., Cui P., Yu P., Shi C., Wang B., Ye Y.",Heterogeneous graph attention network,"Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis. © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.",10.1145/3308558.3313562,2019.0,306.0,Graph Analysis; Neural Network; Social Network,Deep learning; Neural networks; Semantics; Social networking (online); World Wide Web; Attention mechanisms; Graph analysis; Graph neural networks; Graph representation; Heterogeneous graph; Interpretability; Research interests; Semantic information; Graph theory,2-s2.0-85066907835
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Ai, Q., Azizi, V., Chen, X., Zhang, Y., Learning heterogeneous knowledge base embeddings for explainable recommendation (2018) Algorithms, 11 (9), p. 137. , Sep",Learning heterogeneous knowledge base embeddings for explainable recommendation,"Ai Q., Azizi V., Chen X., Zhang Y.",Learning heterogeneous knowledge base embeddings for explainable recommendation,"Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms-especially the collaborative filtering (CF)-based approaches with shallow or deep models-usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users' historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines. © 2018 by the authors.",10.3390/a11090137,2018.0,88.0,Collaborative filtering; Explainable recommendation; Knowledge-base embedding; Recommender systems,Electronic commerce; Knowledge based systems; Recommender systems; Content-based approach; Explainable recommendation; Heterogeneous Knowledge; Knowledge base; Personalized recommendation systems; Recommendation algorithms; Recommendation performance; Structured knowledge; Collaborative filtering,2-s2.0-85053805293
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Wang, X., He, X., Cao, Y., Liu, M., Chua, T.-S., KGAT: Knowledge graph attention network for recommendation (2019) Proc. 25th Acm Sigkdd Int. Conf. Knowl. Discovery Data Mining, pp. 950-958. , Jul",KGAT: Knowledge graph attention network for recommendation,"Wang X., He X., Cao Y., Liu M., Chua T.-S.",KGAT: Knowledge graph attention network for recommendation,"To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations - which connect two items with one or multiple linked attributes - are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM [11] and RippleNet [29]. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism. We release the codes and datasets at https://github.com/xiangwang1223/knowledge_graph_attention_network. © 2019 Association for Computing Machinery.",10.1145/3292500.3330989,2019.0,280.0,Collaborative Filtering; Embedding Propagation; Graph Neural Network; Higher-order Connectivity; Knowledge Graph; Recommendation,Backpropagation; Collaborative filtering; Embeddings; Factorization machines; Graph neural networks; Higher-order; Knowledge graphs; Recommendation; Recommendation methods; State-of-the-art methods; Supervised learning problems; Data mining,2-s2.0-85071195192
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Zhong, W., Xu, J., Tang, D., Xu, Z., Duan, N., Zhou, M., Wang, J., Yin, J., (2019) Reasoning over Semantic-level Graph for Fact Checking, , http://arxiv.org/abs/1909.03745",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Hanselowski, A., Stab, C., Schulz, C., Li, Z., Gurevych, I., (2019) A Richly Annotated Corpus for Different Tasks in Automated Fact-checking, , http://arxiv.org/abs/1911.01214",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Luken, J., Jiang, N., De Marneffe, M.-C., QED: A fact verification system for the FEVER shared task (2018) Proc. 1stWorkshop Fact Extraction Verification (FEVER), pp. 156-160",QED: A fact verification system for the FEVER shared task,"Luken Jackson, Jiang Nanjiang, de Marneffe Marie-Catherine",QED: A fact verification system for the FEVER shared task,[No abstract available],,2018.0,5.0,,,2-s2.0-85084052217
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Parikh, A.P., Täckström, O., Das, D., Uszkoreit, J., (2016) A Decomposable Attention Model for Natural Language Inference, , http://arxiv.org/abs/1606.01933",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Hanselowski, A., Zhang, H., Li, Z., Sorokin, D., Schiller, B., Schulz, C., Gurevych, I., (2018) UKP-athene: Multi-sentence Textual Entailment for Claim Verification, , http://arxiv.org/abs/1809.01479",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Yoneda, T., Mitchell, J., Welbl, J., Stenetorp, P., Riedel, S., UCL machine reading group: Four factor framework for fact finding (HexaF) (2018) Proc. 1st Workshop Fact Extraction Verification (FEVER), pp. 97-102",UCL machine reading group: Four factor framework for fact finding (HexaF),,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Hidey, C., Diab, M., Team SWEEPer: Joint sentence extraction and fact checking with pointer networks (2018) Proc. 1st Workshop Fact Extraction Verification (FEVER), pp. 150-155",Team SWEEPer: Joint sentence extraction and fact checking with pointer networks,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Chen, Q., Zhu, X., Ling, Z., Wei, S., Jiang, H., Inkpen, D., (2016) Enhanced Lstm for Natural Language Inference, , http://arxiv.org/abs/1609.06038",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Malon, C., (2019) Team Papelo: Transformer Networks at Fever, , http://arxiv.org/abs/1901.02534",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Radford, A., Narasimhan, K., Salimans, T., Sutskever, I., (2018) Improving Language Understanding by Generative Pre-Training, , https://s3-us-west-2.amazonaws.com/openai-assets/researchcovers/languageunsupervised/languageunderstandingpaper.pdf",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Zhao, C., Xiong, C., Rosset, C., Song, X., Bennett, P., Tiwary, S., Transformer-XH: Multi-evidence reasoning with extra hop attention (2019) Proc. Int. Conf. Learn. Represent., pp. 1-16",Transformer-XH: Multi-evidence reasoning with extra hop attention,"Zhao C., Xiong C., Rosset C., Song X., Bennett P., Tiwary S.",Transformer-XH: Multi-evidence reasoning with extra hop attention,[No abstract available],,2019.0,1.0,,,2-s2.0-85091239823
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Liu, Z., Xiong, C., Sun, M., Liu, Z., Fine-grained fact verification with kernel graph attention network (2020) Proc. 58th Annu. Meeting Assoc. Comput. Linguistics, pp. 7342-7351",Fine-grained fact verification with kernel graph attention network,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Shi, P., Lin, J., (2019) Simple Bert Models for Relation Extraction and Semantic Role Labeling, , http://arxiv.org/abs/1904.05255",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Dagan, I., Glickman, O., Magnini, B., The PASCAL recognising textual entailment challenge (2005) Proc. Mach. Learn. Challenges Workshop. Berlin, pp. 177-190. , Germany: Springer",The PASCAL recognising textual entailment challenge,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Williams, A., Nangia, N., Bowman, S.R., (2017) A Broad-coverage Challenge Corpus for Sentence Understanding through Inference, , http://arxiv.org/abs/1704.05426",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Bowman, S.R., Angeli, G., Potts, C., Manning, C.D., (2015) A Large Annotated Corpus for Learning Natural Language Inference, , http://arxiv.org/abs/1508.05326",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Sha, L., Chang, B., Sui, Z., Li, S., Reading and thinking: Re-read LSTM unit for textual entailment recognition (2016) Proc. Coling, pp. 2870-2879. , 26th Int. Conf. Comput. Linguistics, Tech. Papers",Reading and thinking: Re-read LSTM unit for textual entailment recognition,"Sha L., Chang B., Sui Z., Li S.",Reading and thinking: Re-read LSTM unit for textual entailment recognition,"Recognizing Textual Entailment (RTE) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI1) corpus has made it possible to develop and evaluate deep neural network methods for the RTE task. Previous neural network based methods usually try to encode the two sentences and send them together into multi-layer perceptron, or use LSTM-RNN to link two sentence together while using attention mechanic to enhance the model's ability. In this paper, we propose to use the intensive reading mechanic, which means to re-read the sentence (read the sentence again) according to the memory of the other sentence for a better understanding of the sentence pair. The re-read process can be applied alternatively between the two sentences. Experiments show that we achieve results better than current state-of-art equivalents. © 1963-2018 ACL.",,2016.0,39.0,,Computational linguistics; Deep neural networks; Natural language processing systems; Text processing; Multi layer perceptron; Natural languages; Neural network method; Recognizing textual entailments; Stanford; Textual entailment; Long short-term memory,2-s2.0-85031900378
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Conneau, A., Kiela, D., Schwenk, H., Barrault, L., Bordes, A., (2017) Supervised Learning of Universal Sentence Representations from Natural Language Inference Data, , http://arxiv.org/abs/1705.02364",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Ghaeini, R., Hasan, S.A., Datla, V., Liu, J., Lee, K., Qadir, A., Ling, Y., Farri, O., (2018) DR-BiLSTM: Dependent Reading Bidirectional Lstm for Natural Language Inference, , http://arxiv.org/abs/1802.05577",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Xu, K., Hu, W., Leskovec, J., Jegelka, S., (2018) How Powerful Are Graph Neural Networks?, , http://arxiv.org/abs/1810.00826",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Kipf, T.N., Welling, M., (2016) Semi-supervised Classification with Graph Convolutional Networks, , http://arxiv.org/abs/1609.02907",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Velikovi, P., Cucurull, G., Casanova, A., Romero, A., Lio, P., Bengio, Y., (2017) Graph Attention Networks, , http://arxiv.org/abs/1710.10903",,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Carreras, X., Màrquez, L., Introduction to the CoNLL-2005 shared task: Semantic role labeling (2005) Proc. 9th Conf. Comput. Natural Lang. Learn. (CONLL), pp. 152-164",Introduction to the CoNLL-2005 shared task: Semantic role labeling,"Carreras X., Márquez L.",Introduction to the CoNLL-2005 shared task: Semantic role labeling,"In this paper we describe the CoNLL- 2005 shared task on Semantic Role Labeling. We introduce the specification and goals of the task, describe the data sets and evaluation methods, and present a general overview of the 19 systems that have contributed to the task, providing a comparative description and results. © 2005 Association for Computational Linguistics.",,2005.0,379.0,,Data sets; Evaluation Method; Semantic role labeling; Semantics,2-s2.0-84862288194
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Fellbaum, C., Word net (2012) the Encyclopedia of Applied Linguistics. Oxford, , U.K.: Blackwell",Word net,Fellbaum C.,Word net,[No abstract available],,2012.0,1.0,,,2-s2.0-85091206985
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Cao, Y., Wang, X., He, X., Hu, Z., Chua, T.-S., Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences (2019) Proc. World Wide Web Conf. (WWW), pp. 151-161",Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences,"Cao Y., Wang X., He X., Hu Z., Chua T.-S.",Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences,"Incorporating knowledge graph (KG) into recommender system is promising in improving the recommendation accuracy and explainability. However, existing methods largely assume that a KG is complete and simply transfer the ""knowledge"" in KG at the shallow level of entity raw data or embeddings. This may lead to suboptimal performance, since a practical KG can hardly be complete, and it is common that a KG has missing facts, relations, and entities. Thus, we argue that it is crucial to consider the incomplete nature of KG when incorporating it into recommender system. In this paper, we jointly learn the model of recommendation and knowledge graph completion. Distinct from previous KG-based recommendation methods, we transfer the relation information in KG, so as to understand the reasons that a user likes an item. As an example, if a user has watched several movies directed by (relation) the same person (entity), we can infer that the director relation plays a critical role when the user makes the decision, thus help to understand the user's preference at a finer granularity. Technically, we contribute a new translation-based recommendation model, which specially accounts for various preferences in translating a user to an item, and then jointly train it with a KG completion model by combining several transfer schemes. Extensive experiments on two benchmark datasets show that our method outperforms state-of-the-art KG-based recommendation methods. Further analysis verifies the positive effect of joint training on both tasks of recommendation and KG completion, and the advantage of our model in understanding user preference. We publish our project at https://github.com/TaoMiner/joint-kg-recommender. © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.",10.1145/3308558.3313705,2019.0,112.0,Embedding; Item Recommendation; Joint Model; Knowledge Graph,Embeddings; HTTP; World Wide Web; Embedding; Item Recommendation; Joint modeling; Knowledge graphs; Recommendation accuracy; Recommendation methods; Relation information; Sub-optimal performance; Recommender systems,2-s2.0-85066889141
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Speer, R., Chin, J., Havasi, C., ConceptNet 5.5: An open multilingual graph of general knowledge (2017) Proc. Nat. Conf. Artif. Intell., pp. 4444-4451",ConceptNet 5.5: An open multilingual graph of general knowledge,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Yang, Z., Dai, Z., Yang, Y., Carbonell, J., Salakhutdinov, R.R., Le, Q.V., Xlnet: Generalized autoregressive pretraining for language understanding (2019) Proc. Adv. Neural Inf. Process. Syst., pp. 5753-5763",Xlnet: Generalized autoregressive pretraining for language understanding,,,,,,,,,
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Lin, Y., Liu, Z., Sun, M., Liu, Y., Zhu, X., Learning entity and relation embeddings for knowledge graph completion (2015) Proc. 29th Aaai Conf. Artif. Intell., pp. 2181-2187",Learning entity and relation embeddings for knowledge graph completion,"Lin Y., Liu Z., Sun M., Liu Y., Zhu X.",Learning entity and relation embeddings for knowledge graph completion,"Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https://github.com/mrlyk423/relation-extraction. © Copyright 2015, Association for the Advancement of Artificial Intelligence (www.aaa1.org). All rights reserved.",,2015.0,1122.0,,Semantics; Common spaces; Corresponding relations; Fact extraction; Knowledge graphs; Link prediction; Semantic Space; Source codes; State of the art; Artificial intelligence,2-s2.0-84959863917
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Bollacker, K., Evans, C., Paritosh, P., Sturge, T., Taylor, J., Freebase: A collaboratively created graph database for structuring human knowledge (2008) Proc. Acm Sigmod Int. Conf. Manage. Data, pp. 1247-1250",Freebase: A collaboratively created graph database for structuring human knowledge,"Bollacker K., Evans C., Paritosh P., Sturge T., Taylor J.",Freebase: A collaboratively created graph database for structuring human knowledge,"Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Free-base currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.",10.1145/1376616.1376746,2008.0,2377.0,Design; Human factors; Languages,Applications.; Data queries; Graph databases; Human factors; Human knowledges; Languages; Manipulation languages; Object-oriented; Database systems; Human engineering; Linguistics; Query languages; Object oriented programming,2-s2.0-57149137628
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Qiu, J., Tang, J., Ma, H., Dong, Y., Wang, K., Tang, J., DeepInf: Social influence prediction with deep learning (2018) Proc. 24th Acm Sigkdd Int. Conf. Knowl. Discovery Data Mining, London, U.K., pp. 2110-2119",DeepInf: Social influence prediction with deep learning,"Qiu J., Tang J., Ma H., Dong Y., Wang K., Tang J.",DeepInf: Social influence prediction with deep learning,"Social and information networking activities such as on Facebook, Twitter, WeChat, and Weibo have become an indispensable part of our everyday life, where we can easily access friends' behaviors and are in turn influenced by them. Consequently, an effective social influence prediction for each user is critical for a variety of applications such as online recommendation and advertising. Conventional social influence prediction approaches typically design various hand-crafted rules to extract user- and network-specific features. However, their effectiveness heavily relies on the knowledge of domain experts. As a result, it is usually difficult to generalize them into different domains. Inspired by the recent success of deep neural networks in a wide range of computing applications, we design an end-to-end framework, DeepInf1, to learn users' latent feature representation for predicting social influence. In general, DeepInf takes a user's local network as the input to a graph neural network for learning her latent social representation. We design strategies to incorporate both network structures and user-specific features into convolutional neural and attention networks. Extensive experiments on Open Academic Graph, Twitter, Weibo, and Digg, representing different types of social and information networks, demonstrate that the proposed end-to-end model, DeepInf, significantly outperforms traditional feature engineering-based approaches, suggesting the effectiveness of representation learning for social applications. © 2018 Association for Computing Machinery.",10.1145/3219819.3220077,2018.0,137.0,Graph Attention; Graph Convolution; Network Embedding; Representation Learning; Social Influence; Social Networks,Convolution; Data mining; Deep neural networks; Forecasting; Information services; Social networking (online); Computing applications; Feature representation; Graph Attention; Information networking; Network embedding; Representation Learning; Social influence; Social representations; Economic and social effects,2-s2.0-85051486121
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Hamilton, W.L., Ying, R., Leskovec, J., Inductive representation learning on large graphs (2017) Proc. Adv. Neural Inf. Process. Syst., pp. 1024-1034. , Long Beach, CA, USA",Inductive representation learning on large graphs,"Hamilton W.L., Ying R., Leskovec J.",Inductive representation learning on large graphs,"Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions. © 2017 Neural information processing systems foundation. All rights reserved.",,2017.0,2169.0,,Classification (of information); Graphic methods; Proteins; Content recommendations; Feature information; Local neighborhoods; Low dimensional; Prediction tasks; Protein functions; Protein-protein interactions; Text attributes; Graph theory,2-s2.0-85046897776
2-s2.0-85091207880,Robust reasoning over heterogeneous textual information for fact verification," Shen, D., Zhang, X., Henao, R., Carin, L., (2018) Improved Semanticaware Network Embedding with Fine-grained Word Alignment, , http://arxiv.org/abs/1808.09633",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification,"Ali, T., Asghar, S., Multi-label scientific document classification (2018) J. Internet Technol., 19 (6), pp. 1707-1716",Multi-label scientific document classification,"Ali T., Asghar S.",Multi-label scientific document classification,"Scientific document label identification is a significant research area having numerous applications like digital libraries. The author assigns a category or categories to their document manually. Likewise, categories are structured in taxonomy in the form of tree such as ACM CCS. The dilemma becomes more complex when a document belongs to multiple categories. The problem of manual assignment becomes more complicated when the number of expected labels increases. Moreover, the accession schemes are insufficient for solutions with higher accuracy on real scientific document datasets. One way to handle the multi-label classification is to change the problem into a single-label classification. Another way is the variation of the algorithm to handle multi-label classification. The focus of our research is on conversion. Moreover, we propose a solution stimulated from the particle swarm optimization algorithm that can consign a label from the taxonomy. A set of similarity measures is evaluated as well for documentation relatedness that are used in the proposed approach. The designed solution is evaluated on two documents dataset that are retrieved from J. UCS and ACM with an average accuracy of 77 percent as compared to the state of the art algorithms. © 2018 Taiwan Academic Network Management Committee. All rights reserved.",10.3966/160792642018111906008,2018.0,2.0,Digital libraries; Multi-label classification; PSO; Text similarity,Digital libraries; Particle swarm optimization (PSO); Taxonomies; Multi label classification; Multi-label; Particle swarm optimization algorithm; Scientific documents; Similarity measure; State-of-the-art algorithms; Text similarity; Text processing,2-s2.0-85060703467
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Liu, L., Mu, F., Li, P., Mu, X., Tang, J., Ai, X., Fu, R., Zhou, X., Neuralclassifier: An open-source neural hierarchical multi-label text classification toolkit (2019) Proc. 57th Annu. Meeting Assoc. Comput. Linguistics, Syst. Demonstrations, pp. 87-92",Neuralclassifier: An open-source neural hierarchical multi-label text classification toolkit,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Liu, J., Chang, W.-C., Wu, Y., Yang, Y., Deep learning for extreme multi-label text classification (2017) Proc. 40th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr. (SIGIR), pp. 115-124",Deep learning for extreme multi-label text classification,"Liu J., Chang W.-C., Wu Y., Yang Y.",Deep learning for extreme multi-label text classification,"Extreme multi-label text classification (XMTC) refers to the problem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. The huge label space raises research challenges such as data sparsity and scalability. Significant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for XMTC, despite its big successes in other related areas. This paper presents the first attempt at applying deep learning to XMTC, with a family of new Convolutional Neural Network (CNN) models which are tailored for multi-label classification in particular. With a comparative evaluation of 7 state-of-The-Art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed CNN approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in particular, it outperformed the second best method by 11:7% ∼ 15:3% in precision@K and by 11:5% ∼ 11:7% in NDCG@K for K = 1,3,5. © 2017 Copyright held by the owner/author(s).",10.1145/3077136.3080834,2017.0,222.0,,Classification (of information); Deep learning; Information retrieval; Information retrieval systems; Learning systems; Neural networks; Vector spaces; Benchmark datasets; Comparative evaluations; Convolutional neural network; Machine learning methods; Multi label classification; Multi-label text classification; Research challenges; State-of-the-art methods; Text processing,2-s2.0-85029387475
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Frank Cooper, J.K., Multiobjective feature selection: Classification using educational datasets in an ensemble validation scheme (2019) Data Sci. Pattern Recognit., 3 (1), pp. 9-34",Multiobjective feature selection: Classification using educational datasets in an ensemble validation scheme,"Frank J., Cooper K.",Multiobjective feature selection: Classification using educational datasets in an ensemble validation scheme,[No abstract available],,2019.0,2.0,,,2-s2.0-85081052615
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Aggarwal, C.C., Zhai, C., (2012) Mining Text Data, , Berlin, Germany: Springer",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Stein, R.A., Jaques, P.A., Valiati, J.F., An analysis of hierarchical text classification using word embeddings (2019) Inf. Sci., 471, pp. 216-232. , Jan",An analysis of hierarchical text classification using word embeddings,"Stein R.A., Jaques P.A., Valiati J.F.",An analysis of hierarchical text classification using word embeddings,"Efficient distributed numerical word representation models (word embeddings) combined with modern machine learning algorithms have recently yielded considerable improvement on automatic document classification tasks. However, the effectiveness of such techniques has not been assessed for the hierarchical text classification (HTC) yet. This study investigates the application of those models and algorithms on this specific problem by means of experimentation and analysis. We trained classification models with prominent machine learning algorithm implementations—fastText, XGBoost, SVM, and Keras’ CNN—and noticeable word embeddings generation methods—GloVe, word2vec, and fastText—with publicly available data and evaluated them with measures specifically appropriate for the hierarchical context. FastText achieved an LCAF1 of 0.893 on a single-labeled version of the RCV1 dataset. An analysis indicates that using word embeddings and its flavors is a very promising approach for HTC. © 2018 Elsevier Inc.",10.1016/j.ins.2018.09.001,2019.0,67.0,fastText; Gradient tree boosting; Hierarchical text classification; Support vector machines; Word embeddings,Artificial intelligence; Classification (of information); Information retrieval systems; Support vector machines; Text processing; Classification models; Document Classification; Embeddings; fastText; Gradient tree boosting; Models and algorithms; Text classification; Word representations; Learning algorithms,2-s2.0-85052975887
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Revanasiddappa, M.B., Harish, B.S., A novel text representation model to categorize text documents using convolution neural network (2019) Int. J. Intell. Syst. Appl., 11 (5), pp. 36-45. , May",A novel text representation model to categorize text documents using convolution neural network,"Revanasiddappa M.B., Harish B.S.",A novel text representation model to categorize text documents using convolution neural network,[No abstract available],,2019.0,1.0,,,2-s2.0-85081050420
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Xie, M., Yin, H., Wang, H., Xu, F., Chen, W., Wang, S., Learning graph-based POI embedding for location-based recommendation (2016) Proc. 25th ACM Int. Conf. Inf. Knowl. Manage. (CIKM), pp. 15-24",Learning graph-based POI embedding for location-based recommendation,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Li, J., Peng, H., Liu, L., Xiong, G., Du, B., Ma, H., Wang, L., Zakirul Alam Bhuiyan, M., Graph CNNs for urban traffic passenger flows prediction (2018) Proc. IEEE SmartWorld, Ubiquitous Intell. Comput., Adv. Trusted Comput., Scalable Comput. Commun., Cloud Big Data Comput., Internet People Smart City Innov. (Smart-World/SCALCOM/UIC/ATC/CBDCom/IOP/SCI), pp. 29-36. , Oct",Graph CNNs for urban traffic passenger flows prediction,"Li J., Peng H., Liu L., Xiong G., Du B., Ma H., Wang L., Zakirul Alam Bhuiyan M.",Graph CNNs for urban traffic passenger flows prediction,"Urban traffic passenger flows prediction has always been a great challenge in transportation field. Efficiently and correctly predicting the future flows of various regions can improve traffic resources scheduling and reduce the possibility of accidents. However, factors which affect the change of traffic passenger flows are complex, including interlaced lines and stations in large areas, diversified traveling demands for people, accidents and bad weathers. So the predicting algorithms or models should be more sensitive to multiply elements and their effecting patterns. Recently, deep learning performs the excellent ability to extract high dimensional spatial-temporal characters in regression and classification tasks. In this paper, we propose a new modeling method for urban traffic passenger flows. Instead of the grid matrices, we quantify the relationship between stations and represent it by a undirected graph. Then we sort the stations by their passenger flows and construct the two-channels graph flows matrices as the input of deep convolutional neural networks. To increase the temporal information of inputs, we also combine the input matrices with recent historical samples. In addition, we add date markers to correct the final prediction flows to further improve the accuracy. Finally we evaluate our model with the real Beijing subway data and compare with other traditional models on short-term passenger flows prediction tasks. Experiments show that our model including multidimensional flows graph matrices and the deep learning model can significantly improve the prediction accuracy. © 2018 IEEE.",10.1109/SmartWorld.2018.00041,2018.0,19.0,Deep learning; Graph cnn; Graph flows matrices; Urban traffic passenger flows prediction,Accidents; Big data; Deep learning; Deep neural networks; Forecasting; Matrix algebra; Neural networks; Smart city; Trusted computing; Ubiquitous computing; Urban transportation; Classification tasks; Deep convolutional neural networks; Graph cnn; Multidimensional flow; Passenger flows; Prediction accuracy; Temporal information; Traditional models; Flow graphs,2-s2.0-85060285351
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Peng, H., Li, J., Gong, Q., Song, Y., Ning, Y., Lai, K., Yu, P.S., Finegrained event categorization with heterogeneous graph convolutional networks (2019) Proc. Int. Joint Conf. Artif. Intell., pp. 1-9",Finegrained event categorization with heterogeneous graph convolutional networks,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Peng, H., Li, J., He, Y., Liu, Y., Bao, M., Wang, L., Song, Y., Yang, Q., Large-scale hierarchical text classification with recursively regularized deep graph-cnn (2018) Proc. World Wide Web Conf., Int. World Wide Web Conf. Steering Committee, pp. 1063-1072",Large-scale hierarchical text classification with recursively regularized deep graph-cnn,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Peng, H., Li, J., Wang, S., Wang, L., Gong, Q., Yang, R., Li, B., He, L., Hierarchical taxonomy-aware and attentional graph capsule RCNNs for large-scale multi-label text classification IEEE Trans. Knowl. Data Eng., , to be published",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Peng, H., Du, B., Ma, H., Bhuiyan, M.Z.A.B., Jianwei, L., Wang, L., Yu, P.S., Spatial temporal incidence dynamic graph neural networks for traffic flow forecasting Inf. Sci., , to be published",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," He, Y., Li, J., Song, Y., He, M., Peng, H., Time-evolving text classification with deep neural networks (2018) Proc. 27th Int. Joint Conf. Artif. Intell., pp. 2241-2247. , Jul",Time-evolving text classification with deep neural networks,"He Y., Li J., Song Y., He M., Peng H.",Time-evolving text classification with deep neural networks,"Traditional text classification algorithms are based on the assumption that data are independent and identically distributed. However, in most non-stationary scenarios, data may change smoothly due to long-term evolution and short-term fluctuation, which raises new challenges to traditional methods. In this paper, we present the first attempt to explore evolutionary neural network models for time-evolving text classification. We first introduce a simple way to extend arbitrary neural networks to evolutionary learning by using a temporal smoothness framework, and then propose a diachronic propagation framework to incorporate the historical impact into currently learned features through diachronic connections. Experiments on real-world news data demonstrate that our approaches greatly and consistently outperform traditional neural network models in both accuracy and stability. © 2018 International Joint Conferences on Artificial Intelligence. All right reserved.",10.24963/ijcai.2018/310,2018.0,19.0,,Backpropagation; Classification (of information); Long Term Evolution (LTE); Neural networks; Text processing; Evolutionary Learning; Evolutionary neural network; Neural network model; Nonstationary; Real-world; Short term; Text classification; Deep neural networks,2-s2.0-85055700079
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Kao, C.-C., Chang, J.-W., Wang, T.-I., Huang, Y.-M., Chiu, P.-S., Design and development of the sentence-based collocation recommender with error detection for academic writing (2019) J. Internet Technol., 20 (1), pp. 229-236",Design and development of the sentence-based collocation recommender with error detection for academic writing,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Liu, P., Qiu, X., Huang, X., (2016) Recurrent Neural Network for Text Classification with Multi-task Learning, , http://arxiv.org/abs/1605.05101",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Shen, T., Zhou, T., Long, G., Jiang, J., Zhang, C., (2018) Bi-directional Block Self-attention for Fast and Memory-efficient Sequence Modeling, , http://arxiv.org/abs/1804.00857",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudinov, R., Zemel, R., Bengio, Y., Show, attend and tell: Neural image caption generation with visual attention (2015) Proc. Int. Conf. Mach. Learn., pp. 2048-2057",attend and tell: Neural image caption generation with visual attention,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Donahue, J., Hendricks, L.A., Rohrbach, M., Venugopalan, S., Guadarrama, S., Saenko, K., Darrell, T., Long-term recurrent convolutional networks for visual recognition and description (2017) IEEE Trans. Pattern Anal. Mach. Intell., 39 (4), pp. 677-691. , Apr",Long-term recurrent convolutional networks for visual recognition and description,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Dos Santos, C., Gatti, M., Deep convolutional neural networks for sentiment analysis of short texts (2014) Proc. 25th Int. Conf. Comput. Linguistics (COLING), pp. 69-78",Deep convolutional neural networks for sentiment analysis of short texts,"Dos Santos C.N., Gatti M.",Deep convolutional neural networks for sentiment analysis of short texts,"Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that exploits from character- To sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: The Stanford Sentiment Treebank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7% accuracy, and fine-grained classification, with 48.3% accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4%.",,2014.0,774.0,,Classification (of information); Convolution; Data mining; Linguistics; Neural networks; Social networking (online); Contextual information; Convolutional neural network; Different domains; Positive/negative classifications; Prediction accuracy; Prior knowledge; Sentiment analysis; State of the art; Computational linguistics,2-s2.0-84932166511
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., Darrell, T., Caffe: Convolutional architecture for fast feature embedding (2014) Proc. 22nd ACM Int. Conf. Multimedia, pp. 675-678",Caffe: Convolutional architecture for fast feature embedding,"Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick R., Guadarrama S., Darrell T.",Caffe: Convolutional architecture for fast feature embedding,"Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.",10.1145/2647868.2654889,2014.0,6989.0,Computer vision; Machine learning; Neural networks; Open source; Parallel computation,C++ (programming language); Computer vision; Convolution; Convolutional neural networks; Deep learning; Graphics processing unit; Industrial research; Learning systems; Network architecture; Neural networks; Object oriented programming; Cloud environments; Feature embedding; Open sources; Parallel Computation; Reference models; Seamless switching; Separating models; State of the art; Learning algorithms,2-s2.0-84913580146
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2017) Commun. ACM, 60 (6), pp. 84-90. , May",ImageNet classification with deep convolutional neural networks,"Krizhevsky A., Sutskever I., Hinton G.E.",ImageNet classification with deep convolutional neural networks,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",,2012.0,54278.0,,Convolutional neural network; Different class; GPU implementation; High resolution image; Max-pooling; Overfitting; Regularization methods; Test errors; Convolution; Neural networks,2-s2.0-84876231242
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Krizhevsky, A., Sutskever, I., Hinton, G.E., ImageNet classification with deep convolutional neural networks (2017) Commun. ACM, 60 (6), pp. 84-90. , May",ImageNet classification with deep convolutional neural networks,"Krizhevsky A., Sutskever I., Hinton G.E.",ImageNet classification with deep convolutional neural networks,"We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. © 2017 ACM.",10.1145/3065386,2017.0,5124.0,,Convolution; Image classification; Neural networks; Convolutional neural network; Different class; GPU implementation; High resolution image; Max-pooling; Overfitting; Regularization methods; State of the art; Deep neural networks,2-s2.0-85020126914
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Arif, M.H., Li, J., Iqbal, M., Peng, H., Optimizing XCSR for text classification (2017) Proc. IEEE Symp. Service-Oriented System Eng. (SOSE), pp. 86-95. , Apr",Optimizing XCSR for text classification,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Dong, L., Zhao, H., Hierarchical feature selection with orthogonal transfer (2019) J. Internet Technol., 20 (4), pp. 1205-1212",Hierarchical feature selection with orthogonal transfer,"Dong L., Zhao H.",Hierarchical feature selection with orthogonal transfer,"Feature selection is an indispensable preprocessing step in high-dimensional data classification, which has an effect on both the running time and the result quality of the subsequent classification processing steps. Most existing approaches use flat strategies, which treat each category or class separately and ignore hierarchical structure. In this paper, we propose a hierarchical feature selection algorithm with orthogonal transfer. We first compute the weight of the feature to the category by hierarchical SVM with orthogonal transfer. More specifically, we use an objective that is a convex function of the normal vectors to compute the weight. Then, we select features using the weight and predict the class label for a test sample according to classifier. Finally, extensive experimental results on various real-life datasets have demonstrated the superiority of the proposed algorithm. © 2019 Taiwan Academic Network Management Committee. All rights reserved.",10.3966/160792642019072004019,2019.0,1.0,Feature selection; Hierarchical classification; Orthogonal transfer,Clustering algorithms; Feature extraction; Functions; Support vector machines; Convex functions; Hierarchical classification; Hierarchical features; Hierarchical structures; High dimensional data; Orthogonal transfer; Pre-processing step; Real life datasets; Classification (of information),2-s2.0-85071732168
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Nickel, M., Kiela, D., Poincare embeddings for learning hierarchical representation (2017) Proc. Adv. Neural Inf. Process. Syst., pp. 6338-6347",Poincare embeddings for learning hierarchical representation,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Ye, J., Ni, J., Yi, Y., Deep learning hierarchical representations for image steganalysis (2017) IEEE Trans. Inf. Forensics Security, 12 (11), pp. 2545-2557. , Nov",Deep learning hierarchical representations for image steganalysis,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Lee, H., Grosse, R., Ranganath, R., Ng, A.Y., Unsupervised learning of hierarchical representations with convolutional deep belief networks (2011) Commun. ACM, 54 (10), p. 95. , Oct",Unsupervised learning of hierarchical representations with convolutional deep belief networks,"Lee H., Grosse R., Ranganath R., Ng A.Y.",Unsupervised learning of hierarchical representations with convolutional deep belief networks,"There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks (DBNs); however, scaling such models to full-sized, highdimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model that scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique that shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images. © 2011 ACM.",10.1145/2001269.2001295,2011.0,268.0,,Bottom-up and top-down; Excellent performance; Generative model; Hierarchical representation; High-dimensional images; Natural scenes; Novel techniques; Probabilistic inference; Realistic images; Translation invariants; Visual feature; Visual recognition; Convolution; Unsupervised learning; Bayesian networks,2-s2.0-80053540444
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., Attention is all you need (2017) Proc. Adv. Neural Inf. Process. Syst., pp. 5998-6008",Attention is all you need,"Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser Ł., Polosukhin I.",Attention is all you need,"The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. © 2017 Neural information processing systems foundation. All rights reserved.",,2017.0,11162.0,,Convolution; Decoding; Network architecture; Program processors; Signal encoding; Attention mechanisms; Best model; Bleu scores; Convolutional neural network; Single models; Training costs; Two machines; Recurrent neural networks,2-s2.0-85043317328
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L., Polosukhin, I., Attention is all you need (2017) Proc. Adv. Neural Inf. Process. Syst., pp. 5998-6008",Attention is all you need,"Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I.",Attention is all you need,[No abstract available],,2017.0,3632.0,,,2-s2.0-85038368581
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Devlin, J., Chang, M.-W., Lee, K., Toutanova, K., (2018) BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding, , http://arxiv.org/abs/1810.04805",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Chiu, B., Crichton, G., Korhonen, A., Pyysalo, S., How to train good word embeddings for biomedical NLP (2016) Proc. 15th Workshop Biomed. Natural Lang. Process, pp. 166-174",How to train good word embeddings for biomedical NLP,"Chiu B., Crichton G., Korhonen A., Pyysalo S.",How to train good word embeddings for biomedical NLP,[No abstract available],,2016.0,169.0,,,2-s2.0-85021655548
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Bengio, Y., Ducharme, R., Vincent, P., Jauvin, C., A neural probabilistic language model (2003) J. Mach. Learn. Res., 3, pp. 1137-1155. , Feb",A neural probabilistic language model,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Lewis, D.D., Yang, Y., Rose, T.G., Li, F., RCV1: A new benchmark collection for text categorization research (2004) J. Mach. Learn. Res., 5, pp. 361-397. , Dec",RCV1: A new benchmark collection for text categorization research,"Lewis D.D., Yang Y., Rose T.G., Li F.",RCV1: A new benchmark collection for text categorization research,"Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices. © 2004 David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li.",,2004.0,1714.0,Applications; Automated indexing; Controlled vocabulary indexing; Effectiveness measures; Evaluation; Feature selection; K-NN; Methodology; Multiclass; Multilabel; Nearest neighbor; News articles; Operational systems; Rocchio; SCut; SCutFBR; Support vector machines; SVMs; Term weighting; Test collection; Text classification; Thresholding,Applications; Classification (of information); Feature extraction; Indexing (of information); Quality control; Semantics; Support vector machines; Taxonomies; Effectiveness measure; Evaluation; Methodology; Multi-label; Multiclass; Nearest neighbors; News articles; Operational systems; Rocchio; SCut; SCutFBR; SVMs; Term weighting; Test Collection; Text classification; Thresholding; Text processing,2-s2.0-84876811202
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Agrawal, R., Gupta, A., Prabhu, Y., Varma, M., Multi-label learning with millions of labels: Recommending advertiser bid phrases for Web pages (2013) Proc. 22ndInt. Conf. World Wide Web, pp. 13-24",Multi-label learning with millions of labels: Recommending advertiser bid phrases for Web pages,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Conneau, A., Schwenk, H., Barrault, L., Lecun, Y., (2016) Very Deep Convolutional Networks for Text Classification, , http://arxiv.org/abs/1606.01781",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Chen, H., Sun, M., Tu, C., Lin, Y., Liu, Z., Neural sentiment classification with user and product attention (2016) Proc. Conf. Empirical Methods Natural Lang. Process., pp. 1650-1659",Neural sentiment classification with user and product attention,"Chen H., Sun M., Tu C., Lin Y., Liu Z.",Neural sentiment classification with user and product attention,"Document-level sentiment classification aims to predict user's overall sentiment in a document about a product. However, most of existing methods only focus on local text information and ignore the global user preference and product characteristics. Even though some works take such information into account, they usually suffer from high model complexity and only consider word-level preference rather than semantic levels. To address this issue, we propose a hierarchical neural network to incorporate global user and product information into sentiment classification. Our model first builds a hierarchical LSTM model to generate sentence and document representations. Afterwards, user and product information is considered via attentions over different semantic levels due to its ability of capturing crucial semantic components. The experimental results show that our model achieves significant and consistent improvements compared to all state-of-the-art methods. The source code of this paper can be obtained from https://github.com/thunlp/NSC. © 2016 Association for Computational Linguistics",10.18653/v1/d16-1171,2016.0,196.0,,Classification (of information); Information retrieval systems; Natural language processing systems; Semantics; Document Representation; Hierarchical neural networks; Product characteristics; Product information; Semantic components; Sentiment classification; State-of-the-art methods; Text information; Long short-term memory,2-s2.0-85045912242
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Wehrmann, J., Cerri, R., Barros, R., Hierarchical multi-label classification networks (2018) Proc. Int. Conf. Mach. Learn., pp. 5225-5234",Hierarchical multi-label classification networks,"Wehrmann J., Cerri R., Barros R.C.",Hierarchical multi-label classification networks,"One of the most challenging machine learning problems is a particular case of data classification in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with applications in text classification, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC. © 35th International Conference on Machine Learning, ICML 2018.All Rights Reserved.",,2018.0,12.0,,Computer aided diagnosis; Learning systems; Network architecture; Neural networks; Text processing; Global informations; Hierarchical multi-label classifications; Machine learning problem; Novel neural network; Protein function prediction; State-of-the-art approach; Statistical significance; Text classification; Classification (of information),2-s2.0-85057303988
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E., Hierarchical attention networks for document classification (2016) Proc. Conf. North Amer. Chapter Assoc. Comput. Linguistics, Hum. Lang. Technol., pp. 1480-1489",Hierarchical attention networks for document classification,"Yang Z., Yang D., Dyer C., He X., Smola A., Hovy E.",Hierarchical attention networks for document classification,"We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences. ©2016 Association for Computational Linguistics.",10.18653/v1/n16-1174,2016.0,2239.0,,Classification (of information); Computational linguistics; Text processing; Attention mechanisms; Document Classification; Document Representation; Hierarchical structures; Proposed architectures; Sentence level; Text classification; Information retrieval systems,2-s2.0-84994158553
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Gopal, S., Yang, Y., Hierarchical Bayesian inference and recursive regularization for large-scale classification (2015) ACM Trans. Knowl. Discov. Data, 9 (3), pp. 1-23. , Apr",Hierarchical Bayesian inference and recursive regularization for large-scale classification,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Prabhu, Y., Varma, M., Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning (2014) Proc. 20th ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, pp. 263-272",accurate and stable tree-classifier for extreme multi-label learning,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Bhatia, K., Jain, H., Kar, P., Varma, M., Jain, P., Sparse local embeddings for extreme multi-label classification (2015) Proc. Adv. Neural Inf. Process. Syst., pp. 730-738",Sparse local embeddings for extreme multi-label classification,"Bhatia K., Jain H., Kar P., Varma M., Jain P.",Sparse local embeddings for extreme multi-label classification,"The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies, or scale to large problems as the low rank assumption is violated in most real world applications. In this paper we develop the SLEEC classifier to address both limitations. The main technical contribution in SLEEC is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring (tail) labels. This allows SLEEC to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors. We conducted extensive experiments on several real-world, as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that SLEEC can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based (by as much as 35%) as well as tree-based (by as much as 6%) methods. SLEEC can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.",,2015.0,230.0,,Forecasting; Information science; Accurate prediction; Classification accuracy; Multi label classification; Multi-label learning; Pairwise distances; Prediction accuracy; State-of-the-art methods; Technical contribution; Classification (of information),2-s2.0-84965155847
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Prabhu, Y., Kag, A., Harsola, S., Agrawal, R., Varma, M., Para-bel: Partitioned label trees for extreme classification with application to dynamic search advertising (2018) Proc. World Wide Web Conf., pp. 993-1002",Para-bel: Partitioned label trees for extreme classification with application to dynamic search advertising,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Pennington, J., Socher, R., Manning, C., Glove: Global vectors for word representation (2014) Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP), pp. 1532-1543",Glove: Global vectors for word representation,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Liu, Y., Peng, H., Li, J., Song, Y., Li, X., Event detection and evolution in multi-lingual social streams (2019) Frontiers Comput. Sci., pp. 1-23",Event detection and evolution in multi-lingual social streams,"Liu Y., Peng H., Li J., Song Y., Li X.",Event detection and evolution in multi-lingual social streams,[No abstract available],,2019.0,4.0,,,2-s2.0-85074940755
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Myroniv, B., Wu, C., Ren, Y., Christian, A., Bajo, E., Tseng, Y.C., Analyzing user emotions via physiology signals (2017) Data Sci. Pattern Recognit., 1 (2), pp. 11-25",Analyzing user emotions via physiology signals,"Myroniv B., Wu C.-W., Ren Y., Christian A.B., Bajo E., Tseng Y.-C.",Analyzing user emotions via physiology signals,[No abstract available],,2017.0,16.0,,,2-s2.0-85046772970
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Zhang, Y., Jin, R., Zhou, Z.-H., Understanding bag-of-words model: A statistical framework (2010) Int. J. Mach. Learn. Cyber., 1 (11), pp. 43-52. , Dec",Understanding bag-of-words model: A statistical framework,"Zhang Y., Jin R., Zhou Z.-H.",Understanding bag-of-words model: A statistical framework,"The bag-of-words model is one of the most popular representation methods for object categorization. The key idea is to quantize each extracted key point into one of visual words, and then represent each image by a histogram of the visual words. For this purpose, a clustering algorithm (e.g., K-means), is generally used for generating the visual words. Although a number of studies have shown encouraging results of the bag-of-words representation for object categorization, theoretical studies on properties of the bag-of-words model is almost untouched, possibly due to the difficulty introduced by using a heuristic clustering process. In this paper, we present a statistical framework which generalizes the bag-of-words representation. In this framework, the visual words are generated by a statistical process rather than using a clustering algorithm, while the empirical performance is competitive to clustering-based method. A theoretical analysis based on statistical consistency is presented for the proposed framework. Moreover, based on the framework we developed two algorithms which do not rely on clustering, while achieving competitive performance in object categorization when compared to clustering-based bag-ofwords representations. © Springer-Verlag 2010.",10.1007/s13042-010-0001-0,2010.0,435.0,Bag of words model; Object recognition; Rademacher complexity,Bag of words; Bag of words model; Clustering process; Empirical performance; K-means; Keypoints; Object categorization; Rademacher complexity; Representation method; Statistical framework; Statistical process; Theoretical study; Visual word; Object recognition; Statistical methods; Clustering algorithms,2-s2.0-79952313379
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Filliat, D., A visual bag of words method for interactive qualitative localization and mapping (2007) Proc. IEEEInt. Conf. Robot. Autom., pp. 3921-3926. , Apr",A visual bag of words method for interactive qualitative localization and mapping,Filliat D.,A visual bag of words method for interactive qualitative localization and mapping,Localization for low cost humanoid or animal-like personal robots has to rely on cheap sensors and has to be robust to user manipulations of the robot. We present a visual localization and map-learning system that relies on vision only and that is able to incrementally learn to recognize the different rooms of an apartment from any robot position. This system is inspired by visual categorization algorithms called bag of words methods that we modified to make fully incremental and to allow a user-interactive training. Our system is able to reliably recognize the room in which the robot is after a short training time and is stable for long term use. Empirical validation on a real robot and on an image database acquired in real environments are presented. © 2007 IEEE.,10.1109/ROBOT.2007.364080,2007.0,167.0,,Algorithms; Database systems; Image analysis; Learning systems; Manipulators; User interfaces; Image databases; Interactive qualitative localization; Real robots; Visual localization; Anthropomorphic robots,2-s2.0-36348999309
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Ramos, J., Using TF-IDF to determine word relevance in document queries (2003) Proc. 1st Instructional Conf. Mach. Learn., 242, pp. 133-142. , Piscataway, NJ, USA",Using TF-IDF to determine word relevance in document queries,Ramos J.,Using TF-IDF to determine word relevance in document queries,[No abstract available],,2003.0,929.0,,,2-s2.0-77950327510
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Aizawa, A., An information-theoretic perspective of TF-IDF measures (2003) Inf. Process. Manage., 39 (1), pp. 45-65. , Jan",An information-theoretic perspective of TF-IDF measures,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Wu, B., Li, C., Wang, B., Event detection and evolution based on entity separation (2011) Proc. 8th Int. Conf. Fuzzy Syst. Knowl. Discovery (FSKD), pp. 1-7. , Jul",Event detection and evolution based on entity separation,"Wu B., Li C., Wang B.",Event detection and evolution based on entity separation,"By computing the relevance of follow-up stories, the traditional topic tracking approaches could track the stories. However, subtopics derived from one topic and the evolution process of these subtopics could not be identified with traditional approaches. A method is proposed to detect event and subtopics and get the evolution process of event from media data. Firstly creating entity vectors with entities in the media data and computing similarity between two entity vectors to separate single event. Then creating full vectors with all keywords in the dataset of an event and computing similarity between two full vectors to get several subtopics of an event and the evolution process of the event. Experiments show that this method can get events and subtopics of them effectively. © 2011 IEEE.",10.1109/FSKD.2011.6019835,2011.0,3.0,entity; evolution; similarity; subtopic,Data sets; entity; Event detection; evolution; Evolution process; similarity; Single event; subtopic; Topic tracking; Fuzzy systems; Information retrieval systems; Separation; Vectors,2-s2.0-80053427384
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Rousseau, F., Kiagias, E., Vazirgiannis, M., Text categorization as a graph classification problem (2015) Proc. 53rd Annu. Meeting Assoc. Com-put. Linguistics, 7th Int. Joint Conf. Natural Language Process., 1, pp. 1702-1712",Text categorization as a graph classification problem,"Rousseau F., Kiagias E., Vazirgiannis M.",Text categorization as a graph classification problem,"In this paper, we consider the task of text categorization as a graph classification problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part - its main core - speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accuracy and macro-Averaged F1-score compared to baseline approaches. © 2015 Association for Computational Linguistics.",10.3115/v1/p15-1164,2015.0,74.0,,Classification (of information); Computational linguistics; Text processing; Discriminative features; Frequent subgraph mining; Graph classification; Graph representation; Prediction performance; Text categorization; Text classification; Textual documents; Natural language processing systems,2-s2.0-84943773848
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Peng, H., Li, J., Gong, Q., Wang, S., Ning, Y., Yu, P.S., (2018) Graph Convolutional Neural Networks Via Motif-based Attention, , http://arxiv.org/abs/1811.08270",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Mao, Q., Li, J., Wang, S., Zhang, Y., Peng, H., He, M., Wang, L., Aspect-based sentiment classification with attentive neural turing machines (2019) Proc. 28thInt. Joint Conf. Artif. Intell., pp. 5139-5145. , Aug",Aspect-based sentiment classification with attentive neural turing machines,"Mao Q., Li J., Wang S., Zhang Y., Peng H., He M., Wang L.",Aspect-based sentiment classification with attentive neural turing machines,"Aspect-based sentiment classification aims to identify sentiment polarity expressed towards a given opinion target in a sentence. The sentiment polarity of the target is not only highly determined by sentiment semantic context but also correlated with the concerned opinion target. Existing works cannot effectively capture and store the inter-dependence between the opinion target and its context. To solve this issue, we propose a novel model of Attentive Neural Turing Machines (ANTM). Via interactive read-write operations between an external memory storage and a recurrent controller, ANTM can learn the dependable correlation of the opinion target to context and concentrate on crucial sentiment information. Specifically, ANTM separates the information of storage and computation, which extends the capabilities of the controller to learn and store sequential features. The read and write operations enable ANTM to adaptively keep track of the interactive attention history between memory content and controller state. Moreover, we append target entity embeddings into both input and output of the controller in order to augment the integration of target information. We evaluate our model on SemEval2014 dataset which contains reviews of Laptop and Restaurant domains and Twitter review dataset. Experimental results verify that our model achieves state-of-the-art performance on aspect-based sentiment classification. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.",10.24963/ijcai.2019/714,2019.0,10.0,,,2-s2.0-85074917337
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Lai, S., Xu, L., Liu, K., Zhao, J., Recurrent convolutional neural networks for text classification (2015) Proc. 29th AAAI Conf. Artif. Intell., pp. 2267-2273",Recurrent convolutional neural networks for text classification,"Lai S., Xu L., Liu K., Zhao J.",Recurrent convolutional neural networks for text classification,"Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets. © Copyright 2015, Association for the Advancement of Artificial Intelligence (www.aaa1.org). All rights reserved.",,2015.0,944.0,,Artificial intelligence; Classification (of information); Convolution; Neural networks; Recurrent neural networks; Contextual information; Convolutional neural network; Knowledge basis; State-of-the-art methods; Text classification; Text classifiers; Window-based; Word representations; Text processing,2-s2.0-84959872385
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Hochreiter, S., Schmidhuber, J., LSTM can solve hard long time lag problems (1997) Proc. Adv. Neural Inf. Process. Syst., pp. 473-479",LSTM can solve hard long time lag problems,"Hochreiter S., Schmidhuber J.",LSTM can solve hard long time lag problems,"Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.",,1997.0,397.0,,Algorithms; Hard problems; Random searches; Random weight; Time lag; Problem solving,2-s2.0-0000370416
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Cho, K., Van Merrienboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., Bengio, Y., (2014) Learning Phrase Representations Using Rnn Encoder-decoder for Statistical Machine Translation, , http://arxiv.org/abs/1406.1078",,,,,,,,,,
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Ko-Wei Huang, Y.-M.L., Lin, C.-C., Wu, Z.-X., A deep learning and image recognition system for image recognition (2019) Data Sci. Pattern Recognit., 3 (2), pp. 1-23",A deep learning and image recognition system for image recognition,"Huang K.W., Lin C.C., Lee Y.M., Wu Z.X.",A deep learning and image recognition system for image recognition,[No abstract available],,2019.0,8.0,,,2-s2.0-85081045682
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Peng, H., Li, J., Song, Y., Liu, Y., Incrementally learning the hierarchical softmax function for neural language models (2017) Proc. 31st AAAI Conf. Artif. Intell., pp. 3267-3273. , London, U.K.: AAAI Press",Incrementally learning the hierarchical softmax function for neural language models,"Peng H., Li J., Song Y., Liu Y.",Incrementally learning the hierarchical softmax function for neural language models,"Neural network language models (NNLMs) have attracted a lot of attention recently. In this paper, we present a training method that can incrementally train the hierarchical softmax function for NNMLs. We split the cost function to model old and update corpora separately, and factorize the objective function for the hierarchical softmax. Then we provide a new stochastic gradient based method to update all the word vectors and parameters, by comparing the old tree generated based on the old corpus and the new tree generated based on the combined (old and update) corpus. Theoretical analysis shows that the mean square error of the parameter vectors can be bounded by a function of the number of changed words related to the parameter node. Experimental results show that incremental training can save a lot of time. The smaller the update corpus is, the faster the update training process is, where an up to 30 times speedup has been achieved. We also use both word similarity/relatedness tasks and dependency parsing task as our benchmarks to evaluate the correctness of the updated word vectors. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2017.0,35.0,,Artificial intelligence; Cost functions; Forestry; Mean square error; Stochastic systems; Dependency parsing; Incremental training; Network language; Objective functions; Parameter vectors; Stochastic gradient; Training methods; Training process; Computational linguistics,2-s2.0-85028599250
2-s2.0-85081058031,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification," Peng, H., Bao, M., Li, J., Bhuiyan, M., Liu, Y., He, Y., Yang, E., Incremental term representation learning for social network analysis (2018) Future Gener. Comput. Syst., 86, pp. 1503-1512. , Sep",Incremental term representation learning for social network analysis,"Peng H., Bao M., Li J., Bhuiyan M.Z.A., Liu Y., He Y., Yang E.",Incremental term representation learning for social network analysis,"Term representation methods as computable and semantic tools have been widely applied in social network analysis. This paper provides a new perspective that can incrementally factorize co-occurrence matrix to query latest semantic vectors. We divide the streaming social network data into old and updated training tasks respectively, and factorize the training objective function based on stochastic gradient methods to update vectors. We prove that the incremental objective function is convergent. Experimental results demonstrate that our incremental factorizing can save a substantial amount of time by speeding up training convergence. The smaller the updated data is, the faster the update factorizing process can be, even 30 times faster than existing methods in certain cases. To evaluate the correctness of incremental representation, social text similarity/relatedness, linguistic tasks, network event detection, social user multi-label classification and user clustering for social network analysis are employed as benchmarks in this paper. © 2017 Elsevier B.V.",10.1016/j.future.2017.05.020,2018.0,14.0,GloVe model; Incremental learning; Social network analysis; Term representation,Gradient methods; Knowledge representation; Semantics; Social networking (online); Stochastic systems; Text processing; Co-occurrence-matrix; Incremental learning; Multi label classification; Objective functions; Semantic vectors; Stochastic gradient methods; Term representation; User clustering; Classification (of information),2-s2.0-85020123700
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model","Aamodt, A., Plaza, E., Case-Based Reasoning: Foundational Issues, Methodological Variations, and System Approaches (1994) AI Commun, 7 (1), pp. 39-59",and System Approaches,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Arditi, D., Tokdemir, B., Comparison of Case-Based Reasoning and Artificial INeural Networks (1999) J Comput Civ Eng, 13 (3), pp. 578-583",Comparison of Case-Based Reasoning and Artificial INeural Networks,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Bates, T., (2005) Technology, e-learning and distance education, , Routledge, London",,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Bouquet, P., Molinari, A., Using Semantic Technologies in E-Learning Platforms: A Case Study (2016) International Journal of Information and Education Technology, 6 (5)",Using Semantic Technologies in E-Learning Platforms: A Case Study,"Bouquet P., Molinari A.",Using Semantic Technologies in E-Learning Platforms: A Case Study,[No abstract available],,2016.0,2.0,,,2-s2.0-85073820020
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Bozkurt, A., Akgun-Ozbek, E., Trends in Distance Education Research: A Content Analysis of Journals 2009-2013 (2015) Int Rev Res Open Dist Learn, 16 (1), pp. 330-363",Trends in Distance Education Research: A Content Analysis of Journals 2009-2013,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Connolly, P., Neill, J., Constructions of locality and gender and their impact on the educational aspirations of working class children (2011) Int Stud Sociol Educ, 11 (2), pp. 107-130",Constructions of locality and gender and their impact on the educational aspirations of working class children,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," David, P., A Design Requirements Framework for Distance Learning Environments (2007) J Comput, 2 (4), pp. 99-113",A Design Requirements Framework for Distance Learning Environments,David P.,A Design Requirements Framework for Distance Learning Environments,[No abstract available],,2007.0,2.0,,,2-s2.0-85073833673
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Ergen, T., Kozat, S.S., Neural Networks based Online Learning (2017) IEEE 25th Signal Processing and Communications Applications Conference",Neural Networks based Online Learning,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," https://www.scribd.com/document/165064823/GAT-Subject-Syllabus, GAT Test Bank Accessed on Oct 2016",,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Guarino, N., Welty, C.A., An Overview of OntoClean (2004) Handbook on Ontologies, pp. 151-171. , Springer Berlin Heidelberg, Berlin, Heidelberg",An Overview of OntoClean,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Jahankhani, H., Tawil, R., Adaptive E-learning Approach based on Semantic Web Technology (2015) International Journal of Webology, 10 (2)",Adaptive E-learning Approach based on Semantic Web Technology,"Jahankhani H., Tawil R.",Adaptive E-learning Approach based on Semantic Web Technology,[No abstract available],,2015.0,1.0,,,2-s2.0-85073825313
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Kaur, P., Classification and prediction based on DM algorithm for slow learners (2015) International Conference on Recent Trends in Computing",Classification and prediction based on DM algorithm for slow learners,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Lafore, R., (2001) Object-Oriented Programming in C++, , Sams Publishing, Published December 29th",,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Liem, B., Beek, W., Gracia, J., Lozano, E., DynaLearn–An Intelligent Learning Environment for Learning Conceptual Knowledge (2013) AI Mag, 34 (4), pp. 46-65",DynaLearn–An Intelligent Learning Environment for Learning Conceptual Knowledge,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Lin, C.C., A case study on SCORM – based eLearning in computer aided drafting course with user satisfaction survey (2008) WSEAS Trans Inf Sci Appl, 5 (10), pp. 1416-1427",A case study on SCORM – based eLearning in computer aided drafting course with user satisfaction survey,Lin C.C.,A case study on SCORM – based eLearning in computer aided drafting course with user satisfaction survey,[No abstract available],,2008.0,1.0,,,2-s2.0-85073814933
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Luis, E., Rofio, A., A recommender system for educational resources in specific learning content (2013) International Conference on Computer Science and Education",A recommender system for educational resources in specific learning content,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Mihăescu, M.C., Classification of Learners Using Linear Regression (2011) Proceedings of the Federated Conference on Computer Science and Information Systems, pp. 717-721. , ISBN 978-83-60810-22-4",Classification of Learners Using Linear Regression,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Mohammad, T.Z., Mahmoud, A.M., Classification Model of English Course e-Learning System for Slow Learners “Recent Advances in Information Science (2014) International Journal of Computer Science (IIJCS, , 978-960-474-304-9",Classification Model of English Course e-Learning System for Slow Learners “Recent Advances in Information Science,"Mohammad T.Z., Mahmoud A.M.",Classification Model of English Course e-Learning System for Slow Learners “Recent Advances in Information Science,[No abstract available],,2014.0,1.0,,,2-s2.0-85075770924
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," (2015) OOP Quizes, , https://www.tutorialspoint.com/cplusplus/cpp_online_quiz.htm, Accessed on Sep",,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Poveda-Villalón, M., Suárez-Figueroa, M.C., Gómez-Pérez, A., Validating Ontologies with OOPS! (2012) Lecture Notes in Computer Science, pp. 267-281. , Springer Berlin Heidelberg, Berlin, Heidelberg",Validating Ontologies with OOPS!,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Rani, M., Srivastava, K.V., Vyas, O.P., An Ontological Learning Management System (2016) Comput Appl Eng Educ, 24 (5), pp. 706-722",An Ontological Learning Management System,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Romero, C., Ventura, S., Educational Data Mining: A Survey from 1995 to 2005 (2007) Expert Syst Appl, 33 (1), pp. 135-146",Educational Data Mining: A Survey from 1995 to 2005,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Safyan, M., Qayyum, Z., Sarwar, S., (2017) Context-Aware Personalized Activity Modeling in Concurrent Environment, , Internet of Things (IoT",,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Salam, F., Shambour, Q., A Framework of semantic recommender system for e-learning (2015) Journal of Software, 10, pp. 317-330",A Framework of semantic recommender system for e-learning,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Saleena, B., Srivastava, K., Using concept similarity in cross ontology for adaptive e-learning (2015) Journal of King Saud University- Computer and Information Sciences, 27 (1), pp. 1-12",Using concept similarity in cross ontology for adaptive e-learning,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Sarma Cakula, M.S., Development of Personalized e-learning model (2013) ICTE in Regional Development, 26 (4), pp. 113-120",Development of Personalized e-learning model,Sarma Cakula M.S.,Development of Personalized e-learning model,[No abstract available],,2013.0,1.0,,,2-s2.0-85073811425
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Sarwar, S., Ul Qayyum, Z., Castro, R.G., Safyan, M., Ontology based E-learning Systems: A Step towards context aware content recommendation (2018) International Journal of Information and Educational Technology, 8 (10), pp. 10-19",Ontology based E-learning Systems: A Step towards context aware content recommendation,"Sarwar S., Ul Qayyum Z., Castro R.G., Safyan M.",Ontology based E-learning Systems: A Step towards context aware content recommendation,[No abstract available],,2018.0,1.0,,,2-s2.0-85075767501
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Sarwar, S., García-Castro, R., Qayyum, Z.U., Safyan, M., Ontology-based Learner Categorization through Case Based Reasoning and Fuzzy Logic (2017) International Conference on E-Learning (IADIS), pp. 159-163",Ontology-based Learner Categorization through Case Based Reasoning and Fuzzy Logic,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Sarwar, S., Ul Qayyum, Z., Safyan, M., Munir, F., (2016) Ontology Based Adaptive, Semantic E-Learning Framework (OASEF), , Springer LNEE ICISA",,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Seteres, V., Ossevroot, M.A., Influence of student characteristics on use of adaptive e-learning material (2012) Int Journal of Computers & Education, 58 (3), pp. 942-952",Influence of student characteristics on use of adaptive e-learning material,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Shen, L., Shen, R., Ontology based Content Recommendation (2005) International Journal of Continued Engineering and Education, 15 (1), pp. 13-26",Ontology based Content Recommendation,"Shen L., Shen R.",Ontology based Content Recommendation,[No abstract available],,2005.0,1.0,,,2-s2.0-85073833077
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Shute, V., Towle, B., Adaptive e-learning (2010) Educ Psychol, 38 (2), pp. 105-114",Adaptive e-learning,"Shute V., Towle B.",Adaptive e-learning,"It has long been known that differences among individuals have an effect on learning. Dick Snow's research on aptitude-treatment interactions (ATIs) was designed to investigate and quantify these effects, and more recent research in this vein has clearly established that these effects can be quantified and predicted. Technology has now reached a point where we have the opportunity to capitalize on these effects to the benefit of learners. In this article, we review some of the demonstrated effects of ATIs, describe how ATI research naturally leads to adaptive e-learning, and describe one way in which an adaptive e-learning system might be implemented to take advantage of these effects.",10.1207/S15326985EP3802_5,2003.0,156.0,,,2-s2.0-0037903187
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Strumiłło, P., Kamiński, W., Radial Basis Function Neural Networks: Theory and Applications, Neural Networks and Soft Computing (2013) Advances in Soft Computing, 19 (5), pp. 107-119",Neural Networks and Soft Computing,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Tambe, S., Kadam, G., An Efficient framework for E-Learning Recommendation system using fuzzy Logic and Ontology (2016) International Research Journal of Engineering and Technology (IRJET), 3 (6), pp. 2062-2067",An Efficient framework for E-Learning Recommendation system using fuzzy Logic and Ontology,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Vanbelle, S., A New Interpretation of the Weighted Kappa Coefficients (2016) Psychometrica, 81 (2), pp. 399-410",A New Interpretation of the Weighted Kappa Coefficients,Vanbelle S.,A New Interpretation of the Weighted Kappa Coefficients,"Reliability and agreement studies are of paramount importance. They do contribute to the quality of studies by providing information about the amount of error inherent to any diagnosis, score or measurement. Guidelines for reporting reliability and agreement studies were recently provided. While the use of the kappa-like family is advised for categorical and ordinal scales, no further guideline in the choice of a weighting scheme is given. In the present paper, a new simple and practical interpretation of the linear- and quadratic-weighted kappa coefficients is given. This will help researchers in motivating their choice of a weighting scheme. © 2014, The Psychometric Society.",10.1007/s11336-014-9439-4,2016.0,50.0,agreement; linear; ordinal scale; quadratic; reliability,human; observer variation; psychometry; reproducibility; statistical model; statistics; Humans; Linear Models; Observer Variation; Psychometrics; Reproducibility of Results; Statistics as Topic,2-s2.0-84971247409
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Viola, S.R., Graf, S., Kinshuk, T.L., Analysis of Felder-Silverman Index of Learning Styles by a Data-driven Statistical Approach (2006) IEEE International Symposium on Multimedia (ISM'06), pp. 7695-7704",Analysis of Felder-Silverman Index of Learning Styles by a Data-driven Statistical Approach,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," West, D.M., Learning, M., Transforming Education, Engaging Students and Improving Outcomes (2013) International Journal of ICT, E-Management and E-Learning, 4",Engaging Students and Improving Outcomes,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Yarandi, M., A Personalized Adaptive E-Learning Approach Based On Semantic Web Technology (2013) Journal of Webology, 10 (2), pp. 751-766",A Personalized Adaptive E-Learning Approach Based On Semantic Web Technology,,,,,,,,,
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Yarandi, M., Jahankhani, H., Tawil, A.-R., A personalized adaptive e-learning approach based on semantic web technology (2013) Webology, 10 (2)",A personalized adaptive e-learning approach based on semantic web technology,"Yarandi M., Jahankhani H., Tawil A.-R.H.",A personalized adaptive e-learning approach based on semantic web technology,"Recent developments in semantic web technologies heightened the need for online adaptive learning environment. Adaptive learning is an important research topic in the field of web-based systems as there are no fixed learning paths which are appropriate for all learners. However, most studies in this field have only focused on learning styles and habits of learners. Far too little attention has been paid on understanding the ability of learners. Therefore, it is becoming increasingly difficult to ignore adaptation in the field of e-learning systems. Many researchers are adopting semantic web technologies to find new ways for designing adaptive learning systems based on describing knowledge using ontological models. Ontologies have the potential to design content and learner models required to create adaptive e-learning systems based on various characteristics of learners. The aim of this paper is to present an ontology-based approach to develop adaptive e-learning system based on the design of semantic content, learner and domain models to tailor the teaching process for individual learner's needs. The proposed new adaptive e-learning has the ability to support personalization based on learner's ability, learning style, preferences and levels of knowledge. In our approach the ontological user profile is updated based on achieved learner's abilities. © 2013, Maryam Yarandi, Hossein Jahankhani, & Abdel-Rahman H. Tawil.",,2013.0,45.0,Adaptive learning; E-learning; Ontology; Personalized learning; Semantic web,,2-s2.0-84938071442
2-s2.0-85071338621,"Ontology based E-learning framework: A personalized, adaptive and context aware model"," Yathongchai, C., Leamer Classification Based on Learning Behavior and Performance (2013) IEEE Conference on Open Systems, , ICOS",Leamer Classification Based on Learning Behavior and Performance,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning,"Álvez, J., Lucio, P., Rigau, G., Adimen-SUMO: reengineering an ontology for first-order reasoning (2012) Int J Semant Web Inf Syst, 8 (4), pp. 80-116",Adimen-SUMO: reengineering an ontology for first-order reasoning,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Basile, V., Cabrio, E., Schon, C., KNEWS: Using logical and lexical semantics to extract knowledge from natural language (2016) Proceedings of the European Conference on Artificial Intelligence (ECAI) 2016 Conference",KNEWS: Using logical and lexical semantics to extract knowledge from natural language,"Basile V., Cabrio E., Schon C.",KNEWS: Using logical and lexical semantics to extract knowledge from natural language,[No abstract available],,2016.0,11.0,,,2-s2.0-85033444430
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Bender, M., Pelzer, B., Schon, C., System description: E-KRHyper 1.4 (2013) International Conference on Automated Deduction, pp. 126-134. , Springer",System description: E-KRHyper 1.4,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Byrne, R.M.J., Johnson-Laird, P.N., ’if’ and the problems of conditional reasoning (2009) Trends Cogn Sci, 13, pp. 282-287",’if’ and the problems of conditional reasoning,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Cariani, F., Grossi, D., Meheus, J., Parent, X., (2014) Deontic Logic and Normative systems—12th International Conference, DEON 2014, Ghent, Belgium, Proceedings, p. 8554. , https://doi.org/10.1007/978-3-319-08615-6, LNAI, Springer",,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," d’Avila Garcez, A.S., Broda, K., Gabbay, D.M., Symbolic knowledge extraction from trained neural networks: a sound approach (2001) Artif Intell, 125 (1-2), pp. 155-207",Symbolic knowledge extraction from trained neural networks: a sound approach,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Furbach, U., Schon, C., Commonsense reasoning meets theorem proving (2016) Proceedings, Lecture Notes in Computer Science, 9872, pp. 3-17. , https://doi.org/10.1007/978-3-319-45889-2_1, M. Klusch, R. Unland, O. Shehory, A. Pokahr, S. Ahrndt (eds.) Multiagent System Technologies—14th German Conference, MATES 2016, Klagenfurt, Österreich, September 27-30, Springer",Commonsense reasoning meets theorem proving,"Furbach U., Schon C.",Commonsense reasoning meets theorem proving,"The area of commonsense reasoning aims at the creation of systems able to simulate the human way of rational thinking. This paper describes the use of automated reasoning methods for tackling commonsense reasoning benchmarks. For this we use a benchmark suite introduced in literature. Our goal is to use general purpose background knowledge without domain specific hand coding of axioms, such that the approach and the result can be used as well for other domains in mathematics and science. Furthermore, we discuss the modeling of normative statements in commonsense reasoning and in robot ethics (This paper is an extended version of the informal proceedings [9] and [10]). © Springer International Publishing Switzerland 2016.",10.1007/978-3-319-45889-2_1,2016.0,4.0,,Theorem proving; Automated reasoning; Back-ground knowledge; Benchmark suites; Commonsense reasoning; Domain specific; Extended versions; Hand coding; Robot ethics; Multi agent systems,2-s2.0-84988672424
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Furbach, U., Schon, C., Stolzenburg, F., Weis, K.H., Wirth, C.P., The RatioLog project: rational extensions of logical reasoning (2015) KI, 29 (3), pp. 271-277",The RatioLog project: rational extensions of logical reasoning,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput, 9 (8), pp. 1735-1780",Long short-term memory,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Hoder, K., Voronkov, A., Sine qua non for large theory reasoning (2011) Automated deduction - CADE-23, 6803, pp. 299-314. , Bjørner N, Sofronie-Stokkermans V, (eds), Lecture notes computer science, Springer, Berlin",Sine qua non for large theory reasoning,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Johnson-Laird, P.N., (1983) Mental models: towards a cognitive science of language, inference, and consciousness, , Cambridge University Press, Cambridge",,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Khemlani, S.S., Barbey, A.K., Johnson-Laird, P.N., Causal reasoning with mental models (2014) Front Hum Neurosci, 8, p. 849",Causal reasoning with mental models,"Khemlani S.S., Barbey A.K., Johnson-Laird P.N.",Causal reasoning with mental models,"This paper outlines the model-based theory of causal reasoning. It postulates that the core meanings of causal assertions are deterministic and refer to temporally-ordered sets of possibilities: A causes B to occur means that given A, B occurs, whereas A enables B to occur means that given A, it is possible for B to occur. The paper shows how mental models represent such assertions, and how these models underlie deductive, inductive, and abductive reasoning yielding explanations. It reviews evidence both to corroborate the theory and to account for phenomena sometimes taken to be incompatible with it. Finally, it reviews neuroscience evidence indicating that mental models for causal inference are implemented within lateral prefrontal cortex. © 2014 Khemlani, Barbey and Johnson-Laird.",10.3389/fnhum.2014.00849,2014.0,32.0,Causal reasoning; Enabling conditions; Explanations; Lateral prefrontal cortex; Mental models,brain damage; causal reasoning; cognition; comprehension; human; lateral prefrontal cortex; learning; mental model; model; nonhuman; prefrontal cortex; Review; thinking,2-s2.0-84933671372
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Lenat, D.B., Cyc: a large-scale investment in knowledge infrastructure (1995) Commun ACM, 38 (11), pp. 33-38",Cyc: a large-scale investment in knowledge infrastructure,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Levesque, H.J., The winograd schema challenge (2011) Logical Formalizations of Commonsense Reasoning, Papers from the 2011 AAAI Spring Symposium, Technical Report SS-11-06, , http://www.aaai.org/ocs/index.php/SSS/SSS11/paper/view/2502, Stanford, California, USA, March 21-23, 2011. AAAI",The winograd schema challenge,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Luo, Z., Sha, Y., Zhu, K.Q., Won Hwang S, Wang Z Commonsense causal reasoning between short texts (2016) Proceeding of 15Th Int. Conf. on Principles of Knowledge Representation and Reasonging (KR’2016, , Cape Town, South Africa",Wang Z Commonsense causal reasoning between short texts,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Maslan, N., Roemmele, M., Gordon, A.S., One hundred challenge problems for logical formalizations of commonsense psychology (2015) Twelfth International Symposium on Logical Formalizations of Commonsense Reasoning, , Stanford, CA",One hundred challenge problems for logical formalizations of commonsense psychology,"Maslan N., Roemmele M., Gordon A.S.",One hundred challenge problems for logical formalizations of commonsense psychology,[No abstract available],,2015.0,12.0,,,2-s2.0-84985977109
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Miller, G.A., WordNet: a lexical database for english (1995) Commun ACM, 38 (11), pp. 39-41",WordNet: a lexical database for english,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Mostafazadeh, N., Roth, M., Louis, A., Chambers, N., Allen, J., LSDSem 2017 shared task: The story cloze test (2017) Proceedings of the 2Nd Workshop on Linking Models of Lexical, Sentential and Discourse-Level Semantics, pp. 46-51",LSDSem 2017 shared task: The story cloze test,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Niles, I., Pease, A., Towards a standard upper ontology (2001) Proceedings of the International Conference on Formal Ontology in Information Systems-, 2001, pp. 2-9. , ACM",Towards a standard upper ontology,"Niles I., Pease A.",Towards a standard upper ontology,"The Suggested Upper Merged Ontology (SUMO) is an upper level ontology that has been proposed as a starter document for The Standard Upper Ontology Working Group, an IEEE-sanctioned working group of collaborators from the fields of engineering, philosophy, and information science. The SUMO provides definitions for general-purpose terms and acts as a foundation for more specific domain ontologies. In this paper we outline the strategy used to create the current version of the SUMO, discuss some of the challenges that we faced in constructing the ontology, and describe in detail its most general concepts and the relations between them.",10.1145/505168.505170,2001.0,1056.0,Knowledge Interchange Format; Ontologies,Computer software; Knowledge based systems; Knowledge representation; Semantics; World Wide Web; Ontology; Artificial intelligence,2-s2.0-0035789772
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Nute, D., Defeasible deontic logic (1997) Synthese Library: Studies in Epistemology, Logic, Methodology, and Philosophy of Science, 263. , https://doi.org/10.1007/978-94-015-8851-5, Springer, Berlin",Defeasible deontic logic,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Ostermann, S., Roth, M., Modi, A., Thater, S., Pinkal, M., SemEval-2018 task 11: Machine comprehension using commonsense knowledge (2018) Proceedings of the 12Th International Workshop on Semantic Evaluation, pp. 747-757",SemEval-2018 task 11: Machine comprehension using commonsense knowledge,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Roemmele, M., Bejan, C.A., Gordon, A.S., Choice of plausible alternatives: An evaluation of commonsense causal reasoning (2011) AAAI Spring Symposium: Logical Formalizations of Commonsense Reasoning",Choice of plausible alternatives: An evaluation of commonsense causal reasoning,"Roemmele M., Bejan C.A., Gordon A.S.",Choice of plausible alternatives: An evaluation of commonsense causal reasoning,"Research in open-domain commonsense reasoning has been hindered by the lack of evaluation metrics for judging progress and comparing alternative approaches. Taking inspiration from large-scale question sets used in natural language processing research, we authored one thousand English-language questions that directly assess commonsense causal reasoning, called the Choice Of Plausible Alternatives (COPA) evaluation. Using a forced-choice format, each question gives a premise and two plausible causes or effects, where the correct choice is the alternative that is more plausible than the other. This paper describes the authoring methodology that we used to develop a validated question set with sufficient breadth to advance open-domain commonsense reasoning research. We discuss the design decisions made during the authoring process, and explain how these decisions will affect the design of high-scoring systems. We also present the performance of multiple baseline approaches that use statistical natural language processing techniques, establishing initial benchmarks for future systems. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.",,2011.0,79.0,,Benchmarking; Authoring process; Causal reasoning; Commonsense reasoning; Design decisions; English languages; Evaluation metrics; NAtural language processing; Statistical natural language processing; Natural language processing systems,2-s2.0-80051542005
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Siebert, S., Schon, C., Stolzenburg, F., Commonsense reasoning using theorem proving and machine learning (2019) CD-MAKE 2019 – Machine Learning and Knowledge Extraction, , Holzinger A, Kieseberg P, Weippl E, Tjoa AM, LNCS. Springer Nature Switzerland, Canterbury, UK. To appear",Commonsense reasoning using theorem proving and machine learning,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Speer, R., Chin, J., Havasi, C., ConceptNet 5.5: An open multilingual graph of general knowledge (2017) AAAI Conference on Artificial Intelligence, pp. 4444-4451. , http://aaai.org/ocs/index.php/AAAI/AAAI17/paper/view/14972",ConceptNet 5.5: An open multilingual graph of general knowledge,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Spohn, W., (2012) The laws of belief: ranking theory and its philosophical applications, , Oxford University Press, Wiesbaden",,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Suchanek, F.M., Kasneci, G., Weikum, G., YAGO: a large ontology from wikipedia and WordNet (2008) Web Semant, 6 (3), pp. 203-217",YAGO: a large ontology from wikipedia and WordNet,,,,,,,,,
2-s2.0-85077011935,The CoRg Project: Cognitive Reasoning," Wirth, C.P., Stolzenburg, F., A series of revisions of David Poole’s specificity (2016) Ann Math Artif Intell, 78 (3), pp. 205-258. , Special issue on Belief Change and Argumentation Multi-Agent Scenarios. Issue editors: Jürgen Dix, Sven Ove Hansson, Gabriele Kern-Isberner, Guillermo Simari",A series of revisions of David Poole’s specificity,"Wirth C.-P., Stolzenburg F.",A series of revisions of David Poole’s specificity,"In the middle of the 1980s, David Poole introduced a semantic, model-theoretic notion of specificity to the artificial-intelligence community. Since then it has found further applications in non-monotonic reasoning, in particular in defeasible reasoning. Poole tried to approximate the intuitive human concept of specificity, which seems to be essential for reasoning in everyday life with its partial and inconsistent information. His notion, however, turns out to be intricate and problematic, which — as we show — can be overcome to some extent by a closer approximation of the intuitive human concept of specificity. Besides the intuitive advantages of our novel specificity orderings over Poole’s specificity relation in the classical examples of the literature, we also report some hard mathematical facts: Contrary to what was claimed before, we show that Poole’s relation is not transitive in general. The first of our specificity orderings (CP1) captures Poole’s original intuition as close as we could get after the correction of its technical flaws. The second one (CP2) is a variation of CP1 and presents a step toward similar notions that may eventually solve the intractability problem of Poole-style specificity relations. The present means toward deciding our novel specificity relations, however, show only slight improvements over the known ones for Poole’s relation; therefore, we suggest a more efficient workaround for applications in practice. © 2015, Springer International Publishing Switzerland.",10.1007/s10472-015-9471-9,2016.0,2.0,Artificial intelligence; Defeasible reasoning; Non-monotonic reasoning; Positive-conditional specification; Specificity,,2-s2.0-84944909121
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis,"Cambria, E., Rajagopal, D., Olsher, D., Das, D., Big Social Data Analysis (2013) Big Data Computing, pp. 401-414. , R. Akerkar Chapman and Hall/CRC",Big Social Data Analysis,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Chaturvedi, I., Ragusa, E., Gastaldo, P., Zunino, R., Cambria, E., Bayesian network based extreme learning machine for subjectivity detection (2018) J. Franklin Inst., 355 (4), pp. 1780-1797",Bayesian network based extreme learning machine for subjectivity detection,"Chaturvedi I., Ragusa E., Gastaldo P., Zunino R., Cambria E.",Bayesian network based extreme learning machine for subjectivity detection,"Subjectivity detection is a task of natural language processing that aims to remove ‘factual’ or ‘neutral’ content, i.e., objective text that does not contain any opinion, from online product reviews. Such a pre-processing step is crucial to increase the accuracy of sentiment analysis systems, as these are usually optimized for the binary classification task of distinguishing between positive and negative content. In this paper, we extend the extreme learning machine (ELM) paradigm to a novel framework that exploits the features of both Bayesian networks and fuzzy recurrent neural networks to perform subjectivity detection. In particular, Bayesian networks are used to build a network of connections among the hidden neurons of the conventional ELM configuration in order to capture dependencies in high-dimensional data. Next, a fuzzy recurrent neural network inherits the overall structure generated by the Bayesian networks to model temporal features in the predictor. Experimental results confirmed the ability of the proposed framework to deal with standard subjectivity detection problems and also proved its capacity to address portability across languages in translation tasks. © 2017 The Franklin Institute",10.1016/j.jfranklin.2017.06.007,2018.0,87.0,,Education; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Knowledge acquisition; Learning systems; Natural language processing systems; Recurrent neural networks; Binary classification; Detection problems; Extreme learning machine; Fuzzy recurrent neural networks; High dimensional data; Online product reviews; Pre-processing step; Sentiment analysis; Bayesian networks,2-s2.0-85024868159
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Chaturvedi, I., Ragusa, E., Gastaldo, P., Zunino, R., Cambria, E., Bayesian network based extreme learning machine for subjectivity detection (2018) J. Franklin Inst., 355 (4), pp. 1780-1797",Bayesian network based extreme learning machine for subjectivity detection,"Chaturvedi I., Ragusa E., Gastaldo P., Zunino R., Cambria E.",Bayesian network based extreme learning machine for subjectivity detection,[No abstract available],,2017.0,9.0,,,2-s2.0-85042347247
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Cheng, K., Li, J., Tang, J., Liu, H., Unsupervised sentiment analysis with signed social networks (2017) AAAI",Unsupervised sentiment analysis with signed social networks,"Cheng K., Li J., Tang J., Liu H.",Unsupervised sentiment analysis with signed social networks,"Huge volumes of opinion-rich data is user-generated in social media at an unprecedented rate, easing the analysis of individual and public sentiments. Sentiment analysis has shown to be useful in probing and understanding emotions, expressions and attitudes in the text. However, the distinct characteristics of social media data present challenges to traditional sentiment analysis. First, social media data is often noisy, incomplete and fast-evolved which necessitates the design of a sophisticated learning model. Second, sentiment labels are hard to collect which further exacerbates the problem by not being able to discriminate sentiment polarities. Meanwhile, opportunities are also unequivocally presented. Social media contains rich sources of sentiment signals in textual terms and user interactions, which could be helpful in sentiment analysis. While there are some attempts to leverage implicit sentiment signals in positive user interactions, little attention is paid on signed social networks with both positive and negative links. The availability of signed social networks motivates us to investigate if negative links also contain useful sentiment signals. In this paper, we study a novel problem of unsupervised sentiment analysis with signed social networks. In particular, we incorporate explicit sentiment signals in textual terms and implicit sentiment signals from signed social networks into a coherent model SignedSenti for unsupervised sentiment analysis. Empirical experiments on two real-world datasets corroborate its effectiveness. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2017.0,26.0,,Artificial intelligence; Social networking (online); Empirical experiments; Learning models; Public sentiments; Real-world datasets; Sentiment analysis; Social media datum; User interaction; User-generated; Data mining,2-s2.0-85029067784
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Howard, N., Cambria, E., Intention awareness: improving upon situation awareness in human-centric environments (2013) Human-centric Comput. Inf. Sci., 3 (9)",Intention awareness: improving upon situation awareness in human-centric environments,"Howard N., Cambria E.",Intention awareness: improving upon situation awareness in human-centric environments,"As the gap between human and machine shrinks, it becomes increasingly important to develop computer systems that incorporate or enhance existing Situation Awareness. However, these tend to focus on raw quantitative parameters, such as position and speed of objects. When these situations are governed by human actors, such parameters leave significant margins of uncertainty. In this paper, we discuss the potential of applying the characteristics intrinsic to the human actors that comprise a given situation to Situation Awareness, and the capacity that these concepts have to improve situation-aware systems. We argue that intention-aware based systems offer an advantage over situation-aware based systems in that they reduce the informational burden on humans without limiting effectiveness. We argue that computational analysis and tracking of semantic and affective information associated with human actors' intentions are an effective way to minimize miscommunication and uncertainty, particularly in time-sensitive and information-saturated situations. © 2013, Howard and Cambria; licensee BioMed Central Ltd.",10.1186/2192-1962-3-9,2013.0,65.0,Intention awareness; Sentic computing; Situation awareness,Semantics; Computational analysis; Human actor; Human-centric; Intention awareness; Quantitative parameters; Sentic Computing; Situation awareness; Situation-aware; Uncertainty analysis,2-s2.0-84941104015
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Abdon Miranda-Correa, J., Khomami, M., Sebe, N., Patras, I., (2017), Amigos: A dataset for mood, personality and affect research on individuals and groups",,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Huang, X., Rao, Y., Xie, H., Wong, T.-L., Wang, F.L., Cross-domain sentiment classification via topic-related tradaboost (2017) AAAI",Cross-domain sentiment classification via topic-related tradaboost,"Huang X., Rao Y., Xie H., Wong T.-L., Wang F.L.",Cross-domain sentiment classification via topic-related tradaboost,"Cross-domain sentiment classification aims to tag sentiments for a target domain by labeled data from a source domain. Due to the difference between domains, the accuracy of a trained classifier may be very low. In this paper, we propose a boosting-based learning framework named TR-TrAdaBoost for cross-domain sentiment classification. We firstly explore the topic distribution of documents, and then combine it with the unigram TrAdaBoost. The topic distribution captures the domain information of documents, which is valuable for cross-domain sentiment classification. Experimental results indicate that TR-TrAdaBoost represents documents well and boost the performance and robustness of TrAdaBoost. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,2017.0,21.0,,Artificial intelligence; Classification (of information); Cross-domain; Domain informations; Labeled data; Learning frameworks; Sentiment classification; Target domain; Topic distributions; Tradaboost; Information retrieval systems,2-s2.0-85028593472
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Krizhevsky, A., Sutskever, I., Hinton, G.E., Imagenet classification with deep convolutional neural networks (2012) NIPS, pp. 1097-1105. , Curran Associates, Inc",Imagenet classification with deep convolutional neural networks,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Cambria, E., Fu, J., Bisio, F., Poria, S., Affectivespace 2: enabling affective intuition for concept-level sentiment analysis (2015) AAAI, pp. 508-514",Affectivespace 2: enabling affective intuition for concept-level sentiment analysis,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Cambria, E., Hussain, A., Havasi, C., Eckl, C., Common sense computing: from the society of mind to digital intuition and beyond (2009) Biometric ID Management and Multimodal Communication, Lecture Notes in Computer Science, 5707, pp. 252-259. , J. Fierrez J. Ortega A. Esposito A. Drygajlo M. Faundez-Zanuy Springer Berlin Heidelberg",Common sense computing: from the society of mind to digital intuition and beyond,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Cambria, E., Poria, S., Hazarika, D., Kwok, K., SenticNet 5: discovering conceptual primitives for sentiment analysis by means of context embeddings (2018) AAAI, pp. 1795-1802",SenticNet 5: discovering conceptual primitives for sentiment analysis by means of context embeddings,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Cambria, E., Hussain, A., Havasi, C., Eckl, C., Sentic computing: exploitation of common sense for the development of emotion-sensitive systems (2010) Development of Multimodal Interfaces: Active Listening and Synchrony, pp. 148-156. , A. Esposito N. Campbell C. Vogel A. Hussain A. Nijholt Springer Berlin",Sentic computing: exploitation of common sense for the development of emotion-sensitive systems,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Cambria, E., Livingstone, A., Hussain, A., The hourglass of emotions (2012) Cognitive Behavioral Systems, Lecture Notes in Computer Science, 7403, pp. 144-157. , A. Esposito A. Vinciarelli R. Hoffmann V. Muller Springer Berlin Heidelberg",The hourglass of emotions,"Cambria E., Livingstone A., Hussain A.",The hourglass of emotions,"Human emotions and their modelling are increasingly understood to be a crucial aspect in the development of intelligent systems. Over the past years, in fact, the adoption of psychological models of emotions has become a common trend among researchers and engineers working in the sphere of affective computing. Because of the elusive nature of emotions and the ambiguity of natural language, however, psychologists have developed many different affect models, which often are not suitable for the design of applications in fields such as affective HCI, social data mining, and sentiment analysis. To this end, we propose a novel biologically-inspired and psychologically-motivated emotion categorisation model that goes beyond mere categorical and dimensional approaches. Such model represents affective states both through labels and through four independent but concomitant affective dimensions, which can potentially describe the full range of emotional experiences that are rooted in any of us. © 2012 Springer-Verlag.",10.1007/978-3-642-34584-5_11,2012.0,166.0,Affective HCI; Cognitive and Affective Modelling; NLP,Affective Computing; Affective state; Human emotion; In-field; Natural languages; NLP; Psychological model; Sentiment analysis; Social data mining; Human computer interaction; Intelligent systems; Cognitive systems,2-s2.0-84870364413
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Cambria, E., Hussain, A., Havasi, C., Eckl, C., SenticSpace: visualizing opinions and sentiments in a multi-dimensional vector space (2010) Knowledge-Based and Intelligent Information and Engineering Systems, Lecture Notes in Artificial Intelligence, 6279, pp. 385-393. , R. Setchi I. Jordanov R. Howlett L. Jain Springer Berlin",SenticSpace: visualizing opinions and sentiments in a multi-dimensional vector space,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Poria, S., Chaturvedi, I., Cambria, E., Hussain, A., Convolutional MKL based multimodal emotion recognition and sentiment analysis (2016) ICDM, Barcelona, pp. 439-448",Convolutional MKL based multimodal emotion recognition and sentiment analysis,"Poria S., Chaturvedi I., Cambria E., Hussain A.",Convolutional MKL based multimodal emotion recognition and sentiment analysis,"Technology has enabled anyone with an Internet connection to easily create and share their ideas, opinions and content with millions of other people around the world. Much of the content being posted and consumed online is multimodal. With billions of phones, tablets and PCs shipping today with built-in cameras and a host of new video-equipped wearables like Google Glass on the horizon, the amount of video on the Internet will only continue to increase. It has become increasingly difficult for researchers to keep up with this deluge of multimodal content, let alone organize or make sense of it. Mining useful knowledge from video is a critical need that will grow exponentially, in pace with the global growth of content. This is particularly important in sentiment analysis, as both service and product reviews are gradually shifting from unimodal to multimodal. We present a novel method to extract features from visual and textual modalities using deep convolutional neural networks. By feeding such features to a multiple kernel learning classifier, we significantly outperform the state of the art of multimodal emotion recognition and sentiment analysis on different datasets. © 2016 IEEE.",10.1109/ICDM.2016.178,2017.0,248.0,Convolutional neural networks; Deep learning; Multimodal sentiment analysis; Multiple kernel learning,Classification (of information); Convolution; Data mining; Deep learning; Deep neural networks; Neural networks; Speech recognition; Convolutional neural network; Internet connection; Internet wills; Multimodal emotion recognition; Multiple Kernel Learning; Product reviews; Sentiment analysis; State of the art; Modal analysis,2-s2.0-85014552954
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Giatsoglou, M., Vozalis, M.G., Diamantaras, K., Vakali, A., Sarigiannidis, G., Chatzisavvas, K.C., Sentiment analysis leveraging emotions and word embeddings (2017) Expert Syst. Appl., 69, pp. 214-224",Sentiment analysis leveraging emotions and word embeddings,"Giatsoglou M., Vozalis M.G., Diamantaras K., Vakali A., Sarigiannidis G., Chatzisavvas K.C.",Sentiment analysis leveraging emotions and word embeddings,"Sentiment analysis and opinion mining are valuable for extraction of useful subjective information out of text documents. These tasks have become of great importance, especially for business and marketing professionals, since online posted products and services reviews impact markets and consumers shifts. This work is motivated by the fact that automating retrieval and detection of sentiments expressed for certain products and services embeds complex processes and pose research challenges, due to the textual phenomena and the language specific expression variations. This paper proposes a fast, flexible, generic methodology for sentiment detection out of textual snippets which express people's opinions in different languages. The proposed methodology adopts a machine learning approach with which textual documents are represented by vectors and are used for training a polarity classification model. Several documents’ vector representation approaches have been studied, including lexicon-based, word embedding-based and hybrid vectorizations. The competence of these feature representations for the sentiment classification task is assessed through experiments on four datasets containing online user reviews in both Greek and English languages, in order to represent high and weak inflection language groups. The proposed methodology requires minimal computational resources, thus, it might have impact in real world scenarios where limited resources is the case. © 2016 Elsevier Ltd",10.1016/j.eswa.2016.10.043,2017.0,174.0,Hybrid vectorization; Machine learning; Multilingual sentiment analysis; Online user reviews; Text analysis; Vector representation,Artificial intelligence; Classification (of information); Commerce; Data mining; Learning systems; Natural language processing systems; Online users; Sentiment analysis; Text analysis; Vector representations; Vectorization; Information retrieval systems,2-s2.0-84994061017
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Tang, D., Qin, B., Liu, T., Aspect level sentiment classification with deep memory network (2016) EMNLP, pp. 214-224",Aspect level sentiment classification with deep memory network,"Tang D., Qin B., Liu T.",Aspect level sentiment classification with deep memory network,"We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation. © 2016 Association for Computational Linguistics",10.18653/v1/d16-1021,2016.0,347.0,,Arts computing; Natural language processing systems; Network layers; Support vector machines; Attention model; Context-word; External memory; Feature-based; Importance degrees; Neural models; Sentiment classification; Text representation; Long short-term memory,2-s2.0-85072835413
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Yang, M., Tu, W., Wang, J., Xu, F., Chen, X., Attention based LSTM for target dependent sentiment classification (2017) AAAI",Attention based LSTM for target dependent sentiment classification,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Xu, J., Chen, D., Qiu, X., Huang, X., Cached long short-term memory neural networks for document-level sentiment classification (2016) EMNLP, pp. 1660-1669",Cached long short-term memory neural networks for document-level sentiment classification,"Xu J., Chen D., Qiu X., Huang X.",Cached long short-term memory neural networks for document-level sentiment classification,"Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets. © 2016 Association for Computational Linguistics",10.18653/v1/d16-1172,2016.0,72.0,,Brain; Information retrieval systems; Long short-term memory; Semantics; Sentiment analysis; Cache mechanism; Feature engineerings; Memory units; Semantic information; Sentiment classification; State of the art; Cache memory,2-s2.0-85072846529
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Fischer, A., Igel, C., An introduction to restricted Boltzmann machines (2012) ICPR, pp. 14-36. , Springer",An introduction to restricted Boltzmann machines,"Fischer A., Igel C.",An introduction to restricted Boltzmann machines,"Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided. © 2012 Springer-Verlag.",10.1007/978-3-642-33275-3_2,2012.0,327.0,,Basic concepts; Building blockes; Computational power; Deep belief networks; GraphicaL model; Machine learning problem; Markov chain Monte Carlo method; Probabilistic graphical models; Restricted boltzmann machine; Stochastic neural network; Graphic methods; Image analysis; Learning algorithms; Learning systems; Markov processes; Neural networks; Computer vision,2-s2.0-84865595751
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Chaturvedi, I., Ong, Y.-S., Tsang, I.W., Welsch, R.E., Cambria, E., Learning word dependencies in text by means of a deep recurrent belief network (2016) Knowl. Based Syst., 108, pp. 144-154",Learning word dependencies in text by means of a deep recurrent belief network,"Chaturvedi I., Ong Y.-S., Tsang I.W., Welsch R.E., Cambria E.",Learning word dependencies in text by means of a deep recurrent belief network,"We propose a deep recurrent belief network with distributed time delays for learning multivariate Gaussians. Learning long time delays in deep belief networks is difficult due to the problem of vanishing or exploding gradients with increase in delay. To mitigate this problem and improve the transparency of learning time-delays, we introduce the use of Gaussian networks with time-delays to initialize the weights of each hidden neuron. From our knowledge of time delays, it is possible to learn the long delays from short delays in a hierarchical manner. In contrast to previous works, here dynamic Gaussian Bayesian networks over training samples are evolved using Markov Chain Monte Carlo to determine the initial weights of each hidden layer of neurons. In this way, the time-delayed network motifs of increasing Markov order across layers can be modeled hierarchically using a deep model. To validate the proposed Variable-order Belief Network (VBN) framework, it is applied for modeling word dependencies in text. To explore the generality of VBN, it is further considered for a real-world scenario where the dynamic movements of basketball players are modeled. Experimental results obtained showed that the proposed VBN could achieve over 30% improvement in accuracy on real-world scenarios compared to the state-of-the-art baselines. © 2016",10.1016/j.knosys.2016.07.019,2016.0,59.0,Deep belief networks; Gaussian networks; Markov Chain Monte Carlo; Time-delays; Variable-order,Chains; Gaussian distribution; Markov processes; Monte Carlo methods; Time delay; Deep belief networks; Distributed time delays; Dynamic movements; Gaussian bayesian networks; Gaussian networks; Markov Chain Monte-Carlo; Real-world scenario; Variable order; Bayesian networks,2-s2.0-84981722706
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Lee, H., Grosse, R., Ranganath, R., Ng, A.Y., Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations (2009) Proceedings of the 26th annual international conference on machine learning, pp. 609-616. , ACM",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,"Lee H., Grosse R., Ranganath R., Ng A.Y.",Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations,"There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.",,2009.0,1670.0,,Belief networks; Bottom-up and top-down; Excellent performance; Generative model; Hierarchical representation; High-dimensional images; Natural scenes; Novel techniques; Probabilistic inference; Realistic images; Translation invariants; Visual feature; Visual recognition; Bayesian networks; Convolution; Robot learning; Unsupervised learning; Education,2-s2.0-71149119164
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Rajagopal, D., Cambria, E., Olsher, D., Kwok, K., A graph-based approach to commonsense concept extraction and semantic similarity detection (2013) WWW, pp. 565-570",A graph-based approach to commonsense concept extraction and semantic similarity detection,"Rajagopal D., Cambria E., Olsher D., Kwok K.",A graph-based approach to commonsense concept extraction and semantic similarity detection,"Commonsense knowledge representation and reasoning support a wide variety of potential applications in fields such as document auto-categorization, Web search enhancement, topic gisting, social process modeling, and concept-level opinion and sentiment analysis. Solutions to these problems, however, demand robust knowledge bases capable of supporting exible, nuanced reasoning. Populating such knowledge bases is highly time-consuming, making it necessary to develop techniques for deconstructing natural language texts into commonsense concepts. In this work, we propose an approach for effective multi-word commonsense expression extraction from unrestricted English text, in addition to a semantic similarity detection technique allowing additional matches to be found for specific concepts not already present in knowledge bases.",10.1145/2487788.2487995,2013.0,53.0,Ai; Commonsense knowledge representation and reasoning; Natural language processing; Semantic similarity,Artificial intelligence; Extraction; Knowledge representation; Semantics; Sentiment analysis; World Wide Web; Commonsense knowledge; Concept extraction; Concept levels; Knowledge basis; NAtural language processing; Natural language text; Semantic similarity; Social process; Graphic methods,2-s2.0-84890636931
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Tanaka, M., Okutomi, M., A novel inference of a restricted Boltzmann machine (2014) ICPR, pp. 1526-1531",A novel inference of a restricted Boltzmann machine,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Baranyi, P., Lei, K.-F., Yam, Y., Complexity reduction of singleton based neuro-fuzzy algorithm (2000) IEEE SMC, 4, pp. 2503-2508 vol.4",Complexity reduction of singleton based neuro-fuzzy algorithm,"Baranyi Peter, Lei Kin-fong, Yam Yeung",Complexity reduction of singleton based neuro-fuzzy algorithm,During the past few years efficient singular value-based complexity reduction tools have been developed to fuzzy logic techniques. This paper introduces a singular value-based reduction method to the generalised type neural network. The method conducts singular value decomposition of the weighting functions defined on the connections among the neurons and generates certain linear combinations of the original weighting functions to form a new connection-net for the complexity reduced neural network.,,2000.0,22.0,,Algorithms; Computational complexity; Fuzzy sets; Mathematical models; Complexity reduction tools; Neural networks,2-s2.0-0034505020
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Morency, L.-P., Mihalcea, R., Doshi, P., Towards multimodal sentiment analysis: Harvesting opinions from the web (2011) ICMI, pp. 169-176. , ACM",Towards multimodal sentiment analysis: Harvesting opinions from the web,"Morency L.-P., Mihalcea R., Doshi P.",Towards multimodal sentiment analysis: Harvesting opinions from the web,"With more than 10,000 new videos posted online every day on social websites such as YouTube and Facebook, the internet is becoming an almost infinite source of information. One crucial challenge for the coming decade is to be able to harvest relevant information from this constant flow of multimodal data. This paper addresses the task of multimodal sentiment analysis, and conducts proof-of-concept experiments that demonstrate that a joint model that integrates visual, audio, and textual features can be effectively used to identify sentiment in Web videos. This paper makes three important contributions. First, it addresses for the first time the task of tri-modal sentiment analysis, and shows that it is a feasible task that can benefit from the joint exploitation of visual, audio and textual modalities. Second, it identifies a subset of audio-visual features relevant to sentiment analysis and present guidelines on how to integrate these features. Finally, it introduces a new dataset consisting of real online data, which will be useful for future research in this area. © 2011 ACM.",10.1145/2070481.2070509,2011.0,197.0,audio-visual integration; multimodal signal processing; subjectivity and sentiment analysis; YouTube videos,Audio-visual features; Audio-visual integration; Constant flow; Data sets; Facebook; Joint models; Multi-modal; Multi-modal data; multimodal signal processing; Online data; Proof of concept; Sentiment analysis; Web video; YouTube; Data processing; Interactive computer systems; Signal processing; Websites; Data mining,2-s2.0-83455176765
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Pérez-Rosas, V., Mihalcea, R., Morency, L.-P., Utterance-level multimodal sentiment analysis (2013) ACL, pp. 973-982",Utterance-level multimodal sentiment analysis,"Pérez-Rosas V., Mihalcea R., Morency L.-P.",Utterance-level multimodal sentiment analysis,"During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. This paper presents a method for multimodal sentiment classification, which can identify the sentiment expressed in utterance-level visual datastreams. Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality. © 2013 Association for Computational Linguistics.",,2013.0,109.0,,Computational linguistics; Speech recognition; Error rate reduction; Facebook; Multi-modal; Multi-modal dataset; New sources; Opinion analysis; Sentiment analysis; Sentiment classification; Data mining,2-s2.0-84907316816
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Bollegala, D., Mu, T., Goulermas, J.Y., Cross-domain sentiment classification using sentiment sensitive embeddings (2016) IEEE Trans. Knowl. Data Eng., 28 (2), pp. 398-410",Cross-domain sentiment classification using sentiment sensitive embeddings,,,,,,,,,
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Long, M., Wang, J., Cao, Y., Sun, J., Yu, P.S., Deep learning of transferable representation for scalable domain adaptation (2016) IEEE TKDE, 28 (8), pp. 2027-2040",Deep learning of transferable representation for scalable domain adaptation,"Long M., Wang J., Cao Y., Sun J., Yu P.S.",Deep learning of transferable representation for scalable domain adaptation,"Domain adaptation generalizes a learning model across source domain and target domain that are sampled from different distributions. It is widely applied to cross-domain data mining for reusing labeled information and mitigating labeling consumption. Recent studies reveal that deep neural networks can learn abstract feature representation, which can reduce, but not remove, the cross-domain discrepancy. To enhance the invariance of deep representation and make it more transferable across domains, we propose a unified deep adaptation framework for jointly learning transferable representation and classifier to enable scalable domain adaptation, by taking the advantages of both deep learning and optimal two-sample matching. The framework constitutes two inter-dependent paradigms, unsupervised pre-training for effective training of deep models using deep denoising autoencoders, and supervised fine-tuning for effective exploitation of discriminative information using deep neural networks, both learned by embedding the deep representations to reproducing kernel Hilbert spaces (RKHSs) and optimally matching different domain distributions. To enable scalable learning, we develop a linear-time algorithm using unbiased estimate that scales linearly to large samples. Extensive empirical results show that the proposed framework significantly outperforms state of the art methods on diverse adaptation tasks: sentiment polarity prediction, email spam filtering, newsgroup content categorization, and visual object recognition. © 1989-2012 IEEE.",10.1109/TKDE.2016.2554549,2016.0,102.0,deep learning; denoising autoencoder; Domain adaptation; multiple kernel learning; neural network; two-sample test,Clustering algorithms; Learning systems; Neural networks; Object recognition; Auto encoders; Deep learning; Domain adaptation; Multiple Kernel Learning; Two-sample tests; Data mining,2-s2.0-84978759588
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Dredze, M., Crammer, K., Pereira, F., Confidence-weighted linear classification (2008) ICML, pp. 264-271",Confidence-weighted linear classification,"Dredze M., Crammer K., Pereira F.",Confidence-weighted linear classification,"We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and eovarianee of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training. Copyright 2008 by the author(s)/owner(s).",10.1145/1390156.1390190,2008.0,292.0,,Machine learning; Classifiers; Parallel algorithms; Robot learning; Classifier combination; Confidence information; Empirical evaluations; Linear classification; Linear classifiers; On-line algorithms; Parallel training; Parameter vectors; Classification (of information); Learning systems; Batch methods; Classifier combinations; Confidence informations; Empirical evaluations; Gaussian; Linear classifications; Linear classifiers; On-line algorithms; On-line settings; Parallel trainings; Parameter vectors; State-of-the arts,2-s2.0-56449101965
2-s2.0-85065463296,Fuzzy commonsense reasoning for multimodal sentiment analysis," Kim, Y., Convolutional neural networks for sentence classification (2014) EMNLP, pp. 1746-1751",Convolutional neural networks for sentence classification,Kim Y.,Convolutional neural networks for sentence classification,"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification. © 2014 Association for Computational Linguistics.",10.3115/v1/d14-1181,2014.0,4151.0,,Convolution; Neural networks; Sentiment analysis; Classification tasks; Convolutional neural network; Hyper-parameter; Learning tasks; Question classification; Sentence classifications; Simple modifications; State of the art; Vectors,2-s2.0-84961376850
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing,"Lindsay, R.K., Buchanan, B.G., Feigenbaum, E.A., Lederberg, J., (1980) Applications of Artificial Intelligence for Organic Chemistry: The Dendral Project, , New York NY USA, McGraw-Hill",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Quinlan, J.R., Induction of decision trees (1986) Mach. Learn, 1 (1), pp. 81-106",Induction of decision trees,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Muggleton, S., Inductive logic programming (1991) New Gener. Comput, 8 (4), pp. 295-318. , http://www.doc.ic.ac.uk/~shm/Papers/ilp.pdf, Feb, [Online]",Inductive logic programming,Muggleton S.,Inductive logic programming,"A new research area, Inductive Logic Programming, is presently emerging. While inheriting various positive characteristics of the parent subjects of Logic Programming and Machine Learning, it is hoped that the new area will overcome many of the limitations of its forebears. The background to present developments within this area is discussed and various goals and aspirations for the increasing body of researchers are identified. Inductive Logic Programming needs to be based on sound principles from both Logic and Statistics. On the side of statistical justification of hypotheses we discuss the possible relationship between Algorithmic Complexity theory and Probably-Approximately-Correct (PAC) Learning. In terms of logic we provide a unifying framework for Muggleton and Buntine's Inverse Resolution (IR) and Plotkin's Relative Least General Generalisation (RLGG) by rederiving RLGG in terms of IR. This leads to a discussion of the feasibility of extending the RLGG framework to allow for the invention of new predicates, previously discussed only within the context of IR. © 1991 Ohmsha, Ltd. and Springer.",10.1007/BF03037089,1991.0,464.0,induction; information compression; inverse resolution; Learning; logic programming; predicate invention,,2-s2.0-0000640432
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Brewka, G., (1991) Nonmonotonic Reasoning: Logical Foundations of Common-sense, , Cambridge, U.K.: Cambridge Univ. Press",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Doyle, J., The ins and outs of reason maintenance (1983) Proc. 8th Int. Joint Conf. Artif Intell. IJCAI, pp. 349-351. , Los Altos, CA, USA",The ins and outs of reason maintenance,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Kakas, A.C., Kowalski, R.A., Toni, F., Abductive logic programming (1993) J. Log. Comput, 2 (6), pp. 719-770. , Dec",Abductive logic programming,"Kakas A.C., Kowalski R.A., Toni F.",Abductive logic programming,This paper is a survey and critical overview of recent work on the extension of logic programming to perform abductive reasoning (abductive logic programming). We outline the general framework of abduction and its applications to knowledge assimilation and default reasoning; and we introduce an argumentation-theoretic approach to the use of abduction as an interpretation for negation as failure. We also analyse the links between abduction and the extension of logic programming obtained by adding a form of explicit negation. Finally we discuss the relation between abduction and truth maintenance. © 1993 Oxford University Press.,10.1093/logcom/2.6.719,1992.0,473.0,,,2-s2.0-77957187070
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Hinton, G.E., Preface to the special issue on connectionist symbol processing (1990) Artif. Intell, 46 (1-2), p. 1. , Nov",Preface to the special issue on connectionist symbol processing,Hinton G.E.,Preface to the special issue on connectionist symbol processing,[No abstract available],10.1016/0004-3702(90)90002-H,1990.0,32.0,,,2-s2.0-0009438133
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Touretzky, D.S., BoltzCONS: Dynamic symbol structures in a connectionist network (1990) Artif. Intell, 46 (1-2), pp. 5-6. , Nov",BoltzCONS: Dynamic symbol structures in a connectionist network,Touretzky D.S.,BoltzCONS: Dynamic symbol structures in a connectionist network,"BoltzCONS is a connectionist model that dynamically creates and manipulates composite symbol structures. These structures are implemented using a functional analog of linked lists, but BoltzCONS employs distributed representations and associative retrieval in place of a conventional memory organization. Associative retrieval leads to some interesting properties, e.g., the model can instantaneously access any uniquely-named internal node of a tree. But the point of the work is not to reimplement linked lists in some peculiar new way; it is to show how neural networks can exhibit compositionality and distal access (the ability to reference a complex structure via an abbreviated tag), two properties that distinguish symbol processing from lower-level cognitive functions such as pattern recognition. Unlike certain other neural net models, BoltzCONS represents objects as a collection of superimposed activity patterns rather than as a set of weights. It can therefore create new structured objects dynamically, without reliance on iterative training procedures, without rehearsal of previously-learned patterns, and without resorting to grandmother cells. © 1990.",10.1016/0004-3702(90)90003-I,1990.0,49.0,,Data Processing--Data Structures; Connectionist Networks; Symbol Processing; Neural Networks,2-s2.0-0025521210
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Schlichtkrull, M., Kipf, T.N., Bloem, P., van den Berg, R., Titov, I., Welling, M., Modeling relational data with graph convolutional networks (2018) Proc. Eur. Semantic Web Conf. (ESWC, pp. 593-607",Modeling relational data with graph convolutional networks,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Trouillon, T., Welbl, J., Riedel, S., Gaussier, E., Bouchard, G., Complex embeddings for simple link prediction (2016) Proc. 33nd Int. Conf. Mach. Learn. (ICML, 2016, pp. 2071-2080. , New York, NY, USA",Complex embeddings for simple link prediction,"Trouillon T., Welbl J., Riedel S., Gaussier É., Bouchard G.",Complex embeddings for simple link prediction,[No abstract available],,2016.0,435.0,,,2-s2.0-85019189005
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Rocktaschel, T., Riedel, S., End-to-end differentiable proving (2017) Proc. Annu. Conf. Neural Inf. Process. Syst, pp. 3788-3800",End-to-end differentiable proving,"Rocktäschel T., Riedel S.",End-to-end differentiable proving,"We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules. © 2017 Neural information processing systems foundation. All rights reserved.",,2017.0,101.0,,Formal logic; Knowledge based systems; Radial basis function networks; Vectors; Backward chaining; First order logic; Gradient descent; Incomplete knowledge; Radial basis function kernels; State of the art; Symbolic reasoning; Vector representations; Vector spaces,2-s2.0-85047004235
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Minervini, P., Bosnjak, M., Rocktaschel, T., Riedel, S., (2018) Towards Neural Theorem Proving at Scale, , https://arxiv.org/abs/1807.08204, arXiv: 1807, 08204, [Online]",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Dong, H., Mao, J., Lin, T., Wang, C., Li, L., Zhou, D., Neural logic machines (2019) Proc. Int. Conf. Learn. Represent, pp. 1-22. , New Orleans, LA, USA",Neural logic machines,"Dong H., Mao J., Lin T., Wang C., Li L., Zhou D.",Neural logic machines,"We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks-as function approximators, and logic programming-as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone. © 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved.",,2019.0,31.0,,Computer circuits; Decision making; Learning systems; Trees (mathematics); Blocks worlds; Function approximators; General graph; Inductive learning; Logic machines; Logic reasoning; Relational reasoning; Shortest path; Inductive logic programming (ILP),2-s2.0-85083952231
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Tellex, S., Katz, B., Lin, J., Marton, G., Fernandes, A., Quantitative evaluation of passage retrieval algorithms for question answering (2003) Proc. 26th Annu. Int. ACM SIGIR Conf. Res. Develop. Inf. Retr, pp. 41-47. , Toronto, ON, Canada, Aug",Quantitative evaluation of passage retrieval algorithms for question answering,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Sequiera, R., Baruah, G., Tu, Z., Mohammed, S., Rao, J., Zhang, H., Lin, J., (2017) Exploring the Effectiveness of Convolutional Neural Networks for Answer Selection in End-to-end Question Answering, , https://arxiv.org/abs/1707.07804, arXiv: 1707.07804, [Online]",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Thomas, N.T., An e-business chatbot using AIML and LSA (2016) Proc. Int. Conf. Adv. Comput., Commun. Inform. ICACCI, pp. 2740-2742. , Jaipur, India, Sep",An e-business chatbot using AIML and LSA,Thomas N.T.,An e-business chatbot using AIML and LSA,"The e-business has completely changed the way of selling products. E-commerce is one of the e-business models which mostly do business over the internet. The major drawback of this field is quality of customer service they provide. In every e-business model, customers have to wait for a long time to get response from the customer service representative. Especially in case of live chat, they talk to multiple customers at a time. The responses may not be relevant as they copy paste pre-written answers. Also, the slow response and the long time wait for the service agent is the biggest headache in this field of online services. As a solution to this problem, we propose a chatbot which automatically gives immediate responses to the users based on the data set of Frequently Answered Questions(FAQs), using Artificial Intelligence Markup Language (AIML) and Latent Semantic Analysis (LSA). Template based questions like greetings and general questions will be answered using AIML and other service related questions use LSA to give responses. © 2016 IEEE.",10.1109/ICACCI.2016.7732476,2016.0,46.0,AIML; E-business; LSA,Information science; Markup languages; Sales; Semantics; AIML; Artificial intelligence mark up languages; Customer service representatives; Customer services; E-business models; eBusiness; Latent Semantic Analysis; On-line service; Electronic commerce,2-s2.0-85007325164
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Cui, L., Wei, F., Huang, S., Tan, C., Duan, C., Zhou, M., SuperA-Gent: A customer service chatbot for e-commerce websites (2017) Proc. 55th Annu. Meeting Assoc. Comput. Linguistics-Syst. Demonstrations, pp. 97-102. , Jul",SuperA-Gent: A customer service chatbot for e-commerce websites,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Bhargava, H., Power, D., Decision support systems and Web technologies: A status report (2001) Proc. Amer. Conf. Inf. Syst, p. 46. , Boston, MA, USA, Dec",Decision support systems and Web technologies: A status report,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Kohn, M.S., Sun, J., Knoop, S., Shabo, A., Carmeli, B., Sow, D., Syed-Mahmood, T., Rapp, W., IBM's health analytics and clinical decision support (2014) Yearbook Med. Inf, 9 (1), pp. 154-162. , Aug",IBM's health analytics and clinical decision support,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Sodhro, A., Li, Y., Shah, M., Energy-efficient adaptive transmission power control for wireless body area networks (2016) IET Commun, 10 (1), pp. 81-90. , Jan",Energy-efficient adaptive transmission power control for wireless body area networks,"Sodhro A.H., Li Y., Shah M.A.",Energy-efficient adaptive transmission power control for wireless body area networks,"An important constraint in wireless body area network (WBAN) is to maximise the energy-efficiency of wearable devices due to their limited size and light weight. Two experimental scenarios; 'right wrist to right hip' and 'chest to right hip' with body posture of walking are considered. It is analyzed through extensive real-time data sets that due to large temporal variations in the wireless channel, a constant transmission power and a typical conventional transmission power control (TPC) methods are not suitable choices for WBAN. To overcome these problems a novel energy-efficient adaptive power control (APC) algorithm is proposed that adaptively adjusts transmission power (TP) level based on the feedback from base station. The main advantages of the proposed algorithm are saving more energy with acceptable packet loss ratio (PLR) and lower complexity in implementation of desired tradeoffbetween energy savings and link reliability. We adapt, optimise and theoretically analyse the required parameters to enhance the system performance. The proposed algorithm sequentially achieves significant higher energy savings of 40.9%, which is demonstrated by Monte Carlo simulations in MATLAB. However, the only limitation of proposed algorithm is a slightly higher PLR in comparison to conventional TPC such as Gao's and Xiao's methods. © The Institution of Engineering and Technology.",10.1049/iet-com.2015.0368,2016.0,78.0,,Adaptive control systems; Algorithms; Chip scale packages; Complex networks; Energy conservation; Intelligent systems; MATLAB; Monte Carlo methods; Networks (circuits); Power control; Wearable technology; Wireless local area networks (WLAN); Wireless networks; Adaptive power control; Adaptive transmission; Packet loss ratio; Temporal variation; Transmission power; Transmission power control; Wearable devices; Wireless body area network; Energy efficiency,2-s2.0-84956856994
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Sodhro, A., Pirbhulal, S., Lodro, M., Shah, M., Energy-efficiency in wireless body sensor networks (2017) Networks of the Future Architectures, Technologies, and Implementations, p. 492. , Boca Raton, FL, USA, CRC Press",Energy-efficiency in wireless body sensor networks,"Sodhro A.H., Fortino G., Pirbhulal S., Muhammad Lodro M., Shah M.A.",Energy-efficiency in wireless body sensor networks,[No abstract available],,2017.0,6.0,,,2-s2.0-85064108789
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Sodhro, A., Sangaiah, A., Sodhro, G., Sekhari, A., Ouzrout, Y., Pirbhulal, S., Energy-efficiency of tools and applications on Internet (2018) Computational Intelligence for Multimedia Big Data on the Cloud with Engineering Applications (Intelligent Data-Centric Systems: Sensor Collected Intelligence), , Amsterdam, The Netherlands, Elsevier",Energy-efficiency of tools and applications on Internet,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," A. S. d'Avila Garcez*K. Broda*D. M. Gabbay (2002) Neural-Symbolic Learning Systems: Foundations and Applications, , London, U.K.: Springer, Verlag",A. S. d'Avila Garcez*K. Broda*D. M. Gabbay,[No author name available],A. S. d'Avila Garcez*K. Broda*D. M. Gabbay,[No abstract available],,2002.0,1.0,,,2-s2.0-85078327952
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Shavlik, J.W., Towell, G.G., An approach to combining explanation-based and neural learning algorithms (1989) Connection Set, 1 (3), pp. 231-253",An approach to combining explanation-based and neural learning algorithms,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Towell, G.G., Shavlik, J.W., Knowledge-based artificial neural networks (1994) Artif. Intell, 70 (1-2), pp. 119-165. , Oct",Knowledge-based artificial neural networks,"Towell G.G., Shavlik J.W.",Knowledge-based artificial neural networks,"Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for accurately classifying examples not seen during training. The challenge of hybrid learning systems is to use the information provided by one source of information to offset information missing from the other source. By so doing, a hybrid learning system should learn more effectively than systems that use only one of the information sources. KBANN (Knowledge-Based Artificial Neural Networks) is a hybrid learning system built on top of connectionist learning techniques. It maps problem-specific ""domain theories"", represented in propositional logic, into neural networks and then refines this reformulated knowledge using backpropagation. KBANN is evaluated by extensive empirical tests on two problems from molecular biology. Among other results, these tests show that the networks created by KBANN generalize better than a wide variety of learning systems, as well as several techniques proposed by biologists. © 1994.",10.1016/0004-3702(94)90105-8,1994.0,459.0,Computational biology; Connectionism; Explanation-based learning; Hybrid algorithms; Machine learning; Theory refinement,Algorithms; Biology; Computational methods; Formal logic; Information use; Knowledge based systems; Learning systems; Man machine systems; Computational theory; Connectionism; Explanation based learning; Hybrid algorithms; Knowledge based artificial neural networks; Machine learning; Theory refinement; Neural networks,2-s2.0-0028529307
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Garcez, A.S.A., Zaverucha, G., The connectionist inductive learning and logic programming system (1999) Appl. Intell, 11 (1), pp. 59-77. , Jul",The connectionist inductive learning and logic programming system,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Shastri, L., Neurally motivated constraints on the working memory capacity of a production system for parallel processing: Implications of a connectionist model based on temporal synchrony (1992) Proc. 14th Annu. Conf. Cognit. Sci. Soc, 14, p. 159. , Bloomington, IN, USA: Psychology Press, Jul./Aug",Neurally motivated constraints on the working memory capacity of a production system for parallel processing: Implications of a connectionist model based on temporal synchrony,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Ding, L., Neural prolog-the concepts, construction and mechanism (1995) Proc. IEEE Int. Conf. Syst., Man Cybern., Intell. Syst. 21st Century, 4, pp. 3603-3608. , Oct",construction and mechanism,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Franca, M.V.M., Zaverucha, G., D'Avila Garcez, A.S., Fast relational learning using bottom clause propositionalization with artificial neural networks (2014) Mach. Learn, 94 (1), pp. 81-104. , Jan",Fast relational learning using bottom clause propositionalization with artificial neural networks,"França M.V.M., Zaverucha G., D'Avila Garcez A.S.",Fast relational learning using bottom clause propositionalization with artificial neural networks,"Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph mining and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integrated BCP with a well-known neural-symbolic system, C-IL2P, to perform learning from numerical vectors. C-IL2P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, handles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary results indicating that a reduction of more than 90 % of features can be achieved with a small loss of accuracy. © 2013 The Author(s).",10.1007/s10994-013-5392-1,2014.0,53.0,Artificial neural networks; Inductive logic programming; Neural-symbolic integration; Propositionalization; Relational learning,Back-ground knowledge; Machine learning applications; Neural-symbolic integration; Neural-symbolic systems; Propositional logic; Propositionalization; Relational learning; Statistical features; C (programming language); Feature extraction; Formal logic; Neural networks; Semantics; Inductive logic programming (ILP),2-s2.0-84891373440
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Komendantskaya, E., Unification neural networks: Unification by error-correction learning (2011) Log. J. IGPL, 19 (6), pp. 821-847. , Dec",Unification neural networks: Unification by error-correction learning,Komendantskaya E.,Unification neural networks: Unification by error-correction learning,"We show that the conventional first-order algorithm of unification can be simulated by finite artificial neural networks with one layer of neurons. In these unification neural networks, the unification algorithm is performed by error-correction learning. Each time-step of adaptation of the network corresponds to a single iteration of the unification algorithm. We present this result together with the library of learning functions and examples fully formalised in MATLAB Neural Network Toolbox. © The Author 2010. Published by Oxford University Press. All rights reserved.",10.1093/jigpal/jzq012,2011.0,5.0,Connectionism; Error-correction Learning; Hybrid networks; Neural network learning; Neuro-symbolic networks; Unification,,2-s2.0-80054760691
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Holldobler, S., A structured connectionist unification algorithm (1990) Proc. 8th Nat. Conf. Artif. Intell, 2, pp. 587-593. , Boston, MA, USA",A structured connectionist unification algorithm,Hölldobler S.,A structured connectionist unification algorithm,[No abstract available],,1990.0,19.0,,,2-s2.0-4243893462
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Sourek, G., Aschenbrenner, V., Zelezny, F., Kuzelka, O., Lifted relational neural networks (2015) Proc. Int. Conf. Cogn. Comput., Integrating Neural Symbolic Approaches, , Montreal, QC, Canada",Lifted relational neural networks,"Sourek G., Aschenbrenner V., Zelezny F., Kuzelka O.",Lifted relational neural networks,[No abstract available],,2015.0,28.0,,,2-s2.0-84977515818
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Cohen, W.W., (2016) Tensorlog: A Differentiable Deductive Database, , https://arxiv.org/abs/1605.06523, arXiv: 1605.06523, [Online]",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Serafini, L., D'Avila Garcez, A.S., Logic tensor networks: Deep learning and logical reasoning from data and knowledge (2016) Proc. 11th Int. Workshop Neural-Symbolic Learn. Reasoning NeSy, pp. 1-12. , New York, NY, USA",Logic tensor networks: Deep learning and logical reasoning from data and knowledge,"Serafini L., D'Avila Garcez A.S.",Logic tensor networks: Deep learning and logical reasoning from data and knowledge,[No abstract available],,2016.0,1.0,,,2-s2.0-85078348967
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Lai, T., Bui, T., Li, S., Lipka, N., A simple end-to-end question answering model for product information (2018) Proc. 1st Workshop Econ. Natural Lang. Process, pp. 38-43. , Jul",A simple end-to-end question answering model for product information,"Lai T., Bui T., Li S., Lipka N.",A simple end-to-end question answering model for product information,[No abstract available],,2018.0,2.0,,,2-s2.0-85062221445
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Tay, Y., Tuan, L.A., Hui, S.C., Hyperbolic representation learning for fast and efficient neural question answering (2018) Proc. 11th ACM Int. Conf. Web Search Data Mining, pp. 583-591. , Los Angeles, CA, USA, Feb",Hyperbolic representation learning for fast and efficient neural question answering,"Tay Y., Tuan L.A., Hui S.C.",Hyperbolic representation learning for fast and efficient neural question answering,"The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, HyperQA, is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple QA benchmarks. The novelty behind HyperQA is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space. This empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks. © 2018 Association for Computing Machinery.",10.1145/3159652.3159664,2018.0,50.0,Deep learning; Learning to rank; Question answering,Complex networks; Data mining; Information retrieval; Network architecture; Neural networks; Websites; Architectural innovation; Attention mechanisms; Competitive performance; Convolutional encoders; Feature engineerings; Learning architectures; Learning to rank; Question Answering; Deep learning,2-s2.0-85046889344
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Peng, B., Lu, Z., Li, H., Wong, K., (2015) Towards Neural Network-based Reasoning, , https://arxiv.org/abs/1508.05508, arXiv: 1508.05508, [Online]",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Weissenborn, D., (2016) Separating Answers from Queries for Neural Reading Comprehension, , https://arxiv.org/abs/1607.03316, arXiv: 1607.03316, [Online]",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Shen, Y., Huang, P., Gao, J., Chen, W., Reasonet: Learning to stop reading in machine comprehension (2017) Proc. 23rd ACM SIGKDD Int. Conf. Knowl. Discovery Data Mining, pp. 1047-1055. , Barcelona, Spain, Aug",Reasonet: Learning to stop reading in machine comprehension,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Bratko, I., (1990) Prolog Programming for Artificial Intelligence, p. 597. , 2nd ed. Reading, MA, USA, Addison-Wesley",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Sutskever, I., Vinyals, O., Le, Q.V., Sequence to sequence learning with neural networks (2014) Proc. NIPS, pp. 3104-3112. , Montreal, QC, Canada",Sequence to sequence learning with neural networks,"Sutskever I., Vinyals O., Le Q.V.",Sequence to sequence learning with neural networks,"Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",,2014.0,8371.0,,Optimization; Statistical tests; Deep neural networks; Learning tasks; Long short term memory; Optimization problems; Out of vocabulary words; Sequence learning; Sequence structure; Target sequences; Information science,2-s2.0-84928547704
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Bahdanau, D., Cho, K., Bengio, Y., Neural machine translation by jointly learning to align and translate (2015) Proc. ICLR, pp. 1-15. , San Diego, CA, USA",Neural machine translation by jointly learning to align and translate,"Bahdanau D., Cho K., Bengio Y.",Neural machine translation by jointly learning to align and translate,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. © 2015 International Conference on Learning Representations, ICLR. All rights reserved.",,2015.0,4477.0,,Computational linguistics; Decoding; Signal encoding; Decoder architecture; Hard segments; Machine translations; New approaches; Qualitative analysis; State of the art; Statistical machine translation; Target words; Computer aided language translation,2-s2.0-85083953689
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., Polosukhi, I., Attention Is All You Need (2017) Proc. 31st Conf. Neural Inf. Process. Syst, pp. 5998-6008. , Long Beach, CA, USA",Attention Is All You Need,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Mikolov, T., Chen, K., Corrado, G., Dean, J., Efficient estimation of word representations in vector space (2013) Proc. ICLR, pp. 1-12. , Scottsdale, AZ, USA",Efficient estimation of word representations in vector space,"Mikolov T., Chen K., Corrado G., Dean J.",Efficient estimation of word representations in vector space,"We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. © 2013 International Conference on Learning Representations, ICLR. All rights reserved.",,2013.0,12416.0,,Semantics; Vectors; Computational costs; Efficient estimation; Model architecture; State-of-the-art performance; Vector representations; Very large datum; Word representations; Word similarity; Vector spaces,2-s2.0-85083951332
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Mikolov, T., Chen, K., Corrado, G., Dean, J., Efficient estimation of word representations in vector space (2013) Proc. ICLR, pp. 1-12. , Scottsdale, AZ, USA",Efficient estimation of word representations in vector space,"Mikolov T., Chen K., Corrado G., Dean J.",Efficient estimation of word representations in vector space,[No abstract available],,2013.0,790.0,,,2-s2.0-84903761492
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J., Distributed representations of words and phrases and their compositionality (2013) Proc. NIPS, pp. 3111-3119. , Lake Tahoe, NV, USA",Distributed representations of words and phrases and their compositionality,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Mikolov, T., Yih, W., Zweig, G., Linguistic regularities in continuous space word representations (2013) Proc. NAACL HLT, pp. 746-751. , Atlanta, GA, USA",Linguistic regularities in continuous space word representations,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Gray, F., (1953) Pulse Code Communication, , U.S, Patent 2632 058 A, Mar. 17",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Kanayama, H., Miyao, Y., Prager, J., Answering Yes/no questions via question inversion Proc. 24th Int. Conf. Comput. Linguistics, 201, pp. 1377-1392. , Mumbai, India, Dec",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Ravichandran, D., Hovy, E., Learning surface text patterns for a question answering system (2002) Proc. 40th Annu. Meeting Assoc. Comput. Linguistics, pp. 41-47. , Jul",Learning surface text patterns for a question answering system,"Ravichandran D., Hovy E.",Learning surface text patterns for a question answering system,[No abstract available],,2002.0,491.0,,,2-s2.0-1642397455
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Higashinaka, R., Isozaki, H., Corpus-based question answering for why-questions (2008) Proc. IJCNLP, pp. 418-425. , Hyderabad, India",Corpus-based question answering for why-questions,"Higashinaka R., Isozaki H.",Corpus-based question answering for why-questions,[No abstract available],,2008.0,49.0,,,2-s2.0-44649163645
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Comput, 9 (8), pp. 1735-1780",Long short-term memory,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Elman, J.L., Finding structure in time (1990) Cognit. Sci, 14 (2), pp. 179-211. , Mar",Finding structure in time,Elman J.L.,Finding structure in time,"Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction. © 1990.",10.1016/0364-0213(90)90002-E,1990.0,5662.0,,,2-s2.0-26444565569
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Goodfellow, I., Warde-Farley, D., Mirza, M., Courville, A., Bengio, Y., Maxout networks (2013) Proc. 30th Int. Conf. Mach. Learn, pp. 1-9. , Atlanta, GA, USA",Maxout networks,"Goodfellow I.J., Warde-Farley D., Mirza M., Courville A., Bengio Y.",Maxout networks,"We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. Copyright 2013 by the author(s).",,2013.0,548.0,,Benchmarking; Classification (of information); Approximate model; Benchmark datasets; Classification performance; State of the art; Learning systems,2-s2.0-84897543523
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Kingma, D., Ba, J., (2014) Adam: A Method for Stochastic Optimization, , https://arxiv.org/abs/1412.6980, arXiv: 1412.6980, [Online]",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Maas, A., Hannu, A., Ng, A., Rectifier nonlinearities improve neural network acoustic models (2013) Proc. 30th Int. Conf. Mach. Learn, p. 3. , Atlanta, GA, USA",Rectifier nonlinearities improve neural network acoustic models,"Maas A.L., Hannun A.Y., Ng A.Y.",Rectifier nonlinearities improve neural network acoustic models,[No abstract available],,2013.0,2965.0,,,2-s2.0-84893676344
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," (2018), https://www.kinsources.net, Kinsources: A Collaborative Web Platform for Kinship Data Sharing. Accessed, May 19, [Online]",,,,,,,,,,
2-s2.0-85078352116,Question Answering Systems with Deep Learning-Based Symbolic Processing," Tang, L.R., Mooney, R.J., Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing (2000) Proc. SIGDAT Conf. Empirical Methods Natural Lang. Process. Very Large Corpora (EMNLP/VLC), Hong Kong, pp. 133-141. , Oct",Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing,"Tang L.R., Mooney R.J.",Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing,[No abstract available],,2000.0,35.0,,,2-s2.0-22344445654
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning,"Ahn, S., Choi, H., Pärnamaa, T., Bengio, Y., (2016), A neural knowledge language model. arXiv preprint. arXiv:1608.00318",,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Bahdanau, D., Cho, K., Bengio, Y., Neural machine translation by jointly learning to align and translate (2015) ICLR",Neural machine translation by jointly learning to align and translate,"Bahdanau D., Cho K., Bengio Y.",Neural machine translation by jointly learning to align and translate,"Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. © 2015 International Conference on Learning Representations, ICLR. All rights reserved.",,2015.0,4477.0,,Computational linguistics; Decoding; Signal encoding; Decoder architecture; Hard segments; Machine translations; New approaches; Qualitative analysis; State of the art; Statistical machine translation; Target words; Computer aided language translation,2-s2.0-85083953689
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Dhingra, B., Li, L., Li, X., Gao, J., Chen, Y.-N., Ahmed, F., End-to-end reinforcement learning of dialogue agents for information access (2017) ACL",End-to-end reinforcement learning of dialogue agents for information access,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Fatemi, M., El Asri, L., Schulz, H., Suleman, H.J., K. (2016). Policy networks with two-stage training for dialogue systems. In 17th annual meeting of the special interest group on discourse and dialogue (p. 101)",K.,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Graves, A., Wayne, G., Danihelka, I., (2014), Neural turing machines. arXiv preprint. arXiv:1410.5401",,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Graves, A., Wayne, G., Reynolds, M., Harley, T., Danihelka, I., Grabska-Barwińska, A., Hybrid computing using a neural network with dynamic external memory (2016) Nature, 538 (7626), pp. 471-476",Hybrid computing using a neural network with dynamic external memory,"Graves A., Wayne G., Reynolds M., Harley T., Danihelka I., Grabska-Barwińska A., Colmenarejo S.G., Grefenstette E., Ramalho T., Agapiou J., Badia A.P., Hermann K.M., Zwols Y., Ostrovski G., Cain A., King H., Summerfield C., Blunsom P., Kavukcuoglu K., Hassabis D.",Hybrid computing using a neural network with dynamic external memory,"Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read-write memory. © 2016 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",10.1038/nature20101,2016.0,635.0,,artificial neural network; computer; computer simulation; data acquisition; machine learning; timescale; Article; artificial neural network; computer memory; data processing; experiment; family; hybrid computer; machine learning; model; priority journal,2-s2.0-84993949467
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Gulcehre, C., Chandar, S., Cho, K., Bengio, Y., (2016), Dynamic neural turing machine with soft and hard addressing schemes. arXiv preprint. arXiv:1607.00036",,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," He, J., Chen, J., He, X., Gao, J., Li, L., Deng, L., (2015), Deep reinforcement learning with a natural language action space. arXiv preprint. arXiv:1511.04636",,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Henaff, M., Weston, J., Szlam, A., Bordes, A., LeCun, Y., Tracking the world state with recurrent entity networks (2017) ICLR",Tracking the world state with recurrent entity networks,"Henaff M., Weston J., Szlam A., Bordes A., LeCun Y.",Tracking the world state with recurrent entity networks,"We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass. © ICLR 2019 - Conference Track Proceedings. All rights reserved.",,2017.0,47.0,,Parallel architectures; Turing machines; Competitive performance; Language understanding; Large-scale datasets; Long term memory; Neural computers; State of the art; Training example; Write operations; Large dataset,2-s2.0-85088230593
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Hochreiter, S., Schmidhuber, J., Long short-term memory (1997) Neural Computation, 9 (8), pp. 1735-1780",Long short-term memory,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Hu, Z., Ma, X., Liu, Z., Hovy, E., Xing, E., Harnessing deep neural networks with logic rules (2016) ACL",Harnessing deep neural networks with logic rules,"Hu Z., Ma X., Liu Z., Hovy E., Xing E.P.",Harnessing deep neural networks with logic rules,"Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems. © 2016 Association for Computational Linguistics.",10.18653/v1/p16-1228,2016.0,170.0,,Computational linguistics; Computer circuits; Distillation; Formal logic; Iterative methods; Sentiment analysis; Distillation method; First order logic; Logic rules; Named entity recognition; Neural models; State of the art; Structured information; Deep neural networks,2-s2.0-85012027098
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Johnson, M., The body in the mind: The bodily basis of meaning, imagination, and reason (2013), University of Chicago Press",and reason,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Johnson, D.D., Learning graphical state transitions (2017) ICLR",Learning graphical state transitions,Johnson D.D.,Learning graphical state transitions,"Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that produces sequences from graph-structured input. In this work I introduce the Gated Graph Transformer Neural Network (GGT-NN), an extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and modify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a family of Turing machines. © ICLR 2019 - Conference Track Proceedings. All rights reserved.",,2017.0,34.0,,Graphic methods; Graph sequences; Graph structured data; Intermediate representations; Model relationships; State transitions; Turing machines,2-s2.0-85084086160
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Kansky, K., Silver, T., Mély, D.A., Eldawy, M., Lázaro-Gredilla, M., Lou, X., (2017), Schema networks: Zero-shot transfer with a generative causal model of intuitive physics. arXiv preprint. arXiv:1706.04317",,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Kumar, A., Irsoy, O., Su, J., Bradbury, J., English, R., Pierce, B., Ask me anything: Dynamic memory networks for natural language processing (2016) ICLR",Ask me anything: Dynamic memory networks for natural language processing,"Kumar A., Irsoy O., Su J., Bradbury J., English R., Pierce B., Ondruska P., Gulrajani I., Socher R.",Ask me anything: Dynamic memory networks for natural language processing,[No abstract available],,2015.0,382.0,,,2-s2.0-84978835300
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Kurach, K., Andrychowicz, M., Sutskever, I., Neural random-access machines (2016) ICLR",Neural random-access machines,"Kurach K., Andrychowicz M., Sutskever I.",Neural random-access machines,"In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions. © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.",,2016.0,23.0,,Backpropagation algorithms; Binary trees; Memory architecture; Network architecture; Trees (mathematics); Constant time; Input-output; Memory access; Random access machines; Random access memory; Variable sizes; Random access storage,2-s2.0-85083953578
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Lakoff, G., Women, fire, and dangerous things: What categories reveal about the mind (1989), University of Chicago Press",and dangerous things: What categories reveal about the mind,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Langacker, R.W., Grammar and conceptualization, Vol. 14 (1999), Walter de Gruyter",Vol. 14,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Lee, M., Yih, H.X., W.-t., Gao, J., Deng, L., & Smolensky, P. (2015). Reasoning in vector space: An exploratory study of question answering. arXiv preprint. arXiv:1511.06426",P.,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Marblestone, A.H., Wayne, G., Kording, K.P., Toward an integration of deep learning and neuroscience (2016) Frontiers in Computational Neuroscience, 10",Toward an integration of deep learning and neuroscience,"Marblestone A.H., Wayne G., Kording K.P.",Toward an integration of deep learning and neuroscience,"Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses. © 2016 Marblestone, Wayne and Kording.",10.3389/fncom.2016.00094,2016.0,241.0,Cognitive architecture; Cost functions; Neural networks; Neuroscience,Costs; Deep learning; Digital storage; Electrophysiology; Network architecture; Neural networks; Neurology; Cognitive architectures; Computational problem; Credit assignment; Dedicated systems; Neuroscience; Specialized systems; Specific problems; Training procedures; Cost functions; attention; behavior; brain; learning; long term memory; nerve cell; neuroscience; storage,2-s2.0-84989345063
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Santoro, A., Raposo, D., Barrett, D.G., Malinowski, M., Pascanu, R., Battaglia, P., A simple neural network module for relational reasoning (2017) Advances in neural information processing systems, pp. 4974-4983",A simple neural network module for relational reasoning,"Santoro A., Raposo D., Barrett D.G.T., Malinowski M., Pascanu R., Battaglia P., Lillicrap T.",A simple neural network module for relational reasoning,"Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties. © 2017 Neural information processing systems foundation. All rights reserved.",,2017.0,503.0,,Convolution; Central component; Computational burden; Convolutional networks; Human performance; Intelligent behavior; Physical systems; Question Answering; Relational reasoning; Complex networks,2-s2.0-85047012589
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Sukhbaatar, S., Weston, J., Fergus, R., End-to-end memory networks (2015) NIPS",End-to-end memory networks,"Sukhbaatar S., Szlam A., Weston J., Fergus R.",End-to-end memory networks,"We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.",,2015.0,1139.0,,Information science; Natural language processing systems; Attention model; End to end; External memory; Language model; Memory network; Question Answering; Treebanks; Modeling languages,2-s2.0-84965143740
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Tieleman, T., Hinton, G., Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude (2012) COURSERA: Neural Networks for Machine Learning, 4 (2)",Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude,"Tieleman T., Hinton G.",Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude,[No abstract available],,2012.0,2319.0,,,2-s2.0-84893343292
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Weston, J., Bordes, A., Chopra, S., Rush, A.M., van Merriënboer, B., Joulin, A., (2015), Towards ai-complete question answering: A set of prerequisite toy tasks. arXiv preprint. arxiv:1502.05698",,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Weston, J., Chopra, S., Bordes, A., Memory networks (2015) ICLR",Memory networks,"Weston J., Chopra S., Bordes A.",Memory networks,"We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs. © 2015 International Conference on Learning Representations, ICLR. All rights reserved.",,2015.0,206.0,,Knowledge base; Learning models; Long term memory; Memory network; Question Answering; Knowledge based systems,2-s2.0-85083951616
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Williams, R.J., Simple statistical gradient-following algorithms for connectionist reinforcement learning (1992) Machine Learning, 8 (3-4), pp. 229-256",Simple statistical gradient-following algorithms for connectionist reinforcement learning,,,,,,,,,
2-s2.0-85049856582,Learning to activate logic rules for textual reasoning," Xiong, C., Merity, S., Socher, R., Dynamic memory networks for visual and textual question answering (2016) ICLR",Dynamic memory networks for visual and textual question answering,"Xiong C., Merity S., Socher R.",Dynamic memory networks for visual and textual question answering,[No abstract available],,2016.0,229.0,,,2-s2.0-84980035974
2-s2.0-85032371736,Semi-supervised learning for big social data analysis,"Poria, S., Cambria, E., Bajpai, R., Hussain, A., A review of affective computing: from unimodal analysis to multimodal fusion (2017) Inf. Fus., 37, pp. 98-125",A review of affective computing: from unimodal analysis to multimodal fusion,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Cambria, E., Hussain, A., Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis (2015), Springer Cham, Switzerland",Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Cambria, E., Olsher, D., Kwok, K., Sentic activation: a two-level affective common sense reasoning framework (2012) Proceedings of the AAAI, pp. 186-192. , Toronto",Sentic activation: a two-level affective common sense reasoning framework,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Speer, R., Havasi, C., ConceptNet 5: A large semantic network for relational knowledge (2012) Theory and Applications of Natural Language Processing, , E. Hovy M. Johnson G. Hirst Springer",ConceptNet 5: A large semantic network for relational knowledge,"Speer R., Havasi C.",ConceptNet 5: A large semantic network for relational knowledge,[No abstract available],,2012.0,15.0,,,2-s2.0-84876932893
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Strapparava, C., Valitutti, A., WordNet-Affect: an affective extension of WordNet (2004) Proceedings of the International Conference on Language Resources and Evaluation, pp. 1083-1086. , Lisbon",WordNet-Affect: an affective extension of WordNet,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Cambria, E., Das, D., Bandyopadhyay, S., Feraco, A., A Practical Guide to Sentiment Analysis (2017), Springer Cham, Switzerland",A Practical Guide to Sentiment Analysis,"Cambria E., Das D., Bandyopadhyay S., Feraco A.",A Practical Guide to Sentiment Analysis,[No abstract available],,2017.0,153.0,,,2-s2.0-85037568228
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Bisio, F., Gastaldo, P., Zunino, R., Decherchi, S., Semi-supervised machine learning approach for unknown malicious software detection (2014) Proceedings of the IEEE International Symposium on Innovations in Intelligent Systems and Applications (INISTA), pp. 52-59. , IEEE",Semi-supervised machine learning approach for unknown malicious software detection,"Bisio F., Gastaldo P., Zunino R., Decherchi S.",Semi-supervised machine learning approach for unknown malicious software detection,"Inductive bias represents an important factor in learning theory, as it can shape the generalization properties of a learning machine. This paper shows that biased regularization can be used as inductive bias to effectively tackle the semi-supervised classification problem. Thus, semi-supervised learning is formalized as a supervised learning problem biased by an unsupervised reference solution. The proposed framework has been tested on a malware-detection problem. Experimental results confirmed the effectiveness of the semi-supervised methodology presented in this paper. © 2014 IEEE.",10.1109/INISTA.2014.6873597,2014.0,6.0,biased regularization; malware detection; semi-supervised; SVM,Computer crime; Intelligent systems; Malware; biased regularization; Machine learning approaches; Malicious software detections; Malware detection; Semi-supervised; Semi-supervised classification; Supervised learning problems; SVM; Supervised learning,2-s2.0-84906696338
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Poria, S., Cambria, E., Hazarika, D., Mazumder, N., Zadeh, A., Morency, L.-P., Context-dependent sentiment analysis in user-generated videos (2017) Proceedings of the ACL, pp. 873-883",Context-dependent sentiment analysis in user-generated videos,"Poria S., Mazumder N., Cambria E., Hazarika D., Morency L.-P., Zadeh A.",Context-dependent sentiment analysis in user-generated videos,"Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the inter-dependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10% performance improvement over the state of the art and high robustness to generalizability. © 2017 Association for Computational Linguistics.",10.18653/v1/P17-1081,2017.0,209.0,,Computational linguistics; Data mining; Linguistics; Long short-term memory; Classification process; Context dependent; Contextual information; High robustness; Inter-dependencies; Sentiment analysis; State of the art; User-generated video; Classification (of information),2-s2.0-85039788178
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Chaturvedi, I., Ragusa, E., Gastaldo, P., Zunino, R., Cambria, E., Bayesian network based extreme learning machine for subjectivity detection (2017) J. Frankl. Inst.",Bayesian network based extreme learning machine for subjectivity detection,"Chaturvedi I., Ragusa E., Gastaldo P., Zunino R., Cambria E.",Bayesian network based extreme learning machine for subjectivity detection,"Subjectivity detection is a task of natural language processing that aims to remove ‘factual’ or ‘neutral’ content, i.e., objective text that does not contain any opinion, from online product reviews. Such a pre-processing step is crucial to increase the accuracy of sentiment analysis systems, as these are usually optimized for the binary classification task of distinguishing between positive and negative content. In this paper, we extend the extreme learning machine (ELM) paradigm to a novel framework that exploits the features of both Bayesian networks and fuzzy recurrent neural networks to perform subjectivity detection. In particular, Bayesian networks are used to build a network of connections among the hidden neurons of the conventional ELM configuration in order to capture dependencies in high-dimensional data. Next, a fuzzy recurrent neural network inherits the overall structure generated by the Bayesian networks to model temporal features in the predictor. Experimental results confirmed the ability of the proposed framework to deal with standard subjectivity detection problems and also proved its capacity to address portability across languages in translation tasks. © 2017 The Franklin Institute",10.1016/j.jfranklin.2017.06.007,2018.0,87.0,,Education; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Knowledge acquisition; Learning systems; Natural language processing systems; Recurrent neural networks; Binary classification; Detection problems; Extreme learning machine; Fuzzy recurrent neural networks; High dimensional data; Online product reviews; Pre-processing step; Sentiment analysis; Bayesian networks,2-s2.0-85024868159
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Chaturvedi, I., Ragusa, E., Gastaldo, P., Zunino, R., Cambria, E., Bayesian network based extreme learning machine for subjectivity detection (2017) J. Frankl. Inst.",Bayesian network based extreme learning machine for subjectivity detection,"Chaturvedi I., Ragusa E., Gastaldo P., Zunino R., Cambria E.",Bayesian network based extreme learning machine for subjectivity detection,[No abstract available],,2017.0,9.0,,,2-s2.0-85042347247
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Xing, F., Cambria, E., Welsch, R., Natural language based financial forecasting: a survey (2018) Artif. Intell. Rev.",Natural language based financial forecasting: a survey,"Xing F.Z., Cambria E., Welsch R.E.",Natural language based financial forecasting: a survey,"Natural language processing (NLP), or the pragmatic research perspective of computational linguistics, has become increasingly powerful due to data availability and various techniques developed in the past decade. This increasing capability makes it possible to capture sentiments more accurately and semantics in a more nuanced way. Naturally, many applications are starting to seek improvements by adopting cutting-edge NLP techniques. Financial forecasting is no exception. As a result, articles that leverage NLP techniques to predict financial markets are fast accumulating, gradually establishing the research field of natural language based financial forecasting (NLFF), or from the application perspective, stock market prediction. This review article clarifies the scope of NLFF research by ordering and structuring techniques and applications from related work. The survey also aims to increase the understanding of progress and hotspots in NLFF, and bring about discussions across many different disciplines. © 2017, Springer Science+Business Media B.V.",10.1007/s10462-017-9588-9,2018.0,117.0,Computational finance; Financial forecasting; Knowledge engineering; Natural language processing; Predictive analytics; Text mining,Commerce; Data mining; Engineering research; Finance; Financial markets; Forecasting; Knowledge engineering; Predictive analytics; Semantics; Surveys; Computational finance; Data availability; Financial forecasting; Natural languages; Nlp techniques; Research fields; Stock market prediction; Text mining; Natural language processing systems,2-s2.0-85032466713
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Ebrahimi, M., Hossein, A., Sheth, A., Challenges of sentiment analysis for dynamicevents (2017) IEEE Intell. Syst., 32 (5)",Challenges of sentiment analysis for dynamicevents,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Cambria, E., Hussain, A., Durrani, T., Havasi, C., Eckl, C., Munro, J., Sentic computing for patient centered application (2010) Proceedings of the IEEE International Conference on Signal Processing, pp. 1279-1282. , Beijing",Sentic computing for patient centered application,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Valdivia, A., Luzon, V., Herrera, F., Sentiment analysis in tripadvisor (2017) IEEE Intell. Syst., 32 (4), pp. 2-7",Sentiment analysis in tripadvisor,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Cavallari, S., Zheng, V., Cai, H., Chang, K., Cambria, E., Joint node and community embedding on graphs (2017) Proceedings of the International Conference on Information and Knowledge Management",Joint node and community embedding on graphs,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Mihalcea, R., Garimella, A., What men say, what women hear: finding gender-specific meaning shades (2016) IEEE Intell. Syst., 31 (4), pp. 62-67",what women hear: finding gender-specific meaning shades,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Cambria, E., Poria, S., Gelbukh, A., Thelwall, M., Sentiment analysis is a big suitcase (2017) IEEE Intell. Syst., 32 (6)",Sentiment analysis is a big suitcase,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Poria, S., Chaturvedi, I., Cambria, E., Bisio, F., Sentic LDA: improving on LDA with semantic similarity for aspect-based sentiment analysis (2016) Proceedings of the International Joint Conference on Neural Networks, pp. 4465-4473",Sentic LDA: improving on LDA with semantic similarity for aspect-based sentiment analysis,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Ma, Y., Cambria, E., Gao, S., Label embedding for zero-shot fine-grained named entity typing (2016) Proceedings of the International Conference on Computational Linguistics, pp. 171-180. , Osaka",Label embedding for zero-shot fine-grained named entity typing,"Ma Y., Cambria E., Gao S.",Label embedding for zero-shot fine-grained named entity typing,"Named entity typing is the task of detecting the types of a named entity in context. For instance, given ""Eric is giving a presentation"", our goal is to infer that 'Eric' is a speaker or a presenter and a person. Existing approaches to named entity typing cannot work with a growing type set and fails to recognize entity mentions of unseen types. In this paper, we present a label embedding method that incorporates prototypical and hierarchical information to learn pre-trained label embeddings. In addition, we adapt a zero-shot framework that can predict both seen and previously unseen entity types. We perform evaluation on three benchmark datasets with two settings: 1) few-shots recognition where all types are covered by the training set; and 2) zero-shot recognition where fine-grained types are assumed absent from training set. Results show that prior knowledge encoded using our label embedding methods can significantly boost the performance of classification for both cases. © 1963-2018 ACL.",,2016.0,65.0,,Computational linguistics; Benchmark datasets; Embedding method; Entity-types; Fine grained; Hierarchical information; Named entities; Prior knowledge; Training sets; Classification (of information),2-s2.0-85054979013
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Xia, Y., Cambria, E., Hussain, A., Zhao, H., Word polarity disambiguation using Bayesian model and opinion-level features (2015) Cognit. Comput., 7 (3), pp. 369-380",Word polarity disambiguation using Bayesian model and opinion-level features,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Zhong, X., Sun, A., Cambria, E., Time expression analysis and recognition using syntactic token types and general heuristic rules (2017) Proceedings of the ACL, pp. 420-429",Time expression analysis and recognition using syntactic token types and general heuristic rules,"Zhong X., Sun A., Cambria E.",Time expression analysis and recognition using syntactic token types and general heuristic rules,"Extracting time expressions from free text is a fundamental task for many applications. We analyze time expressions from four different datasets and find that only a small group of words are used to express time information and that the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach named SynTime1 for time expression recognition. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related token regular expressions. On the types we design general heuristic rules to recognize time expressions. In recognition, SynTime first identifies time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a lightweight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text from different domains and different text types. Experiments on benchmark datasets and tweets data show that SynTime outperforms state-of-the-art methods. © 2017 Association for Computational Linguistics.",10.18653/v1/P17-1039,2017.0,35.0,,Computational linguistics; Linguistics; Syntactics; Benchmark datasets; Different domains; Expression analysis; Expression recognition; Heuristic rules; Regular expressions; State-of-the-art methods; Time information; Character recognition,2-s2.0-85038556598
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Majumder, N., Poria, S., Gelbukh, A., Cambria, E., Deep learning-based document modeling for personality detection from text (2017) IEEE Intell. Syst., 32 (2), pp. 74-79",Deep learning-based document modeling for personality detection from text,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Poria, S., Cambria, E., Hazarika, D., Vij, P., A deeper look into sarcastic tweets using deep convolutional neural networks (2016) Proceedings of the International Conference on Computational Linguistics, pp. 1601-1612",A deeper look into sarcastic tweets using deep convolutional neural networks,"Poria S., Cambria E., Hazarika D., Vij P.",A deeper look into sarcastic tweets using deep convolutional neural networks,"Sarcasm detection is a key task for many natural language processing tasks. In sentiment analysis, for example, sarcasm can flip the polarity of an ""apparently positive"" sentence and, hence, negatively affect polarity detection performance. To date, most approaches to sarcasm detection have treated the task primarily as a text categorization problem. Sarcasm, however, can be expressed in very subtle ways and requires a deeper understanding of natural language that standard text categorization techniques cannot grasp. In this work, we develop models based on a pre-trained convolutional neural network for extracting sentiment, emotion and personality features for sarcasm detection. Such features, along with the network's baseline features, allow the proposed models to outperform the state of the art on benchmark datasets. We also address the often ignored generalizability issue of classifying data that have not been seen by the models at learning phase. © 1963-2018 ACL.",,2016.0,103.0,,Computational linguistics; Convolution; Natural language processing systems; Neural networks; Sentiment analysis; Text processing; Benchmark datasets; Convolutional neural network; Deep convolutional neural networks; Detection performance; Learning phase; Natural languages; State of the art; Text categorization; Deep neural networks,2-s2.0-85046580416
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Oneto, L., Bisio, F., Cambria, E., Anguita, D., Statistical learning theory and ELM for big social data analysis (2016) IEEE Comput. Intell. Mag., 11 (3), pp. 45-55",Statistical learning theory and ELM for big social data analysis,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Elliott, C.D., (1992) The affective reasoner: a process model of emotions in a multi-agent system, , Northwestern University Evanston Ph.D. thesis",,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Ortony, A., Clore, G., Collins, A., The Cognitive Structure of Emotions (1988), Cambridge University Press Cambridge",The Cognitive Structure of Emotions,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Wiebe, J., Wilson, T., Cardie, C., Annotating expressions of opinions and emotions in language (2005) Lang. Resour. Eval., 39 (2), pp. 165-210",Annotating expressions of opinions and emotions in language,"Wiebe J., Wilson T., Cardie C.",Annotating expressions of opinions and emotions in language,"This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented. © Springer 2006.",10.1007/s10579-005-7880-9,2005.0,926.0,Affect; Attitudes; Corpus annotation; Emotion; Natural language processing; Opinions; Sentiment; Subjectivity,,2-s2.0-33644632271
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Wilson, T., Wiebe, J., Hoffmann, P., Recognizing contextual polarity in phrase-level sentiment analysis (2005) Proceedings of the Human Language Technology Conference and Empirical Methods in Natural Language Processing, pp. 347-354. , Vancouver",Recognizing contextual polarity in phrase-level sentiment analysis,"Wilson T., Wiebe J., Hoffmann P.",Recognizing contextual polarity in phrase-level sentiment analysis,"This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline. © 2005 Association for Computational Linguistics.",10.3115/1220575.1220619,2005.0,2038.0,,New approaches; Sentiment analysis,2-s2.0-80053247760
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Stevenson, R., Mikels, J., James, T., Characterization of the affective norms for english words by discrete emotional categories (2007) Behav. Res. Methods, 39, pp. 1020-1024",Characterization of the affective norms for english words by discrete emotional categories,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Somasundaran, S., Wiebe, J., Ruppenhofer, J., Discourse level opinion interpretation (2008) Proceedings of the International Conference on Computational Linguistics, pp. 801-808. , Manchester",Discourse level opinion interpretation,"Somasundaran S., Wiebe J., Ruppenhofer J.",Discourse level opinion interpretation,This work proposes opinion frames as a representation of discourse-level associations which arise from related opinion topics. We illustrate how opinion frames help gather more information and also assist disambiguation. Finally we present the results of our experiments to detect these associations. © 2008. Licensed under the Creative Commons.,10.3115/1599081.1599182,2008.0,47.0,,Computational linguistics,2-s2.0-60649117277
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Pang, B., Lee, L., Vaithyanathan, S., Thumbs up? Sentiment classification using machine learning techniques (2002) Proceedings of the Empirical Methods for Natural Language Processing, pp. 79-86. , Philadelphia",Thumbs up? Sentiment classification using machine learning techniques,"Pang B., Lee L., Vaithyanathan S.",Thumbs up? Sentiment classification using machine learning techniques,[No abstract available],,2002.0,5328.0,,,2-s2.0-2442544814
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Goertzel, B., Silverman, K., Hartley, C., Bugaj, S., Ross, M., The Baby Webmind project (2000) Proceedings of the Adaptation in Artificial and Biological Systems, , Birmingham",The Baby Webmind project,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Pang, B., Lee, L., Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales (2005) Proceedings of the ACL, pp. 115-124. , Ann Arbor",Seeing stars: exploiting class relationships for sentiment categorization with respect to rating scales,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Hu, M., Liu, B., Mining and summarizing customer reviews (2004) Proceedings of the Conference on Knowledge Discovery and Data Mining, , Seattle",Mining and summarizing customer reviews,"Hu M., Liu B.",Mining and summarizing customer reviews,"Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.",10.1145/1014052.1014073,2004.0,4580.0,Reviews; Sentiment classification; Summarization; Text mining,Algorithms; Classification (of information); Consumer products; Customer satisfaction; Electronic commerce; Image quality; Purchasing; Text processing; World Wide Web; Sentiment classification; Summarization; Text mining; Data mining,2-s2.0-12244305149
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Velikovich, L., Goldensohn, S., Hannan, K., McDonald, R., The viability of web-derived polarity lexicons (2010) Proceedings of the Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 777-785. , Los Angeles",The viability of web-derived polarity lexicons,"Velikovich L., Blair-Goldensohn S., Hannan K., McDonald R.",The viability of web-derived polarity lexicons,"We examine the viability of building large polarity lexicons semi-automatically from the web. We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009). We then apply this technique to build an English lexicon that is significantly larger than those previously studied. Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems. As a result, the lexicon is not limited to specific word classes - e.g., adjectives that occur in WordNet - and in fact contains slang, misspellings, multiword expressions, etc. We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet. © 2010 Association for Computational Linguistics.",,2010.0,147.0,,Multiword expressions; Part-of-speech tagger; Sentiment analysis; Wordnet; Computational linguistics; Ontology,2-s2.0-84858422546
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Gangemi, A., Presutti, V., Reforgiato, D., Frame-based detection of opinion holders and topics: a model and a tool (2014) IEEE Comput. Intell. Mag., 9 (1), pp. 20-30",Frame-based detection of opinion holders and topics: a model and a tool,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Cambria, E., Poria, S., Bajpai, R., Schuller, B., SenticNet 4: a semantic resource for sentiment analysis based on conceptual primitives (2016) Proceedings of the International Conference on Computational Linguistics, pp. 2666-2677",SenticNet 4: a semantic resource for sentiment analysis based on conceptual primitives,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Fellbaum, C., WordNet: An Electronic Lexical Database (Language, Speech, and Communication) (1998), The MIT Press",and Communication),,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Lenat, D., Guha, R., Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project (1989), Addison-Wesley Boston",Building Large Knowledge-Based Systems: Representation and Inference in the Cyc Project,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Mueller, E., Commonsense Reasoning (2006), Morgan Kaufmann",Commonsense Reasoning,Mueller E.T.,Commonsense Reasoning,"To endow computers with common sense is one of the major long-term goals of Artificial Intelligence research. One approach to this problem is to formalize commonsense reasoning using mathematical logic. Commonsense Reasoning is a detailed, high-level reference on logic-based commonsense reasoning. It uses the event calculus, a highly powerful and usable tool for commonsense reasoning, which Erik T. Mueller demonstrates as the most effective tool for the broadest range of applications. He provides an up-to-date work promoting the use of the event calculus for commonsense reasoning, and bringing into one place information scattered across many books and papers. Mueller shares the knowledge gained in using the event calculus and extends the literature with detailed event calculus solutions to problems that span many areas of the commonsense world. © 2006 Elsevier Inc. All rights reserved.",10.1016/B978-0-12-369388-4.X5054-1,2006.0,167.0,,,2-s2.0-85013875233
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Fauconnier, G., Turner, M., The Way We Think: Conceptual Blending and the Mind's Hidden Complexities (2003), Basic Books",The Way We Think: Conceptual Blending and the Mind's Hidden Complexities,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Jolliffe, I., Principal Component Analysis (2005), Wiley Online Library",Principal Component Analysis,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Menon, A.K., Elkan, C., Fast algorithms for approximating the singular value decomposition (2011) ACM Trans. Knowl. Discov. Data (TKDD), 5 (2), p. 13",Fast algorithms for approximating the singular value decomposition,"Menon A.K., Elkan C.",Fast algorithms for approximating the singular value decomposition,"A low-rank approximation to a matrix A is a matrix with significantly smaller rank than A, and which is close to A according to some norm. Many practical applications involving the use of large matrices focus on low-rank approximations. By reducing the rank or dimensionality of the data, we reduce the complexity of analyzing the data. The singular value decomposition is the most popular low-rank matrix approximation. However, due to its expensive computational requirements, it has often been considered intractable for practical applications involving massive data. Recent developments have tried to address this problem, with several methods proposed to approximate the decomposition with better asymptotic runtime. We present an empirical study of these techniques on a variety of dense and sparse datasets. We find that a sampling approach of Drineas, Kannan and Mahoney is often, but not always, the best performing method. This method gives solutions with high accuracy much faster than classical SVD algorithms, on large sparse datasets in particular. Other modern methods, such as a recent algorithm by Rokhlin and Tygert, also offer savings compared to classical SVD algorithms. The older sampling methods of Achlioptas and McSherry are shown to sometimes take longer than classical SVD. © 2011 ACM.",10.1145/1921632.1921639,2011.0,34.0,Experimental evaluation; Low rank approximation; Singular value decomposition,Computational requirements; Data sets; Empirical studies; Experimental evaluation; Fast algorithms; Low rank approximation; Low rank approximations; Low-rank matrices; Massive data; matrix; Runtimes; Sampling method; Singular values; Algorithms; Singular value decomposition,2-s2.0-79952555116
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Osgood, C., May, W., Miron, M., Cross-Cultural Universals of Affective Meaning (1975), University of Illinois Press",Cross-Cultural Universals of Affective Meaning,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Balduzzi, D., Randomized co-training: from cortical neurons to machine learning and back again, (2013)., arXiv:1310.6536",,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Lee, H., Grosse, R., Ranganath, R., Ng, A.Y., Unsupervised learning of hierarchical representations with convolutional deep belief networks (2011) Commun. ACM, 54 (10), pp. 95-103",Unsupervised learning of hierarchical representations with convolutional deep belief networks,"Lee H., Grosse R., Ranganath R., Ng A.Y.",Unsupervised learning of hierarchical representations with convolutional deep belief networks,"There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks (DBNs); however, scaling such models to full-sized, highdimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model that scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique that shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images. © 2011 ACM.",10.1145/2001269.2001295,2011.0,268.0,,Bottom-up and top-down; Excellent performance; Generative model; Hierarchical representation; High-dimensional images; Natural scenes; Novel techniques; Probabilistic inference; Realistic images; Translation invariants; Visual feature; Visual recognition; Convolution; Unsupervised learning; Bayesian networks,2-s2.0-80053540444
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Bingham, E., Mannila, H., Random projection in dimensionality reduction: applications to image and text data (2001) Proceedings of the ACM SIGKDD, pp. 245-250",Random projection in dimensionality reduction: applications to image and text data,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Sarlos, T., Improved approximation algorithms for large matrices via random projections (2006) Proceedings of the Foundations of Computer Science, pp. 143-152",Improved approximation algorithms for large matrices via random projections,Sarlós T.,Improved approximation algorithms for large matrices via random projections,"Recently several results appeared that show significant reduction in time for matrix multiplication, singular value decomposition as well as linear (ℓ2) regression, all based on data dependent random sampling. Our key idea is that low dimensional embeddings can be used to eliminate data dependence and provide more versatile, linear time pass efficient matrix computation. Our main contribution is summarized as follows. Independent of the recent results of Har-Peled and of Deshpande and Vempala, one of the first-and to the best of our knowledge the most efficient - relative error (1 + ε) ∥ A - A k∥F approximation algorithms for the singular value decomposition of an m × n matrix A with M non-zero entries that requires 2 passes over the data and runs in time O ((M k/∈ + k log k) + (n + m) (k/∈ + k log k)2) log i/δ). The first o(nd2) time (1 + ∈) relative error approximation algorithm for n × d linear (ℓ2) regression. A matrix multiplication and norm approximation algorithm that easily applies to implicitly given matrices and can be used as a black box probability boosting tool. © 2006 IEEE.",10.1109/FOCS.2006.37,2006.0,356.0,,Data reduction; Random processes; Regression analysis; Singular value decomposition; Low dimensional embeddings; Matrix multiplication; Non-zero entries; Approximation algorithms,2-s2.0-35348901208
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Achlioptas, D., Database-friendly random projections: Johnson–lindenstrauss with binary coins (2003) J. Comput. Syst. Sci., 66 (4), pp. 671-687",Database-friendly random projections: Johnson–lindenstrauss with binary coins,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Lu, Y., Dhillon, P., Foster, D.P., Ungar, L., Faster ridge regression via the subsampled randomized Hadamard transform (2013) Advances in Neural Information Processing Systems, pp. 369-377",Faster ridge regression via the subsampled randomized Hadamard transform,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Tropp, J.A., Improved analysis of the subsampled randomized Hadamard transform (2011) Adv. Adapt. Data Anal., 3 (01n02), pp. 115-126",Improved analysis of the subsampled randomized Hadamard transform,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Ailon, N., Chazelle, B., Faster dimension reduction (2010) Commun. ACM, 53 (2), pp. 97-104",Faster dimension reduction,"Ailon N., Chazelle B.",Faster dimension reduction,"Data represented geometrically in high-dimensional vector spaces can be found in many applications. Images and videos, are often represented by assigning a dimension for every pixel (and time). Text documents may be represented in a vector space where each word in the dictionary incurs a dimension. The need to manipulate such data in huge corpora such as the web and to support various query types gives rise to the question of how to represent the data in a lower-dimensional space to allow more space and time efficient computation. Linear mappings are an attractive approach to this problem because the mapped input can be readily fed into popular algorithms that operate on linear spaces (such as principal-component analysis, PCA) while avoiding the curse of dimensionality. The fact that such mappings even exist became known in computer science following seminal work by Johnson and Lindenstrauss in the early 1980s. The underlying technique is often called ""random projection."" The complexity of the mapping itself, essentially the product of a vector with a dense matrix, did not attract much attention until recently. In 2006, we discovered a way to ""sparsify"" the matrix via a computational version of Heisenberg's Uncertainty Principle. This led to a significant speedup, which also retained the practical simplicity of the standard Johnson-Lindenstrauss projection. We describe the improvement in this article, together with some of its applications. © 2010 ACM.",10.1145/1646353.1646379,2010.0,54.0,,Curse of dimensionality; Dense matrices; Dimension reduction; Dimensional spaces; Efficient computation; Heisenberg's Uncertainty principle; High-dimensional; Linear mapping; Linear spaces; matrix; Query types; Random projections; Space and time; Text document; Computational efficiency; Mapping; Principal component analysis; Vector spaces,2-s2.0-75749141980
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Cambria, E., Fu, J., Bisio, F., Poria, S., AffectiveSpace 2: enabling affective intuition for concept-level sentiment analysis (2015) Proceedings of the AAAI, pp. 508-514. , Austin",AffectiveSpace 2: enabling affective intuition for concept-level sentiment analysis,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Yu, T., Simoff, S., Jan, T., VQSVM: a case study for incorporating prior domain knowledge into inductive machine learning (2010) Neurocomputing, 73 (13), pp. 2614-2623",VQSVM: a case study for incorporating prior domain knowledge into inductive machine learning,,,,,,,,,
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Socher, R., Huval, B., Manning, C.D., Ng, A.Y., Semantic compositionality through recursive matrix-vector spaces (2012) Proceedings of the Empirical Methods for Natural Language Processing, pp. 1201-1211",Semantic compositionality through recursive matrix-vector spaces,"Socher R., Huval B., Manning C.D., Ng A.Y.",Semantic compositionality through recursive matrix-vector spaces,"Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. © 2012 Association for Computational Linguistics.",,2012.0,870.0,,Lexical information; Natural languages; Propositional logic; Recursive neural networks; Semantic relationships; State-of-the-art performance; Vector representations; Vector space models; Formal logic; Neural networks; Semantics; Syntactics; Vector spaces; Natural language processing systems,2-s2.0-84870715081
2-s2.0-85032371736,Semi-supervised learning for big social data analysis," Socher, R., Perelygin, A., Wu, J.Y., Chuang, J., Manning, C.D., Ng, A.Y., Potts, C., Recursive deep models for semantic compositionality over a sentiment treebank (2013) Proceedings of the Empirical Methods for Natural Language Processing, pp. 1642-1654",Recursive deep models for semantic compositionality over a sentiment treebank,"Socher R., Perelygin A., Wu J.Y., Chuang J., Manning C.D., Ng A.Y., Potts C.",Recursive deep models for semantic compositionality over a sentiment treebank,"Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. © 2013 Association for Computational Linguistics.",,2013.0,2896.0,,Natural language processing systems; Semantics; Bag of features; Compositionality; Fine grained; Parse trees; Positive/negative classifications; State of the art; Supervised trainings; Word spaces; Forestry; Classification; Languages; Word Processing,2-s2.0-84926358845
2-s2.0-84940937276,Semantic-based regularization for learning and inference,"Baader, F., The Description Logic Handbook: Theory, Implementation, and Applications (2003), Cambridge University Press",and Applications,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Bard, J.F., Practical Bilevel Optimization: Algorithms and Applications (1998) Nonconvex Optimization and Its Applications, 30. , Springer",Practical Bilevel Optimization: Algorithms and Applications,Bard J.F.,Practical Bilevel Optimization: Algorithms and Applications,[No abstract available],,1999.0,13.0,,,2-s2.0-0010822868
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Belkin, M., Niyogi, P., Sindhwani, V., Manifold regularization: a geometric framework for learning from labeled and unlabeled examples (2006) J. Mach. Learn. Res., 7, p. 2434",Manifold regularization: a geometric framework for learning from labeled and unlabeled examples,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Bengio, Y., Curriculum learning (2009) Proceedings of the 26th Annual International Conference on Machine Learning, ICML0, pp. 41-48",Curriculum learning,"Bengio Y., Louradour J., Collobert R., Weston J.",Curriculum learning,"Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ""curriculum learning"". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).",,2009.0,1075.0,,Continuation method; Local minimums; Machine-learning; Nonconvex functions; Set-ups; Speed of convergence; Stochastic neural network; Training process; Training strategy; Animals; Global optimization; Neural networks; Robot learning; Curricula,2-s2.0-71149116544
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Broecheler, M., Mihalkova, L., Getoor, L., Probabilistic similarity logic (2010) Proceedings of the Twenty-Sixth Conference on Uncertainty in Artificial Intelligence, UAI, pp. 73-82",Probabilistic similarity logic,"Bröcheler M., Mihalkova L., Getoor L.",Probabilistic similarity logic,"Many machine learning applications require the ability to learn from and reason about noisy multi-relational data. To address this, several effective representations have been developed that provide both a language for expressing the structural regularities of a domain, and principled support for probabilistic inference. In addition to these two aspects, however, many applications also involve a third aspect-the need to reason about similarities-which has not been directly supported in existing frameworks. This paper introduces probabilistic similarity logic (PSL), a general-purpose framework for joint reasoning about similarity in relational domains that incorporates probabilistic reasoning about similarities and relational structure in a principled way. PSL can integrate any existing domain-specific similarity measures and also supports reasoning about similarities between sets of entities. We provide efficient inference and learning techniques for PSL and demonstrate its effectiveness both in common relational tasks and in settings that require reasoning about similarity.",,2010.0,84.0,,Artificial intelligence; Computer circuits; Learning systems; General purpose framework; Learning techniques; Machine learning applications; Probabilistic inference; Probabilistic reasoning; Relational structures; Similarity measure; Structural regularity; Probabilistic logics,2-s2.0-79952406643
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Caponnetto, A., Micchelli, C.A., Pontil, M., Ying, Y., Universal multi-task kernels (2008) J. Mach. Learn. Res., 9, pp. 1615-1646",Universal multi-task kernels,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Craven, M., Slattery, S., Relational learning with statistical predicate invention: better models for hypertext (2001) Mach. Learn., pp. 97-119",Relational learning with statistical predicate invention: better models for hypertext,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Cumby, C., Roth, D., Learning with feature description logics (2003) Proceedings of the 12th International Conference on Inductive Logic Programming, pp. 32-47. , Springer",Learning with feature description logics,"Cumby C.M., Roth D.",Learning with feature description logics,"We present a paradigm for efficient learning and inference with relational data using propositional means. The paradigm utilizes description logics and concepts graphs in the service of learning relational models using efficient propositional learning algorithms.We introduce a Feature Description Logic (FDL) - a relational (frame based) language that supports efficient inference, along with a generation function that uses inference with descriptions in the FDL to produce features suitable for use by learning algorithms. These are used within a learning framework that is shown to learn efficiently and accurately relational representations in terms of the FDL descriptions. The paradigm was designed to support learning in domains that are relational but where the amount of data and size of representation learned are very large; we exemplify it here, for clarity, on the classical ILP tasks of learning family relations and mutagenesis. This paradigm provides a natural solution to the problem of learning and representing relational data; it extends and unifies several lines of works in KRR and Machine Learning in ways that provide hope for a coherent usage of learning and reasoning methods in large scale intelligent inference. © Springer-Verlag Berlin Heidelberg 2003.",10.1007/3-540-36468-4_3,2003.0,9.0,,Artificial intelligence; Data description; Formal languages; Inference engines; Learning algorithms; Learning systems; Inductive logic programming (ILP); Description logic; Efficient learning; Feature description; Learning frameworks; Reasoning methods; Relational Model; Relational representations; Support learning; Inductive logic programming (ILP); Learning algorithms,2-s2.0-7044224298
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Cumby, C., Roth, D., On kernel methods for relational learning (2003) Proceedings of the Twentieth International Conference on Machine Learning, ICML, pp. 107-114",On kernel methods for relational learning,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Diligenti, M., Gori, M., Maggini, M., Rigutini, L., Bridging logic and kernel machines (2012) Mach. Learn., 86, pp. 57-88",Bridging logic and kernel machines,"Diligenti M., Gori M., Maggini M., Rigutini L.",Bridging logic and kernel machines,"We propose a general framework to incorporate first-order logic (FOL) clauses, that are thought of as an abstract and partial representation of the environment, into kernel machines that learn within a semi-supervised scheme. We rely on a multi-task learning scheme where each task is associated with a unary predicate defined on the feature space, while higher level abstract representations consist of FOL clauses made of those predicates. We re-use the kernel machine mathematical apparatus to solve the problem as primal optimization of a function composed of the loss on the supervised examples, the regularization term, and a penalty term deriving from forcing real-valued constraints deriving from the predicates. Unlike for classic kernel machines, however, depending on the logic clauses, the overall function to be optimized is not convex anymore. An important contribution is to show that while tackling the optimization by classic numerical schemes is likely to be hopeless, a stage-based learning scheme, in which we start learning the supervised examples until convergence is reached, and then continue by forcing the logic clauses is a viable direction to attack the problem. Some promising experimental results are given on artificial learning tasks and on the automatic tagging of bibtex entries to emphasize the comparison with plain kernel machines. © 2011 The Author(s).",10.1007/s10994-011-5243-x,2012.0,37.0,First-order logic; Kernel machines; Learning from constraints; Learning with prior knowledge; Multi-task learning; Semantic-based regularization,First order logic; Kernel machine; Learning from constraints; Multitask learning; Prior knowledge; Semantic-based regularization; Abstracting; Convergence of numerical methods; Formal logic; Functions; Optimization; Semantics; Inductive logic programming (ILP),2-s2.0-84855713701
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Domingos, P., Richardson, M., Markov logic: a unifying framework for statistical relational learning (2004) ICML-2004 Workshop on Statistical Relational Learning, pp. 49-54",Markov logic: a unifying framework for statistical relational learning,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Domingos, P., Sumner, M., The alchemy tutorial (2010), http://alchemy.cs.washington.edu/tutorial/tutorial.pdf",The alchemy tutorial,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Friedman, N., Getoor, L., Koller, D., Pfeffer, A., Learning probabilistic relational models (1999) Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI, pp. 1300-1309",Learning probabilistic relational models,"Friedman N., Getoor L., Koller D., Pfeffer A.",Learning probabilistic relational models,"A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with ""flat"" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning - the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.",,1999.0,485.0,,Data representations; Dependency structures; Learning Bayesian networks; Learning procedures; Probabilistic relational models; Relational Database; Relational structures; Statistical learning methods; Artificial intelligence; Data handling; Relational database systems; Bayesian networks,2-s2.0-84880688943
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Fung, G.M., Mangasarian, O.L., Shavlik, J.W., Knowledge-based support vector machine classifiers (2002) Advances in Neural Information Processing Systems, pp. 521-528",Knowledge-based support vector machine classifiers,"Fung G., Mangasarian O.L., Shavlik J.",Knowledge-based support vector machine classifiers,[No abstract available],,2003.0,58.0,,,2-s2.0-29144457113
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Fung, G.M., Mangasarian, O.L., Shavlik, J.W., Knowledge-based nonlinear kernel classifiers (2003) Learning Theory and Kernel Machines, pp. 102-113. , Springer",Knowledge-based nonlinear kernel classifiers,"Fung G.M., Mangasarian O.L., Shavlik J.W.",Knowledge-based nonlinear kernel classifiers,"Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a nonlinear kernel support vector machine (SVM) classifier. The resulting formulation leads to a linear program that can be solved efficiently. This extends, in a rather unobvious fashion, previous work [3] that incorporated similar prior knowledge into a linear SVM classifier. Numerical tests on standard-type test problems, such as exclusive-or prior knowledge sets and a checkerboard with 16 points and prior knowledge instead of the usual 1000 points, show the effectiveness of the proposed approach in generating sharp nonlinear classifiers based mostly or totally on prior knowledge.",10.1007/978-3-540-45167-9_9,2003.0,32.0,Linear programming; Prior knowledge; Support vector machines,Classifiers; Computer vision; Data acquisition; Linear programming; Neural networks; Nonlinear control systems; Theorem proving; Vectors; Knowledge based systems; Linear programming; Input space; Kernel classifiers; Prior knowledge; Support vector machines (SVM); Knowledge based; Linear programs; Nonlinear classifiers; Nonlinear kernels; Numerical tests; Polyhedral set; Standard type; Knowledge based systems; Support vector machines,2-s2.0-9444280785
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Golomb, S.W., Baumert, L.D., Backtrack programming (1965) J. ACM, 12, pp. 516-524",Backtrack programming,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Gupta, M., Qi, J., Theory of t-norms and fuzzy inference methods (1991) Fuzzy Sets Syst., 40, pp. 431-450",Theory of t-norms and fuzzy inference methods,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Hajek, P., The Metamathematics of Fuzzy Logic (1998), Kluwer",The Metamathematics of Fuzzy Logic,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Haralick, R.M., Elliott, G.L., Increasing tree search efficiency for constraint satisfaction problems (1980) Artif. Intell., 14, pp. 263-313",Increasing tree search efficiency for constraint satisfaction problems,"Haralick R.M., Elliott G.L.",Increasing tree search efficiency for constraint satisfaction problems,"In this paper we explore the number of tree search operations required to solve binary constraint satisfaction problems. We show analytically and experimentally that the two principles of first trying the places most likely to fail and remembering what has been done to avoid repeating the same mistake twice improve the standard backtracking search. We experimentally show that a lookahead procedure called forward checking (to anticipate the future) which employs the most likely to fail principle performs better than standard backtracking, Ullman's, Waltz's, Mackworth's, and Haralick's discrete relaxation in all cases tested, and better than Gaschnig's backmarking in the larger problems. © 1980, All rights reserved.",10.1016/0004-3702(80)90051-X,1980.0,731.0,,Computer programming,2-s2.0-0019067870
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Haussler, D., Convolution kernels on discrete structures (1999), Technical report Department of Computer Science, University of California at Santa Cruz",Convolution kernels on discrete structures,Haussler D.,Convolution kernels on discrete structures,[No abstract available],,1999.0,805.0,,,2-s2.0-0004019973
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Hitzler, P., Holldobler, S., Sedab, A.K., Logic programs and connectionist networks (2004) J. Appl. Log., 2, pp. 245-272",Logic programs and connectionist networks,"Hitzler P., Hölldobler S., Seda A.K.",Logic programs and connectionist networks,"One facet of the question of integration of Logic and Connectionist Systems, and how these can complement each other, concerns the points of contact, in terms of semantics, between neural networks and logic programs. In this paper, we show that certain semantic operators for propositional logic programs can be computed by feedforward connectionist networks, and that the same semantic operators for first-order normal logic programs can be approximated by feedforward connectionist networks. Turning the networks into recurrent ones allows one also to approximate the models associated with the semantic operators. Our methods depend on a well-known theorem of Funahashi, and necessitate the study of when Funahashi's theorem can be applied, and also the study of what means of approximation are appropriate and significant. © 2004 Elsevier B.V. All rights reserved.",10.1016/j.jal.2004.03.002,2004.0,65.0,Connectionist networks; Logic programming; Metric spaces,Approximation theory; Artificial intelligence; Feedforward neural networks; Mathematical models; Mathematical transformations; Polynomials; Semantics; Theorem proving; Connectionist networks; Contraction mappings; Feedforward connectionist network; Funahashi theorem; Metric spaces; Logic programming,2-s2.0-10944269335
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Huynh, T.N., Mooney, R.J., Discriminative structure and parameter learning for Markov logic networks (2008) Proceedings of the 25th International Conference on Machine Learning, ICML, pp. 416-423. , ACM",Discriminative structure and parameter learning for Markov logic networks,"Huynh T.N., Mooney R.J.",Discriminative structure and parameter learning for Markov logic networks,"Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, many relational learning problems involve specific target predicates that must be inferred from given background information. We found that existing MLN methods perform very poorly on several such ILP benchmark problems, and we present improved discriminative methods for learning MLN clauses and weights that outperform existing MLN and traditional ILP methods. Copyright 2008 by the author(s)/owner(s).",10.1145/1390156.1390209,2008.0,66.0,,Computer circuits; Machine learning; Markov processes; Probabilistic logics; Formal logic; Graphic methods; Learning systems; Robot learning; Background information; Bench-mark problems; Discriminative methods; Logical structure; Markov logic networks; Parameter learning; Relational learning; Statistical relational learning; Inductive logic programming (ILP); Education; Background informations; Bench-mark problems; Discriminative methods; Existing methods; First-order logic; Graphical models; Logical structures; Markov logic networks; Parameter learning; Relational learning; Statistical relational learning,2-s2.0-56449093057
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Kok, S., Domingos, P., Learning the structure of Markov logic networks (2005) Proceedings of the 22nd International Conference on Machine Learning, ICML, pp. 441-448. , ACM",Learning the structure of Markov logic networks,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Landwehr, N., Passerini, A., De Raedt, L., Frasconi, P., kfoil: learning simple relational kernels (2006) Proceedings of the AAAI Conference on Artificial Intelligence, pp. 389-394",kfoil: learning simple relational kernels,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Landwehr, N., Passerini, A., Raedt, L., Frasconi, P., Fast learning of relational kernels (2010) Mach. Learn.",Fast learning of relational kernels,"Landwehr N., Passerini A., De Raedt L., Frasconi P.",Fast learning of relational kernels,"We develop a general theoretical framework for statistical logical learning with kernels based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space. In particular, we study the case where structure learning is performed by a simple FOIL-like algorithm, and propose alternative scoring functions for guiding the search process. We present an empirical evaluation on several data sets in the single-task as well as in the multi-task setting. © The Author(s) 2009.",10.1007/s10994-009-5163-1,2010.0,27.0,Dynamic propositionalization; Inductive logic programming; Kernel learning; Kernel methods; Multi-task learning; Statistical relational learning,Artificial intelligence; Inductive logic programming (ILP); Kernel learning; Kernel methods; Multitask learning; Propositionalization; Statistical relational learning; Software engineering,2-s2.0-80053138684
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Laurer, F., Bloch, G., Incorporating prior knowledge in support vector machines for classification: a review (2009) Neurocomputing, 71, pp. 1578-1594",Incorporating prior knowledge in support vector machines for classification: a review,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Le, Q.V., Smola, A.J., Gärtner, T., Simpler knowledge-based support vector machines (2006) Proceedings of the 23rd International Conference on Machine Learning, ICML, pp. 521-528. , ACM",Simpler knowledge-based support vector machines,"Le Q.V., Smola A.J., Gärtner T.",Simpler knowledge-based support vector machines,"If appropriately used, prior knowledge can significantly improve the predictive accuracy of learning algorithms or reduce the amount of training data needed. In this paper we introduce a simple method to incorporate prior knowledge in support vector machines by modifying the hypothesis space rather than the optimization problem. The optimization problem is amenable to solution by the constrained concave convex procedure, which finds a local optimum. The paper discusses different kinds of prior knowledge and demonstrates the applicability of the approach in some characteristic experiments.",,2006.0,14.0,,Data reduction; Knowledge based systems; Learning algorithms; Optimization; Predictive control systems; Problem solving; Constrained concave convex procedure; Optimization problems; Support vector machines; Learning systems,2-s2.0-33749267288
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Lippi, M., Frasconi, P., Prediction of protein β-residue contacts by Markov logic networks with grounding-specific weights (2009) Bioinformatics, 25, pp. 2326-2333",Prediction of protein β-residue contacts by Markov logic networks with grounding-specific weights,"Lippi M., Frasconi P.",Prediction of protein β-residue contacts by Markov logic networks with grounding-specific weights,"Motivation: Accurate prediction of contacts between β-strand residues can significantly contribute towards ab initio prediction of the 3D structure of many proteins. Contacts in the same protein are highly interdependent. Therefore, significant improvements can be expected by applying statistical relational learners that overcome the usual machine learning assumption that examples are independent and identically distributed. Furthermore, the dependencies among β-residue contacts are subject to strong regularities, many of which are known a priori. In this article, we take advantage of Markov logic, a statistical relational learning framework that is able to capture dependencies between contacts, and constrain the solution according to domain knowledge expressed by means of weighted rules in a logical language. Results: We introduce a novel hybrid architecture based on neural and Markov logic networks with grounding-specific weights. On a non-redundant dataset, our method achieves 44.9% F1 measure, with 47.3% precision and 42.7% recall, which is significantly better (P &lt; 0.01) than previously reported performance obtained by 2D recursive neural networks. Our approach also significantly improves the number of chains for which β-strands are nearly perfectly paired (36% of the chains are predicted with F1 ≥ 70% on coarse map). It also outperforms more general contact predictors on recent CASP 2008 targets. © The Author 2009. Published by Oxford University Press. All rights reserved.",10.1093/bioinformatics/btp421,2009.0,40.0,,"ab initio calculation; accuracy; article; artificial neural network; beta sheet; classifier; computer prediction; controlled study; hidden Markov model; machine learning; priority journal; protein secondary structure; structural proteomics; Computational Biology; Databases, Protein; Markov Chains; Neural Networks (Computer); Protein Conformation; Proteins",2-s2.0-69849084410
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Lowd, D., Domingos, P., Efficient weight learning for Markov logic networks (2007) Proceedings of the Eleventh European Conference on Principles and Practice of Knowledge Discovery in Databases, pp. 200-211",Efficient weight learning for Markov logic networks,"Lowd D., Domingos P.",Efficient weight learning for Markov logic networks,"Markov logic networks (MLNs) combine Markov networks and first-order logic, and are a powerful and increasingly popular representation for statistical relational learning. The state-of-the-art method for discriminative learning of MLN weights is the voted perceptron algorithm, which is essentially gradient descent with an MPE approximation to the expected sufficient statistics (true clause counts). Unfortunately, these can vary widely between clauses, causing the learning problem to be highly ill-conditioned, and making gradient descent very slow. In this paper, we explore several alternatives, from per-weight learning rates to second-order methods. In particular, we focus on two approaches that avoid computing the partition function: diagonal Newton and scaled conjugate gradient. In experiments on standard SRL datasets, we obtain order-of-magnitude speedups, or more accurate models given comparable learning times. © Springer-Verlag Berlin Heidelberg 2007.",10.1007/978-3-540-74976-9_21,2007.0,128.0,,Approximation algorithms; Database systems; Learning algorithms; Markov processes; Problem solving; Statistical methods; Efficient weight learning; First-order logic; Markov logic networks (MLN); Statistical relational learning; Computer networks,2-s2.0-38049174896
2-s2.0-84940937276,Semantic-based regularization for learning and inference," McCallum, A., Nigam, K., Rennie, J., Seymore, K., Automating the construction of Internet portals with machine learning (2000) Inf. Retr., 3, pp. 127-163",Automating the construction of Internet portals with machine learning,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Melacci, S., Belkin, M., Laplacian support vector machines trained in the primal (2011) J. Mach. Learn. Res., 12, pp. 1149-1184",Laplacian support vector machines trained in the primal,"Melacci S., Belkin M.",Laplacian support vector machines trained in the primal,"In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data. Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi-supervised classification. In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dual formulation. In particular, training a LapSVM in the primal can be efficiently performed with preconditioned conjugate gradient. We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples. This allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones, considerably reducing the training time. The computational complexity of the training algorithm is reduced from 0(n3) to 0(kn2), where n is the combined number of labeled and unlabeled examples and k is empirically evaluated to be significantly smaller than n. Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large data sets. We present an extensive experimental evaluation on real world data showing the benefits of the proposed approach. © 2011 Stefano Melacci and Mikhail Belkin.",,2011.0,291.0,Classification; Laplacian support vector machines; Manifold regularization; Optimization; Semi-supervised learning,Approximate solution; Classification; Classification accuracy; Dual formulations; Early stopping; Experimental evaluation; Laplacians; Large datasets; Machine learning communities; Manifold regularization; Preconditioned conjugate gradient; Real world data; Regularization approach; Semi-supervised classification; Semi-supervised learning; Speed-ups; State-of-the-art performance; Training algorithms; Training time; Unlabeled data; Computational complexity; Conjugate gradient method; Learning algorithms; Optimization; Supervised learning; Support vector machines; Laplace transforms,2-s2.0-79955855934
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Mihalkova, L., Mooney, R.J., Bottom-up learning of Markov logic network structure (2007) Proceedings of the 24th International Conference on Machine Learning, pp. 625-632. , ACM New York, NY, USA",Bottom-up learning of Markov logic network structure,"Mihalkova L., Mooney R.J.",Bottom-up learning of Markov logic network structure,"Markov logic networks (MLNs) are a statistical relational model that consists of weighted firstorder clauses and generalizes first-order logic and Markov networks. The current state-of-the-art algorithm for learning MLN structure follows a top-down paradigm where many potential candidate structures are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data. Even though this existing algorithm outperforms an impressive array of benchmarks, its greedy search is susceptible to local maxima or plateaus. We present a novel algorithm for learning MLN structure that follows a more bottom-up approach to address this problem. Our algorithm uses a ""propositional"" Markov network learning method to construct ""template"" networks that guide the construction of candidate clauses. Our algorithm significantly improves accuracy and learning time over the existing topdown approach in three real-world domains.",10.1145/1273496.1273575,2007.0,94.0,,Algorithms; Benchmarking; Data reduction; Learning systems; Real time systems; Statistical methods; Candidate clauses; Local maxima; Markov logic networks (MLN); Real-world domains; Markov processes,2-s2.0-34547988135
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Muggleton, S., Lodhi, H., Amini, A., Sternberg, M.J., Support vector inductive logic programming (2005) Discovery Science, pp. 163-175. , Springer",Support vector inductive logic programming,"Muggleton S., Lodhi H., Amini A., Sternberg M.J.E.",Support vector inductive logic programming,"In this paper we explore a topic which is at the intersection of two areas of Machine Learning: namely Support Vector Machines (SVMs) and Inductive Logic Programming (ILP). We propose a general method for constructing kernels for Support Vector Inductive Logic Programming (SVILP). The kernel not only captures the semantic and syntactic relational information contained in the data but also provides the flexibility of using arbitrary forms of structured and non-structured data coded in a relational way. While specialised kernels have been developed for strings, trees and graphs our approach uses declarative background knowledge to provide the learning bias. The use of explicitly encoded background knowledge distinguishes SVILP from existing relational kernels which in ILP-terms work purely at the atomic generalisation level. The SVILP approach is a form of generalisation relative to background knowledge, though the final combining function for the ILP-leamed clauses is an SVM rather than a logical conjunction. We evaluate SVILP empirically against related approaches, including an industry-standard toxin predictor called TOPKAT. Evaluation is conducted on a new broad-ranging toxicity dataset (DSSTox). The experimental results demonstrate that our approach significantly outperforms all other approaches in the study. © Springer-Verlag Berlin Heidelberg 2005.",10.1007/11563983_15,2005.0,41.0,,Database systems; Knowledge based systems; Learning systems; Semantics; Atomic generalisation level; Logical conjunction; Non-structured data; Support Vector Machines (SVM); Logic programming,2-s2.0-33646363498
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Neville, J., Jensen, D., Relational dependency networks (2007) J. Mach. Learn. Res., 8, pp. 653-692",Relational dependency networks,"Neville J., Jensen D.",Relational dependency networks,"Recent work on graphical models for relational data has demonstrated significant improvements in classification and inference when models represent the dependencies among instances. Despite its use in conventional statistical models, the assumption of instance independence is contradicted by most relational data sets. For example, in citation data there are dependencies among the topics of a paper's references, and in genomic data there are dependencies among the functions of interacting proteins. In this paper, we present relational dependency networks (RDNs), graphical models that are capable of expressing and reasoning with such dependencies in a relational setting. We discuss RDNs in the context of relational Bayes networks and relational Markov networks and outline the relative strengths of RDNs - namely, the ability to represent cyclic dependencies, simple methods for parameter estimation, and efficient structure learning techniques. The strengths of RDNs are due to the use of pseudolikelihood learning techniques, which estimate an efficient approximation of the full joint distribution. We present learned RDNs for a number of real-world data sets and evaluate the models in a prediction context, showing that RDNs identify and exploit cyclic relational dependencies to achieve significant performance gains over conventional conditional models. In addition, we use synthetic data to explore model performance under various relational data characteristics, showing that RDN learning and inference techniques are accurate over a wide range of conditions.",,2007.0,188.0,Dependency networks; Graphical models; Knowledge discovery; Probabilistic relational models; Pseudolikelihood estimation; Relational learning,Data mining; Function evaluation; Graphic methods; Markov processes; Mathematical models; Proteins; Dependency networks; Graphical models; Probabilistic relational models; Pseudolikelihood estimation; Relational learning; Learning systems,2-s2.0-33947664999
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Niu, F., Ré, C., Doan, A., Shavlik, J., Tuffy: scaling up statistical inference in Markov logic networks using an RDBMS (2011) Proceedings of the VLDB, pp. 373-384",Tuffy: scaling up statistical inference in Markov logic networks using an RDBMS,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Novák, V., First-order fuzzy logic (1987) Stud. Log., 46, pp. 87-109",First-order fuzzy logic,Novák V.,First-order fuzzy logic,"This paper is an attempt to develop the many-valued first-order fuzzy logic. The set of its truth, values is supposed to be either a finite chain or the interval 〈0, 1〉 of reals. These are special cases of a residuated lattice 〈L, ∨, ∧, ⊗, →, 1, 0〉. It has been previously proved that the fuzzy propositional logic based on the same sets of truth values is semantically complete. In this paper the syntax and semantics of the first-order fuzzy logic is developed. Except for the basic connectives and quantifiers, its language may contain also additional n-ary connectives and quantifiers. Many propositions analogous to those in the classical logic are proved. The notion of the fuzzy theory in the first-order fuzzy logic is introduced and its canonical model is constructed. Finally, the extensions of Gödel's completeness theorems are proved which confirm that the first-order fuzzy logic is also semantically complete. © 1987 Polish Academy of Sciences.",10.1007/BF00396907,1987.0,57.0,,,2-s2.0-0002131962
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Piaget, J., La psychologie de l'intelligence (1961), Armand Colin Paris",La psychologie de l'intelligence,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Poggio, T., Girosi, F., A theory of networks for approximation and learning (1989), Technical report MIT",A theory of networks for approximation and learning,"Poggio T., Girosi F.",A theory of networks for approximation and learning,[No abstract available],,1989.0,315.0,,,2-s2.0-0004030839
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Raedt, L.D., Frasconi, P., Kersting, K.S.M., (2008) Probabilistic Inductive Logic Programming, Lecture Notes in Artificial Intelligence, 4911. , Springer",,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Richardson, M., Domingos, P., Markov logic networks (2006) Mach. Learn., 62, pp. 107-136",Markov logic networks,"Richardson M., Domingos P.",Markov logic networks,"We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a firstorder knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.",10.1007/s10994-006-5833-1,2006.0,1746.0,First-order logic; Graphical models; Inductive logic programming; Knowledge-based model construction; Log-linear models; Markov chain Monte Carlo; Markov networks; Markov random fields; Pseudo-likelihood; Satisfiability; Statistical relational learning,Information analysis; Learning systems; Markov processes; Monte Carlo methods; Optimization; Relational database systems; First-order logic; Graphical models; Inductive logic programming; Knowledge-based model construction; Log-linear models; Markov logic networks (MLN); Markov random fields; Pseudo likelihood; Satisfiability; Statistical relational learning; Computer graphics,2-s2.0-32044466073
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Richardson, M., Domingos, P., Markov logic networks (2006) Mach. Learn., 62, pp. 107-136",Markov logic networks,"Richardson M., Domingos P.",Markov logic networks,[No abstract available],,2006.0,2.0,,,2-s2.0-34748853447
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Rossi, F., Van Beek, P., Walsh, T., Handbook of Constraint Programming (2006), Elsevier",Handbook of Constraint Programming,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Scholkopf, B., Smola, A.J., Learning with Kernels (2001), MIT Press Cambridge, MA, USA",Learning with Kernels,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Shavlik, J.W., Natarajan, S., Speeding up inference in Markov logic networks by preprocessing to reduce the size of the resulting grounded network (2009) Proceedings of the International Joint Conference on Artificial Intelligence, IJCAI, pp. 1951-1956",Speeding up inference in Markov logic networks by preprocessing to reduce the size of the resulting grounded network,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Singla, P., Domingos, P., Memory-efficient inference in relational domains (2006) Proceedings of the 21st AAAI Conference on Artificial Intelligence, pp. 488-493. , AAAI Press",Memory-efficient inference in relational domains,"Singla P., Donaingos P.",Memory-efficient inference in relational domains,"Propositionalization of a first-order theory followed by satisfiability testing has proved to be a remarkably efficient approach to inference in relational domains such as planning (Kautz & Selman 1996) and verification (Jackson 2000). More recently, weighted satisfiability solvers have been used successfully for MPE inference in statistical relational learners (Singla & Domingos 2005). However, fully instantiating a finite first-order theory requires memory on the order of the number of constants raised to the arity of the clauses, which significantly limits the size of domains it can be applied to. In this paper we propose LazySAT, a variation of the WalkSAT solver that avoids this blowup by taking advantage of the extreme sparseness that is typical of relational domains (i.e., only a small fraction of ground atoms are true, and most clauses are trivially satisfied). Experiments on entity resolution and planning problems show that LazySAT reduces memory usage by orders of magnitude compared to Walk-SAT, while taking comparable time to run and producing the same solutions. Copyright © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",,2006.0,43.0,,Data structures; Finite element method; Learning systems; Problem solving; Statistical methods; Storage allocation (computer); Finite first order theory; Propositionalization; Relational domains; Satisfiability testing; Statistical relational learners; Relational database systems,2-s2.0-33750696315
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Starczewski, J.T., Advanced Concepts in Fuzzy Logic and Systems with Membership Uncertainty (2012), Springer",Advanced Concepts in Fuzzy Logic and Systems with Membership Uncertainty,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Tran, S.D., Davis, L.S., Event modeling and recognition using Markov logic networks (2008) Proceedings of the European Conference of Computer Vision, ECCV, pp. 610-623. , Springer",Event modeling and recognition using Markov logic networks,"Tran S.D., Davis L.S.",Event modeling and recognition using Markov logic networks,"We address the problem of visual event recognition in surveillance where noise and missing observations are serious problems. Common sense domain knowledge is exploited to overcome them. The knowledge is represented as first-order logic production rules with associated weights to indicate their confidence. These rules are used in combination with a relaxed deduction algorithm to construct a network of grounded atoms, the Markov Logic Network. The network is used to perform probabilistic inference for input queries about events of interest. The system's performance is demonstrated on a number of videos from a parking lot domain that contains complex interactions of people and vehicles. © 2008 Springer Berlin Heidelberg.",10.1007/978-3-540-88688-4_45,2008.0,135.0,,Computer vision; Markov processes; Probabilistic logics; Domain knowledge; Event recognition; First order logic; Markov logic networks; Missing observations; Probabilistic inference; Production rules; System's performance; Computer circuits,2-s2.0-56749176261
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Tsochantaridis, I., Joachims, T., Hofmann, T., Altun, Y., Large margin methods for structured and interdependent output variables (2005) J. Mach. Learn. Res., pp. 1453-1484",Large margin methods for structured and interdependent output variables,"Tsochantaridis I., Joachims T., Hofmann T., Altun Y.",Large margin methods for structured and interdependent output variables,"Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.",,2005.0,1442.0,,Algorithms; Artificial intelligence; Classification (of information); Information retrieval; Natural language processing systems; Optical character recognition; Optimization; Polynomials; Problem solving; Set theory; Trees (mathematics); Classification algorithms; Computational biology; Interdependent output variables; Large margin methods; Learning systems,2-s2.0-24944537843
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Vapnik, V., The Nature of Statistical Learning Theory (2000), 2nd edn. Springer Verlag",The Nature of Statistical Learning Theory,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Wang, J., Domingos, P., Hybrid Markov logic networks (2008) Proceedings of the 23-rd AAAI Conference on Artificial Intelligence, pp. 1106-1111",Hybrid Markov logic networks,,,,,,,,,
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Williams, P.M., Bayesian regularization and pruning using a Laplace prior (1995) Neural Comput., 7, pp. 117-143",Bayesian regularization and pruning using a Laplace prior,Williams P.M.,Bayesian regularization and pruning using a Laplace prior,[No abstract available],,1995.0,281.0,,,2-s2.0-0000673452
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Zadeh, L.A., Fuzzy sets (1965) Inf. Control, 8, pp. 338-353",Fuzzy sets,Zadeh L.A.,Fuzzy sets,"A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint. © 1965 Academic Press, Inc.",10.1016/S0019-9958(65)90241-X,1965.0,55488.0,,,2-s2.0-34248666540
2-s2.0-84940937276,Semantic-based regularization for learning and inference," Zhou, D., Schölkopf, B., Regularization on discrete spaces (2005) Pattern Recognit., pp. 361-368",Regularization on discrete spaces,"Zhou D., Schölkopf B.",Regularization on discrete spaces,"We consider the classification problem on a finite set of objects. Some of them are labeled, and the task is to predict the labels of the remaining unlabeled ones. Such an estimation problem is generally referred to as transductive inference. It is well-known that many meaningful inductive or supervised methods can be derived from a regularization framework, which minimizes a loss function plus a regularization term. In the same spirit, we propose a general discrete regularization framework defined on finite object sets, which can be thought of as discrete analogue of classical regularization theory. A family of transductive inference schemes is then systemically derived from the framework, including our earlier algorithm for transductive inference, with which we obtained encouraging results on many practical classification problems. The discrete regularization framework is built on discrete analysis and geometry developed by ourselves, in which a number of discrete differential operators of various orders are constructed, which can be thought of as discrete analogues of their counterparts in the continuous case. © Springer-Verlag Berlin Heidelberg 2005.",10.1007/11550518_45,2005.0,111.0,,Algorithms; Classification (of information); Decision making; Finite difference method; Mathematical operators; Parameter estimation; Problem solving; Classical regularization theory; Discrete spaces; Finite set of objects; Regularization framework; Transductive inference; Pattern recognition,2-s2.0-27244449175
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks,"Aho, A.V., Nested stack automata (1969) Journal of the Association for Computing Machinery, 16 (3), pp. 383-406",Nested stack automata,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Aho, A.V., Ullman, J.D., The theory of parsing, translation and compiling (1972), Prentice-Hall",translation and compiling,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Albert, R., Barabási, A.-L., Statistical mechanics of complex networks (2002) Reviews of Modern Physics, 74 (1), pp. 47-97",Statistical mechanics of complex networks,"Albert R., Barabási A.-L.",Statistical mechanics of complex networks,"Complex networks describe a wide range of systems in nature and society. Frequently cited examples include the cell, a network of chemicals linked by chemical reactions, and the Internet, a network of routers and computers connected by physical links. While traditionally these systems have been modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks are governed by robust organizing principles. This article reviews the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, the authors discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, the emerging theory of evolving networks, and the interplay between topology and the network's robustness against failures and attacks.",10.1103/RevModPhys.74.47,2002.0,14535.0,,,2-s2.0-0036013593
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Alvarez-Alvarez, A., Trivino, G., Cordón, O., Human gait modeling using a genetic fuzzy finite state machine (2012) IEEE Transactions on Fuzzy Systems, 20 (2), pp. 205-223",Human gait modeling using a genetic fuzzy finite state machine,"Alvarez-Alvarez A., Trivino G., Cordón O.",Human gait modeling using a genetic fuzzy finite state machine,"Human gait modeling consists of studying the biomechanics of this human movement. Its importance lies in the fact that its analysis can help in the diagnosis of walking and movement disorders or rehabilitation programs, among other medical situations. Fuzzy finite state machines can be used to model the temporal evolution of this type of phenomenon. Nevertheless, the definition of details of the model in each particular case is a complex task for experts. In this paper, we present an automatic method to learn the model parameters that are based on the hybridization of fuzzy finite state machines and genetic algorithms leading to genetic fuzzy finite state machines. This new genetic fuzzy system automatically learns the fuzzy rules and membership functions of the fuzzy finite state machine, while an expert defines the possible states and allowed transitions. Our final goal is to obtain a specific model for each persons gait in such a way that it can generalize well with different gaits of the same person. The obtained model must become an accurate and human friendly linguistic description of this phenomenon, with the capability to identify the relevant phases of the process. A complete experimentation is developed to test the performance of the new proposal when dealing with datasets of 20 different people, comprising a detailed analysis of results, which shows the advantages of our proposal in comparison with some other classical and computational intelligence techniques. © 2012 IEEE.",10.1109/TFUZZ.2011.2171973,2012.0,66.0,Fuzzy finite state machines; fuzzy systems; genetic algorithms (GAs); genetic fuzzy systems; human gait modeling,Automatic method; Complex task; Computational intelligence techniques; Data sets; Genetic fuzzy systems; Genetic-fuzzy; Human gait modeling; Human movements; Human-friendly; Linguistic descriptions; Model parameters; Movement disorders; Rehabilitation programs; Temporal evolution; Artificial intelligence; Biomechanics; Diagnosis; Finite automata; Fuzzy systems; Program diagnostics; Genetic algorithms,2-s2.0-84859705290
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Amari, S.-I., A method of statistical neurodynamics (1974) Kybernetik, 14, pp. 201-215",A method of statistical neurodynamics,Amari S.-i.,A method of statistical neurodynamics,"A method of statistical neurodynamics is presented for treating ensembles of nets of randomly connected neuron-like elements. The concept of a macrostate plays a fundamental role in statistical neurodynamics and a criterion is given for ascertaining that given macroscopic quantities together constitute a macrostate. The activity of a nerve net is shown to be a macrostate and the equation of the dynamics of the activity is elucidated for various ensembles of random nerve nets. It is shown that the distance between two microstates can also be treated as a macrostate in a generalized sense. The equation of its dynamics represents how the distance between two states changes in the course of state transitions. The dynamics of distance reveals interesting microscopic properties of random nerve nets, such as the stability of state-transition, the transient lengths, etc. © 1974 Springer-Verlag.",10.1007/BF00274806,1974.0,126.0,,"anatomy; hemodynamics; mathematical model; model; nerve cell; statistics; theoretical study; Animal; Human; Mathematics; Models, Neurological; Neurons; Statistics",2-s2.0-0016375547
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Barrès, V., Simons, A., 3rd, Arbib, M., Synthetic event-related potentials: A computational bridge between neurolinguistic models and experiments (2013) Neural Networks, 37, pp. 66-92",Synthetic event-related potentials: A computational bridge between neurolinguistic models and experiments,"Barrès V., Simons A., Arbib M.",Synthetic event-related potentials: A computational bridge between neurolinguistic models and experiments,"Our previous work developed Synthetic Brain Imaging to link neural and schema network models of cognition and behavior to PET and fMRI studies of brain function. We here extend this approach to Synthetic Event-Related Potentials (Synthetic ERP). Although the method is of general applicability, we focus on ERP correlates of language processing in the human brain. The method has two components: Phase 1: To generate cortical electro-magnetic source activity from neural or schema network models; and Phase 2: To generate known neurolinguistic ERP data (ERP scalp voltage topographies and waveforms) from putative cortical source distributions and activities within a realistic anatomical model of the human brain and head. To illustrate the challenges of Phase 2 of the methodology, spatiotemporal information from Friederici's 2002 model of auditory language comprehension was used to define cortical regions and time courses of activation for implementation within a forward model of ERP data. The cortical regions from the 2002 model were modeled using atlas-based masks overlaid on the MNI high definition single subject cortical mesh. The electromagnetic contribution of each region was modeled using current dipoles whose position and orientation were constrained by the cortical geometry. In linking neural network computation via EEG forward modeling to empirical results in neurolinguistics, we emphasize the need for neural network models to link their architecture to geometrically sound models of the cortical surface, and the need for conceptual models to refine and adopt brain-atlas based approaches to allow precise brain anchoring of their modules. The detailed analysis of Phase 2 sets the stage for a brief introduction to Phase 1 of the program, including the case for a schema-theoretic approach to language production and perception presented in detail elsewhere. Unlike Dynamic Causal Modeling (DCM) and Bojak's mean field model, Synthetic ERP builds on models of networks that mediate the relation between the brain's inputs, outputs, and internal states in executing a specific task. The neural networks used for Synthetic ERP must include neuroanatomically realistic placement and orientation of the cortical pyramidal neurons. These constraints pose exciting challenges for future work in neural network modeling that is applicable to systems and cognitive neuroscience. © 2012 Elsevier Ltd.",10.1016/j.neunet.2012.09.021,2013.0,18.0,Computational model; Conceptual model; EEG; Event-related potential; Forward model; Language; Neural network; Neurolinguistics; Schema theory; Synthetic Brain Imaging,"Brain imaging; Computational model; Conceptual model; Event related potentials; Forward models; Language; Neurolinguistics; Schema theory; Brain; Brain models; Cognitive systems; Electroencephalography; Linguistics; Mean field theory; Neural networks; Neuroimaging; article; auditory discrimination; auditory nervous system; biological model; brain cortex; brain depth stimulation; brain electrophysiology; brain function; cognition; comprehension; electric potential; electromagnetic field; event related potential; executive function; geometry; language development; language processing; left hemisphere; linguistics; mental task; nerve cell network; neuroanatomy; neuroscience; priority journal; pyramidal nerve cell; right hemisphere; statistical model; artificial neural network; biological model; brain cortex; cytology; electroencephalography; evoked response; human; methodology; nuclear magnetic resonance imaging; physiology; brain function; brain mapping; brain region; functional anatomy; mathematical computing; mathematical model; Cerebral Cortex; Cognition; Electroencephalography; Evoked Potentials; Humans; Magnetic Resonance Imaging; Models, Neurological; Nerve Net; Neural Networks (Computer); Psycholinguistics; Pyramidal Cells",2-s2.0-84870451109
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," beim Graben, P., Drenhaus, H., Computationelle neurolinguistik (2012) Zeitschrift für Germanistische Linguistik, 40 (1), pp. 97-125",Computationelle neurolinguistik,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," beim Graben, P., Gerth, S., Vasishth, S., Towards dynamical system models of language-related brain potentials (2008) Cognitive Neurodynamics, 2 (3), pp. 229-255",Towards dynamical system models of language-related brain potentials,"Beim Graben P., Gerth S., Vasishth S.",Towards dynamical system models of language-related brain potentials,"Event-related brain potentials (ERP) are important neural correlates of cognitive processes. In the domain of language processing, the N400 and P600 reflect lexical-semantic integration and syntactic processing problems, respectively. We suggest an interpretation of these markers in terms of dynamical system theory and present two nonlinear dynamical models for syntactic computations where different processing strategies correspond to functionally different regions in the system's phase space. © 2008 Springer Science+Business Media B.V.",10.1007/s11571-008-9041-5,2008.0,34.0,Computational psycholinguistics; Dynamical systems; Event-related brain potentials; Language processing,article; cognition; controlled study; correlation function; dynamics; event related potential; human; human experiment; language disability; language processing; linguistics; mathematical model; nerve function; nonlinear system; normal human; process optimization; space; systems theory,2-s2.0-49249139448
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," beim Graben, P., Jurish, B., Saddy, D., Frisch, S., Language processing by dynamical systems (2004) International Journal of Bifurcation and Chaos, 14 (2), pp. 599-621",Language processing by dynamical systems,"Graben P.B., Jurish B., Saddy D., Frisch S.",Language processing by dynamical systems,"We describe a part of the stimulus sentences of a German language processing ERP experiment using a context-free grammar and represent different processing preferences by its unambiguous partitions. The processing is modeled by deterministic pushdown automata. Using a theorem proven by Moore, we map these automata onto discrete time dynamical systems acting at the unit square, where the processing preferences are represented by a control parameter. The actual states of the automata are rectangles lying in the unit square that can be interpreted as cylinder sets in the context of symbolic dynamics theory. We show that applying a wrong processing preference to a certain input string leads to an unwanted invariant set in the parsers dynamics. Then, syntactic reanalysis and repair can be modeled by a switching of the control parameter - in analogy to phase transitions observed in brain dynamics. We argue that ERP components are indicators of these bifurcations and propose an ERP-like measure of the parsing model.",10.1142/S0218127404009326,2004.0,14.0,Ambiguity resolution; Cylinder sets; Entropy; Gödel codes; Language processing; Local ambiguity; Pushdown automata; Symbolic dynamics,Automata theory; Brain; Brain models; Codes (symbols); Entropy; Parameter estimation; Phase transitions; Theorem proving; Ambiguity resolution; Cylinder sets; Gödel codes; Language processing; Local ambiguity; Pushdown automata; Symbolic dynamics; Natural language processing systems,2-s2.0-13844297730
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," beim Graben, P., Potthast, R., Inverse problems in dynamic cognitive modeling (2009) Chaos: An Interdisciplinary Journal of Nonlinear Science, 19 (1). , 015103",Inverse problems in dynamic cognitive modeling,"Beim Graben P., Potthast R.",Inverse problems in dynamic cognitive modeling,"Inverse problems for dynamical system models of cognitive processes comprise the determination of synaptic weight matrices or kernel functions for neural networks or neural/dynamic field models, respectively. We introduce dynamic cognitive modeling as a three tier top-down approach where cognitive processes are first described as algorithms that operate on complex symbolic data structures. Second, symbolic expressions and operations are represented by states and transformations in abstract vector spaces. Third, prescribed trajectories through representation space are implemented in neurodynamical systems. We discuss the Amari equation for a neural/dynamic field theory as a special case and show that the kernel construction problem is particularly ill-posed. We suggest a Tikhonov-Hebbian learning method as regularization technique and demonstrate its validity and robustness for basic examples of cognitive computations. © 2009 American Institute of Physics.",10.1063/1.3097067,2009.0,36.0,,"algorithm; animal; artificial neural network; biological model; cognition; fractal analysis; human; memory; nerve cell; nerve cell network; neuroscience; nonlinear system; physiology; procedures; theoretical model; Algorithms; Animals; Cognition; Fractals; Humans; Memory; Models, Biological; Models, Theoretical; Nerve Net; Neural Networks (Computer); Neurons; Neurosciences; Nonlinear Dynamics",2-s2.0-63849262991
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," beim Graben, P., Rodrigues, S., A biophysical observation model for field potentials of networks of leaky integrate-and-fire neurons (2013) Frontiers in Computational Neuroscience, 6 (100)",A biophysical observation model for field potentials of networks of leaky integrate-and-fire neurons,"Graben P.B., Rodrigues S.",A biophysical observation model for field potentials of networks of leaky integrate-and-fire neurons,"We present a biophysical approach for the coupling of neural network activity as resulting from proper dipole currents of cortical pyramidal neurons to the electric field in extracellular fluid. Starting from a reduced three-compartment model of a single pyramidal neuron, we derive an observation model for dendritic dipole currents in extracellular space and thereby for the dendritic field potential (DFP) that contributes to the local field potential (LFP) of a neural population. This work aligns and satisfies the widespread dipole assumption that is motivated by the ""open-field"" configuration of the DFP around cortical pyramidal cells. Our reduced three-compartment scheme allows to derive networks of leaky integrate-and-fire (LIF) models, which facilitates comparison with existing neural network and observation models. In particular, by means of numerical simulations we compare our approach with an ad hoc model by Mazzoni et al. (2008), and conclude that our biophysically motivated approach yields substantial improvement. © 2013 beim Graben and Rodrigues.",10.3389/fncom.2012.00100,2013.0,8.0,Biophysics; Current dipoles; Extracellular medium; Field potentials; Leaky integrate-and-fire neuron; Neural networks,Current dipoles; Dipole currents; Extracellular fluid; Extracellular medium; Extracellular space; Field potential; Integrate-and-fire model; Integrate-and-fire neurons; Local field potentials; Network activities; Neural populations; Observation model; Pyramidal cell; Pyramidal neuron; Biophysics; Body fluids; Electric fields; Electrophysiology; Independent component analysis; Neurons; Neural networks; article; biophysics; brain electrophysiology; dendritic dipole current; dendritic field potential; extracellular space; local field potential; nerve cell network; pyramidal nerve cell; simulation,2-s2.0-84872080384
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Bengio, Y., Courville, A., Vincent, P., Representation learning: A review and new perspectives (2013) IEEE Transactions on Pattern Analysis and Machine Intelligence, 35 (8), pp. 1798-1828",Representation learning: A review and new perspectives,"Bengio Y., Courville A., Vincent P.",Representation learning: A review and new perspectives,"The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning. © 1979-2012 IEEE.",10.1109/TPAMI.2013.50,2013.0,5505.0,autoencoder; Boltzmann machine; Deep learning; feature learning; neural nets; representation learning; unsupervised learning,Auto encoders; Boltzmann machines; Deep learning; Feature learning; representation learning; Learning systems; Neural networks; Unsupervised learning; Learning algorithms; algorithm; artificial intelligence; artificial neural network; human; review; artificial intelligence; trends; Algorithms; Artificial Intelligence; Humans; Neural Networks (Computer); Algorithms; Artificial Intelligence; Humans; Neural Networks (Computer),2-s2.0-84879854889
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Blutner, R., Taking a broader view: Abstraction and idealization (2011) Theoretical Linguistics, 37 (1-2), pp. 27-35",Taking a broader view: Abstraction and idealization,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Cabessa, J., Siegelmann, H.T., The computational power of interactive recurrent neural networks (2012) Neural Computation, 24 (4), pp. 996-1019",The computational power of interactive recurrent neural networks,"Cabessa J., Siegelmann H.T.",The computational power of interactive recurrent neural networks,"In classical computation, rational- and real-weighted recurrent neural networks were shown to be respectively equivalent to and strictly more powerful than the standard Turing machine model. Here, we study the computational power of recurrent neural networks in a more biologically oriented computational framework, capturing the aspects of sequential interactivity and persistence of memory. In this context, we prove that socalled interactive rational- and real-weighted neural networks show the same computational powers as interactive Turing machines and interactive Turing machineswith advice, respectively. A mathematical characterization of each of these computational powers is also provided. It follows from these results that interactive real-weighted neural networks can perform uncountablymanymore translations of information than interactive Turing machines, making them capable of super-Turing capabilities. © 2012 Massachusetts Institute of Technology.",10.1162/NECO_a_00263,2012.0,26.0,,"artificial neural network; biological model; computer simulation; letter; memory; nerve cell; physiology; Computer Simulation; Memory; Models, Neurological; Neural Networks (Computer); Neurons",2-s2.0-84861112430
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Cabessa, J., Villa, A.E., The expressive power of analog recurrent neural networks on infinite input streams (2012) Theoretical Computer Science, 436, pp. 23-34",The expressive power of analog recurrent neural networks on infinite input streams,"Cabessa J., Villa A.E.P.",The expressive power of analog recurrent neural networks on infinite input streams,"We consider analog recurrent neural networks working on infinite input streams, provide a complete topological characterization of their expressive power, and compare it to the expressive power of classical infinite word reading abstract machines. More precisely, we consider analog recurrent neural networks as language recognizers over the Cantor space, and prove that the classes of ω-languages recognized by deterministic and non-deterministic analog networks correspond precisely to the respective classes of Π20-sets and Σ11-sets of the Cantor space. Furthermore, we show that the result can be generalized to more expressive analog networks equipped with any kind of Borel accepting condition. Therefore, in the deterministic case, the expressive power of analog neural nets turns out to be comparable to the expressive power of any kind of Büchi abstract machine, whereas in the non-deterministic case, analog recurrent networks turn out to be strictly more expressive than any other kind of Büchi or Muller abstract machine, including the main cases of classical automata, 1-counter automata, k-counter automata, pushdown automata, and Turing machines. © 2011 Elsevier B.V. All rights reserved.",10.1016/j.tcs.2012.01.042,2012.0,22.0,ω-Automata; Analog computation; Analog neural networks; Analytic sets; Borel sets; Topology; Turing machines,Abstract machines; Analog computation; Analog neural network; Analog recurrent neural network; Analytic sets; Borel set; Cantor spaces; Expressive power; Infinite word; Input streams; Push-down automata; Recurrent networks; Analog computers; Machinery; Topology; Turing machines; Recurrent neural networks,2-s2.0-84860776133
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Cabessa, J., Villa, A.E., The super-turing computational power of interactive evolving recurrent neural networks (2013) Artificial neural networks and machine learning–ICANN 2013, pp. 58-65. , Springer",The super-turing computational power of interactive evolving recurrent neural networks,"Cabessa J., Villa A.E.P.",The super-turing computational power of interactive evolving recurrent neural networks,"Understanding the dynamical and computational capabilities of neural models represents an issue of central importance. Here, we consider a model of first-order recurrent neural networks provided with the possibility to evolve over time and involved in a basic interactive and memory active computational paradigm. In this context, we prove that the so-called interactive evolving recurrent neural networks are computationally equivalent to interactive Turing machines with advice, hence capable of super-Turing potentialities. We further provide a precise characterisation of the ω-translations realised by these networks. Therefore, the consideration of evolving capabilities in a first-order neural model provides the potentiality to break the Turing barrier. © 2013 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-642-40728-4_8,2013.0,14.0,analog computation; interactive computation; neural computation; recurrent neural networks; super-Turing; Turing machines with advice,Analog computation; Computational capability; Computational paradigm; Computational power; Interactive computation; Neural computations; super-Turing; Turing barriers; Analog computers; Machinery; Turing machines; Recurrent neural networks,2-s2.0-84884914245
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Carmantini, G.S., (2015) Turing neural networks. GitHub repository, , https://github.com/TuringMachinegun/Turing_Neural_Networks",,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Carmantini, G.S., Beim Graben, P., Desroches, M., Rodrigues, S., Turing computation with recurrent artificial neural networks (2015) Proceedings of the NIPS workshop on cognitive computation: integrating neural and symbolic approaches, pp. 5-13. , [cs.NE]. arXiv:1511.01427",Turing computation with recurrent artificial neural networks,"Carmantini G.S., Beim Graben P., Desroches M., Rodrigues S.",Turing computation with recurrent artificial neural networks,[No abstract available],,2015.0,1.0,,,2-s2.0-84994333207
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Christiansen, M.H., Chater, N., Toward a connectionist model of recursion in human linguistic performance (1999) Cognitive Science, 23 (4), pp. 157-205",Toward a connectionist model of recursion in human linguistic performance,"Christiansen M.H., Chater N.",Toward a connectionist model of recursion in human linguistic performance,"Naturally occurring speech contains only a limited amount of complex recursive structure, and this is reflected in the empirically documented difficulties that people experience when processing such structures. We present a connectionist model of human performance in processing recursive language structures. The model is trained on simple artificial languages. We find that the qualitative performance profile of the model matches human behavior, both on the relative difficulty of center-embedding and cross-dependency, and between the processing of these complex recursive structures and right-branching recursive constructions. We analyze how these differences in performance are reflected in the internal representations of the model by performing discriminant analyses on these representations both before and after training. Furthermore, we show how a network trained to process recursive structures can also generate such structures in a probabilistic fashion. This work suggests a novel explanation of people's limited recursive performance, without assuming the existence of a mentally represented competence grammar allowing unbounded recursion. © 1999 Cognitive Science Society, Inc.",10.1207/s15516709cog2302_2,1999.0,203.0,,,2-s2.0-0000046043
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Collins, J.J., Richmond, S.A., Hard-wired central pattern generators for quadrupedal locomotion (1994) Biological Cybernetics, 71 (5), pp. 375-385",Hard-wired central pattern generators for quadrupedal locomotion,"Collins J.J., Richmond S.A.",Hard-wired central pattern generators for quadrupedal locomotion,"Animal locomotion is generated and controlled, in part, by a central pattern generator (CPG), which is an intraspinal network of neurons capable of producing rhythmic output. In the present work, it is demonstrated that a hard-wired CPG model, made up of four coupled nonlinear oscillators, can produce multiple phase-locked oscillation patterns that correspond to three common quadrupedal gaits - the walk, trot, and bound. Transitions between the different gaits are generated by varying the network's driving signal and/or by altering internal oscillator parameters. The above in numero results are obtained without changing the relative strengths or the polarities of the system's synaptic interconnections, i.e., the network maintains an invariant coupling architecture. It is also shown that the ability of the hard-wired CPG network to produce and switch between multiple gait patterns is a model-independent phenomenon, i.e., it does not depend upon the detailed dynamics of the component oscillators and/or the nature of the inter-oscillator coupling. Three different neuronal oscillator models - the Stein neuronal model, the Van der Pol oscillator, and the FitzHugh-Nagumo model -and two different coupling schemes are incorporated into the network without impeding its ability to produce the three quadrupedal gaits and the aforementioned gait transitions. © 1994 Springer-Verlag.",10.1007/BF00198915,1994.0,174.0,,Biocontrol; Brain models; Gait analysis; Mathematical models; Oscillations; FitzHugh Nagumo model; Hard wired central pattern generator; Phase locked oscillation patterns; Quadrupedal locomotion; Stein neuronal model; Van der Pol oscillator; Biomechanics,2-s2.0-0028715081
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Collins, S.H., Ruina, A., A bipedal walking robot with efficient and human-like gait (2005) Proceedings of the 2005 IEEE international conference on robotics and automation, ICRA 2005, pp. 1983-1988. , IEEE",A bipedal walking robot with efficient and human-like gait,"Collins S.H., Ruina A.",A bipedal walking robot with efficient and human-like gait,"Here we present the design of a passive-dynamics based, fully autonomous, 3-D, bipedal walking robot that uses simple control, consumes little energy, and has human-like morphology and gait. Design aspects covered here include the freely rotating hip joint with angle bisecting mechanism; freely rotating knee joints with latches; direct actuation of the ankles with a spring, release mechanism, and reset motor; wide feet that are shaped to aid lateral stability; and the simple control algorithm. The biomechanics context of this robot is discussed in more detail in [1], and movies of the robot walking are available at Science On- line and http://www.tam.cornell.edu/~ruina/powerwalk.html. This robot adds evidence to the idea that passive-dynamic approaches might help design walking robots that are simpler, more efficient and easier to control. ©2005 IEEE.",10.1109/ROBOT.2005.1570404,2005.0,320.0,Biped; Efficiency; Locomotion; Passive- Dynamic,Angle bisecting mechanisms; Walking robots; Biomechanics; Energy utilization; Gait analysis; Joints (anatomy); Motors; Position control; Mobile robots,2-s2.0-33645797164
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Davis, M.D., Sigal, R., Weyuker, E.J., (1994) Computability, complexity, and languages: fundamentals of theoretical computer science, , Academic Press, Harcourt, Brace and Company",,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Desroches, M., Krupa, M., Rodrigues, S., Inflection, canards and excitability threshold in neuronal models (2013) Journal of Mathematical Biology, 67 (4), pp. 989-1017",canards and excitability threshold in neuronal models,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Dominey, P.F., Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning (1995) Biological Cybernetics, 73 (3), pp. 265-274",Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning,Dominey P.F.,Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning,"A novel neural network model is presented that learns by trial-and-error to reproduce complex sensory-motor sequences. One subnetwork, corresponding to the prefrontal cortex (PFC), is responsible for generating unique patterns of activity that represent the continuous state of sequence execution. A second subnetwork, corresponding to the striatum, associates these state-encoding patterns with the correct response at each point in the sequence execution. From a neuroscience perspective, the model is based on the known cortical and subcortical anatomy of the primate oculomotor system. From a theoretical perspective, the architecture is similar to that of a finite automaton in which outputs and state transitions are generated as a function of inputs and the current state. Simulation results for complex sequence reproduction and sequence discrimination are presented. © 1995 Springer-Verlag.",10.1007/BF00201428,1995.0,93.0,,"animal; article; artificial neural network; computer simulation; human; physiology; psychomotor performance; theoretical model; Animal; Computer Simulation; Human; Models, Theoretical; Neural Networks (Computer); Psychomotor Performance; Support, Non-U.S. Gov't",2-s2.0-0029352040
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Eliasmith, C., Stewart, T.C., Choo, X., Bekolay, T., DeWolf, T., Tang, Y., A large-scale model of the functioning brain (2012) Science, 338 (6111), pp. 1202-1205",A large-scale model of the functioning brain,"Eliasmith C., Stewart T.C., Choo X., Bekolay T., DeWolf T., Tang C., Rasmussen D.",A large-scale model of the functioning brain,"A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called ""Spaun"") that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.",10.1126/science.1225266,2012.0,490.0,,neurotransmitter; anatomy; brain; cognition; neurology; numerical model; psychology; article; biological model; brain function; brain nerve cell; learning; memory; nerve cell network; neuroanatomy; neurophysiology; neuropsychology; neuroscience; priority journal; simulation; task performance,2-s2.0-84870209909
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Elman, J.L., Finding structure in time (1990) Cognitive Science, 14, pp. 179-211",Finding structure in time,Elman J.L.,Finding structure in time,"Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction. © 1990.",10.1016/0364-0213(90)90002-E,1990.0,5662.0,,,2-s2.0-26444565569
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Elman, J.L., Distributed representations, simple recurrent networks, and grammatical structure (1991) Machine Learning, 7, pp. 195-225",and grammatical structure,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Elman, J.L., Language as a dynamical system (1995) Mind as motion: explorations in the dynamics of cognition, pp. 195-223. , R.F. Port T. van Gelder MIT Press Cambridge (MA)",Language as a dynamical system,Elman J.L.,Language as a dynamical system,[No abstract available],,1995.0,157.0,,,2-s2.0-0000894759
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Farkas, I., Crocker, M.W., Syntactic systematicity in sentence processing with a recurrent self-organizing network (2008) Neurocomputing, 71, pp. 1172-1179",Syntactic systematicity in sentence processing with a recurrent self-organizing network,"Farkaš I., Crocker M.W.",Syntactic systematicity in sentence processing with a recurrent self-organizing network,"As potential candidates for explaining human cognition, connectionist models of sentence processing must demonstrate their ability to behave systematically, generalizing from a small training set. It has recently been shown that simple recurrent networks and, to a greater extent, echo-state networks possess some ability to generalize in artificial language learning tasks. We investigate this capacity for a recently introduced model that consists of separately trained modules: a recursive self-organizing module for learning temporal context representations and a feedforward two-layer perceptron module for next-word prediction. We show that the performance of this architecture is comparable with echo-state networks. Taken together, these results weaken the criticism of connectionist approaches, showing that various general recursive connectionist architectures share the potential of behaving systematically. © 2008 Elsevier B.V. All rights reserved.",10.1016/j.neucom.2007.11.025,2008.0,15.0,Next-word prediction; Recurrent neural network; Self-organization; Systematicity,Data processing; Feedforward neural networks; Learning algorithms; Self organizing maps; Syntactics; Artificial language learning; Next-word prediction; Syntactic systematicity; Recurrent neural networks; accuracy; article; artificial neural network; automatic speech recognition; computer language; computer model; controlled study; information processing; intermethod comparison; network learning; perceptron; prediction; priority journal; process control; speech articulation; system analysis,2-s2.0-40649098781
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Fernández-García, S., Desroches, M., Krupa, M., Clément, F., A multiple time scale coupling of piecewise linear oscillators. Application to a neuroendocrine system (2015) SIAM Journal on Applied Dynamical Systems, 14 (2), pp. 643-673",A multiple time scale coupling of piecewise linear oscillators. Application to a neuroendocrine system,"Fernández-Garćia S., Desroches M., Krupa M., Clément F.",A multiple time scale coupling of piecewise linear oscillators. Application to a neuroendocrine system,"We analyze a four-dimensional slow-fast piecewise linear system consisting of two coupled McKean caricatures of the FitzHugh-Nagumo system. Each oscillator is a continuous slow-fast piecewise linear system with three zones of linearity. The coupling is one-way, that is, one subsystem evolves independently and is forcing the other subsystem. In contrast to the original FitzHugh-Nagumo system, we consider a negative slope of the linear nullcline in both the forcing and the forced system. In the forcing system, this lets us, by just changing one parameter, pass from a system having one equilibrium and a relaxation cycle to a system with three equilibria keeping the relaxation cycle. Thus, we can easily control the changes in the oscillation frequency of the forced system. The case with three equilibria and a linear slow nullcline is a new configuration of the McKean caricature, where the existence of the relaxation cycle was not studied previously. We also consider a negative slope of the y-nullcline in the forced system that enables us to reproduce a quasi-steady state called the surge. We analyze not only the qualitative behavior of the four-dimensional system, but also quantitative aspects such as the period, frequency, and amplitude of the oscillations. The system is used to reproduce all the features endowed in a former smooth model and reproduce the secretion pattern of the hypothalamic neurohormone GnRH along the ovarian cycle in different species. © 2015 Society for Industrial and Applied Mathematics.",10.1137/140984464,2015.0,13.0,Coupled oscillators; GnRH secretion; Piecewise linear systems; Relaxation oscillations; Slow-fast dynamics,Linear systems; Oscillators (electronic); Oscillators (mechanical); Physiology; Relaxation oscillators; Coupled oscillators; GnRH secretion; Piece-wise linear systems; Relaxation oscillation; Slow-fast dynamics; Piecewise linear techniques,2-s2.0-84937898515
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Frank, S.L., Otten, L.J., Galli, G., Vigliocco, G., The ERP response to the amount of information conveyed by words in sentences (2015) Brain and Language, 140, pp. 1-11",The ERP response to the amount of information conveyed by words in sentences,"Frank S.L., Otten L.J., Galli G., Vigliocco G.",The ERP response to the amount of information conveyed by words in sentences,"Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence's hierarchical structure for generating expectations about the upcoming word. © 2014 The Authors.",10.1016/j.bandl.2014.10.006,2015.0,111.0,Entropy; Event-related potentials; Information theory; Reading; Sentence comprehension; Surprisal,adult; amplitude modulation; Article; artificial neural network; controlled study; entropy; event related potential; female; grammar; human; human experiment; language processing; male; measurement accuracy; normal human; reading; signal transduction; word recognition; evoked response; language; linguistics; physiology; probability; reading; Adult; Evoked Potentials; Female; Humans; Language; Linguistics; Male; Probability; Reading,2-s2.0-84910611051
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Frisch, S., beim Graben, P., Schlesewsky, M., Parallelizing grammatical functions: P600 and P345 reflect different cost of reanalysis (2004) International Journal of Bifurcation and Chaos, 14 (2), pp. 531-549",Parallelizing grammatical functions: P600 and P345 reflect different cost of reanalysis,"Frisch S., Graben P.B., Schlesewsky M.",Parallelizing grammatical functions: P600 and P345 reflect different cost of reanalysis,"It is well-known from psycholinguistic literature that the human language processing system exhibits preferences when sentence constituents are ambiguous with respect to their grammatical function. Generally, many theories assume that an interpretation towards the subject is preferred in such cases. Later disambiguations which contradict such a preference induce enhanced processing difficulty (i.e. reanalysis) which reflects itself in late positive deflections (P345/P600) in event-related brain potentials (ERPs). In the case of phoric elements such as pronouns, a second strategy is known according to which an ambiguous pronoun preferentially receives the grammatical function that its antecedent has (parallel function strategy). In an ERP study, we show that this strategy can in principle override the general subject preference strategy (known for both pronominal and nonpronominal constituents) and induce an object preference, in case that the pronoun's antecedent is itself an object. Interestingly, the revision of a subject preference leads to a P600 component, whereas the revision of an object preference induces an earlier positivity (P345). In order to show that the latter component is indeed a positivity and not an N400-like negativity in the same time range, we apply an additional analysis based on symbolic dynamics which allows to determine the polarity of an ERP effect on purely methodological grounds. With respect to the two positivities, we argue that the latency differences reflect qualitative differences in the reanalysis processes.",10.1142/S0218127404009533,2004.0,17.0,Event-related brain potentials (ERP); P345; P600; Parallel function; Subject preference; Symbolic dynamics,Codes (symbols); Computational linguistics; Computer programming languages; Context free grammars; Formal languages; Information analysis; Parallel processing systems; Syntactics; Event-related brain potentials (ERP); P345; P600; Parallel functions; Subject preference; Symbolic dynamics; Functions,2-s2.0-3242723750
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Gayler, R.W., Vector symbolic architectures are a viable alternative for Jackendoff's challenges (2006) Behavioral and Brain Sciences, 29, pp. 78-79",Vector symbolic architectures are a viable alternative for Jackendoff's challenges,Gayler R.W.,Vector symbolic architectures are a viable alternative for Jackendoff's challenges,"The authors, on the basis of brief arguments, have dismissed tensor networks as a viable response to Jackendoff's challenges. However, there are reasons to believe that connectionist approaches descended from tensor networks are actually very well suited to answering Jackendoff's challenges. I rebut their arguments for dismissing tensor networks and briefly compare the approaches. © 2006 Cambridge University Press.",10.1017/S0140525X06309028,2006.0,13.0,,article; artificial neural network; automated pattern recognition; computer language; computer memory; computer network; computer program; intermethod comparison; mathematical computing; theoretical study,2-s2.0-33644992342
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Gayler, R.W., Levy, S.D., Bod, R., Explanatory aspirations and the scandal of cognitive neuroscience (2010) Proceedings of the 2010 conference on biologically inspired cognitive architectures 2010: proceedings of the first annual meeting of the BICA society, pp. 42-51. , A.V. Samsonovich K.R. Johannsdottir A. Chella B. Goertzel IOS Press Amsterdam",Explanatory aspirations and the scandal of cognitive neuroscience,"Gayler R.W., Levy S.D., Bod R.",Explanatory aspirations and the scandal of cognitive neuroscience,"In this position paper we argue that BICA must simultaneously be compatible with the explanation of human cognition and support the human design of artificial cognitive systems. Most cognitive neuroscience models fail to provide a basis for implementation because they neglect necessary levels of functional organisation in jumping directly from physical phenomena to cognitive behaviour. Of those models that do attempt to include the intervening levels, most either fail to implement the required cognitive functionality or do not scale adequately. We argue that these problems of functionality and scaling arise because of identifying computational entities with physical resources such as neurons and synapses. This issue can be avoided by introducing appropriate virtual machines. We propose a tool stack that introduces such virtual machines and supports design of cognitive architectures by simplifying the design task through vertical modularity. © 2010 The authors and IOS Press. All rights reserved.",10.3233/978-1-60750-661-4-42,2010.0,1.0,cognitive neuroscience; Data-Oriented Processing; Jackendoff's challenges; Neural Engineering Framework; Vector Symbolic Architecture,Architecture; Data handling; Neurology; Cognitive architectures; Cognitive neurosciences; Computational entities; Data-oriented processing; Jackendoff's challenges; Neural engineering; Physical phenomena; Physical resources; Cognitive systems,2-s2.0-78149384084
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Gigley, H.M., Computational neurolinguistics: What is it all about? (1985) Proceedings of the 9th international joint conference on artificial intelligence, 1, pp. 260-266. , IJCAI’85, San Francisco (CA)",Computational neurolinguistics: What is it all about?,Gigley H.,Computational neurolinguistics: What is it all about?,[No abstract available],,1985.0,2.0,,,2-s2.0-84994376644
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Girardi-Schappo, M., Tragtenberg, M., Kinouchi, O., A brief history of excitable map-based neurons and neural networks (2013) Journal of Neuroscience Methods, 220 (2), pp. 116-130",A brief history of excitable map-based neurons and neural networks,"Girardi-Schappo M., Tragtenberg M.H.R., Kinouchi O.",A brief history of excitable map-based neurons and neural networks,"This review gives a short historical account of the excitable maps approach for modeling neurons and neuronal networks. Some early models, due to Pasemann (1993), Chialvo (1995) and Kinouchi and Tragtenberg (1996), are compared with more recent proposals by Rulkov (2002) and Izhikevich (2003). We also review map-based schemes for electrical and chemical synapses and some recent findings as critical avalanches in map-based neural networks. We conclude with suggestions for further work in this area like more efficient maps, compartmental modeling and close dynamical comparison with conductance-based models. © 2013 Elsevier B.V.",10.1016/j.jneumeth.2013.07.014,2013.0,40.0,Bursting; Coupled map lattices; Difference equations; Excitable dynamics; Excitable media; Map-based neuron; Map-based synapses; Neural networks; Neuron models,"brain mapping; computer model; Hodgkin Huxley equation; ion current; mathematical model; membrane potential; molecular dynamics; nerve cell; nerve cell excitability; nerve cell network; nerve conduction; oscillator; pacemaker; presynaptic nerve; priority journal; review; Bursting; Coupled map lattices; Difference equations; Excitable dynamics; Excitable media; Map-based neuron; Map-based synapses; Neural networks; Neuron models; Action Potentials; Animals; Humans; Models, Neurological; Nerve Net; Neural Networks (Computer); Neurons",2-s2.0-84887618382
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Gödel, K., Über formal unentscheidbare sätze der Principia mathematica und verwandter systeme I (1931) Monatshefte für Mathematik und Physik, 38, pp. 173-198",Über formal unentscheidbare sätze der Principia mathematica und verwandter systeme I,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Golubitsky, M., Stewart, I., Buono, P.-L., Collins, J.J., A modular network for legged locomotion (1998) Physica D, 115 (1-2), pp. 56-72",A modular network for legged locomotion,"Golubitsky M., Stewart I., Buono P.-L., Collins J.J.",A modular network for legged locomotion,"In this paper we use symmetry methods to study networks of coupled cells, which are models for central pattern generators (CPGs). In these models the cells obey identical systems of differential equations and the network specifies how cells are coupled. Previously, Collins and Stewart showed that the phase relations of many of the standard gaits of quadrupeds and hexapods can be obtained naturally via Hopf bifurcation in small networks. For example, the networks they used to study quadrupeds all had four cells, with the understanding that each cell determined the phase of the motion of one leg. However, in their work it seemed necessary to employ several different four-oscillator networks to obtain all of the standard quadrupedal gaits. We show that this difficulty with four-oscillator networks is unavoidable, but that the problems can be overcome by using a larger network. Specifically, we show that the standard gaits of a quadruped, including walk, trot and pace, cannot all be realized by a single four-cell network without introducing unwanted conjugacies between trot and pace - conjugacies that imply a dynamic equivalence between these gaits that seems inconsistent with observations. In this sense a single network with four cells cannot model the CPG of a quadruped. We also introduce a single eight-cell network that can model all of the primary gaits of quadrupeds without these unwanted conjugacies. Moreover, this network is modular in that it naturally generalizes to provide models of gaits in hexapods, centipedes, and millipedes. The analysis of models for many-legged animals shows that wave-like motions, similar to those obtained by Kopell and Ermentrout, can be expected. However, our network leads to a prediction that the wavelength of the wave motion will divide twice the length of the animal. Indeed, we reproduce illustrations of wave-like motions in centipedes where the animal is approximately one-and-a-half wavelength long - motions that are consistent with this prediction. We discuss the implications of these results for the development of modular control networks for adaptive legged robots. Copyright © 1998 Elsevier Science B.V.",10.1016/S0167-2789(97)00222-4,1998.0,163.0,Control pattern generator; Gaits; Hopf bifurcation; Symmetry,,2-s2.0-0002659841
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Golubitsky, M., Stewart, I., Buono, P.-L., Collins, J.J., Symmetry in locomotor central pattern generators and animal gaits (1999) Nature, 401 (6754), pp. 693-695",Symmetry in locomotor central pattern generators and animal gaits,"Golubitsky M., Stewart I., Buono P.-L., Collins J.J.",Symmetry in locomotor central pattern generators and animal gaits,"Animal locomotion is controlled, in part, by a central pattern generator (CPG), which is an intraspinal network of neurons capable of generating a rhythmic output. The spatio-temporal symmetries of the quadrupedal gaits walk, trot and pace lead to plausible assumptions about the symmetries of locomotor CPGs. These assumptions imply that the CPG of a quadruped should consist of eight nominally identical subcircuits, arranged in an essentially unique matter. Here we apply analogous arguments to myriapod CPGs. Analyses based on symmetry applied to these networks lead to testable predictions, including a distinction between primary and secondary gaits, the existence of a new primary gait called 'jump', and the occurrence of half-integer wave numbers in myriapod gaits. For bipeds, our analysis also predicts two gaits with the out-of-phase symmetry of the walk and two gaits with the in-phase symmetry of the hop. We present data that support each of these predictions. This work suggests that symmetry can be used to infer a plausible class of CPG network architectures from observed patterns of animal gaits.",10.1038/44416,1999.0,281.0,,"article; gait; locomotion; nerve cell network; prediction; priority journal; Animals; Gait; Locomotion; Models, Biological; Nerve Net; Neurons; Spinal Cord",2-s2.0-0033554720
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Graves, A., Wayne, G., Danihelka, I., (2014) Neural turing machines, , Preprint. [cs.NE]. arXiv:1511.01427",,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Grefenstette, E., Hermann, K.M., Suleyman, M., Blunsom, P., Learning to transduce with unbounded memory (2015) Advances in neural information processing systems, pp. 1819-1827",Learning to transduce with unbounded memory,"Grefenstette E., Hermann K.M., Suleyman M., Blunsom P.",Learning to transduce with unbounded memory,"Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.",,2015.0,109.0,,Bacteriophages; Information science; Continuously differentiable; Generalisation; Machine translations; Natural languages; Recurrent networks; Unbounded memory; Recurrent neural networks,2-s2.0-84965153738
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Grillner, S., Zangger, P., How detailed is the central pattern generation for locomotion? (1975) Brain Research, 88 (2), pp. 367-371",How detailed is the central pattern generation for locomotion?,"Grillner S., Zangger P.",How detailed is the central pattern generation for locomotion?,[No abstract available],10.1016/0006-8993(75)90401-1,1975.0,152.0,,"cat; cuneiform nucleus; electromyography; extensor muscle; flexor muscle; hindlimb; locomotion; theoretical study; Animal; Cats; Denervation; Electric Stimulation; Electromyography; Ganglia, Spinal; Hindlimb; Locomotion; Neural Pathways; Neurons, Afferent; Spinal Cord; Spinal Nerves",2-s2.0-0016770333
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Hebb, D.O., The organization of behavior (1949), Wiley New York (NY) Partly reprinted in J. A. Anderson and E. Rosenfeld (1988), pp. 45ff",The organization of behavior,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Hertz, J., Krogh, A., Palmer, R.G., (1991) Introduction to the theory of neural computation, Lecture notes of the Santa Fe institute studies in the science of complexity, , Perseus Books Cambridge (MA)",,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Hinaut, X., Dominey, P.F., Real-time parallel processing of grammatical structure in the fronto-striatal system: A recurrent network simulation study using reservoir computing (2013) PLoS One, 8 (2), p. e52946",Real-time parallel processing of grammatical structure in the fronto-striatal system: A recurrent network simulation study using reservoir computing,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Hinaut, X., Petit, M., Pointeau, G., Dominey, P.F., Exploring the acquisition and production of grammatical constructions through human–robot interaction with echo state networks (2014) Frontiers in Neurorobotics, 8 (16)",Exploring the acquisition and production of grammatical constructions through human–robot interaction with echo state networks,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Hodgkin, A.L., Huxley, A.F., A quantitative description of membrane current and its application to conduction and excitation in nerve (1952) The Journal of Physiology, 117 (4), p. 500",A quantitative description of membrane current and its application to conduction and excitation in nerve,"Hodgkin A.L., Huxley A.F.",A quantitative description of membrane current and its application to conduction and excitation in nerve,[No abstract available],10.1113/jphysiol.1952.sp004764,1952.0,13225.0,,article; nerve fiber; AXONS; Axons,2-s2.0-35649001607
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Hopcroft, J.E., Ullman, J.D., Introduction to automata theory, languages, and computation (1979), Addison–Wesley Menlo Park, California",and computation,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Huyck, C.R., A psycholinguistic model of natural language parsing implemented in simulated neurons (2009) Cognitive Neurodynamics, 3 (4), pp. 317-330",A psycholinguistic model of natural language parsing implemented in simulated neurons,Huyck C.R.,A psycholinguistic model of natural language parsing implemented in simulated neurons,"A natural language parser implemented entirely in simulated neurons is described. It produces a semantic representation based on frames. It parses solely using simulated fatiguing Leaky Integrate and Fire neurons, that are a relatively accurate biological model that is simulated efficiently. The model works on discrete cycles that simulate 10 ms of biological time, so the parser has a simple mapping to psychological parsing time. Comparisons to human parsing studies show that the parser closely approximates this data. The parser makes use of Cell Assemblies and the semantics of lexical items is represented by overlapping hierarchical Cell Assemblies so that semantically related items share neurons. This semantic encoding is used to resolve prepositional phrase attachment ambiguities encountered during parsing. Consequently, the parser provides a neurally-based cognitive model of parsing. © 2009 Springer Science+Business Media B.V.",10.1007/s11571-009-9080-6,2009.0,19.0,Fatiguing Leaky Integrate and Fire (fLIF) neurons; Natural language parsing; Prepositional phrase attachment; Timing,ambiguity; article; biological model; controlled study; language; linguistics; nerve cell; neuropsychology; semantics; simulation; speech,2-s2.0-72249096236
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Ibarz, B., Casado, J.M., Sanjuán, M.A., Map-based models in neuronal dynamics (2011) Physics Reports, 501 (1), pp. 1-74",Map-based models in neuronal dynamics,"Ibarz B., Casado J.M., Sanjuán M.A.F.",Map-based models in neuronal dynamics,"Ever since the pioneering work of Hodgkin and Huxley, biological neuron models have consisted of ODEs representing the evolution of the transmembrane voltage and the dynamics of ionic conductances. It is only recently that discrete dynamical systems-also known as maps-have begun to receive attention as valid phenomenological neuron models. The present review tries to provide a coherent perspective of map-based biological neuron models, describing their dynamical properties; stressing the similarities and differences, both among them and in relation to continuous-time models; exploring their behavior in networks; and examining their wide-ranging possibilities of application in computational neuroscience. © 2010 Elsevier B.V.",10.1016/j.physrep.2010.12.003,2011.0,169.0,Discrete-time; Map-based; Neural networks; Neuron models; Nonlinear dynamics,,2-s2.0-79953039734
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Ijspeert, A.J., Central pattern generators for locomotion control in animals and robots: a review (2008) Neural Networks, 21 (4), pp. 642-653",Central pattern generators for locomotion control in animals and robots: a review,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Jaeger, H., The echo state approach to analysing and training recurrent neural networks-with an erratum note. Bonn, Germany: German National Research Center for Information Technology GMD Technical Report, 148:34 (2001)",148:34,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Jansen, B.H., Rit, V.G., Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns (1995) Biological Cybernetics, 73, pp. 357-366",Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns,"Jansen B.H., Rit V.G.",Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns,"This study deals with neurophysiologically based models simulating electrical brain activity (i.e., the electroencephalogram or EEG, and evoked potentials or EPs). A previously developed lumped-parameter model of a single cortical column was implemented using a more accurate computational procedure. Anatomically acceptable values for the various model parameters were determined, and a multi-dimensional exploration of the model parameter-space was conducted. It was found that the model could produce a large variety of EEG-like waveforms and rhythms. Coupling two models, with delays in the interconnections to simulate the synaptic connections within and between cortical areas, made it possible to replicate the spatial distribution of alpha and beta activity. EPs were simulated by presenting pulses to the input of the coupled models. In general, the responses were more realistic than those produced using a single model. Our simulations also suggest that the scalp-recorded EP is at least partially due to a phase reordering of the ongoing activity. © 1995 Springer-Verlag.",10.1007/BF00199471,1995.0,634.0,,"animal; article; biological model; brain cortex; cat; electroencephalography; evoked visual response; human; mathematics; nerve cell; physiology; scalp; Animal; Cats; Cerebral Cortex; Electroencephalography; Evoked Potentials, Visual; Human; Mathematics; Models, Neurological; Neurons; Scalp",2-s2.0-0029374946
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Joulin, A., Mikolov, T., Inferring algorithmic patterns with stack-augmented recurrent nets (2015) Advances in neural information processing systems, pp. 190-198",Inferring algorithmic patterns with stack-augmented recurrent nets,"Joulin A., Mikolov T.",Inferring algorithmic patterns with stack-augmented recurrent nets,"Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.",,2015.0,141.0,,Algorithms; Artificial intelligence; Information science; Learning systems; Deep learning; Recurrent networks; Sequence prediction; Sequential data; Complex networks,2-s2.0-84965117324
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Kleene, S., Neural nets and automata (1956) Automata studies, pp. 3-43",Neural nets and automata,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Kohonen, T., Self-organized formation of topologically correct feature maps (1982) Biological Cybernetics, 43 (1), pp. 59-69",Self-organized formation of topologically correct feature maps,Kohonen T.,Self-organized formation of topologically correct feature maps,"This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails. © 1982 Springer-Verlag.",10.1007/BF00337288,1982.0,5891.0,,auditory system; biological model; brain; central nervous system; computer model; cortex; gustatory system; nerve cell network; nervous system; nonbiological model; normal human; olfactory system; pattern recognition; perception; vestibular system; visual system,2-s2.0-0020068152
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Kohonen, T., Somervuo, P., Self-organizing maps of symbol strings (1998) Neurocomputing, 21 (1), pp. 19-30",Self-organizing maps of symbol strings,"Kohonen T., Somervuo P.",Self-organizing maps of symbol strings,"Unsupervised self-organizing maps (SOMs), as well as supervised learning by Learning Vector Quantization (LVQ) can be defined for string variables, too. Their computing becomes possible when the SOM and the LVQ algorithms are expressed as batch versions, and when the average over a list of symbol strings is defined to be the string that has the smallest sum of generalized distance functions from all the other strings.Unsupervised self-organizing maps (SOMs), as well as supervised learning by Learning Vector Quantization (LVQ) can be defined for string variables, too. Their computing becomes possible when the SOM and the LVQ algorithms are expressed as batch versions, and when the average over a list of symbol strings is defined to be the string that has the smallest sum of generalized distance functions from all the other strings.",10.1016/S0925-2312(98)00031-9,1998.0,192.0,Learning vector quantization; Self-organizing map; String clustering,Algorithms; Codes (symbols); Learning systems; Vector quantization; Learning vector quantization (LVQ); Self-organizing map (SOM); String clustering; Symbol strings; Neural networks; algorithm; article; associative memory; clinical article; cluster analysis; human; learning; mathematical computing; nerve cell network; phoneme; priority journal; symbolism; system analysis; word recognition,2-s2.0-0344972931
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Krupa, M., Robust heteroclinic cycles (1997) Journal of Nonlinear Science, 7 (2), pp. 129-176",Robust heteroclinic cycles,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Lawrence, S., Giles, C.L., Fong, S., Natural language grammatical inference with recurrent neural networks (2000) The IEEE Transactions on Knowledge and Data Engineering, 12 (1), pp. 126-140",Natural language grammatical inference with recurrent neural networks,"Lawrence S., Giles C.L., Fong S.",Natural language grammatical inference with recurrent neural networks,"This paper examines the inductive inference of a complex grammar with neural networks - specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-through-time training algorithm, significant learning was possible. It was found that certain architectures are better able to learn an appropriate grammar. The operation of the networks and their training is analyzed. Finally, the extraction of rules in the form of deterministic finite state automata is investigated. © 2000 IEEE.",10.1109/69.842255,2000.0,89.0,Automata extraction; Government-and-binding theory; Gradient descent; Grammatical inference; Natural language processing; Principles-and-parameters framework; Recurrent neural networks; Simulated annealing,,2-s2.0-33747598711
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Lewis, R.L., Reanalysis and limited repair parsing: Leaping off the garden path (1998) Reanalysis in sentence processing, pp. 247-285. , J.D. Fodor F. Ferreira Kluwer Dordrecht",Reanalysis and limited repair parsing: Leaping off the garden path,Lewis R.L.,Reanalysis and limited repair parsing: Leaping off the garden path,[No abstract available],,1998.0,36.0,,,2-s2.0-0008662253
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Li, D., A tutorial survey of architectures, algorithms, and applications for deep learning (2014) APSIPA Transactions on Signal and Information Processing, 3",and applications for deep learning,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Lind, D., Marcus, B., An introduction to symbolic dynamics and coding (1995), Cambridge University Press Cambridge (UK) Reprint 1999",An introduction to symbolic dynamics and coding,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Lopes da Silva, F.H., Hoecks, A., Smits, H., Zetterberg, L.H., Model of brain rhythmic activity: The Alpha-rhythm of the thalamus (1974) Kybernetik, 15, pp. 27-37",Model of brain rhythmic activity: The Alpha-rhythm of the thalamus,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Maass, W., Natschläger, T., Markram, H., Real-time computing without stable states: A new framework for neural computation based on perturbations (2002) Neural Computation, 14 (11), pp. 2531-2560",Real-time computing without stable states: A new framework for neural computation based on perturbations,"Maass W., Natschläger T., Markram H.",Real-time computing without stable states: A new framework for neural computation based on perturbations,"A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.",10.1162/089976602760407955,2002.0,1896.0,,"action potential; article; artificial neural network; biological model; computer; computer simulation; computer system; nerve cell; physiology; Action Potentials; Computer Simulation; Computer Systems; Computers; Models, Neurological; Neural Networks (Computer); Neurons",2-s2.0-0036834701
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," McClelland, J.L., Elman, J.L., The TRACE model of speech perception (1986) Cognitive Psychology, 18 (1), pp. 1-86",The TRACE model of speech perception,"McClelland J.L., Elman J.L.",The TRACE model of speech perception,"We describe a model called the TRACE model of speech perception. The model is based on the principles of interactive activation. Information processing takes place through the excitatory and inhibitory interactions of a large number of simple processing units, each working continuously to update its own activation on the basis of the activations of other units to which it is connected. The model is called the TRACE model because the network of units forms a dynamic processing structure called ""the Trace,"" which serves at once as the perceptual processing mechanism and as the system's working memory. The model is instantiated in two simulation programs. TRACE I, described in detail elsewhere, deals with short segments of real speech, and suggests a mechanism for coping with the fact that the cues to the identity of phonemes vary as a function of context. TRACE II, the focus of this article, simulates a large number of empirical findings on the perception of phonemes and words and on the interactions of phoneme and word perception. At the phoneme level, TRACE II simulates the influence of lexical information on the identification of phonemes and accounts for the fact that lexical effects are found under certain conditions but not others. The model also shows how knowledge of phonological constraints can be embodied in particular lexical items but can still be used to influence processing of novel, nonword utterances. The model also exhibits categorical perception and the ability to trade cues off against each other in phoneme identification. At the word level, the model captures the major positive feature of Marslen-Wilson's COHORT model of speech perception, in that it shows immediate sensitivity to information favoring one word or set of words over others. At the same time, it overcomes a difficulty with the COHORT model: it can recover from underspecification or mispronunciation of a word's beginning. TRACE II also uses lexical information to segment a stream of speech into a sequence of words and to find word beginnings and endings, and it simulates a number of recent findings related to these points. The TRACE model has some limitations, but we believe it is a step toward a psychologically and computationally adequate model of the process of speech perception. © 1986.",10.1016/0010-0285(86)90015-0,1986.0,1755.0,,"article; association; computer program; human; model; phonetics; semantics; speech perception; Cues; Human; Models, Psychological; Phonetics; Semantics; Software; Speech Perception; Support, U.S. Gov't, Non-P.H.S.; Support, U.S. Gov't, P.H.S.",2-s2.0-0022571705
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," McCulloch, W.S., Pitts, W., A logical calculus of ideas immanent in nervous activity (1943) Bulletin of Mathematical Biophysics, 5, pp. 115-133",A logical calculus of ideas immanent in nervous activity,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," McGhee, R.B., Some finite state aspects of legged locomotion (1968) Mathematical Biosciences, 2 (1-2), pp. 67-84",Some finite state aspects of legged locomotion,McGhee R.B.,Some finite state aspects of legged locomotion,Animal locomotion systems making use of legs as the basic component for support and propulsion can be studied from the point of view of finite state machine theory by regarding each leg as an elementary two-state sequential machine. The two states are simply the state of being in contact with the supporting surface and the state of being raised above it. This idealization permits the construction of a general theory of locomotion equally applicable to animals and legged locomotion machines. Such a theory can be made sufficiently complete to permit the synthesis of finite control algorithms capable of coordinating limb movements in either animals or machines. The validity of the finite state approach has been established by the construction and testing of an artificial quadruped based entirely upon finite state principles. © 1968.,10.1016/0025-5564(68)90007-2,1968.0,110.0,,,2-s2.0-34848851673
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Minsky, M., Size and structure of universal Turing machines using tag systems (1962) Recursive function theory: proceedings, symposium in pure mathematics, 5, pp. 229-238",Size and structure of universal Turing machines using tag systems,Minsky M.,Size and structure of universal Turing machines using tag systems,[No abstract available],,1962.0,55.0,,,2-s2.0-0040534095
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Minsky, M.L., Computation: finite and infinite machines (1967), Prentice-Hall, Inc",Computation: finite and infinite machines,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Mizraji, E., Context-dependent associations in linear distributed memories (1989) Bulletin of Mathematical Biology, 51 (2), pp. 195-205",Context-dependent associations in linear distributed memories,Mizraji E.,Context-dependent associations in linear distributed memories,"In this article we present a method that allows conditioning of the response of a linear distributed memory to a variable context. This method requires a system of two neural networks. The first net constructs the Kronecker product between the vector input and the vector context, and the second net supports a linear associative memory. This system is easily adaptable for different goals. We analyse here its capacity for the conditional extraction of features from a complex perceptual input, its capacity to perform quasi-logical operations (for instance, of the kind of ""exclusive-or""), and its capacity to structurate a memory for temporal sequences which access is conditioned by the context. Finally, we evaluate the potential importance of the capacity to establish arbitrary contexts, for the evolution of biological cognitive systems. © 1989 Society for Mathematical Biology.",10.1007/BF02458441,1989.0,34.0,,"article; artificial intelligence; biological model; human; memory; model; nerve cell network; physiology; theoretical model; Artificial Intelligence; Human; Memory; Models, Neurological; Models, Psychological; Models, Theoretical; Nerve Net",2-s2.0-0024526764
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Moore, C., Unpredictability and undecidability in dynamical systems (1990) Physical Review Letters, 64 (20), pp. 2354-2357",Unpredictability and undecidability in dynamical systems,Moore C.,Unpredictability and undecidability in dynamical systems,"We show that motion with as few as three degrees of freedom (for instance, a particle moving in a three-dimensional potential) can be equivalent to a Turing machine, and so be capable of universal computation. Such systems possess a type of unpredictability qualitatively stronger than that which has been previously discussed in the study of low-dimensional chaos: Even if the initial conditions are known exactly, virtually any question about their long-term dynamics is undecidable. © 1990 The American Physical Society.",10.1103/PhysRevLett.64.2354,1990.0,229.0,,,2-s2.0-0001102743
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Moore, C., Generalized shifts: unpredictability and undecidability in dynamical systems (1991) Nonlinearity, 4, pp. 199-230",Generalized shifts: unpredictability and undecidability in dynamical systems,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Neary, T., Woods, D., Four small universal Turing machines (2009) Fundamenta Informaticae, 91 (1), pp. 123-144",Four small universal Turing machines,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Osterhout, L., Holcomb, P.J., Swinney, D.A., Brain potentials elicited by garden-path sentences: Evidence of the application of verb information during parsing (1994) Journal of Experimental Psychology: Learning, Memory, and Cognition, 20 (4), pp. 786-803",Brain potentials elicited by garden-path sentences: Evidence of the application of verb information during parsing,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Rabinovich, M.I., Huerta, R., Varona, P., Afraimovich, V.S., Transient cognitive dynamics, metastability, and decision making (2008) PLoS Computational Biology, 4 (5), p. e1000072",and decision making,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Schöner, G., Jiang, W.Y., Kelso, J.A.S., A synergetic theory of quadrupedal gaits and gait transitions (1990) Journal of Theoretical Biology, 142 (3), pp. 359-391",A synergetic theory of quadrupedal gaits and gait transitions,"Schöner G., Jiang W.Y., Kelso J.A.S.",A synergetic theory of quadrupedal gaits and gait transitions,"We present a theoretical analysis of the patterns of interlimb co-ordination in the gaits of quadrupedal locomotion. Introducing as collective variables a set of relative phases that describe the co-ordination patterns, we classify gaits by their symmetry properties, which can be expressed as invariances under groups of transformations. We define dynamics of the collective variables, on which we impose symmetry restrictions. The stable observable gait patterns correspond to atractors of these dynamics. A non-trivial consequence of this theoretical viewpoint is that gait transitions can take the form of non-equilibrium phase transitions that are accompanied by loss of stability. We show how various types of such phase transitions involving hysteresis, slowing down and fluctuation enhancement can occur. Also the difference between smooth and abrupt transitions is given theoretical foundation. While existing experimental evidence is consistent with the theory developed here, we propose new experimental measures that can serve to test the present theoretical framework. Finally, the influence of underlying symmetries of the dynamics on the nature of the gait patterns and their stability is analyzed. For example, breaking of a front-hind symmetry can lead to a change from absolute to relative co-ordination in the sense of von Holst (1939, Ergebnisse der Physiologie 42, 228). Also, differential stability of straight and reverse gaits results from thus lowering the symmetry. © 1990 Academic Press Limited.",10.1016/S0022-5193(05)80558-2,1990.0,237.0,,"article; gait; horse; locomotion; nonhuman; priority journal; theoretical study; Gait; Models, Biological; Support, U.S. Gov't, Non-P.H.S.; Support, U.S. Gov't, P.H.S.; Equus caballus",2-s2.0-0025064144
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Sejnowski, T.J., Rosenberg, C.R., Parallel networks that learn to pronounce English text (1987) Complex Systems, 1, pp. 145-168",Parallel networks that learn to pronounce English text,"Sejnowski T.J., Rosenberg C.R.",Parallel networks that learn to pronounce English text,[No abstract available],,1987.0,922.0,,,2-s2.0-0000383868
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Shik, M.L., Severin, F.V., Orlovsky, G.N., Control of walking and running by means of electrical stimulation of mid-brain (1966) Biophysics-USSR, 11 (4), p. 756",Control of walking and running by means of electrical stimulation of mid-brain,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Siegelmann, H.T., Sontag, E.D., Turing computability with neural nets (1991) Applied Mathematics Letters, 4 (6), pp. 77-80",Turing computability with neural nets,"Siegelmann H.T., Sontag E.D.",Turing computability with neural nets,"This paper shows the existence of a finite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 105 synchronously evolving processors, interconnected linearly. High-order connections are not required. © 1991.",10.1016/0893-9659(91)90080-F,1991.0,200.0,,,2-s2.0-0002663413
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Siegelmann, H.T., Sontag, E.D., On the computational power of neural nets (1995) Journal of Computer and System Sciences, 50 (1), pp. 132-150",On the computational power of neural nets,"Siegelmann H.T., Sontag E.D.",On the computational power of neural nets,"This paper deals with finite size networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a ""sigmoidal"" function to a linear combination of the previous states of all units. We prove that one may simulate all Turing machines by such nets. In particular, one can simulate any multi-stack Turing machine in real time, and there is a net made up of 886 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Non-deterministic Turing machines can be simulated by non-deterministic rational nets, also in real time. The simulation result has many consequences regarding the decidability, or more generally the complexity, of questions about recursive nets. © 1995 by Academic Press, Inc.",10.1006/jcss.1995.1013,1995.0,389.0,,Computability and decidability; Computational complexity; Computer simulation; Finite automata; Graph theory; Interconnection networks; Real time systems; Recursive functions; Synchronization; Turing machines; Non deterministic rational nets; Processor nets; Rational valued coefficient; Saturated linear function; Sigmoidal functions; Neural networks,2-s2.0-0029255891
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Sipser, M., Introduction to the theory of computation (2006), Thomson Course Technology Boston",Introduction to the theory of computation,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Smith, J.C., Abdala, A.P., Borgmann, A., Rybak, I.A., Paton, J.F., Brainstem respiratory networks: building blocks and microcircuits (2013) Trends in Neurosciences, 36 (3), pp. 152-162",Brainstem respiratory networks: building blocks and microcircuits,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Smith, J.C., Abdala, A., Koizumi, H., Rybak, I.A., Paton, J.F., Spatial and functional architecture of the mammalian brain stem respiratory network: a hierarchy of three oscillatory mechanisms (2007) Journal of Neurophysiology, 98 (6), pp. 3370-3387",Spatial and functional architecture of the mammalian brain stem respiratory network: a hierarchy of three oscillatory mechanisms,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Smolensky, P., Information processing in dynamical systems: Foundations of harmony theory (1986) Parallel distributed processing: explorations in the microstructure of cognition, Vol. I, pp. 194-281. , D.E. Rumelhart J.L. McClelland MIT Press Cambridge (MA) PDP Research Group (Chapter 6)",Information processing in dynamical systems: Foundations of harmony theory,Smolensky P.,Information processing in dynamical systems: Foundations of harmony theory,[No abstract available],,1986.0,1212.0,,,2-s2.0-0000329993
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Smolensky, P., Tensor product variable binding and the representation of symbolic structures in connectionist systems (1990) Artificial Intelligence, 46 (1-2), pp. 159-216",Tensor product variable binding and the representation of symbolic structures in connectionist systems,Smolensky P.,Tensor product variable binding and the representation of symbolic structures in connectionist systems,"A general method, the tensor product representation, is defined for the connectionist representation of value/variable bindings. The technique is a formalization of the idea that a set of value/variable pairs can be represented by accumulating activity in a collection of units each of which computes the product of a feature of a variable and a feature of its value. The method allows the fully distributed representation of bindings and symbolic structures. Fully and partially localized special cases of the tensor product representation reduce to existing cases of connectionist representations of structured data. The representation rests on a principled analysis of structure; it saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; and it enables analysis of the interference of symbolic structures stored in associative memories. It has also served as the basis for working connectionist models of high-level cognitive tasks. © 1990.",10.1016/0004-3702(90)90007-M,1990.0,427.0,,Data Processing--Data Structures; Connectionist Networks; Symbolic Structures; Neural Networks,2-s2.0-0025516779
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Smolensky, P., Legendre, G., The harmonic mind. From neural computation to optimality-theoretic grammar (2006) Cognitive architecture, 1. , MIT Press Cambridge (MA)",The harmonic mind. From neural computation to optimality-theoretic grammar,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Smolensky, P., Legendre, G., (2006) The harmonic mind. From neural computation to optimality-theoretic grammar, Linguistic and philosophic implications, 2. , MIT Press Cambridge (MA)",,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Spröwitz, A., Moeckel, R., Vespignani, M., Bonardi, S., Ijspeert, A.J., Roombots: A hardware perspective on 3d self-reconfiguration and locomotion with a homogeneous modular robot (2014) Robotics and Autonomous Systems, 62 (7), pp. 1016-1033",Roombots: A hardware perspective on 3d self-reconfiguration and locomotion with a homogeneous modular robot,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Steil, J.J., Backpropagation-decorrelation: online recurrent learning with O(N) complexity (2004) Proceedings of the 2004 IEEE international joint conference on neural networks, Vol. 2, pp. 843-848. , IEEE",Backpropagation-decorrelation: online recurrent learning with O(N) complexity,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Stewart, T.C., Choo, X., Eliasmith, C., Sentence processing in spiking neurons: A biologically plausible left-corner parser (2014) Proceedings of the cognitive science conference",Sentence processing in spiking neurons: A biologically plausible left-corner parser,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Sukhbaatar, S., Weston, J., Fergus, R., End-to-end memory networks (2015) Advances in neural information processing systems, pp. 2431-2439",End-to-end memory networks,"Sukhbaatar S., Szlam A., Weston J., Fergus R.",End-to-end memory networks,"We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.",,2015.0,1139.0,,Information science; Natural language processing systems; Attention model; End to end; External memory; Language model; Memory network; Question Answering; Treebanks; Modeling languages,2-s2.0-84965143740
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Tabor, W., Fractal encoding of context-free grammars in connectionist networks (2000) Expert Systems: The International Journal of Knowledge Engineering and Neural Networks, 17 (1), pp. 41-56",Fractal encoding of context-free grammars in connectionist networks,Tabor W.,Fractal encoding of context-free grammars in connectionist networks,"Connectionist network learning of context-free languages has so far been applied only to very simple cases and has often made use of an external stack. Learning complex context-free languages with a homogeneous neural mechanism looks like a much harder problem. The current paper takes a step toward solving this problem by analyzing context-free grammar computation (without addressing learning) in a class of analog computers called dynamical automata, which are naturally implemented in connectionist networks. The result is a widely applicable method of using fractal sets to organize infinite-state computations in a bounded state space. An appealing consequence is the development of parameter-space maps, which locate various complex computers in spatial relationships to one another. An example suggests that such a global perspective on the organization of the parameter space may be helpful for solving the hard problem of getting connectionist networks to learn complex grammars from examples.",10.1111/1468-0394.00126,2000.0,35.0,,Automata theory; Computational complexity; Context free grammars; Context free languages; Encoding (symbols); Fractals; Learning systems; Connectionist networks; Neural networks,2-s2.0-0033743045
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Tabor, W., Learning exponential state-growth languages by hill climbing (2003) IEEE Transactions on Neural Networks, 14 (2), pp. 444-446",Learning exponential state-growth languages by hill climbing,Tabor W.,Learning exponential state-growth languages by hill climbing,"Training recurrent neural networks on infinite state languages has been successful with languages in which the minimal number of machine states grows linearly wilh sentence length, but has faired poorly with exponential state-growth languages. A new architecture learns several exponential state-growth languages nearly perfectly by hill climbing.",10.1109/TNN.2003.809421,2003.0,12.0,,Automata theory; Computer simulation; Constraint theory; Context free languages; Fractals; Learning systems; Natural language processing systems; Probability distributions; Radial basis function networks; Fractal learning neural network; Gaussian activation function; Pushdown dynamical automata; Recurrent neural networks,2-s2.0-0037360272
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Tabor, W., Recursion and recursion-like structure in ensembles of neural elements (2011) Proceedings of the VIII international conference on complex systems, pp. 1494-1508",Recursion and recursion-like structure in ensembles of neural elements,Tabor W.,Recursion and recursion-like structure in ensembles of neural elements,[No abstract available],,2011.0,6.0,,,2-s2.0-84880331266
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Tabor, W., Cho, P.W., Szkudlarek, E., Fractal analyis illuminates the form of connectionist structural gradualness (2013) Topics in Cognitive Science, 5, pp. 634-667",Fractal analyis illuminates the form of connectionist structural gradualness,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Tabor, W., Juliano, C., Tanenhaus, M.K., Parsing in a dynamical system: An attractor-based account of the interaction of lexical and structural constraints in sentence processing (1997) Language and Cognitive Processes, 12 (2-3), pp. 211-271",Parsing in a dynamical system: An attractor-based account of the interaction of lexical and structural constraints in sentence processing,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Tsuda, I., Toward an interpretation of dynamic neural activity in terms of chaotic dynamical systems (2001) Behavioral and Brain Sciences, 24, pp. 793-810",Toward an interpretation of dynamic neural activity in terms of chaotic dynamical systems,Tsuda I.,Toward an interpretation of dynamic neural activity in terms of chaotic dynamical systems,"Using the concepts of chaotic dynamical systems, we present an interpretation of dynamic neural activity found in cortical and subcortical areas. The discovery of chaotic itinerancy in high-dimensional dynamical systems with and without a noise term has motivated a new interpretation of this dynamic neural activity, cast in terms of the high-dimensional transitory dynamics among ""exotic"" attractors. This interpretation is quite different from the conventional one, cast in terms of simple behavior on low-dimensional attractors. Skarda and Freeman (1987) presented evidence in support of the conclusion that animals cannot memorize odor without chaotic activity of neuron populations. Following their work, we study the role of chaotic dynamics in biological information processing, perception, and memory. We propose a new coding scheme of information in chaos-driven contracting systems we refer to as Cantor coding. Since these systems are found in the hippocampal formation and also in the olfactory system, the proposed coding scheme should be of biological significance. Based on these intensive studies, a hypothesis regarding the formation of episodic memory is given.",10.1017/s0140525x01000097,2001.0,347.0,Cantor coding; Chaotic itinerancy; Dynamic aspects of the brain; Dynamic associative memory; Episodic memory; High-dimensional dynamical systems; SCND attractors,article; basal ganglion; brain cortex; chaotic dynamics; coding; controlled study; hippocampus; information processing; memory; nerve cell; nervous system; odor; olfactory system; perception,2-s2.0-0035495017
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Turing, A.M., On computable numbers, with an application to the Entscheidungsproblem (1937) Proceedings of the London Mathematical Society, 42",with an application to the Entscheidungsproblem,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Wegner, P., Interactive foundations of computing (1998) Theoretical Computer Science, 192, pp. 315-351",Interactive foundations of computing,Wegner P.,Interactive foundations of computing,"The claim that interactive systems have richer behavior than algorithms is surprisingly easy to prove. Turing machines cannot model interaction machines (which extend Turing machines with interactive input/output) because interaction is not expressible by a finite initial input string. Interaction machines extend the Chomsky hierarchy, are modeled by interaction grammars, and precisely capture fuzzy concepts like open systems and empirical computer science. Computable functions cannot model real-world behavior because functions are too strong an abstraction, sacrificing the ability to model time and other real-world properties to realize formal tractability. Part I of this paper examines extensions to interactive models for algorithms, machines, grammars, and semantics, while Part II considers the expressiveness of different forms of interaction. Interactive identity machines are already more powerful than Turing machines, while noninteractive parallelism and distribution are algorithmic. The extension of Turing to interaction machines parallels that of the lambda to the pi calculus. Asynchronous and nonserializable interaction are shown to be more expressive than sequential interaction (multiple streams are more expressive than a single stream). In Part III. it is shown that interaction machines cannot be described by sound and complete first-order logics (a form of Godel incompleteness), and that incompleteness is inherently necessary to realize greater expressiveness. In the final section the robustness of interactive models in expressing open systems, programming in the large, graphical user interfaces, and agent-oriented artificial intelligence is compared to the robustness of Turing machines. Less technical discussion of these ideas may be found in [25-27]. Applications of interactive models to coordination, objects and components, patterns and frameworks, software engineering, and AI are examined elsewhere [28,29]. The propositions P1-P36 embody the principal claims, while observations O1 through O40 provide additional insights.",10.1016/S0304-3975(97)00154-0,1998.0,134.0,Constraints; Coordination; Emergent behavior; Empirical computer science; Games; Grammars; Incompleteness; Interaction; Logic; Models; On-line algorithms; Process models; Time; Turing machines,,2-s2.0-0002973118
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Weir, D.J., Linear iterated pushdowns (1994) Computational Intelligence, 10 (4), pp. 431-439",Linear iterated pushdowns,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Wennekers, T., Palm, G., Syntactic sequencing in Hebbian cell assemblies (2009) Cognitive Neurodynamics, 3 (4), pp. 429-441",Syntactic sequencing in Hebbian cell assemblies,"Wennekers T., Palm G.",Syntactic sequencing in Hebbian cell assemblies,"Hebbian cell assemblies provide a theoretical framework for the modeling of cognitive processes that grounds them in the underlying physiological neural circuits. Recently we have presented an extension of cell assemblies by operational components which allows to model aspects of language, rules, and complex behaviour. In the present work we study the generation of syntactic sequences using operational cell assemblies timed by unspecific trigger signals. Syntactic patterns are implemented in terms of hetero-associative transition graphs in attractor networks which cause a directed flow of activity through the neural state space. We provide regimes for parameters that enable an unspecific excitatory control signal to switch reliably between attractors in accordance with the implemented syntactic rules. If several target attractors are possible in a given state, noise in the system in conjunction with a winner-takes-all mechanism can randomly choose a target. Disambiguation can also be guided by context signals or specific additional external signals. Given a permanently elevated level of external excitation the model can enter an autonomous mode, where it generates temporal grammatical patterns continuously. © 2009 Springer Science+Business Media B.V.",10.1007/s11571-009-9095-z,2009.0,25.0,Attractor networks; Behaviour; Cell assemblies; Grammar; Language,article; cognition; conceptual framework; grammar; hebbian cell assembly; language; model; noise; syntactic sequencing,2-s2.0-72249093570
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Werbos, P.J., Backpropagation through time: What it does and how to do it (1990) Proceedings of the IEEE, 78 (10), pp. 1550-1560",Backpropagation through time: What it does and how to do it,,,,,,,,,
2-s2.0-84994319086,A modular architecture for transparent computation in recurrent neural networks," Weston, J., Chopra, S., Bordes, A., (2014) Memory networks. preprint, , [cs:AI]. arXiv:1410.3916",,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation,"Barzilay, R., McKeown, K.R., Sentence fusion for multidocument news summarization (2005) Computational Linguistics, 31 (3), pp. 297-328",Sentence fusion for multidocument news summarization,"Barzilay R., McKeown K.R.",Sentence fusion for multidocument news summarization,"A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources. © 2005 Association for Computational Linguistics.",10.1162/089120105774321091,2005.0,225.0,,Bottom up; Multi-document; News summarization; On-line documents; Sentence fusions; Text generations; Web users,2-s2.0-33646391652
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Bing, L., Li, P., Liao, Y., Lam, W., Guo, W., Passonneau, R., Abstractive multi-document summarization via phrase selection and merging (2015) Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pp. 1587-1597. , Association for Computational Linguistics Beijing, China",Abstractive multi-document summarization via phrase selection and merging,"Bing L., Li P., Liao Y., Lam W., Guo W., Passonneau R.J.",Abstractive multi-document summarization via phrase selection and merging,"We propose an abstraction-based multidocument summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-ofthe-Art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation. © 2015 Association for Computational Linguistics.",10.3115/v1/p15-1153,2015.0,69.0,,Abstracting; Benchmarking; Computational linguistics; Integer programming; Linear programming; Merging; Petroleum reservoir evaluation; Quality control; Syntactics; Benchmark data; Construction constraints; Evaluation metrics; Fine grained; Global optimal solutions; Linear optimization; Multi-document summarization; Quality evaluation; Natural language processing systems,2-s2.0-84943785681
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Bouckaert, R.R., Frank, E., Hall, M., Kirkby, R., Reutemann, P., Seewald, A., Scuse, D., (2013) Weka Manual for Version 3-7-8",,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Boudin, F., Mougard, H., Favre, B., Concept-based summarization using integer linear programming: From concept pruning to multiple optimal solutions (2015) Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 1914-1918. , Association for Computational Linguistics Lisbon, Portugal",Concept-based summarization using integer linear programming: From concept pruning to multiple optimal solutions,"Boudin F., Mougard H., Favre B.",Concept-based summarization using integer linear programming: From concept pruning to multiple optimal solutions,"In concept-based summarization, sentence selection is modelled as a budgeted maximum coverage problem. As this problem is NP-hard, pruning low-weight concepts is required for the solver to find optimal solutions efficiently. This work shows that reducing the number of concepts in the model leads to lower Rouge scores, and more importantly to the presence of multiple optimal solutions. We address these issues by extending the model to provide a single optimal solution, and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference. © 2015 Association for Computational Linguistics.",10.18653/v1/d15-1220,2015.0,31.0,,Approximation algorithms; Budget control; Inference engines; Integer programming; Optimal systems; Budgeted maximum coverage problems; Concept-based; Exact inference; Integer Linear Programming; Multiple optimal solutions; NP-hard; Optimal solutions; Sentence selection; Natural language processing systems,2-s2.0-84957547564
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Chang, C.-C., Lin, C.-J., Libsvm: A library for support vector machines (2011) ACM Transactions on Intelligent Systems and Technology, 2 (3), p. 27",Libsvm: A library for support vector machines,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Clarke, J., Lapata, M., Global inference for sentence compression: An integer linear programming approach (2008) Journal of Artificial Intelligence Research, 31, pp. 399-429",Global inference for sentence compression: An integer linear programming approach,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Cohn, T., Lapata, M., An abstractive approach to sentence compression (2013) ACM Transactions on Intelligent Systems and Technology, 4 (3), pp. 411-4135",An abstractive approach to sentence compression,"Cohn T., Lapata M.",An abstractive approach to sentence compression,"In this article we generalize the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present an experimental study showing that humans can naturally create abstractive sentences using a variety of rewrite operations, not just deletion. We next create a new corpus that is suited to the abstractive compression task and formulate a discriminative tree-to-tree transduction model that can account for structural and lexical mismatches. The model incorporates a grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression-specific loss functions. ©2013 ACM.",10.1145/2483669.2483674,2013.0,22.0,Language generation; Language models; Machine translation; Paraphrases; Sentence compression; Synchronous grammars; Transduction,Language generation; Language model; Machine translations; Paraphrases; Sentence compression; Synchronous grammars; Transduction; Bacteriophages; Forestry; Computational linguistics; Forestry; Languages; Models,2-s2.0-84880227825
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Cortes, C., Vapnik, V., Support-vector networks (1995) Machine Learning, 20 (3), pp. 273-297",Support-vector networks,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Coster, W., Kauchak, D., Simple english wikipedia: A new text simplification task (2011) Acl (Short Papers), pp. 665-669",Simple english wikipedia: A new text simplification task,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," De Marneffe, M.-C., Manning, C.D., Stanford typed dependencies manual (2008) URL Http://nlp. Stanford. Edu/software/dependencies Manual. Pdf",Stanford typed dependencies manual,"De Marneffe M.-C., Manning C.D.",Stanford typed dependencies manual,[No abstract available],,2008.0,480.0,,,2-s2.0-77954188732
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Edmundson, H.P., New methods in automatic extracting (1969) Journal of the ACM, 16 (2), pp. 264-285",New methods in automatic extracting,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Feblowitz, D., Kauchak, D., Sentence simplification as tree transduction (2013) Proceedings of the Second Workshop on Predicting and Improving Text Readability for Target Reader Populations, pp. 1-10. , Association for Computational Linguistics Sofia, Bulgaria",Sentence simplification as tree transduction,"Feblowitz D., Kauchak D.",Sentence simplification as tree transduction,[No abstract available],,2013.0,20.0,,,2-s2.0-84907531672
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Ferreira, R., Lins, R.D., Freitas, F., Cavalcanti, G.D., Lima, R., Simske, S.J., Favaro, L., Assessing sentence scoring techniques for extractive text summarization (2013) Expert Systems with Applications, 40 (14), pp. 5755-5764",Assessing sentence scoring techniques for extractive text summarization,"Ferreira R., De Souza Cabral L., Lins R.D., Pereira E Silva G., Freitas F., Cavalcanti G.D.C., Lima R., Simske S.J., Favaro L.",Assessing sentence scoring techniques for extractive text summarization,"Text summarization is the process of automatically creating a shorter version of one or more text documents. It is an important way of finding relevant information in large text libraries or in the Internet. Essentially, text summarization techniques are classified as Extractive and Abstractive. Extractive techniques perform text summarization by selecting sentences of documents according to some criteria. Abstractive summaries attempt to improve the coherence among sentences by eliminating redundancies and clarifying the contest of sentences. In terms of extractive summarization, sentence scoring is the technique most used for extractive text summarization. This paper describes and performs a quantitative and qualitative assessment of 15 algorithms for sentence scoring available in the literature. Three different datasets (News, Blogs and Article contexts) were evaluated. In addition, directions to improve the sentence extraction results obtained are suggested. © 2013 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2013.04.023,2013.0,180.0,Extractive summarization; Sentence scoring methods; Summarization evaluation,Extractive summarizations; Quantitative and qualitative assessments; Relevant informations; Sentence extraction; Sentence scoring; Summarization evaluation; Text document; Text summarization; Internet; Text processing,2-s2.0-84878347084
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Floridi, L., The method of levels of abstraction (2008) Minds and Machines, 18 (3), pp. 303-329",The method of levels of abstraction,Floridi L.,The method of levels of abstraction,"The use of ""levels of abstraction"" in philosophical analysis (levelism) has recently come under attack. In this paper, I argue that a refined version of epistemological levelism should be retained as a fundamental method, called the method of levels of abstraction. After a brief introduction, in section ""Some Definitions and Preliminary Examples"" the nature and applicability of the epistemological method of levels of abstraction is clarified. In section ""A Classic Application of the Method of Abstraction"", the philosophical fruitfulness of the new method is shown by using Kant's classic discussion of the ""antinomies of pure reason"" as an example. In section ""The Philosophy of the Method of Abstraction"", the method is further specified and supported by distinguishing it from three other forms of ""levelism"": (i) levels of organisation; (ii) levels of explanation and (iii) conceptual schemes. In that context, the problems of relativism and antirealism are also briefly addressed. The conclusion discusses some of the work that lies ahead, two potential limitations of the method and some results that have already been obtained by applying the method to some long-standing philosophical problems. © 2008 Springer Science+Business Media B.V.",10.1007/s11023-008-9113-7,2008.0,160.0,Abstraction; Gradient of abstraction; Level of abstraction; Levelism; Observable; Stance,Clarification; Philosophical aspects; Abstraction; Gradient of abstraction; Level of abstraction; Levelism; Levels of abstraction; Observable; Stance; Abstracting,2-s2.0-50249130726
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Floridi, L., (2009) Philosophical Conceptions of Information, , Springer",,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Floridi, L., (2011) The Philosophy of Information, , Oxford University Press",,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Floridi, L., (2013) The Ethics of Information, , Oxford University Press",,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Ganascia, J.-G., Abstraction of levels of abstraction (2015) Journal of Experimental & Theoretical Artificial Intelligence, 27 (1), pp. 23-35",Abstraction of levels of abstraction,Ganascia J.-G.,Abstraction of levels of abstraction,"The notion of level of abstraction (LoA) is one of the foundations of the Floridi's Philosophy of Information. It also serves for many practical purposes as in information ethics. But the notion of abstraction is not new; it has been given many different meanings in various fields, especially in scientific disciplines and, in particular, in computer science. Our purpose here is to examine the use of abstraction in Floridi's works in conjunction with some of the meanings of abstraction in computer science. The article is divided into five sections. After a general introduction to the Floridi's method of abstraction (MoA) in Section 1, Section 2 revisits Floridi's definition of abstraction and Section 3 gives the different senses of abstraction in computer science. The Section 4 compares them with the Floridi's LoAs and proposes to generalise the Floridi's approach to abstraction using an abstraction of the LoAs, while Section 5 concludes on what we think to be some new arguments in favour of MoA and LoA. © 2014 Taylor & Francis.",10.1080/0952813X.2014.940685,2015.0,4.0,abstract data type; abstract interpretation; abstraction; knowledge level; level of abstraction; method of abstraction; philosophy of information; software engineering,Abstract data types; Birds; Ontology; Software engineering; Abstract interpretations; abstraction; Knowledge level; Level of abstraction; method of abstraction; Philosophy of information; Abstracting,2-s2.0-85027930148
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Genest, P.-E., Lapalme, G., Framework for abstractive summarization using text-to-text generation (2011) Proceedings of the Workshop on Monolingual Text-to-text Generation, pp. 64-73. , Association for Computational Linguistics",Framework for abstractive summarization using text-to-text generation,"Genest P.E., Lapalme G.",Framework for abstractive summarization using text-to-text generation,[No abstract available],,2011.0,80.0,,,2-s2.0-84874648191
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Hasler, L., (2007) From Extracts to Abstracts: Human Summary Production Operations for Computer-aided Summarisation, , University of Wolverhampton Ph.D. thesis",,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Hovy, E., Lin, C.-Y., Automated text summarization and the summarist system (1998) Proceedings of A Workshop on Held at Baltimore, Maryland: October 13-15, 1998, pp. 197-214. , Association for Computational Linguistics",Automated text summarization and the summarist system,"Hovy E., Lin C.-Y.",Automated text summarization and the summarist system,[No abstract available],,1998.0,100.0,,,2-s2.0-0003100146
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Hovy, E., Marcu, D., Automated text summarization (2005) The Oxford Handbook of Computational Linguistics, pp. 583-598. , Ruslan Mitkov Oxford University Press",Automated text summarization,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Huang, M., Shi, X., Jin, F., Zhu, X., Using first-order logic to compress sentences (2012) Aaai",Using first-order logic to compress sentences,"Huang M., Shi X., Jin F., Zhu X.",Using first-order logic to compress sentences,"Sentence compression is one of the most challenging tasks in natural language processing, which may be of increasing interest to many applications such as abstractive summarization and text simplification for mobile devices. In this paper, we present a novel sentence compression model based on first-order logic, using Markov Logic Network. Sentence compression is formulated as a word/phrase deletion problem in this model. By taking advantage of first-order logic, the proposed method is able to incorporate local linguistic features and to capture global dependencies between word deletion operations. Experiments on both written and spoken corpora show that our approach produces competitive performance against the state-of-the-art methods in terms of manual evaluation measures such as importance, grammaticality, and overall quality. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,2012.0,3.0,,Evaluation measures; First order logic; Linguistic features; Markov logic networks; NAtural language processing; Overall quality; Sentence compression; State-of-the-art methods; Artificial intelligence; Formal logic; Natural language processing systems; Mobile devices,2-s2.0-84868280217
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Jing, H., Using hidden markov modeling to decompose human-written summaries (2002) Computational Linguistics, 28 (4), pp. 527-543",Using hidden markov modeling to decompose human-written summaries,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Jing, H., McKeown, K.R., Cut and paste based text summarization (2000) Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference, pp. 178-185. , Association for Computational Linguistics",Cut and paste based text summarization,"Jing H., McKeown K.",Cut and paste based text summarization,[No abstract available],,2000.0,125.0,,,2-s2.0-0039141162
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Kauchak, D., Improving text simplification language modeling using unsimplified text data (2013) Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1537-1546. , Association for Computational Linguistics Sofia, Bulgaria",Improving text simplification language modeling using unsimplified text data,Kauchak D.,Improving text simplification language modeling using unsimplified text data,"In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams. © 2013 Association for Computational Linguistics.",,2013.0,71.0,,Combined model; Input and outputs; Language model; N-grams; Text data; Computational linguistics,2-s2.0-84907300133
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Khan, A., Salim, N., Kumar, Y.J., A framework for multi-document abstractive summarization based on semantic role labelling (2015) Applied Soft Computing, 30, pp. 737-747",A framework for multi-document abstractive summarization based on semantic role labelling,"Khan A., Salim N., Jaya Kumar Y.",A framework for multi-document abstractive summarization based on semantic role labelling,"We propose a framework for abstractive summarization of multi-documents, which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents. In this framework, contents of the source documents are represented by predicate argument structures by employing semantic role labeling. Content selection for summary is made by ranking the predicate argument structures based on optimized features, and using language generation for generating sentences from predicate argument structures. Our proposed framework differs from other abstractive summarization approaches in a few aspects. First, it employs semantic role labeling for semantic representation of text. Secondly, it analyzes the source text semantically by utilizing semantic similarity measure in order to cluster semantically similar predicate argument structures across the text; and finally it ranks the predicate argument structures based on features weighted by genetic algorithm (GA). Experiment of this study is carried out using DUC-2002, a standard corpus for text summarization. Results indicate that the proposed approach performs better than other summarization systems. © 2015 Elsevier B.V. All rights reserved.",10.1016/j.asoc.2015.01.070,2015.0,75.0,Abstractive summary; Genetic algorithm; Language generation; Semantic role labeling; Semantic similarity measure,Computational linguistics; Genetic algorithms; Natural language processing systems; Text processing; Abstractive summary; Argument structures; Language generation; Semantic representation; Semantic role labeling; Semantic similarity measures; Summarization systems; Text summarization; Semantics,2-s2.0-84939483207
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Kumar, M., Das, D., Agarwal, S., Rudnicky, A.I., Non-textual event summarization by applying machine learning to template-based language generation (2009) Proceedings of the 2009 Workshop on Language Generation and Summarisation, pp. 67-71. , Association for Computational Linguistics",Non-textual event summarization by applying machine learning to template-based language generation,"Kumar M., Das D., Agarwal S., Rudnicky A.",Non-textual event summarization by applying machine learning to template-based language generation,[No abstract available],,2009.0,7.0,,,2-s2.0-79959652444
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Lin, C.-Y., Knowledge-based automatic topic identification (1995) Proceedings of the 33rd Annual Meeting on Association for Computational Linguistics, pp. 308-310. , Association for Computational Linguistics",Knowledge-based automatic topic identification,Lin C.Y.,Knowledge-based automatic topic identification,[No abstract available],,1995.0,26.0,,,2-s2.0-4544268174
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Liu, F., Flanigan, J., Thomson, S., Sadeh, N., Smith, N.A., Toward abstractive summarization using semantic representations (2015) Proceedings of the 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 1077-1086. , Association for Computational Linguistics Denver, Colorado",Toward abstractive summarization using semantic representations,"Liu F., Flanigan J., Thomson S., Sadeh N., Smith N.A.",Toward abstractive summarization using semantic representations,"We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-to-graph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-to-text generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on gold-standard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization. © 2015 Association for Computational Linguistics.",10.3115/v1/n15-1114,2015.0,106.0,,Computational linguistics; Natural language processing systems; Semantics; Data driven; Gold standards; Graph Transformation; Semantic graphs; Semantic representation; Source text; Text generators; Treebanks; Abstracting,2-s2.0-84957578529
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Lloret, E., Boldrini, E., Vodolazova, T., Martínez-Barco, P., Muñoz, R., Palomar, M., A novel concept-level approach for ultra-concise opinion summarization (2015) Expert Systems with Applications, 42 (20), pp. 7148-7156",A novel concept-level approach for ultra-concise opinion summarization,"Lloret E., Boldrini E., Vodolazova T., Martínez-Barco P., Muñoz R., Palomar M.",A novel concept-level approach for ultra-concise opinion summarization,"The Web 2.0 has resulted in a shift as to how users consume and interact with the information, and has introduced a wide range of new textual genres, such as reviews or microblogs, through which users communicate, exchange, and share opinions. The exploitation of all this user-generated content is of great value both for users and companies, in order to assist them in their decision-making processes. Given this context, the analysis and development of automatic methods that can help manage online information in a quicker manner are needed. Therefore, this article proposes and evaluates a novel concept-level approach for ultra-concise opinion abstractive summarization. Our approach is characterized by the integration of syntactic sentence simplification, sentence regeneration and internal concept representation into the summarization process, thus being able to generate abstractive summaries, which is one the most challenging issues for this task. In order to be able to analyze different settings for our approach, the use of the sentence regeneration module was made optional, leading to two different versions of the system (one with sentence regeneration and one without). For testing them, a corpus of 400 English texts, gathered from reviews and tweets belonging to two different domains, was used. Although both versions were shown to be reliable methods for generating this type of summaries, the results obtained indicate that the version without sentence regeneration yielded to better results, improving the results of a number of state-of-the-art systems by 9%, whereas the version with sentence regeneration proved to be more robust to noisy data. © 2015 Elsevier Ltd. All rights reserved.",10.1016/j.eswa.2015.05.026,2015.0,28.0,Electronic Word of Mouth; Natural language generation; Text summarization; Ultra-concise opinion summarization,Decision making; Social networking (online); Decision making process; Different domains; Electronic word of mouths; Natural language generation; On-line information; Text summarization; Ultra-concise opinion summarization; User-generated content; Natural language processing systems,2-s2.0-84930636185
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Lloret, E., Palomar, M., Analyzing the use of word graphs for abstractive text summarization (2011) Proceedings of the First International Conference on Advances in Information Mining and Management, Barcelona, Spain, pp. 61-66",Analyzing the use of word graphs for abstractive text summarization,"Lloret E., Palomar M.",Analyzing the use of word graphs for abstractive text summarization,[No abstract available],,2011.0,14.0,,,2-s2.0-84869767046
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Lloret, E., Palomar, M., Text summarisation in progress: A literature review (2012) Artificial Intelligence Review, 37 (1), pp. 1-41",Text summarisation in progress: A literature review,"Lloret E., Palomar M.",Text summarisation in progress: A literature review,"This paper contains a large literature review in the research field of Text Summarisation (TS) based on Human Language Technologies (HLT). TS helps users manage the vast amount of information available, by condensing documents' content and extracting the most relevant facts or topics included in them. The rapid development of emerging technologies poses new challenges to this research field, which still need to be solved. Therefore, it is essential to analyse its progress over the years, and provide an overview of the past, present and future directions, highlighting the main advances achieved and outlining remaining limitations. With this purpose, several important aspects are addressed within the scope of this survey. On the one hand, the paper aims at giving a general perspective on the state-of-the-art, describing the main concepts, as well as different summarisation approaches, and relevant international forums. Furthermore, it is important to stress upon the fact that the birth of new requirements and scenarios has led to new types of summaries with specific purposes (e.g. sentiment-based summaries), and novel domains within which TS has proven to be also suitable for (e.g. blogs). In addition, TS is successfully combined with a number of intelligent systems based on HLT (e.g. information retrieval, question answering, and text classification). On the other hand, a deep study of the evaluation of summaries is also conducted in this paper, where the existing methodologies and systems are explained, as well as new research that has emerged concerning the automatic evaluation of summaries' quality. Finally, some thoughts about TS in general and its future will encourage the reader to think of novel approaches, applications and lines to conduct research in the next years. The analysis of these issues allows the reader to have a wide and useful background on the main important aspects of this research field. © 2011 Springer Science+Business Media B.V.",10.1007/s10462-011-9216-z,2012.0,160.0,Human language technologies; Intelligent systems; Text summarisation,Amount of information; Automatic evaluation; Emerging technologies; Future directions; Human language technologies; Literature reviews; Novel domain; Question Answering; Rapid development; Research fields; Text classification; Text summarisation; Intelligent systems; Quality control; Research; Search engines; Text processing; Information retrieval,2-s2.0-84855347506
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Luhn, H.P., The automatic creation of literature abstracts (1958) IBM Journal of Research and Development, 2 (2), pp. 159-165",The automatic creation of literature abstracts,Luhn H.P.,The automatic creation of literature abstracts,[No abstract available],,1958.0,1619.0,,,2-s2.0-0000880768
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Mani, I., Maybury, M.T., (1999) Advances in Automatic Text Summarization, , the MIT Press",,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," McKeown, K., Rosenthal, S., Thadani, K., Moore, C., Time-efficient creation of an accurate sentence fusion corpus (2010) Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, pp. 317-320. , Association for Computational Linguistics",Time-efficient creation of an accurate sentence fusion corpus,"McKeown K., Rosenthal S., Thadani K., Moore C.",Time-efficient creation of an accurate sentence fusion corpus,"Sentence fusion enables summarization and question-answering systems to produce output by combining fully formed phrases from different sentences. Yet there is little data that can be used to develop and evaluate fusion techniques. In this paper, we present a methodology for collecting fusions of similar sentence pairs using Amazon's Mechanical Turk, selecting the input pairs in a semi-automated fashion. We evaluate the results using a novel technique for automatically selecting a representative sentence from multiple responses. Our approach allows for rapid construction of a high accuracy fusion corpus. © 2010 Association for Computational Linguistics.",,2010.0,18.0,,Amazon's mechanical turks; Fusion techniques; Multiple response; Novel techniques; Question answering systems; Rapid construction; Semi-automated; Artificial intelligence; Computational linguistics,2-s2.0-84858422326
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Mehdad, Y., Carenini, G., Ng, R.T., Abstractive summarization of spoken and written conversations based on phrasal queries (2014) Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1220-1230. , Association for Computational Linguistics Baltimore, Maryland",Abstractive summarization of spoken and written conversations based on phrasal queries,"Mehdad Y., Carenini G., Ng R.T.",Abstractive summarization of spoken and written conversations based on phrasal queries,"We propose a novel abstractive querybased summarization system for conversations, where queries are defined as phrases reflecting a user information needs. We rank and extract the utterances in a conversation based on the overall content and the phrasal query information. We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model. We propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster. A resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation. Automatic and manual evaluation results over meeting, chat and email conversations show that our approach significantly outperforms baselines and previous extractive models. © 2014 Association for Computational Linguistics.",10.3115/v1/p14-1115,2014.0,21.0,,Abstracting; Computational linguistics; Graph theory; Best paths; Evaluation results; Lexical similarity; Query information; Ranking strategy; Summarization systems; User information need; Word graphs; Query processing,2-s2.0-84906930162
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Miller, G.A., Wordnet: A lexical database for english (1995) Communications of the ACM, 38 (11), pp. 39-41",Wordnet: A lexical database for english,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Nenkova, A., Entity-driven rewrite for multidocument summarization (2008) Proceedings of IJCNLP08",Entity-driven rewrite for multidocument summarization,Nenkova A.,Entity-driven rewrite for multidocument summarization,[No abstract available],,2008.0,12.0,,,2-s2.0-79960612461
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Pighin, D., Cornolti, M., Alfonseca, E., Filippova, K., Modelling events through memory-based, open-ie patterns for abstractive summarization (2014) Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, 1, pp. 892-901",open-ie patterns for abstractive summarization,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Platt, J.C., (1999) Advances in Kernel Methods, pp. 185-208. , MIT Press Cambridge, MA, USA",,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Radev, D.R., McKeown, K.R., Generating natural language summaries from multiple on-line sources (1998) Computational Linguistics, 24 (3), pp. 470-500",Generating natural language summaries from multiple on-line sources,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Rush, A.M., Chopra, S., Weston, J., A neural attention model for abstractive sentence summarization (2015) Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pp. 379-389. , Association for Computational Linguistics Lisbon, Portugal",A neural attention model for abstractive sentence summarization,,,,,,,,,
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Saggion, H., A classification algorithm for predicting the structure of summaries (2009) Proceedings of the 2009 Workshop on Language Generation and Summarisation, pp. 31-38. , Association for Computational Linguistics",A classification algorithm for predicting the structure of summaries,Saggion H.,A classification algorithm for predicting the structure of summaries,[No abstract available],,2009.0,12.0,,,2-s2.0-79959662357
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Siddharthan, A., An architecture for a text simplification system (2002) Language Engineering Conference, 2002. Proceedings, pp. 64-71. , IEEE",An architecture for a text simplification system,Siddharthan A.,An architecture for a text simplification system,"We present a pipelined architecture for a text simplification system and describe our implementation of the three stages-analysis, transformation and regeneration. Our architecture allows each component to be developed and evaluated independently. We lay particular emphasis on the discourse level aspects of syntactic simplification as these are crucial to the process and have not been dealt with by previous research in the field. These aspects include generating referring expressions, deciding determiners, deciding sentence order and preserving rhetorical and anaphoric structure. © 2002 IEEE.",10.1109/LEC.2002.1182292,2002.0,61.0,,Architecture; Computational linguistics; Generating referring expressions; Pipelined architecture; Sentence ordering; Computer architecture,2-s2.0-84961806393
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Tanaka, H., Kinoshita, A., Kobayakawa, T., Kumano, T., Kato, N., Syntax-driven sentence revision for broadcast news summarization (2009) Proceedings of the 2009 Workshop on Language Generation and Summarisation, pp. 39-47. , Association for Computational Linguistics",Syntax-driven sentence revision for broadcast news summarization,"Tanaka H., Kinoshita A., Kobayakawa T., Kumano T., Kato N.",Syntax-driven sentence revision for broadcast news summarization,[No abstract available],,2009.0,23.0,,,2-s2.0-84878186415
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Woodsend, K., Lapata, M., Wikisimple: Automatic simplification of wikipedia articles (2011) Aaai",Wikisimple: Automatic simplification of wikipedia articles,"Woodsend K., Lapata M.",Wikisimple: Automatic simplification of wikipedia articles,[No abstract available],,2011.0,4.0,,,2-s2.0-84939551020
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Wubben, S., Van Den Bosch, A., Krahmer, E., Sentence simplification by monolingual machine translation (2012) Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Long Papers-volume 1, pp. 1015-1024. , Association for Computational Linguistics",Sentence simplification by monolingual machine translation,"Wubben S., Van Den Bosch A., Krahmer E.",Sentence simplification by monolingual machine translation,"In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems. © 2012 Association for Computational Linguistics.",,2012.0,131.0,,Human tests; Machine translations; Parallel corpora; Phrase-based machine translations; Re-ranking; State-of-the-art system; Wikipedia; Computational linguistics; Heuristic methods; Websites; Computer aided language translation,2-s2.0-84876797872
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Yamangil, E., Shieber, S.M., Bayesian synchronous tree-substitution grammar induction and its application to sentence compression (2010) Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pp. 937-947. , Association for Computational Linguistics",Bayesian synchronous tree-substitution grammar induction and its application to sentence compression,"Yamangil E., Shieber S.M.",Bayesian synchronous tree-substitution grammar induction and its application to sentence compression,"We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar. © 2010 Association for Computational Linguistics.",,2010.0,12.0,,Expectation Maximization; Gibbs sampling; Grammar induction; Non-parametric Bayesian; Nonparametric inference; Parametric inference; Parse trees; Sentence compression; Training algorithms; Variational bayes; Word alignment; Algorithms; Alignment; Computational linguistics; Inference engines; Forestry; Algorithms; Computation; Optimization; Parametric Equations,2-s2.0-79960613531
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Yoshikawa, K., Hirao, T., Iida, R., Okumura, M., Sentence compression with semantic role constraints (2012) Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-volume 2, pp. 349-353. , Association for Computational Linguistics",Sentence compression with semantic role constraints,"Yoshikawa K., Iida R., Hirao T., Okumura M.",Sentence compression with semantic role constraints,"For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques. © 2012 Association for Computational Linguistics.",,2012.0,17.0,,Empirical evaluations; Linguistic properties; Semantic constraints; Semantic roles; Sentence compression; State-of-the-art techniques; Syntactic dependencies; Syntactic information; Computational linguistics; Semantics,2-s2.0-84876812246
2-s2.0-84957598648,Concept generalization and fusion for abstractive sentence generation," Zajic, D., Dorr, B.J., Lin, J., Schwartz, R., Multi-candidate reduction: Sentence compression as a tool for document summarization tasks (2007) Information Processing & Management, 43 (6), pp. 1549-1570",Multi-candidate reduction: Sentence compression as a tool for document summarization tasks,"Zajic D., Dorr B.J., Lin J., Schwartz R.",Multi-candidate reduction: Sentence compression as a tool for document summarization tasks,"This article examines the application of two single-document sentence compression techniques to the problem of multi-document summarization-a ""parse-and-trim"" approach and a statistical noisy-channel approach. We introduce the multi-candidate reduction (MCR) framework for multi-document summarization, in which many compressed candidates are generated for each source sentence. These candidates are then selected for inclusion in the final summary based on a combination of static and dynamic features. Evaluations demonstrate that sentence compression is a valuable component of a larger multi-document summarization framework. © 2007 Elsevier Ltd. All rights reserved.",10.1016/j.ipm.2007.01.016,2007.0,102.0,Headline generation; Hidden Markov model; Parse-and-trim; Summarization,Problem solving; Spurious signal noise; Statistical methods; Headline generation; Hidden Markov model; Parse-and-trim; Summarization; Text processing,2-s2.0-34547941814
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model,"Anderson, A.H., Bader, M., Bard, E.G., Boyle, E., Doherty, G., Garrod, S., The HCRC Map Task corpus (1991) Language and Speech, 34 (4), pp. 351-366",The HCRC Map Task corpus,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," (1996) Repetition in dialogue, , Niemeyer, Tübingen, C. Bazzanella (Ed.)",,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Branigan, H.P., Pickering, M.J., Cleland, A.A., Syntactic co-ordination in dialogue (2000) Cognition, 25, pp. B13-B25",Syntactic co-ordination in dialogue,"Branigan H.P., Pickering M.J., Cleland A.A.",Syntactic co-ordination in dialogue,"There is substantial evidence that speakers co-ordinate their contributions in dialogue. Until now, experimental studies of co-ordination have concentrated on the development of shared strategies for reference. We present an experiment that employed a novel confederate-scripting technique to investigate whether speakers also co-ordinate syntactic structure in dialogue. Pairs of speakers took it in turns to describe pictures to each other. One speaker was a confederate of the experimenter and produced scripted descriptions that systematically varied in syntactic structure. The syntactic structure of the confederate's description affected the syntactic structure of the other speaker's subsequent description. We suggest that these effects are instances of syntactic priming (Bock, 1986), and provide evidence for a shared level of representation in comprehension and production. We describe how these effects might be realized in a processing model of language production, and relate them to previous findings of linguistic co-ordination in dialogue. © 2000 Elsevier Science B.V.",10.1016/S0010-0277(99)00081-5,2000.0,479.0,Co-ordination; Dialogue; Language production; Syntactic priming; Syntax,article; automation; cognition; comprehension; coordination; human; human experiment; language; linguistics; normal human; priority journal; speech articulation; verbal communication; Humans; Language; Random Allocation; Speech; Verbal Behavior,2-s2.0-0034657936
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Čermák, F., Idioms and morphology (2007) Phraseology. An international handbook of contemporary research, pp. 20-26. , Walter de Gruyter, Berlin, New York, H. Burger, D. Dobrovolskij, P. Kühn, N.R. Norrick (Eds.)",Idioms and morphology,Čermák F.,Idioms and morphology,[No abstract available],,2007.0,6.0,,,2-s2.0-84855934092
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Church, K.W., (2000), pp. 180-186. , Empirical estimates of adaptation: the chance of two noriegas is closer to p/2 than p2. In Proceedings of coling 2000, Saarbrücken",,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Clark, H.H., Wilkes-Gibbs, D., Referring as a collaborative process (1986) Cognition, 22, pp. 1-39",Referring as a collaborative process,"Clark H.H., Wilkes-Gibbs D.",Referring as a collaborative process,"In conversation, speakers and addressees work together in the making of a definite reference. In the model we propose, the speaker initiates the process by presenting or inviting a noun phrase. Before going on to the next contribution, the participants, if necessary, repair, expand on, or replace the noun phrase in an iterative process until they reach a version they mutually accept. In doing so they try to minimize their joint effort. The preferred procedure is for the speaker to present a simple noun phrase and for the addressee to accept it by allowing the next contribution to begin. We describe a communication task in which pairs of people conversed about arranging complex figures and show how the proposed model accounts for many features of the references they produced. The model follows, we suggest, from the mutual responsibility that participants in conversation bear toward the understanding of each utterance. © 1986.",10.1016/0010-0277(86)90010-7,1986.0,1242.0,,"article; human; human relation; interpersonal communication; linguistics; model; speech; Communication; Human; Interpersonal Relations; Linguistics; Models, Psychological; Speech; Support, Non-U.S. Gov't; Support, U.S. Gov't, Non-P.H.S.; Support, U.S. Gov't, P.H.S.",2-s2.0-0022671781
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Dehmer, M., Information processing in complex networks: graph entropy and information functionals (2008) Applied Mathematics and Computation, 201, pp. 82-94",Information processing in complex networks: graph entropy and information functionals,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Garrod, S., Anderson, A., Saying what you mean in dialogue: a study in conceptual and semantic co-ordination (1987) Cognition, 27, pp. 181-218",Saying what you mean in dialogue: a study in conceptual and semantic co-ordination,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Garrod, S., Pickering, M.J., Why is conversation so easy? (2004) Trends in Cognitive Sciences, 8 (1), pp. 8-11",Why is conversation so easy?,"Garrod S., Pickering M.J.",Why is conversation so easy?,Traditional accounts of language processing suggest that monologue - presenting and listening to speeches - should be more straightforward than dialogue - holding a conversation. This is clearly not the case. We argue that conversation is easy because of an interactive processing mechanism that leads to the alignment of linguistic representations between partners. Interactive alignment occurs via automatic alignment channels that are functionally similar to the automatic links between perception and behaviour (the so-called perception-behaviour expressway) proposed in recent accounts of social interaction. We conclude that humans are 'designed' for dialogue rather than monologue.,10.1016/j.tics.2003.10.016,2004.0,394.0,,Social aspects; Speech processing; Speech recognition; Linguistic representations; Cognitive systems; behavior; conversation; human; language; language ability; linguistics; processing; review; social interaction; speech; speech articulation; speech perception,2-s2.0-0346362269
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Garrod, S., Pickering, M.J., Alignment in dialogue (2007) Oxford handbook of psycholinguistics, pp. 443-451. , Oxford University Press, Oxford, UK, (Chapter 26), G. Gaskell (Ed.)",Alignment in dialogue,"Garrod S., Pickering M.J.",Alignment in dialogue,[No abstract available],,2007.0,29.0,,,2-s2.0-42649109299
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Giles, H., Powesland, P.F., (1975) Speech styles and social evaluation, , Academic Press, London",,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Godfrey, J.J., Holliman, E.C., McDaniel, J., SWITCHBOARD: telephone speech corpus for research and development (1992), 1, pp. 517-520. , In Proc. of IEEE ICASSP-92",SWITCHBOARD: telephone speech corpus for research and development,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Hadelich, K., Pickering, M.J., Branigan, H.P., Crocker, M.W., Alignment in dialogue: effects of visual versus verbal-feedback (2004), pp. 35-40. , In Proc. of catalog'04",Alignment in dialogue: effects of visual versus verbal-feedback,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Healey, P.G.T., Howes, C., Purver, M., Does structural priming occur in ordinary conversation? (2010) In Proc. of conference on linguistic evidence., , Tübingen, Germany",Does structural priming occur in ordinary conversation?,"Healey P.G.T., Howes C., Purver M.",Does structural priming occur in ordinary conversation?,[No abstract available],,2010.0,3.0,,,2-s2.0-84861779450
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Howes, C., Healey, P.G.T., Purver, M., Tracking lexical and syntactic alignment in conversation (2010), pp. 2004-2009. , In Proc. of CogSci'10",Tracking lexical and syntactic alignment in conversation,"Howes C., Healey P., Purver M.",Tracking lexical and syntactic alignment in conversation,[No abstract available],,2010.0,16.0,,,2-s2.0-84861779451
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Jackendoff, R., (2002) Foundations of language. Brain, meaning, grammar, evolution, , Oxford University Press, Oxford, UK",,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Kövecses, Z., (2002) Metaphor: a practical introduction, , Oxford University Press, Cary",,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Kraskov, A., Grassberger, P., MIC: mutual information based hierarchical clustering (2008) Information theory and statistical learning, pp. 101-123. , Springer, New York, F. Emmert-Streib, M. Dehmer (Eds.)",MIC: mutual information based hierarchical clustering,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Kuiper, K., (1996) Smooth talkers: the linguistic performance of auctioneers and sportscasters, , Lawrence Earlbaum Associates, Inc., Mahwah, New Jersey",,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Kuiper, K., On the linguistic properties of formulaic speech (2000) Oral Tradition, 15 (2), pp. 279-305",On the linguistic properties of formulaic speech,Kuiper K.,On the linguistic properties of formulaic speech,[No abstract available],,2000.0,11.0,,,2-s2.0-0742276749
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Lücking, A., Bergmann, K., Hahn, F., Kopp, S., Rieser, H., The Bielefeld speech and gesture alignment corpus (SaGA) (2010), 5, pp. 92-98. , In Multimodal corpora: advances in capturing, coding and analyzing multimodality. LREC 2010. Malta",The Bielefeld speech and gesture alignment corpus (SaGA),"Lücking A., Bergmann K., Hahn F., Kopp S., Rieser H.",The Bielefeld speech and gesture alignment corpus (SaGA),[No abstract available],,2010.0,27.0,,,2-s2.0-79952274250
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Mehler, A., Lücking, A., Menke, P., From neural activation to symbolic alignment: a network-based approach to the formation of dialogue lexica (2011), 8, pp. 527-536. , In Proc. of IJCNN 2011. San Jose",From neural activation to symbolic alignment: a network-based approach to the formation of dialogue lexica,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Mehler, A., Lücking, A., Menke, P., Modelling lexical alignment in spontaneous direction dialogue data by means of a lexicon network model (2011), 2. , In Proc. of CICLing 2011. Tokyo",Modelling lexical alignment in spontaneous direction dialogue data by means of a lexicon network model,"Mehler A., Lücking A., Menke P.",Modelling lexical alignment in spontaneous direction dialogue data by means of a lexicon network model,[No abstract available],,2011.0,1.0,,,2-s2.0-84861766372
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Mehler, A., Lücking, A., Weiß, P., A network model of interpersonal alignment in dialogue (2010) Entropy, 12 (6), pp. 1440-1483",A network model of interpersonal alignment in dialogue,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Miller, G., Towards ethnographies of institutional discourse: proposals and suggestions (1994) Journal of Contemporary Ethnography, 23 (3), pp. 280-306",Towards ethnographies of institutional discourse: proposals and suggestions,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Newman, M.E.J., The structure and function of complex networks (2003) SIAM Review, 45, pp. 167-256",The structure and function of complex networks,Newman M.E.J.,The structure and function of complex networks,"The structure and function of complex networks were discussed. The study of networks, in the form of mathematical graph theory, is one of the fundamental pillars of discrete mathematics. Analysis showed that the network properties would affect the behavior of networked systems substantially.",10.1137/S003614450342480,2003.0,12405.0,Complex systems; Computer networks; Graph theory; Networks; Percolation theory; Random graphs; Social networks,Graph theory; Internet; Large scale systems; Percolation (computer storage); Topology; Random graphs; Computer networks,2-s2.0-0038718854
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Norrick, N.R., Proverbs as set phrases (2007) Phraseology. An international handbook of contemporary research, pp. 381-393. , Walter de Gruyter, Berlin, New York, H. Burger, D. Dobrovolskij, P. Kühn, N.R. Norrick (Eds.)",Proverbs as set phrases,Norrick N.R.,Proverbs as set phrases,[No abstract available],,2007.0,10.0,,,2-s2.0-84861768028
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Pickering, M.J., Garrod, S., Toward a mechanistic psychology of dialogue (2004) Journal of Behavioral and Brain Science, 27 (2), pp. 169-190",Toward a mechanistic psychology of dialogue,"Pickering M.J., Garrod S.",Toward a mechanistic psychology of dialogue,"Traditional mechanistic accounts of language processing derive almost entirely from the study of monologue. Yet, the most natural and basic form of language use is dialogue. As a result, these accounts may only offer limited theories of the mechanisms that underlie language processing in general. We propose a mechanistic account of dialogue, the interactive alignment account, and use it to derive a number of predictions about basic language processes. The account assumes that, in dialogue, the linguistic representations employed by the interlocutors become aligned at many levels, as a result of a largely automatic process. This process greatly simplifies production and comprehension in dialogue. After considering the evidence for the interactive alignment model, we concentrate on three aspects of processing that follow from it. It makes use of a simple interactive inference mechanism, enables the development of local dialogue routines that greatly simplify language processing, and explains the origins of self-monitoring in production. We consider the need for a grammatical framework that is designed to deal with language in dialogue rather than monologue, and discuss a range of implications of the account. © 2004 Cambridge University Press.",10.1017/s0140525x04000056,2004.0,1374.0,Common ground; Dialogue; Dialogue routines; Language comprehension; Language production; Monitoring; Perception-behavior link,article; auditory discrimination; autonomic nervous system function; cognition; comprehension; conversation; coordination; experimental model; health status; interpersonal communication; language ability; maze test; mental function; mental health; nerve conduction; phonetics; prediction; reading; self monitoring; semantics; social interaction; speech articulation; speech discrimination; speech intelligibility; theory; human; human relation; language; linguistics; mental function; psychological theory; review; speech; systems theory; Humans; Interpersonal Relations; Language; Mental Processes; Psycholinguistics; Psychological Theory; Speech; Systems Theory,2-s2.0-12744262642
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Pulvermüller, F., (2002) The neuroscience of language: on brain circuits of words and serial order, , Cambridge University Press, Cambridge, UK",,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Reitter, D., Keller, F., Moore, J.D., Computational modelling of structural priming in dialogue (2006), pp. 121-124. , In Proc. of NAACL-Short'06. ACL",Computational modelling of structural priming in dialogue,"Reitter D., Keller F., Moore J.D.",Computational modelling of structural priming in dialogue,[No abstract available],,2006.0,51.0,,,2-s2.0-70450147063
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Reitter, D., Moore, J.D., Keller, F., Priming of syntactic rules in task-oriented dialogue and spontaneous conversation (2006), pp. 685-690. , In Proc. of CogSci'06",Priming of syntactic rules in task-oriented dialogue and spontaneous conversation,"Reitter D., Moore J.D., Keller F.",Priming of syntactic rules in task-oriented dialogue and spontaneous conversation,[No abstract available],,2006.0,64.0,,,2-s2.0-56149098648
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Sacks, H., Schegloff, E.A., Jefferson, G., A simplest systematics for the organization of turn-taking for conversation (1974) Language, 50 (4), pp. 696-735",A simplest systematics for the organization of turn-taking for conversation,"Sacks H., Schegloff E.A., Jefferson G.",A simplest systematics for the organization of turn-taking for conversation,[No abstract available],,1974.0,6960.0,,,2-s2.0-0000098051
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Tannen, D., (2007) Talking voices: repetition, dialogue, and imagery in conversational discourse, , Cambridge University Press, Cambridge, UK",,,,,,,,,,
2-s2.0-84861762399,Assessing cognitive alignment in different types of dialog by means of a network model," Weiß, P., Pfeiffer, T., Schaffranietz, G., Rickheit, G., Coordination in dialog (2008) In Proc. of 8th Annual Meeting of the Cognitive Science Society of Germany, 4, pp. 1-17. , Saarbrücken",Coordination in dialog,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification,"Amaral, C., Cassan, A., Figueira, H., Martins, A., Mendes, A., Mendes, P., Pinto, C., Vidal, D., Priberam's question answering system in QA@CLEF 2007 (2008) Advances in Multilingual and Multimodal Information Retrieval: 8th Workshop of the Cross-language Evaluation Forum, CLEF 2007, pp. 364-371. , http://dx.doi.org/10.1007/978-3-540-85760-0_46, Budapest, Hungary, September 19-21, 2007. Revised Selected Papers, Berlin, Heidelberg, 2008. Springer. ISBN 978-3-540-85759-4",Priberam's question answering system in QA@CLEF 2007,"Amaral C., Cassan A., Figueira H., Martins A., Mendes A., Mendes P., Pinto C., Vidal D.",Priberam's question answering system in QA@CLEF 2007,"This paper accounts for Priberam's participation in the monolingual question answering (QA) track of CLEF 2007. In previous participations, Priberam's QA system obtained encouraging results both in monolingual and cross-language tasks. This year we endowed the system with syntactical processing, in order to capture the syntactic structure of the question. The main goal was to obtain a more tuned question categorisation and consequently a more precise answer extraction. Besides this, we provided our system with the ability to handle topic-related questions and to use encyclopaedic sources like Wikipedia. The paper provides a description of the improvements made in the system, followed by the discussion of the results obtained in Portuguese and Spanish monolingual runs. © 2008 Springer-Verlag Berlin Heidelberg.",10.1007/978-3-540-85760-0_46,2008.0,7.0,,Syntactics; Answer extraction; Cross languages; QA system; Question Answering; Question answering systems; Syntactic structure; Wikipedia; Natural language processing systems,2-s2.0-70349832162
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Bhagat, R., Leuski, A., Hovy, E., Shallow semantic parsing despite little training data (2005) Proceedings of the ACL/SIGPARSE 9th International Workshop on Parsing Technologies, , Vancouver, Canada",Shallow semantic parsing despite little training data,"Bhagat R., Leuski A., Hovy E.",Shallow semantic parsing despite little training data,[No abstract available],,2005.0,2.0,,,2-s2.0-70549100459
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Blunsom, P., Kocik, K., Curran, J.R., Question classification with log-linear models (2006) SIGIR '06: Proceedings of the 29th Annual International ACM SIGIR conference on research and development in information retrieval, pp. 615-616. , ACM, New York, USA, ISBN 1-59593-369-7",Question classification with log-linear models,"Blunsom P., Kocik K., Curran J.R.",Question classification with log-linear models,"Question classification has become a crucial step in modern question answering systems. Previous work has demonstrated the effectiveness of statistical machine learning approaches to this problem. This paper presents a new approach to building a question classifier using log-linear models. Evidence from a rich and diverse set of syntactic and semantic features is evaluated, as well as approaches which exploit the hierarchical structure of the question classes.",10.1145/1148170.1148282,2006.0,40.0,Machine Learning; Maximum entropy; Question Answering; Question Classification,Entropy; Learning systems; Mathematical models; Optimization; Statistical methods; Hierarchical systems; Learning systems; Linear systems; Mathematical models; Problem solving; Semantics; Question Answering; Question Classification; Query languages; Query processing; Maximum entropy; Question answering; Question classification,2-s2.0-33750376143
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Carlson, A.J., Cumby, C.M., Rosen, J.L., Roth, D., (1999) Snow User Guide, , Technical report UIUC-DCS-R-99-210, Champaign, IL",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Chang, C-.C., Lin, C-.J., (2001) LIBSVM: A Library for Support Vectormachines, , http://wwwcsie.ntu.edu.tw/cjlin/libsvm, Software available at",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Collins, M.J., (1999) Head-driven Statistical Models for Natural Language Parsing, , PhD thesis, Philadelphia, PA, USA",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Fellbaum, C., (1998) WordNet: An Electronic Lexical Database., , http://books.google.es/books?hl=es&lr=&id=Rehu8OOzMIMC&oi= fnd&pg=PR11, MIT, Cambridge. URL",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Hermjakob, U., Hovy, E., Lin, C-.Y., Automated question answering in Webclopedia: A demonstration. In: Proceedings of the second international conference on human language technology research (2002) Morgan Kaufmann Publishers, pp. 370-371. , Inc, San Francisco",Automated question answering in Webclopedia: A demonstration. In: Proceedings of the second international conference on human language technology research,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Huang, Z., Thint, M., Qin, Z., Question classification using head words and their hypernyms (2008) EMNLP, pp. 927-936",Question classification using head words and their hypernyms,"Huang Z., Thint M., Qin Z.",Question classification using head words and their hypernyms,"Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2%and 89.0%respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%. © 2008 Association for Computational Linguistics.",10.3115/1613715.1613835,2008.0,142.0,,Semantics; Support vector machines; Benchmark datasets; Feature sets; Maximum entropy models; Question Answering; Question classification; Rich features; Semantic features; Word-sense disambiguation; Natural language processing systems,2-s2.0-77956056624
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Judge, J., Cahill, A., Van Genabith, J., Questionbank: Creating a corpus of parse-annotated questions (2006) ACL-44: Proceedings of the 21st International Conference on Computational Linguistics and the 44th Annual Meeting of the Association for Computational Linguistics, pp. 497-504. , http://dx.doi.org/10.3115/1220175.1220238, Association for Computational Linguistics, Morristown, NJ, USA",Questionbank: Creating a corpus of parse-annotated questions,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Krishnan, V., Das, S., Chakrabarti, S., Enhanced answer type inference from questions using sequential models (2005) HLT '05: Proceedings of the Conference on Human Language Technology and Empirical Methods in Natural Language Processing, pp. 315-322. , http://dx.doi.org/10.3115/1220575.1220615, Association for Computational Linguistics, Morristown, NJ, USA",Enhanced answer type inference from questions using sequential models,"Krishnan V., Das S., Chakrabarti S.",Enhanced answer type inference from questions using sequential models,"Question classification is an important step in factual question answering (QA) and other dialog systems. Several attempts have been made to apply statistical machine learning approaches, including Support Vector Machines (SVMs) with sophisticated features and kernels. Curiously, the payoff beyond a simple bag-ofwords representation has been small. We show that most questions reveal their class through a short contiguous token subsequence, which we call its informer span. Perfect knowledge of informer spans can enhance accuracy from 79.4% to 88% using linear SVMs on standard benchmarks. In contrast, standard heuristics based on shallow pattern-matching give only a 3% improvement, showing that the notion of an informer is non-trivial. Using a novel multi-resolution encoding of the question's parse tree, we induce a Conditional Random Field (CRF) to identify informer spans with about 85% accuracy. Then we build a meta-classifier using a linear SVM on the CRF output, enhancing accuracy to 86.2%, which is better than all published numbers. © 2005 Association for Computational Linguistics.",10.3115/1220575.1220615,2005.0,41.0,,Learning algorithms; Pattern matching; Random processes; Support vector machines; Conditional random field; Meta-classifiers; Question Answering; Question classification; Sequential model; Statistical machine learning; Support vector machine (SVMs); Type inferences; Natural language processing systems,2-s2.0-34547441174
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Kwok, C.C.T., Etzioni, O., Weld, D.S., Scaling question answering to the web (2001) WWW '01: Proceedings of the 10th International Conference on World Wide Web, pp. 150-161. , http://doi.acm.org/10.1145/371920.371973, ACMNew York, NY, USA, ISBN 1-58113-348-0",Scaling question answering to the web,"Kwok C.C.T., Etzioni O., Weld D.S.",Scaling question answering to the web,"The wealth of information on the web makes it an at-tractive resource for seeking quick answers to simple, factual questions such as ""who was the first American in space?"" or ""what is the second tallest mountain in the world?"" Yet today's most advanced web search services (e.g., Google and AskJeeves) make it surprisingly te-dious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance. First we introduce Mulder, which we believe to be the first general-purpose, fully-automated question- A nswering system available on the web. Second, we de-scribe Mulder's architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare Mulder's per-formance to that of Google and AskJeeves on questions drawn from the TREC-8 question track. We find that Mulder's recall is more than a factor of three higher than that of AskJeeves. In addition, we find that Google requires 6.6 times as much user effort to achieve the same level of recall as Mulder. © 2001 ACM.",10.1145/371920.371973,2001.0,184.0,,Natural language processing systems; Search engines; World Wide Web; Fully automated; Multiple search; Natural language parsing; Question Answering; Wealth of information; Web searches; Information retrieval,2-s2.0-84944091179
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Li, F., Zhang, X., Yuan, J., Zhu, X., Classifying what-type questions by head noun tagging (2008) Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pp. 481-488. , http://www.aclweb.org/anthology/C08-1061, August, Manchester, UK, Coling 2008 Organizing Committee. URL",Classifying what-type questions by head noun tagging,"Li F., Zhang X., Yuan J., Zhu X.",Classifying what-type questions by head noun tagging,"Classifying what-type questions into proper semantic categories is found more challenging than classifying other types in question answering systems. In this paper, we propose to classify what-type questions by head noun tagging. The approach highlights the role of head nouns as the category discriminator of what-type questions. To reduce the semantic ambiguities of head noun, we integrate local syntactic feature, semantic feature and category dependency among adjacent nouns with Conditional Random Fields (CRFs). Experiments on standard question classification data set show that the approach achieves state-of-the-art performances. © 2008. Licensed under the Creative Commons.",10.3115/1599081.1599142,2008.0,34.0,,Computational linguistics; Semantics; Conditional Random Fields(CRFs); Question answering systems; Question classification; Semantic ambiguities; Semantic category; Semantic features; State-of-the-art performance; Syntactic features; Classification (of information),2-s2.0-79956091185
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Li, X., Roth, D., Learning question classifiers (2002) Proceedings of the 19th International Conference on Computational Linguistics, pp. 1-7. , http://dx.doi.org/10.3115/1072228.1072378, Association for Computational Linguistics, Morristown, NJ, USA",Learning question classifiers,"Li X., Roth D.",Learning question classifiers,[No abstract available],,2002.0,731.0,,,2-s2.0-1542370072
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Mendes, A., Coheur, L., Mamede, N.J., Ribeiro, R.D., De Matos, D.M., Batista, F., QA@L2F, first steps at QA@CLEF (2008) Advances in Multilingual and Multimodal Information Retrieval Volume 5152 of Lecture Notes in Computer Science, , Springer, Berlin",first steps at QA@CLEF,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Metzler, D., Croft, W.B., Analysis of statistical question classification for fact-based questions (2005) Information Retrieval, 8 (3), pp. 481-504. , DOI 10.1007/s10791-005-6995-3",Analysis of statistical question classification for fact-based questions,"Metzler D., Croft W.B.",Analysis of statistical question classification for fact-based questions,"Question classification systems play an important role in question answering systems and can be used in a wide range of other domains. The goal of question classification is to accurately assign labels to questions based on expected answer type. Most approaches in the past have relied on matching questions against hand-crafted rules. However, rules require laborious effort to create and often suffer from being too specific. Statistical question classification methods overcome these issues by employing machine learning techniques. We empirically show that a statistical approach is robust and achieves good performance on three diverse data sets with little or no hand tuning. Furthermore, we examine the role different syntactic and semantic features have on performance. We find that semantic features tend to increase performance more than purely syntactic features. Finally, we analyze common causes of misclassification error and provide insight into ways they may be overcome. © 2005 Springer Science + Business Media, Inc.",10.1007/s10791-005-6995-3,2005.0,106.0,Machine learning; Question answering; Question classification; Semantic features; Support Vector Machines; Syntactic features; WordNet,,2-s2.0-17444401767
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Moldovan, D., Paşca, M., Harabagiu, S., Surdeanu, M., Performance issues and error analysis in an open-domain question answering system (2003) ACM Trans Inf Syst, 21 (2), pp. 133-154",Performance issues and error analysis in an open-domain question answering system,"Moldovan D., Paşca M., Harabagiu S., Surdeanu M.",Performance issues and error analysis in an open-domain question answering system,"This paper presents an in-depth analysis of a state-of-the-art Question Answering system. Several scenarios are examined: (1) the performance of each module in a serial baseline system, (2) the impact of feedbacks and the insertion of a logic prover, and (3) the impact of various retrieval strategies and lexical resources. The main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding.",10.1145/763693.763694,2003.0,145.0,Natural language applications; Performance analysis; Question answering; Text retrieval,Algorithms; Error analysis; Information retrieval; Systems analysis; Text processing; Natural language applications; Performance analysis; Question answering; Text retrieval; Natural language processing systems,2-s2.0-2442589044
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Moldovan, D.I., Harabagiu, S.M., Paşca, M., Mihalcea, R., Girju, R., Goodrum, R., Rus, V., The structure and performance of an open-domain question answering system (2000) ACL",The structure and performance of an open-domain question answering system,"Moldovan D., Harabagiu S., Pasca M., Mihalcea R., Girju R., Goodrum R., Rus V.",The structure and performance of an open-domain question answering system,[No abstract available],,2000.0,92.0,,,2-s2.0-0009872219
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Newell, A., (1990) Unified Theories of Cognition, , Harvard University Press, Harvard",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Pan, Y., Tang, Y., Lin, L., Luo, Y., Question classification with semantic tree kernel (2008) SIGIR '08: Proceedings of the 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 837-838. , http://doi.acm.org/10.1145/1390334.1390530, ACM, New York, NY, USA,ISBN:978-1-60558-164-4",Question classification with semantic tree kernel,"Yan P., Yong T., Luxian L., Yemin L.",Question classification with semantic tree kernel,"Question Classification plays an important role in most Question Answering systems. In this paper, we exploit semantic features in Support Vector Machines (SVMs) for Question Classification. We propose a semantic tree kernel to incorporate semantic similarity information. A diverse set of semantic features is evaluated. Experimental results show that SVMs with semantic features, especially semantic classes, can significantly outperform the state-of-the-art systems.",10.1145/1390334.1390530,2008.0,9.0,Machine learning; Question answering; Question classification; Semantic class; Support vector machines; Tree kernel,Content based retrieval; Image retrieval; Information retrieval; Information services; Learning systems; Natural language processing systems; Research and development management; Semantics; Support vector machines; Vectors; Machine learning; Question answering; Question classification; Semantic class; Tree kernel; Information theory,2-s2.0-57349165324
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Petrov, S., Klein, D., Improved inference for unlexicalized parsing (2007) Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics",Improved inference for unlexicalized parsing,"Petrov S., Klein D.",Improved inference for unlexicalized parsing,"We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar's own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. © 2007 Association for Computational Linguistics.",,2007.0,370.0,,Coarse-to-fine; Multiple languages; Risk minimization; Treebanks; Computational linguistics; Experiments,2-s2.0-84858380058
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Proceedings of the Main Conference, Association for Computational Linguistics, pp. 404-411. , http://www.aclweb.org/anthology/N/N07/N07-1051, Rochester, New York",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Saquete, E., Vicedo, J.L., Martínez-Barco, P., Munoz, R., Llorens, H., Enhancing QA systems with complex temporal question processing capabilities (2009) J Artif Intell Res, 35, pp. 299-330",Enhancing QA systems with complex temporal question processing capabilities,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Sharada, B.A., Girish, P.M., (2004) Wordnet has No 'Recycle Bin'",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Simon Herbert, A., (1969) The Sciences of the Artificial, , 1st edn MIT, Cambridge",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Sun, R., A two-level hybrid architecture for structuring knowledge for commonsense reasoning (1995) Computational Architectures Integrating Neural and Symbolic Processing, pp. 247-282. , Sun R, Bookman LA (eds), chap 8. Kluwer, Dordrecht",A two-level hybrid architecture for structuring knowledge for commonsense reasoning,Sun R.,A two-level hybrid architecture for structuring knowledge for commonsense reasoning,[No abstract available],,1995.0,11.0,,,2-s2.0-0010721465
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Tarski, A., (1956) Logic, Semantics, Metamathematics, , Oxford University Press, London",,,,,,,,,,
2-s2.0-79956078298,From symbolic to sub-symbolic information in question classification," Zhang, D., Lee, W.S., Question classification using support vector machines (2003) SIGIR '03: Proceedings of the 26th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 26-32. , http://doi.acm.org/10.1145/860435.860443, ACM, New York, NY, USA, ISBN:1-58113-646-3",Question classification using support vector machines,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier,"Anagha, M., Kumar, R.R., Sreetha, K., Reghu Raj, P.C., Fuzzy logic based hybrid approach for sentiment analysisl of malayalam movie reviews (2015) Proc. IEEE Int. Conf. Signal Process., Informat., Com-mun. Energy Syst. (SPICES), pp. 1-4. , Feb",Fuzzy logic based hybrid approach for sentiment analysisl of malayalam movie reviews,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Sathe, J.B., Mali, M.P., A hybrid sentiment classification method using neural network and fuzzy logic (2017) Proc. 11th Int. Conf. Intell. Syst. Control (ISCO), pp. 93-96. , Jan",A hybrid sentiment classification method using neural network and fuzzy logic,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Biltawi, M., Etaiwi, W., Tedmori, S., Shaout, A., Fuzzy based sentiment classification in the Arabic language (2019) Intelligent Systems and Applica-tions (Advances in Intelligent Systems and Computing), 868. , K. Arai, S. Kapoor, and R. Bhatia, Eds. Cham, Switzerland: Springer",Fuzzy based sentiment classification in the Arabic language,"Biltawi M., Etaiwi W., Tedmori S., Shaout A.",Fuzzy based sentiment classification in the Arabic language,"Sentiment Analysis is the task of identifying individuals’ positive and negative opinions, emotions and evaluations concerning a specific object. Fuzzy logic in the field of sentiment analysis can be employed to classify the polarity of sentences or documents. Although some efforts have been made by researchers who applied fuzzy logic for Sentiment Analysis on English texts, to the best of the authors’ knowledge, no efforts have been made targeting Arabic texts. This paper proposes a lexicon based approach to extract sentiment polarity from Arabic text using a fuzzy logic approach. The proposed approach consists of two main phases. In the first phase, Arabic text is assigned weights, while in the second phase fuzzy logic is employed to assign the polarity to the inputted sentence. Experiments were conducted on Large Scale Arabic Book Reviews Dataset (LABR), and the results showed 94.87%, 84.04%, 80.59% and 89.13% for recall, precision, accuracy, and F1-measure, respectively. © Springer Nature Switzerland AG 2019.",10.1007/978-3-030-01054-6_42,2018.0,4.0,Arabic fuzzy logic; Arabic natural language processing; Arabic sentiment analysis; Linguistic variables,Classification (of information); Computer circuits; Intelligent systems; Large dataset; Reviews; Sentiment analysis; Arabic languages; Arabic natural language processing; Book reviews; Fuzzy logic approach; Lexicon-based; Linguistic variable; Second phase; Sentiment classification; Fuzzy logic,2-s2.0-85057102916
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Kumari, U., Sharma, A.K., Soni, D., Sentiment analysis of smart phone product review using SVM classification technique (2017) Proc. Int. Conf. Energy, Commun., Data Anal. Soft Comput. (ICECDS), pp. 1469-1474. , Aug",Sentiment analysis of smart phone product review using SVM classification technique,"Kumari U., Sharma A.K., Soni D.",Sentiment analysis of smart phone product review using SVM classification technique,"There is a massive increase in number of people who access various social networking and micro-blogging websites that gives new shape to the impression of today's generation. Several reviews for a specific product, brand, individual personality, forum sand movies etc. are very helpful in directing the perception of people. Hence the analysts are commenced to create algorithms to automate the classification of distinctive reviews on the basis of their polarities particularly: Positive, Negative and Neutral. This automated classification mechanism is referred as Sentiment Analysis. The ultimate aim of this paper is to apply Support Vector Machine (SVM) classification technique to classify the sentiment sand texts for smart phone product review that analyses different datasets used for classification of sentiments and texts. Furthermore, various data sets have been utilized for training as well as testing and implemented using Support Vector Machine (SVM) to investigate polarity of the ambiguous tweets. The experimental work includes three performance features such as Precision, Recall and F-measure. On the basis of these features, the accuracy of the different products has been computed. The obtained result approves high accuracy as predicted on the basis of smart phone reviews. © 2017 IEEE.",10.1109/ICECDS.2017.8389689,2018.0,14.0,Clustering; Sentiment Analysis; SVM; Twitter Reviews,Classification (of information); Data mining; Sentiment analysis; Smartphones; Soft computing; Telephone sets; Automated classification; Clustering; High-accuracy; Micro blogging; Number of peoples; Product reviews; Support vector machine classification techniques; SVM classification; Support vector machines,2-s2.0-85050108754
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Thein, Y., Khin, T.N., Comparing SVM and KNN algorithms for Myanmar news sentiment analysis system Proc. 6th Int. Conf. Comput. Data Eng. (ICCDE), New York, NY, USA: Association for Computing Machinery, 2020, pp. 65-69",,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Singh, S.N., Sarraf, T., Sentiment analysis of a product based on user reviews using random forests algorithm (2020) Proc. 10th Int. Conf. Cloud Comput., Data Sci. Eng. (Confluence), pp. 112-116. , Jan",Sentiment analysis of a product based on user reviews using random forests algorithm,"Singh S.N., Sarraf T.",Sentiment analysis of a product based on user reviews using random forests algorithm,"After many sentiment analysis as well as many types of methods classify the reviews that is based on test data and reviewer's ratings which uses training., after reading reviews it is seen that star rating of reviewer do not always give a precise measure of his sentiment. This paper primarily focuses on analyzing customer reviews from the e-commerce space. Upon surveying popular e-commerce websites it can be observed that in several instances the product rating given by a customer is not consistent with the product review written by him/her. The problem is made complex by the fact that there is no standard scale to measure the rating that the user gives and the rating of the product are instinctive to the customers' view. In several cases it is seen that a product is rated 4 out of 5. However, the reviews detail that the customer's experience with the product is not favourable. Indeed, text reviews are a true picture of the product. To get rid of this problem, the stated system will give a boolean result i.e. whether the product is good or bad and the user does not need to read all the reviews to analyze the product. © 2020 IEEE.",10.1109/Confluence47617.2020.9058128,2020.0,4.0,Bag-of-words; Product reviews; Random forest classifier; Sentiment analysis,Cloud computing; Decision trees; Electronic commerce; Random forests; Sentiment analysis; Customer review; E-commerce websites; Product ratings; Product reviews; Standard scale; Star ratings; Test data; User reviews; Sales,2-s2.0-85083991802
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Ramadhan, W.P., Astri Novianty, S.T.M.T., Casi Setianingsih, S.T.M.T., Sentiment analysis using multinomial logistic regression (2017) Proc. Int. Conf. Control, Electron., Renew. Energy Commun. (ICCREC), pp. 46-49. , Sep",Sentiment analysis using multinomial logistic regression,"Ramadhan W.P., Novianty A., Setianingsih C.",Sentiment analysis using multinomial logistic regression,"Data amount becomes rapidly increased in today's era. Data can be in form of text, picture, voice, and video. Social media is one factor of the data increase as everybody expresses, gives opinion, and even complains in social media. The first step is data collection used API twitter with each candidate names on Jakarta Governor Election. The collected data then became input for preprocessing step. The next step is extracted-each tweet's feature to be listed. The list of features were transformed into feature vector in binary form and transformed again used Tf-idf method. Dataset consists of two kinds of data, training and testing. Training was labeled manually. K-Fold Cross Validation is used to test algorithm performance. Based on the result of the test, accuracy obtained reached 74% in average with composition of training data and testing data by 90:10. Changed folding amount gave no impact to the accuracy level. © 2017 IEEE.",10.1109/ICCEREC.2017.8226700,2017.0,28.0,multinomial logistic regression; softmax regression; text mining; twitter,Data mining; Natural language processing systems; Sentiment analysis; Social networking (online); Statistical tests; Data collection; K fold cross validations; Multinomial logistic regression; Pre-processing step; Softmax regressions; Text mining; Training and testing; twitter; Regression analysis,2-s2.0-85045328391
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Chakraborty, S., Biswas, A., Bose, B., Tiwari, S., Sentiment analysis of review datasets using naive Bayes and K-NN classifier (2016) Proc. IJIEEB, Kolkata, India, 8, pp. 54-62",Sentiment analysis of review datasets using naive Bayes and K-NN classifier,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Es-Sabery, F., Hair, A., An improved ID3 classification algorithm based on correlation function and weighted attribute* (2019) Proc. Int. Conf. Intell. Syst. Adv. Comput. Sci. (ISACS), pp. 1-8. , Dec",An improved ID3 classification algorithm based on correlation function and weighted attribute*,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Severyn, A., Moschitti, A., Twitter sentiment analysis with deep convolutional neural networks (2015) Proc. 38th Int. ACM SIGIR Conf. Res. Develop. Inf. Retr., pp. 959-962. , Aug",Twitter sentiment analysis with deep convolutional neural networks,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Vassilev, A., (2019) BowTie-A Deep Learning Feedforward Neural Network for Sentiment Analysis, , Gaithersburg, MD, USA: National Institute of Standards and Technology",,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Li, D., Qian, J., Text sentiment analysis based on long short-term memory (2016) Proc. 1st IEEE Int. Conf. Comput. Commun. Internet (ICCCI), pp. 471-475. , Oct",Text sentiment analysis based on long short-term memory,"Li D., Qian J.",Text sentiment analysis based on long short-term memory,"With the rapid development of Internet and big explosion of text data, it has been a very significant research subject to extract valuable information from text ocean. To realize multi-classification for text sentiment, this paper promotes a RNN language model based on Long Short Term Memory (LSTM), which can get complete sequence information effectively. Compared with the traditional RNN language model, LSTM is better in analyzing emotion of long sentences. And as a language model, LSTM is applied to achieve multi-classification for text emotional attributes. So though training different emotion models, we can know which emotion the sentence belongs to by using these emotion models. And numerical experiments show that it can produce better accuracy rate and recall rate than the conventional RNN. © 2016 IEEE.",10.1109/CCI.2016.7778967,2016.0,62.0,LSTM; RNN; sentiment analysis,Brain; Classification (of information); Computational linguistics; Data mining; Language model; Long short term memory; LSTM; Multi-classification; Numerical experiments; Research subjects; Sentiment analysis; Sequence informations; Text processing,2-s2.0-85010281232
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Kuta, M., Morawiec, M., Kitowski, J., Sentiment analysis with tree-structured gated recurrent units (2017) Text, Speech, and Dialogue, K. Ekstein, and V. Matousek, Eds. Cham, Switzerland: Springer, pp. 74-82",Sentiment analysis with tree-structured gated recurrent units,"Kuta M., Morawiec M., Kitowski J.",Sentiment analysis with tree-structured gated recurrent units,"Advances in neural network models and deep learning mark great impact on sentiment analysis, where models based on recursive or convolutional neural networks show state-of-the-art results leaving behind non-neural models like SVM or traditional lexicon-based approaches. We present Tree-Structured Gated Recurrent Unit network, which exhibits greater simplicity in comparison to the current state of the art in sentiment analysis, Tree-Structured LSTM model. © Springer International Publishing AG 2017.",10.1007/978-3-319-64206-2_9,2017.0,5.0,Gated Recurrent Unit; Long Short-Term Memory; Recursive neural network; Sentiment analysis; Tree-Structured GRU,Forestry; Long short-term memory; Neural networks; Convolutional neural network; Gated Recurrent Unit; Neural models; Neural network model; Recursive neural networks; Sentiment analysis; State of the art; Tree-structured; Data mining,2-s2.0-85028673833
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Al-Smadi, M., Qawasmeh, O., Al-Ayyoub, M., Jararweh, Y., Gupta, B., Deep recurrent neural network vs. Support vector machine for aspectbased sentiment analysis of arabic hotels' reviews (2018) J. Comput. Sci., 27, pp. 386-393. , Jul",Deep recurrent neural network vs. Support vector machine for aspectbased sentiment analysis of arabic hotels' reviews,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Rezaeinia, S.M., Rahmani, R., Ghodsi, A., Veisi, H., Sentiment analysis based on improved pre-trained word embeddings (2019) Expert Syst. Appl., 117, pp. 139-147. , Mar",Sentiment analysis based on improved pre-trained word embeddings,"Rezaeinia S.M., Rahmani R., Ghodsi A., Veisi H.",Sentiment analysis based on improved pre-trained word embeddings,"Sentiment analysis is a fast growing area of research in natural language processing (NLP) and text classifications. This technique has become an essential part of a wide range of applications including politics, business, advertising and marketing. There are various techniques for sentiment analysis, but recently word embeddings methods have been widely used in sentiment classification tasks. Word2Vec and GloVe are currently among the most accurate and usable word embedding methods which can convert words into meaningful vectors. However, these methods ignore sentiment information of texts and need a large corpus of texts for training and generating exact vectors. As a result, because of the small size of some corpora, researcher often have to use pre-trained word embeddings which were trained on other large text corpora such as Google News with about 100 billion words. The increasing accuracy of pre-trained word embeddings has a great impact on sentiment analysis research. In this paper, we propose a novel method, Improved Word Vectors (IWV), which increases the accuracy of pre-trained word embeddings in sentiment analysis. Our method is based on Part-of-Speech (POS) tagging techniques, lexicon-based approaches, word position algorithm and Word2Vec/GloVe methods. We tested the accuracy of our method via different deep learning models and benchmark sentiment datasets. Our experiment results show that Improved Word Vectors (IWV) are very effective for sentiment analysis. © 2018",10.1016/j.eswa.2018.08.044,2019.0,103.0,Deep learning; GloVe; Natural language processing; Sentiment analysis; Word embeddings; Word2Vec,Classification (of information); Data mining; Deep learning; Linguistics; Marketing; Sentiment analysis; Vectors; Embeddings; GloVe; Learning models; Part of speech tagging; Sentiment classification; Text classification; Word embedding; Word2Vec; Natural language processing systems,2-s2.0-85054032852
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Li, Y., Shen, B., Research on sentiment analysis of microblogging based on LSA and TF-IDF (2017) Proc. 3rd IEEE Int. Conf. Com-put. Commun. (ICCC), pp. 2584-2588. , Dec",Research on sentiment analysis of microblogging based on LSA and TF-IDF,"Li Y., Shen B.",Research on sentiment analysis of microblogging based on LSA and TF-IDF,"As a typical social network application, the impact of microblogging on people has penetrated into all aspects, which attracts more and more scholars to carry out in-depth study on microblogging. The sentiment analysis of microblogging text is the hot studying field now. Feature selection and extraction is one of the core parts of microblogging text sentiment analysis, TF-IDF algorithm is the most widely used method in selecting features. Although the TF-IDF algorithm is simple to use, there is still a problem of semantic deletion on it, that is, it ignores the semantic information contained in the text. To solve the problem, LSA is introduced in this paper. Firstly, the eigenvectors generated by TF-IDF algorithm is decomposed by singular value. Then, calculating the cosine value between the row vectors of the decomposition results to identify the similarity between the words, which realizes the feature extraction and makes up for the deficiency of TF-IDF. Finally, the extracted features are applied into four classification algorithms to verify the effectiveness of the proposed method. The experimental results show that the introcuction of LSA can make improvements of microblogging text classification in accuracy, recall and F value. © 2017 IEEE.",10.1109/CompComm.2017.8323002,2018.0,11.0,eigenvectors; LSA; microblogging; sentiment analysis; TF-IDF,Classification (of information); Data mining; Eigenvalues and eigenfunctions; Extraction; Feature extraction; Semantics; Sentiment analysis; Classification algorithm; Feature selection and extractions; Micro-blogging; Network applications; Semantic information; Text classification; TF-IDF; TF-IDF algorithms; Text processing,2-s2.0-85049689803
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Cummins, N., Amiriparian, S., Ottl, S., Gerczuk, M., Schmitt, M., Schuller, B., Multimodal bag-of-words for cross domains sentiment analysis (2018) Proc. IEEE Int. Conf. Acoust., Speech Signal Process. (ICASSP), pp. 4954-4958. , Apr",Multimodal bag-of-words for cross domains sentiment analysis,"Cummins N., Amiriparian S., Ottl S., Gerczuk M., Schmitt M., Schuller B.",Multimodal bag-of-words for cross domains sentiment analysis,"The advantages of using cross domain data when performing text-based sentiment analysis have been established; however, similar findings have yet to be observed when performing multimodal sentiment analysis. A potential reason for this is that systems based on feature extracted from speech and facial features are susceptible to confounding effecting caused by different recording conditions associated with data collected in different locations. In this regard, we herein explore different Bag-of-Words paradigms to aid sentiment detection by providing training material from an additional dataset. Key results presented indicate that using a Bag-of-Words extraction paradigm that takes into account information from both the test domain and the out of domain datasets yields gains in system performance. © 2018 IEEE.",10.1109/ICASSP.2018.8462660,2018.0,21.0,Bag-of-Words; Cross Domain; Deep Spectrum Features; Multimodal; Sentiment Analysis,,2-s2.0-85054241452
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Liu, B., Zhou, Y., Sun, W., Character-level hybrid convolutional and recurrent neural network for fast-text categorization (2020) Proc. ELM, J. Cao, C. M. Vong, Y. Miche, and A. Lendasse, Eds. Cham, Switzerland: Springer, pp. 108-117",Character-level hybrid convolutional and recurrent neural network for fast-text categorization,"Liu B., Zhou Y., Sun W.",Character-level hybrid convolutional and recurrent neural network for fast-text categorization,[No abstract available],,2020.0,1.0,,,2-s2.0-85106843489
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Modha, S., Mandl, T., Majumder, P., Patel, D., Tracking hate in social media: Evaluation, challenges and approaches (2020) SN Comput. Sci., 1, p. 105. , Mar",challenges and approaches,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Devi, B.L., Bai, V.V., Ramasubbareddy, S., Govinda, K., Sentiment analysis on movie reviews (2020) Emerging Research in Data Engineering Systems and Computer Communications, P. V. Krishna and M. S. Obaidat, Eds. Singapore: Springer, pp. 321-328",Sentiment analysis on movie reviews,"Lakshmi Devi B., Varaswathi Bai V., Ramasubbareddy S., Govinda K.",Sentiment analysis on movie reviews,"Movie reviews help users decide if the movie is worth their time. A summary of all reviews for a movie can help users make this decision by not wasting their time reading all reviews. Movie-rating websites are often used by critics to post comments and rate movies which help viewers decide if the movie is worth watching. Sentiment analysis can determine the attitude of critics depending on their reviews. Sentiment analysis of a movie review can rate how positive or negative a movie review is and hence the overall rating for a movie. Therefore, the process of understanding if a review is positive or negative can be automated as the machine learns through training and testing the data. This project aims to rate reviews using two classifiers and compare which gives better and more accurate results. Classification is a data mining methodology that assigns classes to a collection of data in order to help in more accurate predictions and analysis. Naïve Bayes and decision tree classifications will be used and the results of sentiment analysis compared. © Springer Nature Singapore Pte Ltd 2020.",10.1007/978-981-15-0135-7_31,2020.0,4.0,Decision tree; Movie reviews; Naive Bayes; Prediction; SLIQ,Data mining; Decision trees; Forecasting; Sentiment analysis; Accurate prediction; Decision tree classification; Movie ratings; Movie reviews; Naive bayes; SLIQ; Training and testing; Motion pictures,2-s2.0-85079664314
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Bose, R., Dey, R.K., Roy, S., Sarddar, D., Sentiment Analysis on Online Product Reviews (2020) In: M. Tuba, S. Akashe, and A. Joshi, (Eds.) Inf. Commun. Technol. For Sustain. Develop., pp. 559-569. , Springer, Singapore",Sentiment Analysis on Online Product Reviews,"Bose R., Dey R.K., Roy S., Sarddar D.",Sentiment Analysis on Online Product Reviews,"Today, people are exchanging their thoughts through online Web forums, blogs, and different social media platforms. Sometimes, they are giving reviews and opinions on different products, brand, and their services. Their reviews toward a product not only improve the product quality but also influence purchase decisions of the consumers. Thus, product review analysis is a widely accepted platform where consumer can easily aware of their requirements. In this experiment, we track 568,454 fine food reviews of 74,258 products and 256,059 users on Amazon over a period of ten years. To analyze the result, we select six most popular products and users based on the plain text review, and NRC emotion lexicon is used which can be categorized eight basic emotions and two sentiments. Word cloud also help our research to make comparisons between the eight emotion categories. Our results show that how sentiment analysis will help to identify the consumers’ behaviors and overcome those risks to meet the consumers’ satisfaction. © Springer Nature Singapore Pte Ltd. 2020.",10.1007/978-981-13-7166-0_56,2020.0,3.0,Amazon’s customer reviews; Digital marketing; Electronic word-of-mouth (e-WOM); Sentiment analysis; Unstructured data,Risk assessment; Sentiment analysis; Customer review; Digital marketing; Electronic word of mouths; Online product reviews; Product review analysis; Purchase decision; Social media platforms; Unstructured data; Consumer behavior,2-s2.0-85068195855
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Uban, A.-S., Dinu, L.P., On transfer learning for detecting abusive language online (2019) Advances in Computational Intelligence, I. Rojas, G. Joya, and A. Catala, Eds. Cham, Switzerland: Springer, pp. 688-700",On transfer learning for detecting abusive language online,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Rosa, H., Pereira, N., Ribeiro, R., Ferreira, P.C., Carvalho, J.P., Oliveira, S., Coheur, L., Trancoso, I., Automatic cyberbullying detection: A systematic review (2019) Comput. Hum. Behav., 93, pp. 333-345. , Apr",Automatic cyberbullying detection: A systematic review,"Rosa H., Pereira N., Ribeiro R., Ferreira P.C., Carvalho J.P., Oliveira S., Coheur L., Paulino P., Veiga Simão A.M., Trancoso I.",Automatic cyberbullying detection: A systematic review,"Automatic cyberbullying detection is a task of growing interest, particularly in the Natural Language Processing and Machine Learning communities. Not only is it challenging, but it is also a relevant need given how social networks have become a vital part of individuals' lives and how dire the consequences of cyberbullying can be, especially among adolescents. In this work, we conduct an in-depth analysis of 22 studies on automatic cyberbullying detection, complemented by an experiment to validate current practices through the analysis of two datasets. Results indicated that cyberbullying is often misrepresented in the literature, leading to inaccurate systems that would have little real-world application. Criteria concerning cyberbullying definitions and other methodological concerns seem to be often dismissed. Additionally, there is no uniformity regarding the methodology to evaluate said systems and the natural imbalance of datasets remains an issue. This paper aims to direct future research on the subject towards a viewpoint that is more coherent with the definition and representation of the phenomenon, so that future systems can have a practical and impactful application. Recommendations on future works are also made. © 2018 Elsevier Ltd",10.1016/j.chb.2018.12.021,2019.0,55.0,Abusive language; Automatic cyberbullying detection; Cyberbullying; Machine learning; Natural language processing; Social networks,Artificial intelligence; Learning algorithms; Learning systems; Natural language processing systems; Social networking (online); Abusive language; Current practices; Cyber bullying; In-depth analysis; Machine learning communities; Real-world; Systematic Review; Computer crime; adolescent; article; human; machine learning; natural language processing; social network; systematic review,2-s2.0-85059576433
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Sharma, D., Sabharwal, M., Goyal, V., Vij, M., Sentiment analysis techniques for social media data: A review (2020) Proc. 1st Int. Conf. Sus-tain. Technol. Comput. Intell., A. K. Luhach, J. A. Kosa, R. C. Poonia, X.-Z. Gao, and D. Singh, Eds. Singapore: Springer, pp. 75-90",Sentiment analysis techniques for social media data: A review,"Sharma D., Sabharwal M., Goyal V., Vij M.",Sentiment analysis techniques for social media data: A review,"The world is going to digitize day by day. A lot of data generated by the social website users that play an essential role in decision-making. It is impossible to read the whole text, so sentiment analysis make it easy by providing the polarity to the text and classify text into positive and negative classes. Classification task can be performed by using different algorithms results in a different level of accuracy. The purpose of the survey is to provide an overview of various methods that deal with sentiment analysis. The review also presented a comparative analysis of various sentimental analysis techniques with their performance measurement. © Springer Nature Singapore Pte Ltd. 2020",10.1007/978-981-15-0029-9_7,2020.0,9.0,Decision-making; Opinion mining; Sentiment analysis,Artificial intelligence; Decision making; Analysis techniques; Classification tasks; Comparative analysis; Opinion mining; Performance measurements; Social media datum; Sentiment analysis,2-s2.0-85076848234
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Alom, M.Z., Taha, T.M., Yakopcic, C., Westberg, S., Sidike, P., Nasrin, M.S., Hasan, M., Asari, V.K., A stateof-the-art survey on deep learning theory and architectures (2019) Electronics, 8 (3), p. 292. , Mar",A stateof-the-art survey on deep learning theory and architectures,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Beer, C., Fuzzy thinking: The new science of fuzzy logic. Bart Kosko (1995) Quart. Rev. Biol., 70 (2), p. 210",Fuzzy thinking: The new science of fuzzy logic. Bart Kosko,Beer C.,Fuzzy thinking: The new science of fuzzy logic. Bart Kosko,[No abstract available],,1995.0,1.0,,,2-s2.0-85106798603
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Zadeh, L.A., Fuzzy logic = computing with words (1996) IEEE Trans. Fuzzy Syst., 4 (2), pp. 103-111. , May",Fuzzy logic = computing with words,Zadeh L.A.,Fuzzy logic = computing with words,"As its name suggests, computing with words (CW) is a methodology in which words are used in place of numbers for computing and reasoning. The point of this note is that fuzzy logic plays a pivotal role in CW and vice-versa. Thus, as an approximation, fuzzy logic may be equated to CW. There are two major imperatives for computing with words. First, computing with words is a necessity when the available information is too imprecise to justify the use of numbers, and second, when there is a tolerance for imprecision which can be exploited to achieve tractability, robustness, low solution cost, and better rapport with reality. Exploitation of the tolerance for imprecision is an issue of central importance in CW. In CW, a word is viewed as a label of a granule; that is, a fuzzy set of points drawn together by similarity, with the fuzzy set playing the role of a fuzzy constraint on a variable. The premises are assumed to be expressed as propositions in a natural language. For purposes of computation, the propositions are expressed as canonical forms which serve to place in evidence the fuzzy constraints that are implicit in the premises. Then, the rules of inference in fuzzy logic are employed to propagate the constraints from premises to conclusions. At this juncture, the techniques of computing with words underlie - in one way or another - almost all applications of fuzzy logic. In coming years, computing with words is likely to evolve into a basic methodology in its own right with wide-ranging ramifications on both basic and applied levels. © 1996 IEEE.",10.1109/91.493904,1996.0,2324.0,,Computational linguistics; Constraint theory; Natural language processing systems; Computing with words (CW); Fuzzy constraints; Fuzzy sets,2-s2.0-0030142764
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Es-Sabery, F., Hair, A., A MapReduce C4.5 decision tree algorithm based on fuzzy rule-based system (2020) Fuzzy Inf. Eng., pp. 1-28. , Jun",A MapReduce C4.5 decision tree algorithm based on fuzzy rule-based system,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Jin, N., Wu, J., Ma, X., Yan, K., Mo, Y., Multi-task learning model based on multi-scale CNN and LSTM for sentiment classification (2020) IEEE Access, 8, pp. 77060-77072",Multi-task learning model based on multi-scale CNN and LSTM for sentiment classification,"Jin N., Wu J., Ma X., Yan K., Mo Y.",Multi-task learning model based on multi-scale CNN and LSTM for sentiment classification,"Sentiment classification is an interesting and crucial research topic in the field of natural language processing (NLP). Data-driven methods, including machine learning and deep learning techniques, provide one direct and effective solution to solve the sentiment classification problem. However, the classification performance declines when the input includes review comments for multiple tasks. The most appropriate way of constructing a sentiment classification model under multi-tasking circumstances remains questionable in the related field. In this study, aiming at the multi-tasking sentiment classification problem, we propose a multi-task learning model based on a multi-scale convolutional neural network (CNN) and long short term memory (LSTM) for multi-task multi-scale sentiment classification (MTL-MSCNN-LSTM). The model comprehensively utilizes and properly handles global features and local features of different scales of text to model and represent sentences. The multi-task learning framework improves the encoder quality, simultaneously improving the results of emotion classification. Six different types of commodity review datasets were employed in the experiment. Using accuracy and F1-score as the metrics to evaluate the performance of the proposed model, comparing with methods such as single-task learning and LSTM encoder, the proposed MTL-MSCNN-LSTM model outperforms most of the existing methods. © 2013 IEEE.",10.1109/ACCESS.2020.2989428,2020.0,16.0,long short term memory; multi-scale convolutional neural network; multi-task learning model; Sentiment classification,Convolutional neural networks; Deep learning; Learning algorithms; Learning systems; Linearization; Multi-task learning; Multitasking; Natural language processing systems; Signal encoding; Classification performance; Data-driven methods; Effective solution; Emotion classification; Learning techniques; NAtural language processing; Sentiment classification; Single task learning; Long short-term memory,2-s2.0-85084802469
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Almeida, F., Xexéo, G., (2019) Word Embeddings: A Survey, , http://arxiv.org/abs/1901.09069, arXiv:1901.09069",,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Lan, Y., Hao, Y., Xia, K., Qian, B., Li, C., Stacked residual recurrent neural networks with cross-layer attention for text classification (2020) IEEE Access, 8, pp. 70401-70410",Stacked residual recurrent neural networks with cross-layer attention for text classification,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Lin, Y., Li, J., Yang, L., Xu, K., Lin, H., Sentiment analysis with comparison enhanced deep neural network (2020) IEEE Access, 8, pp. 78378-78384",Sentiment analysis with comparison enhanced deep neural network,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Liu, G., Xu, X., Deng, B., Chen, S., Li, L., A hybrid method for bilingual text sentiment classification based on deep learning (2016) Proc. 17th IEEE/ACIS Int. Conf. Softw. Eng., Artif. Intell., Netw. Paral-lel/Distrib. Comput. (SNPD), pp. 93-98. , May",A hybrid method for bilingual text sentiment classification based on deep learning,"Liu G., Xu X., Deng B., Chen S., Li L.",A hybrid method for bilingual text sentiment classification based on deep learning,"Text sentiment classification has occupied a pivotal position in sentiment analysis research, it offers important opinion mining functions. Nowadays, with explosion of information, many researchers are focusing on sentiment classification research on massive amounts of data. However, the traditional machine learning methods cannot acquire text semantic information and most research achievements are about single language, in this paper, a hybrid method which integrates the deep learning features and shallow learning features is proposed. The hybrid method can not only realize single language text sentiment classification but realize bilingual text sentiment classification as well. Models such as recurrent neural networks (RNNs) with long short term memory(LSTM), Naïve Bayes Support Vector Machine (NB-SVM), word vectors and bag-of-words are explored. Firstly, these models are studied separately in sentiment classification task. The paper then integrates the above methods as a whole to complete the task. Different combination strategies are discussed regarding the contribution of each method. The experiments show that the accuracy can reach 89% and the hybrid method performs much better than any other method individually. The proposed method achieves a performance close to the state-of-the-art methods based on the had-engineered features. What's more, the hybrid model can learn more linguistic phenomena with the growth of the accuracy of emotional tendency discrimination when more background knowledge is available. © 2016 IEEE.",10.1109/SNPD.2016.7515884,2016.0,11.0,deep learning; neural network; opinion mining; sentiment analysis; text sentiment classification,Artificial intelligence; Computational linguistics; Data mining; Learning systems; Neural networks; Recurrent neural networks; Semantics; Software engineering; Support vector machines; Text processing; Combination strategies; Deep learning; Machine learning methods; Opinion mining; Recurrent neural network (RNNs); Sentiment analysis; Sentiment classification; State-of-the-art methods; Classification (of information),2-s2.0-84983372744
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Jain, A., Jain, V., Sentiment classification of Twitter data belonging to renewable energy using machine learning (2019) J. Inf. Optim. Sci., 40 (2), pp. 521-533. , Feb",Sentiment classification of Twitter data belonging to renewable energy using machine learning,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Yenter, A., Verma, A., Deep CNN-LSTM with combined kernels from multiple branches for IMDb review sentiment analysis (2017) Proc. IEEE 8th Annu. Ubiquitous Comput., Electron. Mobile Commun. Conf. (UEMCON), pp. 540-546. , Oct",Deep CNN-LSTM with combined kernels from multiple branches for IMDb review sentiment analysis,"Yenter A., Verma A.",Deep CNN-LSTM with combined kernels from multiple branches for IMDb review sentiment analysis,"Deep learning neural networks have made significant progress in the area of image and video analysis. This success of neural networks can be directed towards improvements in textual sentiment classification. In this paper, we describe a novel approach to sentiment analysis through the use of combined kernel from multiple branches of convolutional neural network (CNN) with Long Short-term Memory (LSTM) layers. Our combination of CNN and LSTM schemes produces a model with the highest reported accuracy on the Internet Movie Database (IMDb) review sentiment dataset. Additionally, we present multiple architecture variations of our proposed model to illustrate our attempts to increase accuracy while minimizing overfitting. We experiment with numerous regularization techniques, network structures, and kernel sizes to create five high-performing models for comparison. These models are capable of predicting the sentiment polarity of reviews from the IMDb dataset with accuracy above 89%. Firstly, the accuracy of our best performing proposed model surpasses the previously published models and secondly it vastly improves upon the baseline CNN+LSTM model. The capability of the combined kernel from multiple branches of CNN based LSTM architecture could also be lucrative towards other datasets for sentiment analysis or simply text classification. Furthermore, the proposed model has the potential in machine learning in video and audio. © 2017 IEEE.",10.1109/UEMCON.2017.8249013,2017.0,58.0,CNN; IMDb; LSTM; Neural network; Sentiment analysis; Text classification,Classification (of information); Deep learning; Mobile telecommunication systems; Multilayer neural networks; Network architecture; Neural networks; Sentiment analysis; Ubiquitous computing; Convolutional neural network; IMDb; Internet movie database; Learning neural networks; LSTM; Regularization technique; Sentiment classification; Text classification; Long short-term memory,2-s2.0-85046357381
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Xing, F.Z., Cambria, E., Welsch, R.E., Intelligent asset allocation via market sentiment views (2018) IEEE Comput. Intell. Mag., 13 (4), pp. 25-34. , Nov",Intelligent asset allocation via market sentiment views,"Xing F.Z., Cambria E., Welsch R.E.",Intelligent asset allocation via market sentiment views,"The sentiment index of market participants has been extensively used for stock market prediction in recent years. Many financial information vendors also provide it as a service. However, utilizing market sentiment under the asset allocation framework has been rarely discussed. In this article, we investigate the role of market sentiment in an asset allocation problem. We propose to compute sentiment time series from social media with the help of sentiment analysis and text mining techniques. A novel neural network design, built upon an ensemble of evolving clustering and long short-term memory, is used to formalize sentiment information into market views. These views are later integrated into modern portfolio theory through a Bayesian approach. We analyze the performance of this asset allocation model from many aspects, such as stability of portfolios, computing of sentiment time series, and profitability in our simulations. Experimental results show that our model outperforms some of the most successful forecasting techniques. Thanks to the introduction of the evolving clustering method, the estimation accuracy of market views is significantly improved. © 2018 IEEE.",10.1109/MCI.2018.2866727,2018.0,45.0,,Bayesian networks; Computation theory; Data mining; Investments; Natural language processing systems; Neural networks; Sentiment analysis; Time series; Time series analysis; Evolving clustering methods; Financial information; Forecasting techniques; Market participants; Modern portfolio theories; Novel neural network; Stock market prediction; Text mining techniques; Commerce,2-s2.0-85055283489
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," González, J.-A., Pla, F., Hurtado, L.-F., ELiRF-UPV at SemEval-2017 task 4: Sentiment analysis using deep learning (2017) Proc. 11th Int. Workshop Semantic Eval. (SemEval-), pp. 723-727",ELiRF-UPV at SemEval-2017 task 4: Sentiment analysis using deep learning,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Wang, G., Qiao, J., Bi, J., Li, W., Zhou, M., TL-GDBN: Growing deep belief network with transfer learning (2019) IEEE Trans. Autom. Sci. Eng., 16 (2), pp. 874-885. , Apr",TL-GDBN: Growing deep belief network with transfer learning,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Hameed, Z., Garcia-Zapirain, B., Sentiment classification using a single-layered BiLSTM model (2020) IEEE Access, 8, pp. 73992-74001",Sentiment classification using a single-layered BiLSTM model,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Han, L., Mihaela, C., Fuzzy rule based systems for interpretable sentiment analysis (2017) Proc. 9th Int. Conf. Adv. Comput. Intell., pp. 129-136. , Feb",Fuzzy rule based systems for interpretable sentiment analysis,"Liu H., Cocea M.",Fuzzy rule based systems for interpretable sentiment analysis,"Sentiment analysis, which is also known as opinion mining, aims to recognise the attitude or emotion of people through natural language processing, text analysis and computational linguistics. In recent years, many studies have focused on sentiment classification in the context of machine learning, e.g. to identify that a sentiment is positive or negative. In particular, the bag-of-words method has been popularly used to transform textual data into structured data, in order to enable the direct use of machine learning algorithms for sentiment classification. Through the bag-of-words method, each single term in a text document is turned into a single attribute to make up a structured data set, which results in high dimensionality of the data set and thus negative impact on the interpretability of computational models for sentiment analysis. This paper proposes the use of fuzzy rule based systems as computational models towards accurate and interpretable analysis of sentiments. The use of fuzzy logic is better aligned with the inherent uncertainty of language, while the 'white box' characteristic of the rule based learning approaches leads to better interpretability of the results. The proposed approach is tested on four datasets containing movie reviews; the aim is to compare its performance in terms of accuracy with two other approaches for sentiment analysis that are known to perform very well. The results indicate that the fuzzy rule based approach performs marginally better than the well-known machine learning techniques, while reducing the computational complexity and increasing the interpretability. © 2017 IEEE.",10.1109/ICACI.2017.7974497,2017.0,35.0,data mining; fuzzy rule based systems; machine learning; sentiment analysis; text classification,Artificial intelligence; Character recognition; Classification (of information); Computation theory; Computational methods; Fuzzy inference; Fuzzy logic; Fuzzy rules; Learning algorithms; Learning systems; Natural language processing systems; Text processing; Computational model; High dimensionality; Machine learning techniques; Rule-based learning; Sentiment analysis; Sentiment classification; Text classification; Use of fuzzy logic; Data mining,2-s2.0-85027447753
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Wu, K., Zhou, M., Lu, X.S., Huang, L., A fuzzy logic-based text classification method for social media data (2017) Proc. IEEE Int. Conf. Syst., Man, Cybern. (SMC), pp. 1942-1947. , Oct",A fuzzy logic-based text classification method for social media data,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Vashishtha, S., Susan, S., Fuzzy rule based unsupervised sentiment analysis from social media posts (2019) Expert Syst. Appl., 138. , Dec",Fuzzy rule based unsupervised sentiment analysis from social media posts,"Vashishtha S., Susan S.",Fuzzy rule based unsupervised sentiment analysis from social media posts,"In this paper, we compute the sentiment of social media posts using a novel set of fuzzy rules involving multiple lexicons and datasets. The proposed fuzzy system integrates Natural Language Processing techniques and Word Sense Disambiguation using a novel unsupervised nine fuzzy rule based system to classify the post into: positive, negative or neutral sentiment class. We perform a comparative analysis of our method on nine public twitter datasets, three sentiment lexicons, four state-of-the-art approaches for unsupervised Sentiment Analysis and one state-of-the-art method for supervised machine learning. Traditionally, Sentiment Analysis of twitter data is performed using a single lexicon. Our results can give an insight to researchers to choose which lexicon is best for social media. The fusion of fuzzy logic with lexicons for sentiment classification provides a new paradigm in Sentiment Analysis. Our method can be adapted to any lexicon and any dataset (two-class or three-class sentiment). The experiments on benchmark datasets yield higher performance for our approach as compared to the state-of-the-art. © 2019 Elsevier Ltd",10.1016/j.eswa.2019.112834,2019.0,44.0,Fuzzy rule; Lexicon; Sentiment analysis; Social media; Twitter,Benchmarking; Fuzzy logic; Fuzzy rules; Learning algorithms; Sentiment analysis; Social networking (online); Supervised learning; Lexicon; NAtural language processing; Sentiment classification; Social media; State-of-the-art approach; Supervised machine learning; Twitter; Word Sense Disambiguation; Fuzzy inference,2-s2.0-85069805425
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Abdul-Jaleel, M., Ali, Y.H., Ibrahim, N.J., Fuzzy logic and genetic algorithm based text classification Twitter (2019) Proc. 2nd Sci. Conf. Comput. Sci. (SCCS), pp. 93-98. , Mar",Fuzzy logic and genetic algorithm based text classification Twitter,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Wang, G., Jia, Q.-S., Qiao, J., Bi, J., Liu, C., A sparse deep belief network with efficient fuzzy learning framework (2020) Neural Netw., 121, pp. 430-440. , Jan",A sparse deep belief network with efficient fuzzy learning framework,"Wang G., Jia Q.-S., Qiao J., Bi J., Liu C.",A sparse deep belief network with efficient fuzzy learning framework,"Deep belief network (DBN) is one of the most feasible ways to realize deep learning (DL) technique, and it has been attracting more and more attentions in nonlinear system modeling. However, DBN cannot provide satisfactory results in learning speed, modeling accuracy and robustness, which is mainly caused by dense representation and gradient diffusion. To address these problems and promote DBN's development in cross-models, we propose a Sparse Deep Belief Network with Fuzzy Neural Network (SDBFNN) for nonlinear system modeling. In this novel framework, the sparse DBN is considered as a pre-training technique to realize fast weight-initialization and to obtain feature vectors. It can balance the dense representation to improve its robustness. A fuzzy neural network is developed for supervised modeling so as to eliminate the gradient diffusion. Its input happens to be the obtained feature vector. As a novel cross-model, SDBFNN combines the advantages of both pre-training technique and fuzzy neural network to improve modeling capability. Its convergence is also analyzed as well. A benchmark problem and a practical problem in wastewater treatment are conducted to demonstrate the superiority of SDBFNN. The extensive experimental results show that SDBFNN achieves better performance than the existing methods in learning speed, modeling accuracy and robustness. © 2019 Elsevier Ltd",10.1016/j.neunet.2019.09.035,2020.0,25.0,Deep belief network; Deep learning; Fuzzy neural network; Nonlinear system modeling; Sparse representation,Deep learning; Deep neural networks; Fuzzy inference; Fuzzy logic; Nonlinear systems; Wastewater treatment; Bench-mark problems; Deep belief network (DBN); Deep belief networks; Feature vectors; Nonlinear system modeling; Practical problems; Sparse representation; Weight initialization; Fuzzy neural networks; article; deep belief network; deep learning; diffusion; nonlinear system; velocity; waste water management; nonlinear system; Deep Learning; Nonlinear Dynamics,2-s2.0-85073015074
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Kim, Y., Convolutional neural networks for sentence classification (2014) Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP), pp. 1746-1751",Convolutional neural networks for sentence classification,Kim Y.,Convolutional neural networks for sentence classification,"We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification. © 2014 Association for Computational Linguistics.",10.3115/v1/d14-1181,2014.0,4151.0,,Convolution; Neural networks; Sentiment analysis; Classification tasks; Convolutional neural network; Hyper-parameter; Learning tasks; Question classification; Sentence classifications; Simple modifications; State of the art; Vectors,2-s2.0-84961376850
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Svozil, D., Kvasnicka, V., Pospichal, J., Introduction to multi-layer feed-forward neural networks (1997) Chemometrics Intell. Lab. Syst., 39 (1), pp. 43-62",Introduction to multi-layer feed-forward neural networks,"Svozil D., Kvasnička V., Pospíchal J.",Introduction to multi-layer feed-forward neural networks,Basic definitions concerning the multi-layer feed-forward neural networks are given. The back-propagation training algorithm is explained. Partial derivatives of the objective function with respect to the weight and threshold coefficients are derived. These derivatives are valuable for an adaptation process of the considered neural network. Training and generalisation of multi-layer feed-forward neural networks are discussed. Improvements of the standard back-propagation algorithm are reviewed. Example of the use of multi-layer feed-forward neural networks for prediction of carbon-13 NMR chemical shifts of alkanes is given. Further applications of neural networks in chemistry are reviewed. Advantages and disadvantages of multilayer feed-forward neural networks are discussed.,10.1016/S0169-7439(97)00061-0,1997.0,726.0,Back-propagation network; Neural networks,algorithm; analytic method; artificial neural network; carbon nuclear magnetic resonance; conference paper; internet; priority journal; protein folding; quantitative structure activity relation; spectroscopy,2-s2.0-0342871690
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Pourjavad, E., Mayorga, R.V., A comparative study and measuring performance of manufacturing systems with mamdani fuzzy inference system (2019) J. Intell. Manuf., 30 (3), pp. 1085-1097. , Mar",A comparative study and measuring performance of manufacturing systems with mamdani fuzzy inference system,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Fukushima, K., Neocognitron: A hierarchical neural network capable of visual pattern recognition (1988) Neural Netw., 1 (2), pp. 119-130",Neocognitron: A hierarchical neural network capable of visual pattern recognition,Fukushima K.,Neocognitron: A hierarchical neural network capable of visual pattern recognition,"A neural network model for visual pattern recognition, called the ""neocognitron,"" was previously proposed by the author. In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism of the neocognitron. The system has been implemented on a minicomputer and has been trained to recognize handwritten numerals. The neocognitron is a hierarchical network consisting of many layers of cells, and has variable connections between the cells in adjoining layers. It can acquire the ability to recognize patterns by learning, and can be trained to recognize any set of patterns. After finishing the process of learning, pattern recognition is performed on the basis of similarity in shape between patterns, and is not affected by deformation, nor by changes in size, nor by shifts in the position of the input patterns. In the hierarchical network of the neocognitron, local features of the input pattern are extracted by the cells of a lower stage, and they are gradually integrated into more global features. Finally, each cell of the highest stage integrates all the information of the input pattern, and responds only to one specific pattern. Thus, the response of the cells of the highest stage shows the final result of the pattern-recognition of the network. During this process of extracting and integrating features, errors in the relative position of local features are gradually tolerated. The operation of tolerating positional error a little at a time at each stage, rather than all in one step, plays an important role in endowing the network with an ability to recognize even distorted patterns. © 1988.",10.1016/0893-6080(88)90014-7,1988.0,564.0,,PATTERN RECOGNITION; HIERARCHICAL NEURAL NETWORK; IMAGE PATTERN RECOGNITION; NEOCOGNITRON; VISUAL PATTERN RECOGNITION; SYSTEMS SCIENCE AND CYBERNETICS,2-s2.0-0023846591
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Chen, Y., (2015) Convolutional Neural Network for Sentence Clas-sification. UWSpace, , http://hdl.handle.net/10012/9592, (Aug.), Accessed: Dec. 29 2020",,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," (2020) The Most Popular Research, Guides, News and More in Artificial Intelli-gence, , https://deepai.org/, Accessed: Dec. 29",,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Yousuf, H., Salloum, S., Survey analysis: Enhancing the security of vectorization by using word2vec and CryptDB (2020) Adv. Sci., Technol. Eng. Syst. J., 5 (4), pp. 374-380",Survey analysis: Enhancing the security of vectorization by using word2vec and CryptDB,"Yousuf H., Salloum S.",Survey analysis: Enhancing the security of vectorization by using word2vec and CryptDB,"Vectorization is extracting data from strings through Natural Language Processing by using different approaches; one of the best approaches used in vectorization is word2vec. To make the vectorized data secure, we must apply a security method, which will be CryptDB. The paper is analyzing the survey, which is created to interview security engineers through the SPSS tool. By analyzing the responses from software security engineers, it is seen that both word2vec and CryptDB works significantly. Word2vec is an effective vectorization approach, while CryptDB is an effective, secure database. In future work, we will be developing a secure vectorization using both approaches. © 2020 ASTES Publishers. All rights reserved.",10.25046/aj050443,2020.0,5.0,CryptDB; Natural Language Processing; Vectorisation; Word2vec,,2-s2.0-85091027234
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," (2020) COVID-19-Sentiments India[20/03/20-31/05/20], , https://kaggle.com/abhaydhiman/covid19-sentiments, Accessed: Dec. 30",,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Gangadharan, V., Gupta, D., Amritha, L., Athira, T.A., Paraphrase detection using deep neural network based word embedding techniques (2020) Proc. 4th Int. Conf. Trends Electron. Informat. (ICOEI), pp. 517-521. , Jun",Paraphrase detection using deep neural network based word embedding techniques,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Pennington, J., Socher, R., Manning, C., Glove: Global vectors for word representation (2014) Proc. Conf. Empirical Methods Natural Lang. Process. (EMNLP), pp. 1532-1543",Glove: Global vectors for word representation,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J., (2013) Distributed Representations of Words and Phrases and Their Compositionality, , http://arxiv.org/abs/1310.4546, arXiv:1310.4546",,,,,,,,,,
2-s2.0-85106847388,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier," Es-Sabery, F., Hair, A., Big data solutions proposed for cluster computing systems challenges: A survey (2020) Proc. 3rd Int. Conf. Netw., Inf. Syst. Secur., pp. 1-7. , Mar",Big data solutions proposed for cluster computing systems challenges: A survey,,,,,,,,,
