ID,Title,reasoning,out-of-distribution generaization,interpretability,reduced data,transferability,NeSy (check if the authors label their work as NeSy),performance,computational cost,comments,eliminate
2-s2.0-85064856751,A simple neural approach to spatial role labelling,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,better than baselines,,"candidate roles are selected based on a rules based algorithm which travers the dependency parse. In that sense, the symbolic structure of the sentence is leveraged to improve perfomramance",None of the fields are required.
2-s2.0-85067488804,Semantic Fake News Detection: A Machine Learning Perspective,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,no mention of reasoning. essentially this is another example of semantic enrichement,"If any of the checkboxes are selected, please provide details in the comments column."
2-s2.0-85072850367,Jointly Learning to Detect Emotions and Predict Facebook Reactions,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,on par with baselines,,FOL is embedded as constraints. none of the promises are relevant here.,
2-s2.0-85075592929,Complementing logical reasoning with sub-symbolic commonsense,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,on par with baselines,higher than baselines,"Logic Tensor Networks (LTNs). 
The authors describe two types of reasoning: ""subsymbolic commonsense"" (strongly correlated with associative learning), and ""axiomatic"" knowledge (predicates and logic formulas) for structured inference.
""LTNs can be be used to do after-training reasoning over combinations of axioms on which it was not trained on""
""Since LTNs are based on Neural Networks, they reach similar results while also achieving high explainability due to the fact that they ground first-order logic.""

Subsymbolic commonsense (word vectors) do not in my opinion represent the kind of commonsense we refer to when we talk about people's ability to use common sense, as in not touching a hot stove, or not 'cutting a door in half to fit the table through it' (as in the example from Marcus on GPT3). Should we make a distinction between commonsense knowledge and commonsense reasoning? and how would we do that? Is it justifiable to call word vetors commonsense knowledge, or is this just linguistic gymnastics to fit an agenda?
",
2-s2.0-85091286980,A Framework for a Comprehensive Conceptualization of Urban Constructs,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,no mention of reasoning. essentially this is another example of semantic enrichement,
2-s2.0-85096590382,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,better than baselines,,"""reasoning and inference on link prediction""
The authors talk about reasoning over graphs to mean link prediction. I'm not sure how this constitutes reasoning.This is a common practice, but perhaps it is misguided.
",
2-s2.0-85113718164,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,link prediction. reasoning is not mentioned explicitly.,
2FCUJH2G,From symbolic to sub-symbolic information in question classification,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,no mention of reasoning.,
JN4IZETG,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,on par with baselines,lower than baselines,"""autoBOT also explores novel representation types such as e.g., knowledgegraph based features, capable of exploiting the knowledge beyond the textual training data considered.""
no mention of reasoning.",
PD2A2ZVV,"Ontology based E-learning framework: A personalized, adaptive and context aware model",FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,NOT TEXT DATA.,1
4EF4BXD9,Solving the twitter sentiment analysis problem based on a machine learning-based approach,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,better than baselines,higher than baselines,VERY POOR QUALITY PAPER. basic feature engineering followed by SVM. ,1
7MMJY5BM,The CoRg Project: Cognitive Reasoning,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,,,"""The objective of CoRg is to develop a system for cognitive computing.""
""The aim of the CoRg project (Cognitive Reasoning) is to successfully complete a reasoning task with commonsense reasoning.""
""we have to find appropriate logics for cognitive reasoning. For a successful reasoning system, nowadays it seems to be important to combine automated reasoning with machine learning technology like recurrent neural networks."" The authors don't explictly state that this is neuro-symbolic. They state that in order to emulate human reasoning, systems need to be flexible, be able to deal with contradicting evidence, evolving evidence, have access to enourmous amounts of background knowledge, and include a combination of different techniques and logics.
",
7Q5JRVK2,Semantic-based regularization for learning and inference,TRUE,FALSE,FALSE,TRUE,FALSE,TRUE,better than baselines,,"symbolic logic is embedded in the regularization.
Reasoning takes place in the translated FOL formulas using t-norms.
The baselines are weak even for 2017. Since then GNNs have surpassed these results by far.",
JGU2SECC,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,better than baselines,,"Embeddings are learned by following the logic expressions encoded in the huffman trees which have been built from the data. In that sense it's fair to say that reasoning is taking place to generate the embeddings. Embeddings are then processed in the usual NN manner.

Kyle: If we could come up with a logic for language (as in news articles), then perhaps this is a promising approach to either finetune LLM, or bild a model from much smaller data.

each node of the tree () is a logic expression, thus hidden layers are interpretable.
Using Huffman trees to represent deep first-order logic knowledge - these are fed into a NN to learn vector embeddings, which in turn are used for classification.
Here knowledge is embedded in the represenations.As such it can be considered a case of semantic enrichment.
""We developed a knowledge-based neural network, in which nerve cells in a hidden layer consist of first-order logic.""
""knowledge embedding (i.e., high-dimensional and continuous vectors) is a feasible approach to complex reasoning that can not only retain the semantic information of knowledge, but also establish the quantifiable relationship among embeddings""
""the core argument against embeddings is the inability to capture more complex patterns of reasoning such as those enabled by first-order logic""



",
7YX447XS,Concept generalization and fusion for abstractive sentence generation,TRUE,TRUE,FALSE,FALSE,FALSE,FALSE,better than baselines,higher than baselines,"a case of semantic enrichment. where symbolic (structural) information is used to infer a hierarchy of concepts. In that sense one might call it reasoning. If I know that cat, dog, and fish entail pets, one can say this is learning entailment. ""The notion of entailment plays the most essential role in human logical thinking because any reasoning must invoke it.""(Cheng_1996)

""From a theoretical point of view the approach addresses some reasoning aspects. The fact that the system we propose tries to abstract sentences by generalizing and fusing concepts is a cognitive effort"". The authors accept to be doing reasoning, but they do not use the language of linguistics (entailment). They instead create their own vocabulary (generalizing and fusing).

weak baselines. in fairness, this paper is from 2016.
",
GS3TRUYZ,Semi-supervised learning for big social data analysis,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,better than baselines,,"AffectiveSpace - see also Chaturvedi2019264
""The key to performing commonsense reasoning is to find a good trade-off for knowledge representation."" Here this translates to the dimanetionality of the AffectiveSpace.
Humans reason by way of analogy. ""concepts and features that are aligned and possess high dot yields are ideal contenders for analogies.""
commonsense reasoning is defeasable.
commonsense knowledge is represented in ConceptNet, a semantic network based on WordNet and Cyc which includes word phrases and relations (isA, etc..) Compare to {Bianchi2019161} where (sub-symbolic) commonsense is the raw data. 
benchmarks are from 5 and 6 years prior to the study at hand.

",
KTEHK4MZ,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,,,"NOT TEXT DATA. ""KG reasoning, also known as KG completion"". 
",1
2PNFHS7L,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,,,reasoning on graphs - domain knowledge is leveraged. supervised dialog data is used for answer generation.,
6DWCY3EC,Assessing cognitive alignment in different types of dialog by means of a network model,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,"this is an older paper (2012), so needs to be seen from that perspective. It may be intereseting to revisit with GNNs.They don't talk about reasoning per se, but I would think that alignment in dialog implies some kind of reasoning on the part of the interlocutors. But that's neither here nor there. Authors draw parallels between human brain synapses and NNs. The paper is jargony. ",
NB39QA35,Learning to activate logic rules for textual reasoning,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,better than baselines,lower than baselines,"authors take the explicit stance that LLMs do not reason (hence, reasoning is/should be explicit). ""There is close, intrinsic relationship between reasoning and human language.""
""our work aims at precise reasoning, good interpretability of models, and perfect performances on small dataset (1k)""
""We redefine textual reasoning as a sequential decision-making problem, and solve it with reinforcement learning (RL). ""
""we generate symbolic relational tuples to take part in external reasoning process.""
",
B47SSE6P,Fuzzy commonsense reasoning for multimodal sentiment analysis,FALSE,FALSE,FALSE,FALSE,TRUE,FALSE,better than baselines,,"AffectiveSpace - see also Hussain20181662
""AffectiveSpace [8], a vector space model of commonsense concepts such as ‘beautiful painting’ or ‘poor writing’ [9].""
here transferability pertains to different languages rather than different domains, and addresses the issue of the lack of annotations in other languages, in particular, Spanish.
The authors use fuzzy logic to classify sentences with emotions. This is the symbolic part, as it requires incorporating explicit membership functions (which are leanred). ",
ACDMDN8W,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,,,"reasoning is refering to the neutrosophic rules for membership. this is mathematical reasoning, not human reasoning.",
EGI547RA,Question Answering Systems with Deep Learning-Based Symbolic Processing,TRUE,TRUE,FALSE,TRUE,FALSE,FALSE,better than baselines,,"reasoning is logical/mathematical. the authors don't use the term reasoning, instead they call it processing.",
3YFVRRKE,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,on par with baselines,,learning text representations by leveraging the linguistic structure (semantic/symbolic part). no mention of reasoning or other benefits.,
74QZV8X9,Robust reasoning over heterogeneous textual information for fact verification,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,worse than baselines,,VERY POOR QUALITY PAPER. unsubstantiated claims.,1
BAMGPUCX,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,better than baselines,lower than baselines,"reasoning takes palce at the fuzzy logic stage. 
""Fuzzy logic is proposed to handle uncertainty and vagueness data. The strength of fuzzy logic is its resemblance to human reasoning and natural language."" statement is backed up with several studies using fuzzy logic to deal with ambiguity, as well as comparing fuzzy logic to human reasoning.",
2-s2.0-85081087069,Cases without borders: Automating knowledge acquisition approach using deep autoencoders and siamese networks in case-based reasoning,FALSE,FALSE,FALSE,TRUE,FALSE,FALSE,better than baselines,lower than baselines,"CBR - reasoning consists of comparing output of NN to cases and selecting the most similar one. To call this reasoning is a stretch, but that's the name of the technique.",
2-s2.0-85073214635,Leveraging Recursive Processing for Neural-Symbolic Affect-Target Associations,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,better than baselines,,"leverages symbolic structure but does not reason in the sense that there is no logic, just rules. The authors refer to this as 'symbolic reasoning'. 
I'm not sure why the authors feel the need to mention ""commonsense principles"".  There is a single sentence in the text which is a tautology. ""Commonsense principles dictate that the context in which an entity exists will be local to the position of the entity in a sentence.""
""Our approach uses a mixture of commonsense principles, symbolic reasoning, and recursive neural learning to construct associations between affective labels and identified targets in natural language expressions.""",
2-s2.0-85083026315,Causal relation classification using convolutional neural networks and grammar tags,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,better than baselines,,"""The underlying hypothesis of the proposed approach establishes the link between language constructs and human thought constructs such as cause-effect pair."" The authors don't call it reasoning, but ""human thought constructs"" can be interpreted as human reasoning. it would be interesting to see if this approach meets the OOD , reduced data, and transferability promises. 
",
2-s2.0-85062227737,Using Deep Learning and an External Knowledge Base to Develop Human-Robot Dialogues,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,VERY POOR QUALITY PAPER. CBR. Reasoning is semantic similarity matching.,1
RDSQSBN7,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,better than baselines,,"""the graph reasoning layer can learn the representation features of different nodes and the mentions of the same entity have similar representations in the feature space."" reasoning over graphs can produce better representations in vector space.",
2-s2.0-85087103850,Automated ontology-based annotation of scientific literature using deep learning,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,better than baselines,,"""Inclusion of ontology semantics via subsumption reasoning yielded modest performance improvement.""
""Augmenting the models with subsumption semantics from the ontology hierarchy showed modest but certain gains in both F-1 score and semantic similarity"".
subsumption reasoning follows heirarchical rules. Here reasoning is rules. this component does not significantly improve performance.",
2-s2.0-85108908770,Web question answering with neurosymbolic program synthesis,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,better than baselines,,The authors don't explictly talk about reasoning. their system can be said to be doing reasoning by following rules using a DSL. ,
2-s2.0-85070277710,A novel NLP-fuzzy system prototype for information extraction from medical guidelines,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,better than baselines,,authors refer to reasoning in the explicit sense of following rules. No additioinal discussion of reasoning is present.,
2-s2.0-85083954234,"The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,better than baselines,lower than baselines,"The authors acknowledge impllict and explicit reasoning, but do not take a stance as to which is the more legitimate.


""We propose to use neural symbolic reasoning as a bridge to jointly learn visual concepts, words, and semantic parsing of sentences.""
""we propose an alternative representation learning approach through joint reasoning with language.""
""Our program executor works in a symbolic and deterministic manner. This feature ensures a transparent execution trace of the program.""
""The explicit program semantics enjoys compositionality, interpretability, and generalizability.""
""Using only 10% of the training images, our model is able to achieve a comparable results with the baselines trained on the full dataset.""
""Our model outperforms all baselines on data efficiency. This comes from the full disentanglement of visual concept learning and symbolic reasoning""
Future work:
Second, our model assumes a domain-specific language for describing formal semantics. The integration of formal semantics into the processing of complex natural language would be meaningful future work (Artzi & Zettlemoyer, 2013; Oh et al., 2017). We hope our paper could motivate future research in visual concept learning, language learning, and compositionality.
",
2-s2.0-85085038180,Team SVMrank: Leveraging feature-rich support vector machines for ranking explanations to elementary science questions,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,worse than baselines,,"athors choose to report performance in terms of the weakest baseline. In fact, their solution is the worst of all entries. Rules are the reasoning component, and the knowledgebase of explanations is refered to as commonsense knowledge.",
2-s2.0-85080593464,Attentive tensor product learning,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,better than baselines,,"TPR with attention replaces LSTM.
""The novelty of our approach lies in its ability to extract the grammatical structure of a sentence"". Perhaps it can be used in downstream tasks that can leverage this structure.",
2-s2.0-85081092649,Hybrid deep neural networks to predict socio-moral reasoning skills,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,,,"even though it has reasoning in the title, this is a classification model where expert knowledge is represented in the attention mechanism.",
2-s2.0-85085049747,Graph enhanced cross-domain text-to-SQL generation,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,better than baselines,,"using GNN is better than baselines, but not better than traditional data augmentation",
2-s2.0-85105153943,Mapping natural-language problems to formal-language solutions using structured neural representations,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,better than baselines,,"TPR
""State-of-the-art performance on two recently developed N2F tasks shows that the TPN2F model has significant structure learning ability on tasks requiring symbolic reasoning through program synthesis.""
""Next, we will combine largescale deep learning models such as BERT with TP-N2F to take advantage of structure learning for other generation tasks.""
The mapping step between the encoder and decoder is refered to as a ""reasoning module"", but no justification for this nomenclature is given. This is simply a MLP which transforms the encoded text input into a TPR. I cannot see the ""reasoning"" part here.
",
2-s2.0-85106687657,Just Add Functions: A Neural-Symbolic Language Model,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,better than baselines,,"NNLM (neural network language model) cannot learn a function such as f(x)=x+1 even when the corpus is very large. By contrast augmenting an NNLM (neural network language model) with a simple micro-models allows the model to generalize well on this simple function.
The downside is that the system requires domain expert knowledge to come up with metric functions and to identify classes of interest. So this is a domain specific solution rather than a general improvement over LLMs. What would be interesting to see is how this technique comapares to other domain specific finetuning techniques. Still, theoretically, the promise of OOD has been met which is likely to not be the case with simple finetuing of domain specific data.",