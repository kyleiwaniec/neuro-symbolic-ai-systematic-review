Authors,Author(s) ID,Title,Year,Source title,Volume,Issue,Art. No.,Page start,Page end,Page count,Cited by,DOI,Link,Abstract,Author Keywords,Index Keywords,Sponsors,Conference name,Conference date,Conference location,Conference code,Document Type,Publication Stage,Open Access,Source,EID
"Zadeh L.A.","24613216300;","Fuzzy sets",1965,"Information and Control","8","3",,"338","353",,55488,"10.1016/S0019-9958(65)90241-X","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34248666540&doi=10.1016%2fS0019-9958%2865%2990241-X&partnerID=40&md5=52dbe1d1e74f322d2c9f6532ac332a8c","A fuzzy set is a class of objects with a continuum of grades of membership. Such a set is characterized by a membership (characteristic) function which assigns to each object a grade of membership ranging between zero and one. The notions of inclusion, union, intersection, complement, relation, convexity, etc., are extended to such sets, and various properties of these notions in the context of fuzzy sets are established. In particular, a separation theorem for convex fuzzy sets is proved without requiring that the fuzzy sets be disjoint. © 1965 Academic Press, Inc.",,,,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-34248666540
"Krizhevsky A., Sutskever I., Hinton G.E.","40761762200;24831264500;7006699573;","ImageNet classification with deep convolutional neural networks",2012,"Advances in Neural Information Processing Systems","2",,,"1097","1105",,54278,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876231242&partnerID=40&md5=621b2cd1757ccc77341281f8a2f2ecaf","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry.",,"Convolutional neural network; Different class; GPU implementation; High resolution image; Max-pooling; Overfitting; Regularization methods; Test errors; Convolution; Neural networks","Winton Capital Management;Google;Pascal2;EMC2 Greenplum;Facebook","26th Annual Conference on Neural Information Processing Systems 2012, NIPS 2012","3 December 2012 through 6 December 2012","Lake Tahoe, NV",96883,Conference Paper,"Final","",Scopus,2-s2.0-84876231242
"Goldberg D.E.","",[No title available],1989,"Genetic Algorithms in Search, Optimization and Machine Learning",,,,"","",,48949,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0003722376
"Vapnik V.","",[No title available],1995,"The Nature of Statistical Learning Theory",,,,"","",,36857,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003450542
"Hastie T., Tibshirani R., Friedman J.","",[No title available],2001,"The Elements of Statistical Learning: Data Mining, Inference, and Prediction",,,,"","",,33651,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003684449
"Hochreiter S., Schmidhuber J.","6602873810;7003514621;","Long Short-Term Memory",1997,"Neural Computation","9","8",,"1735","1780",,32546,"10.1162/neco.1997.9.8.1735","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031573117&doi=10.1162%2fneco.1997.9.8.1735&partnerID=40&md5=6e4ee65c4bc5399487e5a65f4186aa19","Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient-based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O(1). Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms.",,"algorithm; article; artificial neural network; biological model; learning; memory; nerve cell network; physiology; psychological model; short term memory; time; Algorithms; Learning; Memory; Memory, Short-Term; Models, Neurological; Models, Psychological; Nerve Net; Neural Networks (Computer); Time Factors",,,,,,Article,"Final","",Scopus,2-s2.0-0031573117
"Cortes C., Vapnik V.","8850433300;6604096045;","Support-Vector Networks",1995,"Machine Learning","20","3",,"273","297",,30832,"10.1023/A:1022627411411","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34249753618&doi=10.1023%2fA%3a1022627411411&partnerID=40&md5=97a8591c7d55575e8c48344379ee2796","The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data. High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition. © 1995, Kluwer Academic Publishers. All rights reserved.","efficient learning algorithms; neural networks; pattern recognition; polynomial classifiers; radial basis function classifiers",,,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-34249753618
"Kingma D., Ba J.","","Adam: A method for stochastic optimization",2014,"Adam: A Method for Stochastic Optimization",,,,"","",,30786,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84941620184
"Lecun Y., Bengio Y., Hinton G.","55666793600;7003958245;7006699573;","Deep learning",2015,"Nature","521","7553",,"436","444",,30315,"10.1038/nature14539","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930630277&doi=10.1038%2fnature14539&partnerID=40&md5=e324cb9ec992f892ebc74f3e06078083","Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech. © 2015 Macmillan Publishers Limited. All rights reserved.",,"data processing; data set; machine learning; parameterization; automatic speech recognition; classifier; deep learning; human; image processing; language processing; learning; learning algorithm; learning theory; machine learning; nonhuman; pattern recognition; priority journal; recognition; Review; speech discrimination; algorithm; artificial intelligence; artificial neural network; computer; language; trends; Algorithms; Artificial Intelligence; Computers; Language; Neural Networks (Computer)",,,,,,Review,"Final","",Scopus,2-s2.0-84930630277
"Pedregosa F., Varoquaux G., Gramfort A., Michel V., Thirion B., Grisel O., Blondel M., Prettenhofer P., Weiss R., Dubourg V., Vanderplas J., Passos A., Cournapeau D., Brucher M., Perrot M., Duchesnay É.","42762055900;12808763400;23388890700;35752411900;7004721247;54379893900;36622153000;26422339000;57226353962;56633293500;35111982900;55377126700;24342730900;24398447700;7003415534;16174724800;","Scikit-learn: Machine learning in Python",2011,"Journal of Machine Learning Research","12",,,"2825","2830",,27209,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80555140075&partnerID=40&md5=63e53cee7a9711760872d4d103e5453a","Scikit-learn is a Python module integrating a wide range of state-of-the-art machine learning algorithms for medium-scale supervised and unsupervised problems. This package focuses on bringing machine learning to non-specialists using a general-purpose high-level language. Emphasis is put on ease of use, performance, documentation, and API consistency. It has minimal dependencies and is distributed under the simplified BSD license, encouraging its use in both academic and commercial settings. Source code, binaries, and documentation can be downloaded from http://scikit-learn.sourceforge.net.","Model selection; Python; Supervised learning; Unsupervised learning","Commercial settings; Ease of use; Model Selection; Python; Source codes; Learning algorithms; Learning systems; High level languages",,,,,,Article,"Final","",Scopus,2-s2.0-80555140075
"Sutton R.S., Barto A.G.","",[No title available],1998,"Reinforcement Learning: An Introduction",,,,"","",,23402,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004102479
"Chang C.-C., Lin C.-J.","57212663688;57154890600;","LIBSVM: A Library for support vector machines",2011,"ACM Transactions on Intelligent Systems and Technology","2","3","27","","",,23067,"10.1145/1961189.1961199","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955702502&doi=10.1145%2f1961189.1961199&partnerID=40&md5=dd03b423a92c61178036037cfd9e2444","LIBSVM is a library for Support Vector Machines (SVMs). We have been actively developing this package since the year 2000. The goal is to help users to easily apply SVM to their applications. LIBSVM has gained wide popularity in machine learning and many other areas. In this article, we present all implementation details of LIBSVM. Issues such as solving SVM optimization problems theoretical convergence multiclass classification probability estimates and parameter selection are discussed in detail. © 2011 ACM.","Classification LIBSVM optimization regression support vector machines SVM","Machine-learning; Multi-class classification; Parameter selection; Probability estimate; Regression support vector machines; SVM-optimization; Optimization; Vectors; Support vector machines",,,,,,Article,"Final","",Scopus,2-s2.0-79955702502
"Mikolov T., Sutskever I., Chen K., Corrado G., Dean J.","34969425500;24831264500;57304936500;7006502115;16427311000;","Distributed representations ofwords and phrases and their compositionality",2013,"Advances in Neural Information Processing Systems",,,,"","",,15513,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898956512&partnerID=40&md5=dfbefdd1802d61da676977c9ddce4729","The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling. An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of ""Canada"" and ""Air"" cannot be easily combined to obtain ""Air Canada"". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.",,"Compositionality; Distributed representation; High quality; Inherent limitations; SIMPLE method; Training speed; Vector representations; Word representations; Semantics","Air Force Office of Scientific Research (AFOSR);Amazon.com;et al.;Facebook;Google;Microsoft Research","27th Annual Conference on Neural Information Processing Systems, NIPS 2013","5 December 2013 through 10 December 2013","Lake Tahoe, NV",104690,Conference Paper,"Final","",Scopus,2-s2.0-84898956512
"Albert R., Barabási A.-L.","7202686127;7006567633;","Statistical mechanics of complex networks",2002,"Reviews of Modern Physics","74","1",,"47","97",,14535,"10.1103/RevModPhys.74.47","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036013593&doi=10.1103%2fRevModPhys.74.47&partnerID=40&md5=7849e23292c3d114c7641cbe602dba40","Complex networks describe a wide range of systems in nature and society. Frequently cited examples include the cell, a network of chemicals linked by chemical reactions, and the Internet, a network of routers and computers connected by physical links. While traditionally these systems have been modeled as random graphs, it is increasingly recognized that the topology and evolution of real networks are governed by robust organizing principles. This article reviews the recent advances in the field of complex networks, focusing on the statistical mechanics of network topology and dynamics. After reviewing the empirical data that motivated the recent interest in networks, the authors discuss the main models and analytical tools, covering random graphs, small-world and scale-free networks, the emerging theory of evolving networks, and the interplay between topology and the network's robustness against failures and attacks.",,,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0036013593
"Van Der Maaten L., Hinton G.","23092276000;7006699573;","Visualizing data using t-SNE",2008,"Journal of Machine Learning Research","9",,,"2579","2625",,13802,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57249084011&partnerID=40&md5=234c45663c3957ae2268abc330a03438","We present a new technique called ""t-SNE"" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.","Dimensionality reduction; Embedding algorithms; Manifold learning; Multidimensional scaling; Visualization","Learning algorithms; Maps; Numerical analysis; Three dimensional; Visualization; Data sets; Different scales; Dimensional datums; Dimensionality reduction; Embedding algorithms; Locally linear embedding.; Manifold learning; Multidimensional scaling; Multiple classes; Multiple viewpoints; Neighborhood graphs; New techniques; Random walks; Sammon mappings; Stochastic neighbor embedding; Very large datums; Visualization techniques; Data visualization",,,,,,Article,"Final","",Scopus,2-s2.0-57249084011
"Rumelhart D.E., McClelland J.L.","",[No title available],1986,"Parallel Distributed Processing",,,,"","",,13279,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0003444646
"Hodgkin A.L., Huxley A.F.","7004387152;7005746969;","A quantitative description of membrane current and its application to conduction and excitation in nerve",1952,"The Journal of Physiology","117","4",,"500","544",,13225,"10.1113/jphysiol.1952.sp004764","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35649001607&doi=10.1113%2fjphysiol.1952.sp004764&partnerID=40&md5=6aa1026626cee0603551c918b54870ca",[No abstract available],,"article; nerve fiber; AXONS; Axons",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-35649001607
"Pennington J., Socher R., Manning C.D.","22953926600;24766896100;35280197500;","GloVe: Global vectors for word representation",2014,"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,,"1532","1543",,12738,"10.3115/v1/d14-1162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961289992&doi=10.3115%2fv1%2fd14-1162&partnerID=40&md5=53f2b22fdb7676d7ea744a3676c76cc8","Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arithmetic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global logbilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word cooccurrence matrix, rather than on the entire sparse matrix or on individual context windows in a large corpus. The model produces a vector space with meaningful substructure, as evidenced by its performance of 75% on a recent word analogy task. It also outperforms related models on similarity tasks and named entity recognition. © 2014 Association for Computational Linguistics.",,"Factorization; Matrix algebra; Natural language processing systems; Regression analysis; Semantics; Vectors; Learning vectors; Model properties; Named entity recognition; Regression model; Sparse matrices; Statistical information; Word co-occurrence; Word representations; Vector spaces","Carnegie Mellon University Qatar;Facebook;iHorizons;Qatar Computing Research Institute;Qatar National Research Fund (QNRF);Yandex","2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014","25 October 2014 through 29 October 2014",,111414,Conference Paper,"Final","",Scopus,2-s2.0-84961289992
"Rumelhart D.E., Hinton G.E., Williams R.J.","6603375627;7006699573;57224681279;","Learning representations by back-propagating errors",1986,"Nature","323","6088",,"533","536",,12560,"10.1038/323533a0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022471098&doi=10.1038%2f323533a0&partnerID=40&md5=2e1eae82a89c7503560b15a74e4c698d","We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal 'hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure 1. © 1986 Nature Publishing Group.",,"learning; model; nerve cell network; nervous system; nonbiological model; nonhuman; priority journal",,,,,,Article,"Final","",Scopus,2-s2.0-0022471098
"Mikolov T., Chen K., Corrado G., Dean J.","34969425500;57304936500;7006502115;16427311000;","Efficient estimation of word representations in vector space",2013,"1st International Conference on Learning Representations, ICLR 2013 - Workshop Track Proceedings",,,,"","",,12416,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083951332&partnerID=40&md5=20428820e8b09cdfb5078ea812a71f2d","We propose two novel model architectures for computing continuous vector representations of words from very large data sets. The quality of these representations is measured in a word similarity task, and the results are compared to the previously best performing techniques based on different types of neural networks. We observe large improvements in accuracy at much lower computational cost, i.e. it takes less than a day to learn high quality word vectors from a 1.6 billion words data set. Furthermore, we show that these vectors provide state-of-the-art performance on our test set for measuring syntactic and semantic word similarities. © 2013 International Conference on Learning Representations, ICLR. All rights reserved.",,"Semantics; Vectors; Computational costs; Efficient estimation; Model architecture; State-of-the-art performance; Vector representations; Very large datum; Word representations; Word similarity; Vector spaces",,"1st International Conference on Learning Representations, ICLR 2013","2 May 2013 through 4 May 2013",,149796,Conference Paper,"Final","",Scopus,2-s2.0-85083951332
"Newman M.E.J.","7401940367;","The structure and function of complex networks",2003,"SIAM Review","45","2",,"167","256",,12405,"10.1137/S003614450342480","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038718854&doi=10.1137%2fS003614450342480&partnerID=40&md5=8be796efe4c09efdcb0e0de09e90a205","The structure and function of complex networks were discussed. The study of networks, in the form of mathematical graph theory, is one of the fundamental pillars of discrete mathematics. Analysis showed that the network properties would affect the behavior of networked systems substantially.","Complex systems; Computer networks; Graph theory; Networks; Percolation theory; Random graphs; Social networks","Graph theory; Internet; Large scale systems; Percolation (computer storage); Topology; Random graphs; Computer networks",,,,,,Review,"Final","All Open Access, Green",Scopus,2-s2.0-0038718854
"Agrawal R., Srikant R.","","Fast algorithms for mining association rules",1994,"Proceedings of the 20th International Conference on Very Large Databases",,,,"487","499",,12207,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0001882616
"Jang J.-S.R.","7402965041;","ANFIS: Adaptive-Network-Based Fuzzy Inference System",1993,"IEEE Transactions on Systems, Man and Cybernetics","23","3",,"665","685",,11782,"10.1109/21.256541","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0027601884&doi=10.1109%2f21.256541&partnerID=40&md5=1b37c64dea82d83e701b9bd07185ae13","The architecture and learning procedure underlying ANFIS (adaptive-network-based fuzzy inference system) is presented, which is a fuzzy inference system implemented in the framework of adaptive networks. By using a hybrid learning procedure, the proposed ANFIS can construct an input-output mapping based on both human knowledge (in the form of fuzzy if-then rules) and stipulated input-output data pairs. In the simulation, the ANFIS architecture is employed to model nonlinear functions, identify nonlinear components on-Iinely in a control system, and predict a chaotic time series, all yielding remarkable results. Comparisons with artificial neural networks and earlier work on fuzzy modeling are listed and discussed. Other extensions of the proposed ANFIS and promising applications to automatic control and signal processing are also suggested. © 1993 IEEE",,"Adaptive control systems; Computer architecture; Cybernetics; Learning systems; Linguistics; Neural networks; Signal processing; Simulation; Time series analysis; Adaptive network; Fuzzy associative memories; Fuzzy interference system; Human knowledge; Hybrid learning procedure; Input output mapping; Fuzzy sets",,,,,,Article,"Final","",Scopus,2-s2.0-0027601884
"Quinlan J.R.","7103316475;","Induction of Decision Trees",1986,"Machine Learning","1","1",,"81","106",,11626,"10.1023/A:1022643204877","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33744584654&doi=10.1023%2fA%3a1022643204877&partnerID=40&md5=48a99c15698f92848809769c74f260a1","The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions. © 2004, Kluwer Academic Publishers. All rights reserved.","classification; decision trees; expert systems; induction; information theory; knowledge acquisition",,,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-33744584654
"Simon H.A.","",[No title available],1969,"The Sciences of the Artificial",,,,"","",,11589,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003960921
"Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser Ł., Polosukhin I.","55147219200;6505939767;57202055737;51666085000;57192432426;57202060635;15044821700;57193222468;","Attention is all you need",2017,"Advances in Neural Information Processing Systems","2017-December",,,"5999","6009",,11162,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043317328&partnerID=40&md5=3e5a5c2b862c8979ffea845bb707b3c3","The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. © 2017 Neural information processing systems foundation. All rights reserved.",,"Convolution; Decoding; Network architecture; Program processors; Signal encoding; Attention mechanisms; Best model; Bleu scores; Convolutional neural network; Single models; Training costs; Two machines; Recurrent neural networks","","31st Annual Conference on Neural Information Processing Systems, NIPS 2017","4 December 2017 through 9 December 2017",,136033,Conference Paper,"Final","",Scopus,2-s2.0-85043317328
"Chang C.-C., Lin C.-J.","","LIBSVM: A library for support vector machines",2001,"LIBSVM: A Library for Support Vector Machines",,,,"","",,10537,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003710380
"Mnih V., Kavukcuoglu K., Silver D., Rusu A.A., Veness J., Bellemare M.G., Graves A., Riedmiller M., Fidjeland A.K., Ostrovski G., Petersen S., Beattie C., Sadik A., Antonoglou I., King H., Kumaran D., Wierstra D., Legg S., Hassabis D.","15845911500;25646533000;7202151417;56536225100;21744020700;16199466700;56273511600;6603794300;8838993800;36996117900;56535950000;56536156200;56536367700;56536965900;56535990600;8650774800;16551181100;12242205800;15839738700;","Human-level control through deep reinforcement learning",2015,"Nature","518","7540",,"529","533",,9228,"10.1038/nature14236","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924051598&doi=10.1038%2fnature14236&partnerID=40&md5=babda0bcd55350613c26ba3aed808ce4","The theory of reinforcement learning provides a normative account, deeply rooted in psychological and neuroscientific perspectives on animal behaviour, of how agents may optimize their control of an environment. To use reinforcement learning successfully in situations approaching real-world complexity, however, agents are confronted with a difficult task: they must derive efficient representations of the environment from high-dimensional sensory inputs, and use these to generalize past experience to new situations. Remarkably, humans and other animals seem to solve this problem through a harmonious combination of reinforcement learning and hierarchical sensory processing systems, the former evidenced by a wealth of neural data revealing notable parallels between the phasic signals emitted by dopaminergic neurons and temporal difference reinforcement learning algorithms. While reinforcement learning agents have achieved some successes in a variety of domains, their applicability has previously been limited to domains in which useful features can be handcrafted, or to domains with fully observed, low-dimensional state spaces. Here we use recent advances in training deep neural networks to develop a novel artificial agent, termed a deep Q-network, that can learn successful policies directly from high-dimensional sensory inputs using end-to-end reinforcement learning. We tested this agent on the challenging domain of classic Atari 2600 games. We demonstrate that the deep Q-network agent, receiving only the pixels and the game score as inputs, was able to surpass the performance of all previous algorithms and achieve a level comparable to that of a professional human games tester across a set of 49 games, using the same algorithm, network architecture and hyperparameters. This work bridges the divide between high-dimensional sensory inputs and actions, resulting in the first artificial agent that is capable of learning to excel at a diverse array of challenging tasks. © 2015 Macmillan Publishers Limited. All rights reserved.",,"algorithm; artificial intelligence; behavioral ecology; game theory; harmonic analysis; hierarchical system; neurology; parameterization; sensory system; animal behavior; Article; human; learning algorithm; nonhuman; priority journal; reinforcement; sensory stimulation; algorithm; artificial intelligence; artificial neural network; psychological model; recreation; reward; Animalia; Algorithms; Artificial Intelligence; Humans; Models, Psychological; Neural Networks (Computer); Reinforcement (Psychology); Reward; Video Games",,,,,,Article,"Final","",Scopus,2-s2.0-84924051598
"Hopcroft J.E., Ullman J.D.","",[No title available],1979,"Introduction to Automata Theory, Languages, and Computation",,,,"","",,9083,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003620778
"Schölkopf B., Smola A.J.","","Learning with kernels",2002,"Learning with Kernels",,,,"","",,8943,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004094721
"Hebb D.O.","",[No title available],1949,"The Organization of Behavior",,,,"","",,8882,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004230131
"Lakoff G.","",[No title available],1987,"Women, Fire, and Dangerous Things",,,,"","",,8708,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003770368
"McCulloch W.S., Pitts W.","7005416468;8949658600;","A logical calculus of the ideas immanent in nervous activity",1943,"The Bulletin of Mathematical Biophysics","5","4",,"115","133",,8443,"10.1007/BF02478259","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51249194645&doi=10.1007%2fBF02478259&partnerID=40&md5=edb67afceee33d22eaabbf1f8c1dca90","Because of the ""all-or-none"" character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed. © 1943 The University of Chicago Press.",,,,,,,,Article,"Final","",Scopus,2-s2.0-51249194645
"Mitchell M.","",[No title available],1996,"An Introduction to Genetic Algorithms",,,,"","",,8412,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003587805
"Fellbaum C.","",[No title available],1998,"WordNet: An Electronic Lexical Database",,,,"","",,8385,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004289791
"Sutskever I., Vinyals O., Le Q.V.","24831264500;24342311100;12140898300;","Sequence to sequence learning with neural networks",2014,"Advances in Neural Information Processing Systems","4","January",,"3104","3112",,8371,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928547704&partnerID=40&md5=a48ad857cb90914792ad1b1191d7c591","Deep Neural Networks (DNNs) are powerful models that have achieved excellent performance on difficult learning tasks. Although DNNs work well whenever large labeled training sets are available, they cannot be used to map sequences to sequences. In this paper, we present a general end-to-end approach to sequence learning that makes minimal assumptions on the sequence structure. Our method uses a multilayered Long Short-Term Memory (LSTM) to map the input sequence to a vector of a fixed dimensionality, and then another deep LSTM to decode the target sequence from the vector. Our main result is that on an English to French translation task from the WMT'14 dataset, the translations produced by the LSTM achieve a BLEU score of 34.8 on the entire test set, where the LSTM's BLEU score was penalized on out-of-vocabulary words. Additionally, the LSTM did not have difficulty on long sentences. For comparison, a phrase-based SMT system achieves a BLEU score of 33.3 on the same dataset. When we used the LSTM to rerank the 1000 hypotheses produced by the aforementioned SMT system, its BLEU score increases to 36.5, which is close to the previous best result on this task. The LSTM also learned sensible phrase and sentence representations that are sensitive to word order and are relatively invariant to the active and the passive voice. Finally, we found that reversing the order of the words in all source sentences (but not target sentences) improved the LSTM's performance markedly, because doing so introduced many short term dependencies between the source and the target sentence which made the optimization problem easier.",,"Optimization; Statistical tests; Deep neural networks; Learning tasks; Long short term memory; Optimization problems; Out of vocabulary words; Sequence learning; Sequence structure; Target sequences; Information science","Air Force Office of Scientic Research;Amazon;et al.;Facebook;Ketchum Trading, LLC;Microsoft Research","28th Annual Conference on Neural Information Processing Systems 2014, NIPS 2014","8 December 2014 through 13 December 2014",,112962,Conference Paper,"Final","",Scopus,2-s2.0-84928547704
"Miller G.A.","57213955998;","WordNet: A Lexical Database for English",1995,"Communications of the ACM","38","11",,"39","41",,8184,"10.1145/219717.219748","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84976702763&doi=10.1145%2f219717.219748&partnerID=40&md5=0e8111f7c1c3746158f873e0d3e8ca03","This database links English nouns, verbs, adjectives, and adverbs to sets of synonyms that are in turn linked through semantic relations that determine word definitions. © 1995, ACM. All rights reserved.",,,,,,,,Article,"Final","",Scopus,2-s2.0-84976702763
"Demšar J.","7004027053;","Statistical comparisons of classifiers over multiple data sets",2006,"Journal of Machine Learning Research","7",,,"1","30",,7434,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-29644438050&partnerID=40&md5=81992d7f5603cc9729bb9c2c1dcd4f8c","While methods for comparing two learning algorithms on a single data set have been scrutinized for quite some time already, the issue of statistical tests for comparisons of more algorithms on multiple data sets, which is even more essential to typical machine learning studies, has been all but ignored. This article reviews the current practice and then theoretically and empirically examines several suitable tests. Based on that, we recommend a set of simple, yet safe and robust non-parametric tests for statistical comparisons of classifiers: the Wilcoxon signed ranks test for comparison of two classifiers and the Friedman test with the corresponding post-hoc tests for comparison of more classifiers over multiple data sets. Results of the latter can also be neatly presented with the newly introduced CD (critical difference) diagrams.","Comparative studies; Friedman test; Multiple comparisons tests; Statistical methods; Wilcoxon signed ranks test","Algorithms; Computation theory; Data processing; Learning algorithms; Learning systems; Set theory; Multiple data sets; Parametric tests; Statistical comparisons; Wilcoxon signed ranks test; Statistical methods",,,,,,Article,"Final","",Scopus,2-s2.0-29644438050
"Glorot X., Bengio Y.","49861305800;7003958245;","Understanding the difficulty of training deep feedforward neural networks",2010,"Journal of Machine Learning Research","9",,,"249","256",,7005,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862277874&partnerID=40&md5=54f8f51c32d5f983b293ca51949d946e","Whereas before 2006 it appears that deep multilayer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence. Copyright 2010 by the authors.",,"Faster convergence; Gradient descent; Hidden layers; Jacobians; Mean values; Non-linear activation; Non-Linearity; Singular values; Algorithms; Artificial intelligence; Feedforward neural networks; Multilayer neural networks; Chemical activation","PASCAL2;Intel;Microsoft;Google;Institute of Mathematical Statistics","13th International Conference on Artificial Intelligence and Statistics, AISTATS 2010","13 May 2010 through 15 May 2010","Sardinia",90379,Conference Paper,"Final","",Scopus,2-s2.0-84862277874
"Jia Y., Shelhamer E., Donahue J., Karayev S., Long J., Girshick R., Guadarrama S., Darrell T.","56153296600;56433480600;54956247200;50561748500;55457086300;35179333300;6506759870;7003377605;","Caffe: Convolutional architecture for fast feature embedding",2014,"MM 2014 - Proceedings of the 2014 ACM Conference on Multimedia",,,,"675","678",,6989,"10.1145/2647868.2654889","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84913580146&doi=10.1145%2f2647868.2654889&partnerID=40&md5=df3396ee82bd996a1e4e1e3cf474efe9","Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying generalpurpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (≈ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.","Computer vision; Machine learning; Neural networks; Open source; Parallel computation","C++ (programming language); Computer vision; Convolution; Convolutional neural networks; Deep learning; Graphics processing unit; Industrial research; Learning systems; Network architecture; Neural networks; Object oriented programming; Cloud environments; Feature embedding; Open sources; Parallel Computation; Reference models; Seamless switching; Separating models; State of the art; Learning algorithms","ACM SIGMM;et al.;FXPAL;Google;IBM;Symantec","2014 ACM Conference on Multimedia, MM 2014","3 November 2014 through 7 November 2014",,108944,Conference Paper,"Final","",Scopus,2-s2.0-84913580146
"Sacks H., Schegloff E.A., Jefferson G.","","A simplest systematics for the organization of turn-taking for conversation",1974,"Language","50","4",,"696","735",,6960,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0000098051
"Baader F., Calvanese D., McGuinness D., Nardi D., Patel-Schneider P.","",[No title available],2003,"The Description Logic Handbook: Theory, Implementation and Applications",,,,"","",,6491,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003727420
"Wolpert D.H., Macready W.G.","7006620754;6602303017;","No free lunch theorems for optimization",1997,"IEEE Transactions on Evolutionary Computation","1","1",,"67","82",,6418,"10.1109/4235.585893","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031118203&doi=10.1109%2f4235.585893&partnerID=40&md5=fac8c56be911367d556066800e863066","A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of ""no free lunch"" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori ""head-to-head"" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms. © 1997 IEEE.","Evolutionary algorithms; Information theory; Optimization","Algorithms; Information theory; Problem solving; Time varying systems; Head to head minimax distinctions; No free lunch (NFL) theorems; Optimization",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0031118203
"Devlin J., Chang M.-W., Lee K., Toutanova K.","","BERT: Pre-training of deep bidirectional transformers for language understanding",2018,"BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding",,,,"","",,6222,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85057019815
"Devlin J., Chang M.-W., Lee K., Toutanova K.","54879967400;25925685700;56349980800;6506107920;","BERT: Pre-training of deep bidirectional transformers for language understanding",2019,"NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference","1",,,"4171","4186",,6017,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083815650&partnerID=40&md5=4986c6d6076c0c91df84d17216b47216","We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement). © 2019 Association for Computational Linguistics",,"Computational linguistics; Language inference; Language understanding; NAtural language processing; Output layer; Pre-training; Question Answering; Representation model; State of the art; Natural language processing systems","Amazon;ASAPP;Bloomberg Engineering;et al.;facebook;Google","2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019","2 June 2019 through 7 June 2019",,159851,Conference Paper,"Final","",Scopus,2-s2.0-85083815650
"Kohonen T.","7005640110;","Self-organized formation of topologically correct feature maps",1982,"Biological Cybernetics","43","1",,"59","69",,5891,"10.1007/BF00337288","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020068152&doi=10.1007%2fBF00337288&partnerID=40&md5=7d7b7dbf6ea124f7f98e5d5b3e16bf80","This work contains a theoretical study and computer simulations of a new self-organizing process. The principal discovery is that in a simple network of adaptive physical elements which receives signals from a primary event space, the signal representations are automatically mapped onto a set of output responses in such a way that the responses acquire the same topological order as that of the primary events. In other words, a principle has been discovered which facilitates the automatic formation of topologically correct maps of features of observable events. The basic self-organizing system is a one- or two-dimensional array of processing units resembling a network of threshold-logic units, and characterized by short-range lateral feedback between neighbouring units. Several types of computer simulations are used to demonstrate the ordering process as well as the conditions under which it fails. © 1982 Springer-Verlag.",,"auditory system; biological model; brain; central nervous system; computer model; cortex; gustatory system; nerve cell network; nervous system; nonbiological model; normal human; olfactory system; pattern recognition; perception; vestibular system; visual system",,,,,,Article,"Final","",Scopus,2-s2.0-0020068152
"Hart P.E., Nilsson N.J., Raphael B.","24530768300;7005244296;56891815600;","A Formal Basis for the Heuristic Determination of Minimum Cost Paths",1968,"IEEE Transactions on Systems Science and Cybernetics","4","2",,"100","107",,5839,"10.1109/TSSC.1968.300136","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899829959&doi=10.1109%2fTSSC.1968.300136&partnerID=40&md5=1c8bca093a1fb7a88c1a4fbdeb167ff8","Although the problem of determining the minimum cost path through a graph arises naturally in a number of interesting applications, there has been no underlying theory to guide the development of efficient search procedures. Moreover, there is no adequate conceptual framework within which the various ad hoc search strategies proposed to date can be compared. This paper describes how heuristic information from the problem domain can be incorporated into a formal mathematical theory of graph searching and demonstrates an optimality property of a class of search strategies. Copyright © 1968 by The Institute of Electrical and Electronics Engineers, Inc.",,,,,,,,Article,"Final","",Scopus,2-s2.0-84899829959
"Elman J.L.","57225419564;","Finding structure in time",1990,"Cognitive Science","14","2",,"179","211",,5662,"10.1016/0364-0213(90)90002-E","https://www.scopus.com/inward/record.uri?eid=2-s2.0-26444565569&doi=10.1016%2f0364-0213%2890%2990002-E&partnerID=40&md5=91b36562ba1395856ed9903cf264861d","Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction. © 1990.",,,,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-26444565569
"Bengio Y., Courville A., Vincent P.","7003958245;6507291186;57203214842;","Representation learning: A review and new perspectives",2013,"IEEE Transactions on Pattern Analysis and Machine Intelligence","35","8","6472238","1798","1828",,5505,"10.1109/TPAMI.2013.50","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879854889&doi=10.1109%2fTPAMI.2013.50&partnerID=40&md5=92867e41fa47332f88fba4e8d594fbaa","The success of machine learning algorithms generally depends on data representation, and we hypothesize that this is because different representations can entangle and hide more or less the different explanatory factors of variation behind the data. Although specific domain knowledge can be used to help design representations, learning with generic priors can also be used, and the quest for AI is motivating the design of more powerful representation-learning algorithms implementing such priors. This paper reviews recent work in the area of unsupervised feature learning and deep learning, covering advances in probabilistic models, autoencoders, manifold learning, and deep networks. This motivates longer term unanswered questions about the appropriate objectives for learning good representations, for computing representations (i.e., inference), and the geometrical connections between representation learning, density estimation, and manifold learning. © 1979-2012 IEEE.","autoencoder; Boltzmann machine; Deep learning; feature learning; neural nets; representation learning; unsupervised learning","Auto encoders; Boltzmann machines; Deep learning; Feature learning; representation learning; Learning systems; Neural networks; Unsupervised learning; Learning algorithms; algorithm; artificial intelligence; artificial neural network; human; review; artificial intelligence; trends; Algorithms; Artificial Intelligence; Humans; Neural Networks (Computer); Algorithms; Artificial Intelligence; Humans; Neural Networks (Computer)",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84879854889
"Davis L.","",[No title available],1991,"Handbook of Genetic Algorithms",,,,"","",,5483,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003849948
"Pang B., Lee L., Vaithyanathan S.","","Thumbs up? Sentiment classification using machine learning techniques",2002,"Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing (EMNLP)",,,,"79","86",,5328,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-2442544814
"Hertz J., Krogh A., Palmer R.G.","",[No title available],1991,"Introduction to the Theory of Neural Computation",,,,"","",,5158,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003979924
"Krizhevsky A., Sutskever I., Hinton G.E.","40761762200;24831264500;7006699573;","ImageNet classification with deep convolutional neural networks",2017,"Communications of the ACM","60","6",,"84","90",,5124,"10.1145/3065386","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020126914&doi=10.1145%2f3065386&partnerID=40&md5=d920d0868678405baf0e4410804358ec","We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used nonsaturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called ""dropout"" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry. © 2017 ACM.",,"Convolution; Image classification; Neural networks; Convolutional neural network; Different class; GPU implementation; High resolution image; Max-pooling; Overfitting; Regularization methods; State of the art; Deep neural networks",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85020126914
"Salton G., Wong A., Yang C.S.","7005457066;16511588000;7407021752;","A Vector Space Model for Automatic Indexing",1975,"Communications of the ACM","18","11",,"613","620",,4873,"10.1145/361219.361220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0016572913&doi=10.1145%2f361219.361220&partnerID=40&md5=6c22c6b5bf4c38cf8c2e04ae11c78215","In a document retrieval, or other pattern matching environment where stored entities (documents) are compared with each other or with incoming patterns (search requests), it appears that the best indexing (property) space is one where each entity lies as far away from the others as possible; in these circumstances the value of an indexing system may be expressible as a function of the density of the object space; in particular, retrieval performance may correlate inversely with space density. An approach based on space density computations is used to choose an optimum indexing vocabulary for a collection of documents. Typical evaluation results are shown, demonstating the usefulness of the model. © 1975, ACM. All rights reserved.","automatic indexing; automatic information retrieval; content analysis; document space","INFORMATION SCIENCE",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0016572913
"Gen M., Cheng R.","",[No title available],1997,"Genetic Algorithms and Engineering Design",,,,"","",,4617,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003735261
"Hu M., Liu B.","7402639444;36063168200;","Mining and summarizing customer reviews",2004,"KDD-2004 - Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"168","177",,4580,"10.1145/1014052.1014073","https://www.scopus.com/inward/record.uri?eid=2-s2.0-12244305149&doi=10.1145%2f1014052.1014073&partnerID=40&md5=9a70296b63bddb8b219fba6fb104a2b1","Merchants selling products on the Web often ask their customers to review the products that they have purchased and the associated services. As e-commerce is becoming more and more popular, the number of customer reviews that a product receives grows rapidly. For a popular product, the number of reviews can be in hundreds or even thousands. This makes it difficult for a potential customer to read them to make an informed decision on whether to purchase the product. It also makes it difficult for the manufacturer of the product to keep track and to manage customer opinions. For the manufacturer, there are additional difficulties because many merchant sites may sell the same product and the manufacturer normally produces many kinds of products. In this research, we aim to mine and to summarize all the customer reviews of a product. This summarization task is different from traditional text summarization because we only mine the features of the product on which the customers have expressed their opinions and whether the opinions are positive or negative. We do not summarize the reviews by selecting a subset or rewrite some of the original sentences from the reviews to capture the main points as in the classic text summarization. Our task is performed in three steps: (1) mining product features that have been commented on by customers; (2) identifying opinion sentences in each review and deciding whether each opinion sentence is positive or negative; (3) summarizing the results. This paper proposes several novel techniques to perform these tasks. Our experimental results using reviews of a number of products sold online demonstrate the effectiveness of the techniques.","Reviews; Sentiment classification; Summarization; Text mining","Algorithms; Classification (of information); Consumer products; Customer satisfaction; Electronic commerce; Image quality; Purchasing; Text processing; World Wide Web; Sentiment classification; Summarization; Text mining; Data mining","ACM SIGKDD, Spec. Interest Group on Knowl. Discov. and Data Min.","KDD-2004 - Proceedings of the Tenth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","22 August 2004 through 25 August 2004","Seattle, WA",64210,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-12244305149
"Bahdanau D., Cho K., Bengio Y.","57188434700;55722769200;7003958245;","Neural machine translation by jointly learning to align and translate",2015,"3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings",,,,"","",,4477,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083953689&partnerID=40&md5=3c32cc81a2a26902f116a3b9280f1aaf","Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder–decoders and encode a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder–decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition. © 2015 International Conference on Learning Representations, ICLR. All rights reserved.",,"Computational linguistics; Decoding; Signal encoding; Decoder architecture; Hard segments; Machine translations; New approaches; Qualitative analysis; State of the art; Statistical machine translation; Target words; Computer aided language translation",,"3rd International Conference on Learning Representations, ICLR 2015","7 May 2015 through 9 May 2015",,149801,Conference Paper,"Final","",Scopus,2-s2.0-85083953689
"Tobler W.R.","","A computer movie simulating urban growth in the Detroit region",1970,"Economic Geography","46","2",,"234","240",,4403,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0000565591
"Ortony A., Clore G.L., Collins A.","",[No title available],1988,"The Cognitive Structure of Emotions",,,,"","",,4274,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004160296
"Agnar A., Plaza E.","55391786000;7004015031;","Case-Based reasoning: Foundational issues, methodological variations, and system approaches",1994,"AI Communications","7","1",,"39","59",,4264,"10.3233/AIC-1994-7104","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028401306&doi=10.3233%2fAIC-1994-7104&partnerID=40&md5=8b50a5c7cc225150fa48d04a873a3056","Case-based reasoning is a recent approach to problem solving and learning that has got a lot of attention over the last few years. Originating in the US, the basic idea and underlying theories have spread to other continents, and we are now within a period of highly active research in case-based reasoning in Europe as well. This paper gives an overview of the foundational issues related to case-based reasoning, describes some of the leading methodological approaches within the field, and exemplifies the current state through pointers to some systems. Initially, a general framework is defined, to which the subsequent descriptions and discussions will refer. The framework is influenced by recent methodologies for knowledge level descriptions of intelligent systems. The methods for case retrieval, reuse, solution testing, and learning are summarized, and their actual realization is discussed in the light of a few example systems that represent different CBR approaches. We also discuss the role of case-based methods as one type of reasoning and learning method within an integrated system architecture. © 1994 IOS Press and the authors.",,"Inference engines; Knowledge based systems; Case based reasoning; Foundational issues; Methodological variations; Artificial intelligence",,,,,,Article,"Final","",Scopus,2-s2.0-0028401306
"Kim Y.","57187293300;","Convolutional neural networks for sentence classification",2014,"EMNLP 2014 - 2014 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,,"1746","1751",,4151,"10.3115/v1/d14-1181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961376850&doi=10.3115%2fv1%2fd14-1181&partnerID=40&md5=38be18f19001ceea9ee8cacdc60d2e3c","We report on a series of experiments with convolutional neural networks (CNN) trained on top of pre-trained word vectors for sentence-level classification tasks. We show that a simple CNN with little hyperparameter tuning and static vectors achieves excellent results on multiple benchmarks. Learning task-specific vectors through fine-tuning offers further gains in performance. We additionally propose a simple modification to the architecture to allow for the use of both task-specific and static vectors. The CNN models discussed herein improve upon the state of the art on 4 out of 7 tasks, which include sentiment analysis and question classification. © 2014 Association for Computational Linguistics.",,"Convolution; Neural networks; Sentiment analysis; Classification tasks; Convolutional neural network; Hyper-parameter; Learning tasks; Question classification; Sentence classifications; Simple modifications; State of the art; Vectors","Carnegie Mellon University Qatar;Facebook;iHorizons;Qatar Computing Research Institute;Qatar National Research Fund (QNRF);Yandex","2014 Conference on Empirical Methods in Natural Language Processing, EMNLP 2014","25 October 2014 through 29 October 2014",,111414,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84961376850
"Xu K., Ba J.L., Kiros R., Cho K., Courville A., Salakhutdinov R., Zemel R.S., Bengio Y.","57202579207;57189090730;55377012400;55722769200;6507291186;57203057355;7004912699;7003958245;","Show, attend and tell: Neural image caption generation with visual attention",2015,"32nd International Conference on Machine Learning, ICML 2015","3",,,"2048","2057",,4115,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970002232&partnerID=40&md5=bc0cd24211267b34ae29b5202424fb2e","Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr9k, Flickr30k and MS COCO. © Copyright 2015 by International Machine Learning Society (IMLS). All rights reserved.",,"Artificial intelligence; Benchmarking; Learning systems; Variational techniques; Backpropagation techniques; Benchmark datasets; Image caption; Machine translations; Output sequences; Salient objects; State-of-the-art performance; Visual Attention; Behavioral research","","32nd International Conference on Machine Learning, ICML 2015","6 July 2015 through 11 July 2015",,119782,Conference Paper,"Final","",Scopus,2-s2.0-84970002232
"Virtanen P., Gommers R., Oliphant T.E., Haberland M., Reddy T., Cournapeau D., Burovski E., Peterson P., Weckesser W., Bright J., van der Walt S.J., Brett M., Wilson J., Millman K.J., Mayorov N., Nelson A.R.J., Jones E., Kern R., Larson E., Carey C.J., Polat İ., Feng Y., Moore E.W., VanderPlas J., Laxalde D., Perktold J., Cimrman R., Henriksen I., Quintero E.A., Harris C.R., Archibald A.M., Ribeiro A.H., Pedregosa F., van Mulbregt P., Vijaykumar A., Bardelli A.P., Rothberg A., Hilboll A., Kloeckner A., Scopatz A., Lee A., Rokem A., Woods C.N., Fulton C., Masson C., Häggström C., Fitzgerald C., Nicholson D.A., Hagen D.R., Pasechnik D.V., Olivetti E., Martin E., Wieser E., Silva F., Lenders F., Wilhelm F., Young G., Price G.A., Ingold G.-L., Allen G.E., Lee G.R., Audren H., Probst I., Dietrich J.P., Silterra J., Webber J.T., Slavič J., Nothman J., Buchner J., Kulick J., Schönberger J.L., de Miranda Cardoso J.V., Reimer J., Harrington J., Rodríguez J.L.C., Nunez-Iglesias J., Kuczynski J., Tritz K., Thoma M., Newville M., Kümmerer M., Bolingbroke M., Tartre M., Pak M., Smith N.J., Nowaczyk N., Shebanov N., Pavlyk O., Brodtkorb P.A., Lee P., McGibbon R.T., Feldbauer R., Lewis S., Tygier S., Sievert S., Vigna S., Peterson S., More S., Pudlik T., Oshima T., Pingel T.J., Robitaille T.P., Spura T., Jones T.R., Cera T., Leslie T., Zito T., Krauss T., Upadhyay U., Halchenko Y.O., Vázquez-Baeza Y., SciPy 1.0 Contributors","57214794276;57203836453;57223409879;54792832200;57220709605;24342730900;8381708700;7402598824;57214802201;57214803742;15732291000;7101935343;57214804858;23489307400;57214795348;55549633000;57224773854;56110074100;57208874664;55445593700;57214791890;57221137079;57214800830;35111982900;57214806968;56543748800;7801604258;57214790501;55794090700;57214796492;23975452800;57191699148;42762055900;6507701619;57214800295;57214804545;57195628504;36504218000;33067970700;24367402300;55433933100;6507945628;57214797255;57214796281;57214792757;57214803944;57214806110;57193604224;57214802551;6701409422;13405718700;55757782291;57194589306;56354550700;57201360441;57214803383;57214792183;23499021000;7003918050;7402366368;8980876500;56423610700;57214795515;8976319800;56711246600;35218545700;6506383362;55119994400;56414398800;56066652600;56041330400;55795162700;56572272600;7202784101;57220822680;6504594686;36093149800;6603231731;57214792148;7003333387;57031832500;57214805989;35744236400;57214791203;57214807110;57214797480;57214805424;8985341100;57214797956;57214803409;37012700700;57202087840;57198811830;57202826215;16032245800;56267340500;57214800038;16234186800;55964546700;57214793280;28167780500;12787314800;6506578111;7404294787;57214804289;57214807284;24472414200;57214795523;36706670500;6503870081;55580465100;","SciPy 1.0: fundamental algorithms for scientific computing in Python",2020,"Nature Methods","17","3",,"261","272",,3958,"10.1038/s41592-019-0686-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079119586&doi=10.1038%2fs41592-019-0686-2&partnerID=40&md5=dcc316cb47f1306fc8180526a89cfc0e","SciPy is an open-source scientific computing library for the Python programming language. Since its initial release in 2001, SciPy has become a de facto standard for leveraging scientific algorithms in Python, with over 600 unique code contributors, thousands of dependent packages, over 100,000 dependent repositories and millions of downloads per year. In this work, we provide an overview of the capabilities and development practices of SciPy 1.0 and highlight some recent technical developments. © 2020, The Author(s).",,"algorithm; article; human; human experiment; biological model; biology; computer language; computer simulation; history; nonlinear system; procedures; signal processing; software; statistical model; Algorithms; Computational Biology; Computer Simulation; History, 20th Century; History, 21st Century; Linear Models; Models, Biological; Nonlinear Dynamics; Programming Languages; Signal Processing, Computer-Assisted; Software",,,,,,Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85079119586
"Han J., Kamber M., Pei J.","24325399900;7003632479;35273378100;","Data Mining: Concepts and Techniques",2012,"Data Mining: Concepts and Techniques",,,,"","",703,3951,"10.1016/C2009-0-61819-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013808225&doi=10.1016%2fC2009-0-61819-5&partnerID=40&md5=f729b866bc391720c589e10fb0d35f7c","This is the third edition of the premier professional reference on the subject of data mining, expanding and updating the previous market leading edition. This was the first (and is still the best and most popular) of its kind. Combines sound theory with truly practical applications to prepare students for real-world challenges in data mining. Like the first and second editions, Data Mining: Concepts and Techniques, 3rd Edition equips professionals with a sound understanding of data mining principles and teaches proven methods for knowledge discovery in large corporate databases. The first and second editions also established itself as the market leader for courses in data mining, data analytics, and knowledge discovery. Revisions incorporate input from instructors, changes in the field, and new and important topics such as data warehouse and data cube technology, mining stream data, mining social networks, and mining spatial, multimedia and other complex data. This book begins with a conceptual introduction followed by a comprehensive and state-of-the-art coverage of concepts and techniques. Each chapter is a stand-alone guide to a critical topic, presenting proven algorithms and sound implementations ready to be used directly or with strategic modification against live data. Wherever possible, the authors raise and answer questions of utility, feasibility, optimization, and scalability. relational data. -- A comprehensive, practical look at the concepts and techniques you need to get the most out of real business data. -- Updates that incorporate input from readers, changes in the field, and more material on statistics and machine learning, -- Scores of algorithms and implementation examples, all in easily understood pseudo-code and suitable for use in real-world, large-scale data mining projects. -- Complete classroom support for instructors as well as bonus content available at the companion website. A comprehensive and practical look at the concepts and techniques you need in the area of data mining and knowledge discovery. © 2012 Elsevier Inc. All rights reserved.",,,,,,,,Book,"Final","",Scopus,2-s2.0-85013808225
"Szegedy C., Ioffe S., Vanhoucke V., Alemi A.A.","56122593000;7005880161;6507256014;57194151381;","Inception-v4, inception-ResNet and the impact of residual connections on learning",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"4278","4284",,3898,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028013193&partnerID=40&md5=46f5a89e7abca0ba1d9056f3372858ea","Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and nonresidual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Classification (of information); Image recognition; Classification tasks; Computational costs; Convolutional networks; Single frames; State-of-the-art performance; Test sets; Traditional architecture; Network architecture","Amazon;Artificial Intelligence;Baidu;et al.;IBM;Tencent","31st AAAI Conference on Artificial Intelligence, AAAI 2017","4 February 2017 through 10 February 2017",,130407,Conference Paper,"Final","",Scopus,2-s2.0-85028013193
"Bengio Y., Ducharme R., Vincent P., Jauvin C.","7003958245;7004197900;7201678633;6503995197;","A Neural Probabilistic Language Model",2003,"Journal of Machine Learning Research","3","6",,"1137","1155",,3763,"10.1162/153244303322533223","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0142166851&doi=10.1162%2f153244303322533223&partnerID=40&md5=786ebeb149454f1f9d72c189103f8cb8","A goal of statistical language modeling is to learn the joint probability function of sequences of words in a language. This is intrinsically difficult because of the curse of dimensionality: a word sequence on which the model will be tested is likely to be different from all the word sequences seen during training. Traditional but very successful approaches based on n-grams obtain generalization by concatenating very short overlapping sequences seen in the training set. We propose to fight the curse of dimensionality by learning a distributed representation for words which allows each training sentence to inform the model about an exponential number of semantically neighboring sentences. The model learns simultaneously (1) a distributed representation for each word along with (2) the probability function for word sequences, expressed in terms of these representations. Generalization is obtained because a sequence of words that has never been seen before gets high probability if it is made of words that are similar (in the sense of having a nearby representation) to words forming an already seen sentence. Training such large models (with millions of parameters) within a reasonable time is itself a significant challenge. We report on experiments using neural networks for the probability function, showing on two text corpora that the proposed approach significantly improves on state-of-the-art n-gram models, and that the proposed approach allows to take advantage of longer contexts.","Artificial neural networks; Curse of dimensionality; Distributed representation; Statistical language modeling","Mathematical models; Neural networks; Probability; Problem solving; Statistical methods; Distributed representations; Learning systems",,,,,,Conference Paper,"Final","",Scopus,2-s2.0-0142166851
"Manning C.D., Surdeanu M., Bauer J., Finkel J., Bethard S.J., McClosky D.","","The Stanford CoreNLP natural language processing toolkit",2014,"Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics: System Demonstrations",,,,"55","60",,3638,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84909958352
"Vaswani A., Shazeer N., Parmar N., Uszkoreit J., Jones L., Gomez A.N., Kaiser L., Polosukhin I.","","Attention is all you need",2017,"Attention is All You Need",,,,"6000","6010",,3632,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85038368581
"Johnson M.","",[No title available],1987,"The Body in the Mind: The Bodily Basis of Meaning, Imagination, and Reason",,,,"","",,3516,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003681244
"Turing A.M.","36512022900;","On computable numbers, with an application to the entscheidungsproblem",1937,"Proceedings of the London Mathematical Society","s2-42","1",,"230","265",,3372,"10.1112/plms/s2-42.1.230","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960561455&doi=10.1112%2fplms%2fs2-42.1.230&partnerID=40&md5=b1b5cc97fee9501ebdf1eab76fb96ba7",[No abstract available],,,,,,,,Article,"Final","",Scopus,2-s2.0-84960561455
"Silver D., Schrittwieser J., Simonyan K., Antonoglou I., Huang A., Guez A., Hubert T., Baker L., Lai M., Bolton A., Chen Y., Lillicrap T., Hui F., Sifre L., Van Den Driessche G., Graepel T., Hassabis D.","7202151417;57188927606;34870482800;56536965900;57188927052;25925850300;57196116650;57196124277;57196122202;57196119870;57220895088;6503977179;57196116944;55924253100;57188923520;6601966753;15839738700;","Mastering the game of Go without human knowledge",2017,"Nature","550","7676",,"354","359",,3038,"10.1038/nature24270","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031918331&doi=10.1038%2fnature24270&partnerID=40&md5=c270c477058ce3cbd0527e45973484bc","A long-standing goal of artificial intelligence is an algorithm that learns, tabula rasa, superhuman proficiency in challenging domains. Recently, AlphaGo became the first program to defeat a world champion in the game of Go. The tree search in AlphaGo evaluated positions and selected moves using deep neural networks. These neural networks were trained by supervised learning from human expert moves, and by reinforcement learning from self-play. Here we introduce an algorithm based solely on reinforcement learning, without human data, guidance or domain knowledge beyond game rules. AlphaGo becomes its own teacher: a neural network is trained to predict AlphaGo's own move selections and also the winner of AlphaGo's games. This neural network improves the strength of the tree search, resulting in higher quality move selection and stronger self-play in the next iteration. Starting tabula rasa, our new program AlphaGo Zero achieved superhuman performance, winning 100-0 against the previously published, champion-defeating AlphaGo. © 2017 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",,"algorithm; artificial intelligence; artificial neural network; knowledge; supervised learning; Article; artificial intelligence; artificial neural network; game; knowledge; learning algorithm; priority journal; reinforcement; human; recreational game; reinforcement; software; supervised machine learning; unsupervised machine learning; Games, Recreational; Humans; Neural Networks (Computer); Reinforcement (Psychology); Software; Supervised Machine Learning; Unsupervised Machine Learning",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85031918331
"Willia R.J.","57019275700;","Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning",1992,"Machine Learning","8","3",,"229","256",,3021,"10.1023/A:1022672621406","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000337576&doi=10.1023%2fA%3a1022672621406&partnerID=40&md5=50f42911d3a600f4c672696cadbcc06a","This article presents a general class of associative reinforcement learning algorithms for connectionist networks containing stochastic units. These algorithms, called REINFORCE algorithms, are shown to make weight adjustments in a direction that lies along the gradient of expected reinforcement in both immediate-reinforcement tasks and certain limited forms of delayed-reinforcement tasks, and they do this without explicitly computing gradient estimates or even storing information from which such estimates could be computed. Specific examples of such algorithms are presented, some of which bear a close relationship to certain existing algorithms while others are novel but potentially interesting in their own right. Also given are results that show how such algorithms can be naturally integrated with backpropagation. We close with a brief discussion of a number of additional issues surrounding the use of such algorithms, including what is known about their limiting behaviors as well as further considerations that might be used to help develop similar but potentially more powerful reinforcement learning algorithms. © 1992, Kluwer Academic Publishers. All rights reserved.","connectionist networks; gradient descent; mathematical analysis; Reinforcement learning",,,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-0000337576
"Kipf T.N., Welling M.","57202470192;55907170700;","Semi-supervised classification with graph convolutional networks",2017,"5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings",,,,"","",,3007,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086180249&partnerID=40&md5=de472b43f496c76073ce3493990482e3","We present a scalable approach for semi-supervised learning on graph-structured data that is based on an efficient variant of convolutional neural networks which operate directly on graphs. We motivate the choice of our convolutional architecture via a localized first-order approximation of spectral graph convolutions. Our model scales linearly in the number of graph edges and learns hidden layer representations that encode both local graph structure and features of nodes. In a number of experiments on citation networks and on a knowledge graph dataset we demonstrate that our approach outperforms related methods by a significant margin. © ICLR 2019 - Conference Track Proceedings. All rights reserved.",,"Convolution; Graphic methods; Machine learning; Neural networks; Citation networks; Convolutional networks; Convolutional neural network; First-order approximations; Graph structured data; Scalable approach; Semi- supervised learning; Semi-supervised classification; Supervised learning",,"5th International Conference on Learning Representations, ICLR 2017","24 April 2017 through 26 April 2017",,149804,Conference Paper,"Final","",Scopus,2-s2.0-85086180249
"Newell A.","","Unified theories of cognition",1990,"Unified Theories of Cognition",,,,"","",,2992,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004291277
"Maas A.L., Hannun A.Y., Ng A.Y.","","Rectifier nonlinearities improve neural network acoustic models",2013,"Proc. ICML","30",,,"","",,2965,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84893676344
"Cho K., Van Merriënboer B., Gulcehre C., Bahdanau D., Bougares F., Schwenk H., Bengio Y.","","Learning phrase representations using rnn encoder-decoder for statistical machine translation",2014,"Learning Phrase Representations Using Rnn Encoder-decoder for Statistical Machine Translation",,,,"","",,2931,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84919728106
"Socher R., Perelygin A., Wu J.Y., Chuang J., Manning C.D., Ng A.Y., Potts C.","24766896100;56581921600;56576533900;55247620600;35280197500;35410071600;57206534134;","Recursive deep models for semantic compositionality over a sentiment treebank",2013,"EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,,"1631","1642",,2896,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926358845&partnerID=40&md5=aee25e7557c51d87ca49204c286b2813","Semantic word spaces have been very useful but cannot express the meaning of longer phrases in a principled way. Further progress towards understanding compositionality in tasks such as sentiment detection requires richer supervised training and evaluation resources and more powerful models of composition. To remedy this, we introduce a Sentiment Treebank. It includes fine grained sentiment labels for 215,154 phrases in the parse trees of 11,855 sentences and presents new challenges for sentiment composition-ality. To address them, we introduce the Recursive Neural Tensor Network. When trained on the new treebank, this model outperforms all previous methods on several metrics. It pushes the state of the art in single sentence positive/negative classification from 80% up to 85.4%. The accuracy of predicting fine-grained sentiment labels for all phrases reaches 80.7%, an improvement of 9.7% over bag of features baselines. Lastly, it is the only model that can accurately capture the effects of negation and its scope at various tree levels for both positive and negative phrases. © 2013 Association for Computational Linguistics.",,"Natural language processing systems; Semantics; Bag of features; Compositionality; Fine grained; Parse trees; Positive/negative classifications; State of the art; Supervised trainings; Word spaces; Forestry; Classification; Languages; Word Processing","Allen Institute for Artificial Intelligence;Amazon;et al.;Google;Microsoft;Nuance","2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013","18 October 2013 through 21 October 2013",,111413,Conference Paper,"Final","",Scopus,2-s2.0-84926358845
"Kipf T.N., Welling M.","","Semi-supervised Classification with Graph Convolutional Networks",2016,"Semi-supervised Classification with Graph Convolutional Networks",,,,"","",,2877,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85021804857
"Bird S., Klein E., Loper E.","",[No title available],2009,"Natural Language Processing with Python",,,,"","",,2865,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-70349549313
"Johnson-Laird P.N.","","Mental models: Towards a cognitive science of language, inference, and consciousness",1983,"Mental Models: Towards a Cognitive Science of Language, Inference, and Consciousness",,,,"","",,2857,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0003486989
"Belkin M., Niyogi P., Sindhwani V.","57205523518;7004259002;14822592800;","Manifold regularization: A geometric framework for learning from labeled and unlabeled examples",2006,"Journal of Machine Learning Research","7",,,"2399","2434",,2850,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750729556&partnerID=40&md5=02909c0cc31bdebc709cfdc551e272db","We propose a family of learning algorithms based on a new form of regularization that allows us to exploit the geometry of the marginal distribution. We focus on a semi-supervised framework that incorporates labeled and unlabeled data in a general-purpose learner. Some transductive graph learning algorithms and standard methods including support vector machines and regularized least squares can be obtained as special cases. We use properties of reproducing kernel Hilbert spaces to prove new Representer theorems that provide theoretical basis for the algorithms. As a result (in contrast to purely graph-based approaches) we obtain a natural out-of-sample extension to novel examples and so are able to handle both transductive and truly semi-supervised settings. We present experimental evidence suggesting that our semi-supervised algorithms are able to use unlabeled data effectively. Finally we have a brief discussion of unsupervised and fully supervised learning within our general framework.","Graph transduction; Kernel methods; Manifold learning; Regularization; Semi-supervised learning; Spectral graph theory; Support vector machines; Unlabeled data","Data reduction; Geometry; Graph theory; Learning algorithms; Least squares approximations; Graph transduction; Kernel methods; Semi-supervised learning; Spectral graph theory; Support vector machines; Unlabeled data; Learning systems",,,,,,Article,"Final","",Scopus,2-s2.0-33750729556
"Mikolov T., Karafiát M., Burget L., Jan C., Khudanpur S.","34969425500;13008836500;8653241100;42061449300;6603596916;","Recurrent neural network based language model",2010,"Proceedings of the 11th Annual Conference of the International Speech Communication Association, INTERSPEECH 2010",,,,"1045","1048",,2847,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79959829092&partnerID=40&md5=cf6430e6d744999c7bea7c7a7932c9a5","A new recurrent neural network based language model (RNN LM) with applications to speech recognition is presented. Results indicate that it is possible to obtain around 50% reduction of perplexity by using mixture of several RNN LMs, compared to a state of the art backoff language model. Speech recognition experiments show around 18% reduction of word error rate on the Wall Street Journal task when comparing models trained on the same amount of data, and around 5% on the much harder NIST RT05 task, even when the backoff model is trained on much more data than the RNN LM. We provide ample empirical evidence to suggest that connectionist language models are superior to standard n-gram techniques, except their high computational (training) complexity. © 2010 ISCA.","Language modeling; Recurrent neural networks; Speech recognition","Computational linguistics; Modeling languages; Speech communication; Speech recognition; Backoff; Language model; N-grams; State of the art; Wall Street Journal; Word error rate; Recurrent neural networks",,,,,,Conference Paper,"Final","",Scopus,2-s2.0-79959829092
"Hájek P.","",[No title available],1998,"Metamathematics of Fuzzy Logic",,,,"","",,2751,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004130617
"Fauconnier G., Turner M.","",[No title available],2002,"The Way We Think: Conceptual Blending and the Mind's Hidden Complexities",,,,"","",,2745,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0013375571
"Snoek J., Larochelle H., Adams R.P.","15129127600;14827997400;55366781900;","Practical Bayesian optimization of machine learning algorithms",2012,"Advances in Neural Information Processing Systems","4",,,"2951","2959",,2670,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869201485&partnerID=40&md5=14aa83df115308a2cf91468d634a36aa","The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ""black art"" requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.",,"Automatic approaches; Automatic procedures; Bayesian optimization; Convolutional neural network; Expert experience; Generalization performance; Latent Dirichlet allocation; Learning parameters; Cost accounting; Experiments; Learning systems; Neural networks; Optimization; Statistics; Learning algorithms","Winton Capital Management;Google;Pascal2;EMC2 Greenplum;Facebook","26th Annual Conference on Neural Information Processing Systems 2012, NIPS 2012","3 December 2012 through 6 December 2012","Lake Tahoe, NV",96883,Conference Paper,"Final","",Scopus,2-s2.0-84869201485
"Werbos P.J.","7003827430;","Backpropagation Through Time: What It Does and How to Do It",1990,"Proceedings of the IEEE","78","10",,"1550","1560",,2555,"10.1109/5.58337","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025503558&doi=10.1109%2f5.58337&partnerID=40&md5=82cf290beda28d7ebd9cdfbfb80267c5","Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for back-propagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, it describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule forordered derivatives—the theorem which underlies backpropagation—is briefly discussed. © 1990, IEEE",,"Computer Programming--Algorithms; Learning Systems; Pattern Recognition; Backpropagation; Chain Rule; Neural Networks",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0025503558
"Zhu X.","","Semi-supervised learning literature survey",2005,"Technical Report",,,,"","",,2551,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-33745456231
"Bordes A., Usunier N., Garcia-Durán A., Weston J., Yakhnenko O.","18933923000;8558715300;55249506700;8865128200;21735152400;","Translating embeddings for modeling multi-relational data",2013,"Advances in Neural Information Processing Systems",,,,"","",,2395,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899013802&partnerID=40&md5=23c2a9e6dbb1b9be2560df5e24bfa648","We consider the problem of embedding entities and relationships of multi relational data in low-dimensional vector spaces. Our objective is to propose a canonical model which is easy to train, contains a reduced number of parameters and can scale up to very large databases. Hence, we propose TransE, a method which models relationships by interpreting them as translations operating on the low-dimensional embeddings of the entities. Despite its simplicity, this assumption proves to be powerful since extensive experiments show that TransE significantly outperforms state-of-the-art methods in link prediction on two knowledge bases. Besides, it can be successfully trained on a large scale data set with 1M entities, 25k relationships and more than 17M training samples.",,"Canonical modeling; Knowledge basis; Large scale data sets; Link prediction; Relational data; State-of-the-art methods; Training sample; Very large database","Air Force Office of Scientific Research (AFOSR);Amazon.com;et al.;Facebook;Google;Microsoft Research","27th Annual Conference on Neural Information Processing Systems, NIPS 2013","5 December 2013 through 10 December 2013","Lake Tahoe, NV",104690,Conference Paper,"Final","",Scopus,2-s2.0-84899013802
"Bollacker K., Evans C., Paritosh P., Sturge T., Taylor J.","6701444919;52963516700;10243809100;52964660100;57189334931;","Freebase: A collaboratively created graph database for structuring human knowledge",2008,"Proceedings of the ACM SIGMOD International Conference on Management of Data",,,"1376746","1247","1249",,2377,"10.1145/1376616.1376746","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57149137628&doi=10.1145%2f1376616.1376746&partnerID=40&md5=5431f3af0d032747f4ca4358599d20a2","Freebase is a practical, scalable tuple database used to structure general human knowledge. The data in Freebase is collaboratively created, structured, and maintained. Free-base currently contains more than 125,000,000 tuples, more than 4000 types, and more than 7000 properties. Public read/write access to Freebase is allowed through an HTTP-based graph-query API using the Metaweb Query Language (MQL) as a data query and manipulation language. MQL provides an easy-to-use object-oriented interface to the tuple data in Freebase and is designed to facilitate the creation of collaborative, Web-based data-oriented applications.","Design; Human factors; Languages","Applications.; Data queries; Graph databases; Human factors; Human knowledges; Languages; Manipulation languages; Object-oriented; Database systems; Human engineering; Linguistics; Query languages; Object oriented programming","ACM Sigmod","2008 ACM SIGMOD International Conference on Management of Data 2008, SIGMOD'08","9 June 2008 through 12 June 2008","Vancouver, BC",74397,Conference Paper,"Final","",Scopus,2-s2.0-57149137628
"Deb K., Jain H.","7006019904;57197553406;","An evolutionary many-objective optimization algorithm using reference-point-based nondominated sorting approach, Part I: Solving problems with box constraints",2014,"IEEE Transactions on Evolutionary Computation","18","4","6600851","577","601",,2332,"10.1109/TEVC.2013.2281535","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84905581607&doi=10.1109%2fTEVC.2013.2281535&partnerID=40&md5=1cba0c6c5a0edc476f23df0438b79190","Having developed multiobjective optimization algorithms using evolutionary optimization methods and demonstrated their niche on various practical problems involving mostly two and three objectives, there is now a growing need for developing evolutionary multiobjective optimization (EMO) algorithms for handling many-objective (having four or more objectives) optimization problems. In this paper, we recognize a few recent efforts and discuss a number of viable directions for developing a potential EMO algorithm for solving many-objective optimization problems. Thereafter, we suggest a reference-point-based many-objective evolutionary algorithm following NSGA-II framework (we call it NSGA-III) that emphasizes population members that are nondominated, yet close to a set of supplied reference points. The proposed NSGA-III is applied to a number of many-objective test problems with three to 15 objectives and compared with two versions of a recently suggested EMO algorithm (MOEA/D). While each of the two MOEA/D methods works well on different classes of problems, the proposed NSGA-III is found to produce satisfactory results on all problems considered in this paper. This paper presents results on unconstrained problems, and the sequel paper considers constrained and other specialties in handling many-objective optimization problems. © 1997-2012 IEEE.","evolutionary computation; large dimension; Many-objective optimization; multi-criterion optimization; non-dominated sorting; NSGA-III","Evolutionary algorithms; Multiobjective optimization; Large dimensions; Many-objective optimizations; Multi-criterion optimization; Non-dominated Sorting; NSGA-III; Problem solving",,,,,,Article,"Final","",Scopus,2-s2.0-84905581607
"Zadeh L.A.","24613216300;","Fuzzy logic = computing with words",1996,"IEEE Transactions on Fuzzy Systems","4","2",,"103","111",,2324,"10.1109/91.493904","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030142764&doi=10.1109%2f91.493904&partnerID=40&md5=9ea21e7fe41aed719769234bc9d0d6fc","As its name suggests, computing with words (CW) is a methodology in which words are used in place of numbers for computing and reasoning. The point of this note is that fuzzy logic plays a pivotal role in CW and vice-versa. Thus, as an approximation, fuzzy logic may be equated to CW. There are two major imperatives for computing with words. First, computing with words is a necessity when the available information is too imprecise to justify the use of numbers, and second, when there is a tolerance for imprecision which can be exploited to achieve tractability, robustness, low solution cost, and better rapport with reality. Exploitation of the tolerance for imprecision is an issue of central importance in CW. In CW, a word is viewed as a label of a granule; that is, a fuzzy set of points drawn together by similarity, with the fuzzy set playing the role of a fuzzy constraint on a variable. The premises are assumed to be expressed as propositions in a natural language. For purposes of computation, the propositions are expressed as canonical forms which serve to place in evidence the fuzzy constraints that are implicit in the premises. Then, the rules of inference in fuzzy logic are employed to propagate the constraints from premises to conclusions. At this juncture, the techniques of computing with words underlie - in one way or another - almost all applications of fuzzy logic. In coming years, computing with words is likely to evolve into a basic methodology in its own right with wide-ranging ramifications on both basic and applied levels. © 1996 IEEE.",,"Computational linguistics; Constraint theory; Natural language processing systems; Computing with words (CW); Fuzzy constraints; Fuzzy sets",,,,,,Article,"Final","",Scopus,2-s2.0-0030142764
"Tieleman T., Hinton G.","","Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude",2012,"COURSERA: Neural Networks for Machine Learning","4","2",,"","",,2319,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84893343292
"Mihalcea R., Tarau P.","8619220500;6603670264;","TextRank: Bringing order into texts",2004,"Proceedings of the 2004 Conference on Empirical Methods in Natural Language Processing, EMNLP 2004 - A meeting of SIGDAT, a Special Interest Group of the ACL held in conjunction with ACL 2004",,,,"404","411",,2311,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85114317854&partnerID=40&md5=15e2afffa6e0a5b9941ab725ce69c530","In this paper, we introduce TextRank - a graph-based ranking model for text processing, and show how this model can be successfully used in natural language applications. In particular, we propose two innovative unsupervised methods for keyword and sentence extraction, and show that the results obtained compare favorably with previously published results on established benchmarks. © 2005 Association for Computational Linguistics",,"Computational linguistics; Graphic methods; Natural language processing systems; Graph based rankings; Keywords extraction; Natural language applications; Ranking model; Sentence extraction; Text-processing; Unsupervised method; Text processing",,"2004 Conference on Empirical Methods in Natural Language Processing, EMNLP 2004","25 July 2004 through 26 July 2004",,172000,Conference Paper,"Final","",Scopus,2-s2.0-85114317854
"Veličković P., Cucurull G., Casanova A., Romero A., Liò P., Bengio Y.","","Graph attention networks",2018,"ICLR",,,,"","",,2262,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85055102914
"Lefebvre H.","",[No title available],1974,"The Production of Space",,,,"","",,2260,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004128476
"Yang Z., Yang D., Dyer C., He X., Smola A., Hovy E.","57056582700;56145767700;51664869700;37085932700;6701849799;6602910705;","Hierarchical attention networks for document classification",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",,,,"1480","1489",,2239,"10.18653/v1/n16-1174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994158553&doi=10.18653%2fv1%2fn16-1174&partnerID=40&md5=617197a966973cc4fb1d505ec10ad6ea","We propose a hierarchical attention network for document classification. Our model has two distinctive characteristics: (i) it has a hierarchical structure that mirrors the hierarchical structure of documents; (ii) it has two levels of attention mechanisms applied at the wordand sentence-level, enabling it to attend differentially to more and less important content when constructing the document representation. Experiments conducted on six large scale text classification tasks demonstrate that the proposed architecture outperform previous methods by a substantial margin. Visualization of the attention layers illustrates that the model selects qualitatively informative words and sentences. ©2016 Association for Computational Linguistics.",,"Classification (of information); Computational linguistics; Text processing; Attention mechanisms; Document Classification; Document Representation; Hierarchical structures; Proposed architectures; Sentence level; Text classification; Information retrieval systems","Amazon;Baidu;Bloomberg;eBay;et al.;Google","15th Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016","12 June 2016 through 17 June 2016",,124044,Conference Paper,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-84994158553
"Hamilton W.L., Ying R., Leskovec J.","56096744700;57190049577;12241436100;","Inductive representation learning on large graphs",2017,"Advances in Neural Information Processing Systems","2017-December",,,"1025","1035",,2169,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046897776&partnerID=40&md5=e1f06f1d0e9d9edc9db04224446fd32c","Low-dimensional embeddings of nodes in large graphs have proved extremely useful in a variety of prediction tasks, from content recommendation to identifying protein functions. However, most existing approaches require that all nodes in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen nodes. Here we present GraphSAGE, a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Instead of training individual embeddings for each node, we learn a function that generates embeddings by sampling and aggregating features from a node's local neighborhood. Our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and Reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions. © 2017 Neural information processing systems foundation. All rights reserved.",,"Classification (of information); Graphic methods; Proteins; Content recommendations; Feature information; Local neighborhoods; Low dimensional; Prediction tasks; Protein functions; Protein-protein interactions; Text attributes; Graph theory","","31st Annual Conference on Neural Information Processing Systems, NIPS 2017","4 December 2017 through 9 December 2017",,136033,Conference Paper,"Final","",Scopus,2-s2.0-85046897776
"Wilson T., Wiebe J., Hoffmann P.","55510852800;7005437314;35069485000;","Recognizing contextual polarity in phrase-level sentiment analysis",2005,"HLT/EMNLP 2005 - Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,,"347","354",,2038,"10.3115/1220575.1220619","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053247760&doi=10.3115%2f1220575.1220619&partnerID=40&md5=2976fb52f408bf1aaeb3fb5d8fe6c7ed","This paper presents a new approach to phrase-level sentiment analysis that first determines whether an expression is neutral or polar and then disambiguates the polarity of the polar expressions. With this approach, the system is able to automatically identify the contextual polarity for a large subset of sentiment expressions, achieving results that are significantly better than baseline. © 2005 Association for Computational Linguistics.",,"New approaches; Sentiment analysis","eLDa;et al.;Google;Microsoft;SHARP;XEROX - Research Centre Europe","Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, HLT/EMNLP 2005, Co-located with the 2005 Document Understanding Conference, DUC and the 9th International Workshop on Parsing Technologies, IWPT","6 October 2005 through 8 October 2005","Vancouver, BC",86710,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-80053247760
"Jones K.S.","7404727909;","A statistical interpretation of term specificity and its application in retrieval",1972,"Journal of Documentation","28","1",,"11","21",,2032,"10.1108/eb026526","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953744816&doi=10.1108%2feb026526&partnerID=40&md5=3e92d44c84eaf48bae1dea74b91e153b","The exhaustivity of document descriptions and the specificity of index terms are usually regarded as independent. It is suggested that specificity should be interpreted statistically, as a function of term use rather than of term meaning. The effects on retrieval of variations in term specificity are examined, experiments with three test collections showing in particular that frequently-occurring terms are required for good overall performance. It is argued that terms should be weighted according to collection frequency, so that matches on less frequent, more specific, terms are of greater value than matches on frequent terms. Results for the test collections show that considerable improvements in performance are obtained with this very simple procedure. © 1972, MCB UP Limited",,,,,,,,Review,"Final","",Scopus,2-s2.0-84953744816
"Minsky M.L.","",[No title available],1967,"Computation: Finite and Infinite Machines",,,,"","",,1960,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004165618
"Maass W., Natschläger T., Markram H.","7005129380;55972917700;56275180000;","Real-time computing without stable states: A new framework for neural computation based on perturbations",2002,"Neural Computation","14","11",,"2531","2560",,1896,"10.1162/089976602760407955","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0036834701&doi=10.1162%2f089976602760407955&partnerID=40&md5=bd266474cc562ac276ce5de6032dadce","A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology.",,"action potential; article; artificial neural network; biological model; computer; computer simulation; computer system; nerve cell; physiology; Action Potentials; Computer Simulation; Computer Systems; Computers; Models, Neurological; Neural Networks (Computer); Neurons",,,,,,Article,"Final","",Scopus,2-s2.0-0036834701
"Schölkopf B., Burges C.J.C., Smola A.J.","",[No title available],1999,"Advances in Kernel Methods: Support Vector Learning",,,,"","",,1887,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003798627
"Mikolov T., Yih W.-T., Zweig G.","34969425500;23010913500;55142762700;","Linguistic regularities in continuous spaceword representations",2013,"NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference",,,,"746","751",,1858,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926179397&partnerID=40&md5=f1becaf90c3fa1b25a5d1d89f5711f67","Continuous space language models have recently demonstrated outstanding results across a variety of tasks. In this paper, we examine the vector-space word representations that are implicitly learned by the input-layer weights. We find that these representations are surprisingly good at capturing syntactic and semantic regularities in language, and that each relationship is characterized by a relation-specific vector offset. This allows vector-oriented reasoning based on the offsets between words. For example, the male/female relationship is automatically learned, and with the induced vector representations, ""King - Man + Woman"" results in a vector very close to ""Queen."" We demonstrate that the word vectors capture syntactic regularities by means of syntactic analogy questions (provided with this paper), and are able to correctly answer almost 40% of the questions. We demonstrate that the word vectors capture semantic regularities by using the vector offset method to answer SemEval-2012 Task 2 questions. Remarkably, this method outperforms the best previous systems. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Semantics; Syntactics; Vectors; Continuous spaces; Input layers; Language model; Offset method; Vector representations; Word representations; Word vectors; Vector spaces","Appen ButlerHill;et al.;ETS;Google;Microsoft Research;Rakuten","2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2013","9 June 2013 through 14 June 2013",,111457,Conference Paper,"Final","",Scopus,2-s2.0-84926179397
"McClelland J.L., Elman J.L.","57198096249;57225419564;","The TRACE model of speech perception",1986,"Cognitive Psychology","18","1",,"1","86",,1755,"10.1016/0010-0285(86)90015-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022571705&doi=10.1016%2f0010-0285%2886%2990015-0&partnerID=40&md5=b4361c26ac149acefdabc51c62ca7153","We describe a model called the TRACE model of speech perception. The model is based on the principles of interactive activation. Information processing takes place through the excitatory and inhibitory interactions of a large number of simple processing units, each working continuously to update its own activation on the basis of the activations of other units to which it is connected. The model is called the TRACE model because the network of units forms a dynamic processing structure called ""the Trace,"" which serves at once as the perceptual processing mechanism and as the system's working memory. The model is instantiated in two simulation programs. TRACE I, described in detail elsewhere, deals with short segments of real speech, and suggests a mechanism for coping with the fact that the cues to the identity of phonemes vary as a function of context. TRACE II, the focus of this article, simulates a large number of empirical findings on the perception of phonemes and words and on the interactions of phoneme and word perception. At the phoneme level, TRACE II simulates the influence of lexical information on the identification of phonemes and accounts for the fact that lexical effects are found under certain conditions but not others. The model also shows how knowledge of phonological constraints can be embodied in particular lexical items but can still be used to influence processing of novel, nonword utterances. The model also exhibits categorical perception and the ability to trade cues off against each other in phoneme identification. At the word level, the model captures the major positive feature of Marslen-Wilson's COHORT model of speech perception, in that it shows immediate sensitivity to information favoring one word or set of words over others. At the same time, it overcomes a difficulty with the COHORT model: it can recover from underspecification or mispronunciation of a word's beginning. TRACE II also uses lexical information to segment a stream of speech into a sequence of words and to find word beginnings and endings, and it simulates a number of recent findings related to these points. The TRACE model has some limitations, but we believe it is a step toward a psychologically and computationally adequate model of the process of speech perception. © 1986.",,"article; association; computer program; human; model; phonetics; semantics; speech perception; Cues; Human; Models, Psychological; Phonetics; Semantics; Software; Speech Perception; Support, U.S. Gov't, Non-P.H.S.; Support, U.S. Gov't, P.H.S.",,,,,,Article,"Final","",Scopus,2-s2.0-0022571705
"Richardson M., Domingos P.","36139751700;7003565655;","Markov logic networks",2006,"Machine Learning","62","1-2 SPEC. ISS.",,"107","136",,1746,"10.1007/s10994-006-5833-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-32044466073&doi=10.1007%2fs10994-006-5833-1&partnerID=40&md5=0219a59a8c6eb7fa53b1340da59641cf","We propose a simple approach to combining first-order logic and probabilistic graphical models in a single representation. A Markov logic network (MLN) is a firstorder knowledge base with a weight attached to each formula (or clause). Together with a set of constants representing objects in the domain, it specifies a ground Markov network containing one feature for each possible grounding of a first-order formula in the KB, with the corresponding weight. Inference in MLNs is performed by MCMC over the minimal subset of the ground network required for answering the query. Weights are efficiently learned from relational databases by iteratively optimizing a pseudo-likelihood measure. Optionally, additional clauses are learned using inductive logic programming techniques. Experiments with a real-world database and knowledge base in a university domain illustrate the promise of this approach.","First-order logic; Graphical models; Inductive logic programming; Knowledge-based model construction; Log-linear models; Markov chain Monte Carlo; Markov networks; Markov random fields; Pseudo-likelihood; Satisfiability; Statistical relational learning","Information analysis; Learning systems; Markov processes; Monte Carlo methods; Optimization; Relational database systems; First-order logic; Graphical models; Inductive logic programming; Knowledge-based model construction; Log-linear models; Markov logic networks (MLN); Markov random fields; Pseudo likelihood; Satisfiability; Statistical relational learning; Computer graphics",,,,,,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-32044466073
"Lind D., Marcus B.","",[No title available],1995,"An Introduction to Symbolic Dynamics and Coding",,,,"","",,1729,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004111328
"Liu Y., Ott M., Goyal N., Du J., Joshi M., Chen D., Levy O., Lewis M., Zettlemoyer L., Stoyanov V.","","RoBERTa: A Robustly Optimized BERT Pretraining Approach",2019,"Roberta: A Robustly Optimized Bert Pretraining Approach",,,,"","",,1727,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85076489289
"Lewis D.D., Yang Y., Rose T.G., Li F.","55742498400;35231480000;8850442500;8930284900;","RCV1: A new benchmark collection for text categorization research",2004,"Journal of Machine Learning Research","5",,,"361","397",,1714,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876811202&partnerID=40&md5=4ce3319fe7495ab49d17fc47402117d2","Reuters Corpus Volume I (RCV1) is an archive of over 800,000 manually categorized newswire stories recently made available by Reuters, Ltd. for research purposes. Use of this data for research on text categorization requires a detailed understanding of the real world constraints under which the data was produced. Drawing on interviews with Reuters personnel and access to Reuters documentation, we describe the coding policy and quality control procedures used in producing the RCV1 data, the intended semantics of the hierarchical category taxonomies, and the corrections necessary to remove errorful data. We refer to the original data as RCV1-v1, and the corrected data as RCV1-v2. We benchmark several widely used supervised learning methods on RCV1-v2, illustrating the collection's properties, suggesting new directions for research, and providing baseline results for future studies. We make available detailed, per-category experimental results, as well as corrected versions of the category assignments and taxonomy structures, via online appendices. © 2004 David D. Lewis, Yiming Yang, Tony G. Rose, and Fan Li.","Applications; Automated indexing; Controlled vocabulary indexing; Effectiveness measures; Evaluation; Feature selection; K-NN; Methodology; Multiclass; Multilabel; Nearest neighbor; News articles; Operational systems; Rocchio; SCut; SCutFBR; Support vector machines; SVMs; Term weighting; Test collection; Text classification; Thresholding","Applications; Classification (of information); Feature extraction; Indexing (of information); Quality control; Semantics; Support vector machines; Taxonomies; Effectiveness measure; Evaluation; Methodology; Multi-label; Multiclass; Nearest neighbors; News articles; Operational systems; Rocchio; SCut; SCutFBR; SVMs; Term weighting; Test Collection; Text classification; Thresholding; Text processing",,,,,,Article,"Final","",Scopus,2-s2.0-84876811202
"Radford A., Wu J., Child R., Luan D., Amodei D., Sutskever I.","","Language models are unsupervised multitask learners",2019,"Language Models Are Unsupervised Multitask Learners",,"8",,"","",,1709,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85067388978
"Taboada M., Brooke J., Tofiloski M., Voll K., Stede M.","12784775300;35301920800;39763174300;23468222900;6701450094;","Lexicon-basedmethods for sentiment analysis",2011,"Computational Linguistics","37","2",,"267","307",,1708,"10.1162/COLI_a_00049","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958257877&doi=10.1162%2fCOLI_a_00049&partnerID=40&md5=1a3da9caef08c49f971e4ef5dbc99dfa","We present a lexicon-based approach to extracting sentiment from text. The Semantic Orientation CALculator (SO-CAL) uses dictionaries of words annotated with their semantic orientation (polarity and strength), and incorporates intensification and negation. SO-CAL is applied to the polarity classification task, the process of assigning a positive or negative label to a text that captures the text's opinion towards its main subjectmatter.We show that SO-CAL's performance is consistent across domains and on completely unseen data. Additionally, we describe the process of dictionary creation, and our use of Mechanical Turk to check dictionaries for consistency and reliability. © 2011 Association for Computational Linguistics.",,"Semantics; Lexicon-based; Mechanical turks; Polarity classification; Semantic orientation; Sentiment analysis",,,,,,Article,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-79958257877
"Esuli A., Sebastiani F.","15044356100;7004170314;","SENTIWORDNET: A publicly available lexical resource for opinion mining",2006,"Proceedings of the 5th International Conference on Language Resources and Evaluation, LREC 2006",,,,"417","422",,1708,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031789656&partnerID=40&md5=90e374da1f67c13ce93da274568ba34b","Opinion mining (OM) is a recent subdiscipline at the crossroads of information retrieval and computational linguistics which is concerned not with the topic a document is about, but with the opinion it expresses. OM has a rich set of applications, ranging from tracking users' opinions about products or about political candidates as expressed in online forums, to customer relationship management. In order to aid the extraction of opinions from text, recent research has tried to automatically determine the ""PN-polarity"" of subjective terms, i.e. identify whether a term that is a marker of opinionated content has a positive or a negative connotation. Research on determining whether a term is indeed a marker of opinionated content (a subjective term) or not (an objective term) has been, instead, much more scarce. In this work we describe SENTIWORDNET, a lexical resource in which each WORDNET synset s is associated to three numerical scores Obj(s), P os(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. The method used to develop SENTIWORDNET is based on the quantitative analysis of the glosses associated to synsets, and on the use of the resulting vectorial term representations for semi-supervised synset classification. The three scores are derived by combining the results produced by a committee of eight ternary classifiers, all characterized by similar accuracy levels but different classification behaviour. SENTIWORDNET is freely available for research purposes, and is endowed with a Web-based graphical user interface.",,"Classification (of information); Graphical user interfaces; Knowledge engineering; Public relations; User interfaces; Accuracy level; Customer relationship management; Lexical resources; Opinion mining; Recent researches; Research purpose; Semi-supervised; Term representation; Data mining","et al.;European Media Laboratory GmbH (EML);IBM;Regione Liguria;Softissimo;TST-Centrale","5th International Conference on Language Resources and Evaluation, LREC 2006","22 May 2006 through 28 May 2006",,131722,Conference Paper,"Final","",Scopus,2-s2.0-85031789656
"Lee H., Grosse R., Ranganath R., Ng A.Y.","15056237200;34875103900;34875603200;35410071600;","Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations",2009,"Proceedings of the 26th International Conference On Machine Learning, ICML 2009",,,,"609","616",,1670,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71149119164&partnerID=40&md5=58186ca411f85585eab50995c92c80eb","There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks. Scaling such models to full-sized, high-dimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model which scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique which shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images.",,"Belief networks; Bottom-up and top-down; Excellent performance; Generative model; Hierarchical representation; High-dimensional images; Natural scenes; Novel techniques; Probabilistic inference; Realistic images; Translation invariants; Visual feature; Visual recognition; Bayesian networks; Convolution; Robot learning; Unsupervised learning; Education",,"26th International Conference On Machine Learning, ICML 2009","14 June 2009 through 18 June 2009","Montreal, QC",78500,Conference Paper,"Final","",Scopus,2-s2.0-71149119164
"Radford A., Narasimhan K., Salimans T., Sutskever I.","","Improving language understanding by generative pre-training",2018,"Improving Language Understanding by Generative Pre-Training",,,,"","",,1638,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85056475484
"Luhn H.P.","","The automatic creation of literature abstracts",1958,"IBM Journal of Research and Development","2","2",,"159","165",,1619,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0000880768
"Opitz D., Maclin R.","7004700582;6603657355;","Popular Ensemble Methods: An Empirical Study",1999,"Journal of Artificial Intelligence Research","11",,,"169","198",,1607,"10.1613/jair.614","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000551189&doi=10.1613%2fjair.614&partnerID=40&md5=d7fd8a56dda559e553f7af4feb3ede0e","An ensemble consists of a set of individually trained classifiers (such as neural networks or decision trees) whose predictions are combined when classifying novel instances. Previous research has shown that an ensemble is often more accurate than any of the single classifiers in the ensemble. Bagging (Breiman, 1996c) and Boosting (Freund & Schapire, 1996; Schapire, 1990) are two relatively new but popular methods for producing ensembles. In this paper we evaluate these methods on 23 data sets using both neural networks and decision trees as our classification algorithm. Our results clearly indicate a number of conclusions. First, while Bagging is almost always more accurate than a single classifier, it is sometimes much less accurate than Boosting. On the other hand, Boosting can create ensembles that are less accurate than a single classifier - especially when using neural networks. Analysis indicates that the performance of the Boosting methods is dependent on the characteristics of the data set being examined. In fact, further results show that Boosting ensembles may overfit noisy data sets, thus decreasing its performance. Finally, consistent with previous studies, our work suggests that most of the gain in an ensemble's performance comes in the first few classifiers combined; however, relatively large gains can be seen up to 25 classifiers when Boosting decision trees.",,,,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-0000551189
"Kövecses Z.","",[No title available],2002,"Metaphor: A Practical Introduction",,,,"","",,1571,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-3943071217
"Jouppi N.P., Young C., Patil N., Patterson D., Agrawal G., Bajwa R., Bates S., Bhatia S., Boden N., Borchers A., Boyle R., Cantin P.-L., Chao C., Clark C., Coriell J., Daley M., Dau M., Dean J., Gelb B., Ghaemmaghami T.V., Gottipati R., Gulland W., Hagmann R., Richard Ho C., Hogberg D., Hu J., Hundt R., Hurt D., Ibarz J., Jaffey A., Jaworski A., Kaplan A., Khaitan H., Killebrew D., Koch A., Kumar N., Lacy S., Laudon J., Law J., Le D., Leary C., Liu Z., Lucke K., Lundin A., MacKean G., Maggiore A., Mahony M., Miller K., Nagarajan R., Narayanaswami R., Ni R., Nix K., Norrie T., Omernick M., Penukonda N., Phelps A., Ross J., Ross M., Salek A., Samadiani E., Severn C., Sizikov G., Snelham M., Souter J., Steinberg D., Swing A., Tan M., Thorson G., Tian B., Toma H., Tuttle E., Vasudevan V., Walter R., Wang W., Wilcox E., Yoon D.H.","7003278650;55239677000;57214620241;7401930147;56369015400;57195138658;57195129229;57195128653;57195130632;57195134313;57195131955;57195131469;57195126019;57213539294;57195131199;57195130688;57195126929;16427311000;57195128104;57195131762;57195126449;57195126852;57195126236;57195137581;57195127450;57195128331;6603534903;57195131559;57191410963;57195133994;57195128342;57195127240;57195137413;55868980800;57195135559;57214110211;57195131290;6603184330;57195127235;57195127156;57189304786;57195126776;57195131452;57195136413;57195128731;57195137441;57195133372;56073382500;57195132478;57195133159;57195137544;57195136804;57195129669;42962200500;57195136221;57195132957;57195128022;57195133959;57213297608;15072937000;57195127202;24448552500;57195129319;57195130361;57195137748;57195125999;57195137546;6602482988;57195131738;56719211700;57195137961;57202537142;57195128224;57195138024;57195128328;35184626400;","In-datacenter performance analysis of a tensor processing unit",2017,"Proceedings - International Symposium on Computer Architecture","Part F128643",,,"1","12",,1557,"10.1145/3079856.3080246","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85025594365&doi=10.1145%2f3079856.3080246&partnerID=40&md5=308f276a5b6ff510a595cacd935c9f01","Many architects believe that major improvements in cost-energyperformance must now come from domain-specific hardware. This paper evaluates a custom ASIC-called a Tensor Processing Unit (TPU)-deployed in datacenters since 2015 that accelerates the inference phase of neural networks (NN). The heart of the TPU is a 65,536 8-bit MAC matrix multiply unit that offers a peak throughput of 92 TeraOps/second (TOPS) and a large (28 MiB) software-managed on-chip memory. The TPU's deterministic execution model is a better match to the 99th-percentile responsetime requirement of our NN applications than are the time-varying optimizations of CPUs and GPUs that help average throughput more than guaranteed latency. The lack of such features helps explain why, despite having myriad MACs and a big memory, the TPU is relatively small and low power. We compare the TPU to a server-class Intel Haswell CPU and an NVIDIA K80 GPU, which are contemporaries deployed in the same datacenters. Our workload, written in the high-level TensorFlow framework, uses production NN applications (MLPs, CNNs, and LSTMs) that represent 95% of our datacenters' NN inference demand. Despite low utilization for some applications, the TPU is on average about 15X-30X faster than its contemporary GPU or CPU, with TOPS/Watt about 30X-80X higher. Moreover, using the GPU's GDDR5 memory in the TPU would triple achieved TOPS and raise TOPS/Watt to nearly 70X the GPU and 200X the CPU. © 2017 Association for Computing Machinery.","Accelerator; CNN; Deep learning; DNN; Domain-specific architecture; GPU; LSTM; MLP; Neural network; RNN; TensorFlow; TPU","Computer hardware; Deep learning; Deep neural networks; Graphics processing unit; Image coding; Memory architecture; Network architecture; Neural networks; Particle accelerators; Program processors; Tensors; Average throughput; Deterministic execution; Domain specific architectures; LSTM; Neural network (nn); Performance analysis; Processing units; TensorFlow; Computer architecture","ARM;Cavium;et al.;Facebook;Google;Intel","44th Annual International Symposium on Computer Architecture - ISCA 2017","24 June 2017 through 28 June 2017",,128643,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85025594365
"Jackendoff R.","","Foundations of language: Brain, meaning, grammar, evolution",2002,"Foundations of Language: Brain, Meaning, Grammar, Evolution",,,,"","",,1549,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004076559
"Scarselli F., Gori M., Tsoi A.C., Hagenbuchner M., Monfardini G.","6603295429;7005254436;7005107318;6602995698;14039630300;","The graph neural network model",2009,"IEEE Transactions on Neural Networks","20","1",,"61","80",,1542,"10.1109/TNN.2008.2005605","https://www.scopus.com/inward/record.uri?eid=2-s2.0-58649113008&doi=10.1109%2fTNN.2008.2005605&partnerID=40&md5=3f795120994301ae72b2904a50a4703b","Many underlying relationships among data in several areas of science and engineering, e.g., computer vision, molecular chemistry, molecular biology, pattern recognition, and data mining, can be represented in terms of graphs. In this paper, we propose a new neural network model, called graph neural network (GNN) model, that extends existing neural network methods for processing the data represented in graph domains. This GNN model, which can directly process most of the practically useful types of graphs, e.g., acyclic, cyclic, directed, and undirected, implements a function τ (G,n) ∈ Rm that maps a graph G and one of its nodes n into an m-dimensional Euclidean space. A supervised learning algorithm is derived to estimate the parameters of the proposed GNN model. The computational cost of the proposed algorithm is also considered. Some experimental results are shown to validate the proposed learning algorithm, and to demonstrate its generalization capabilities. © 2008 IEEE.","Graph neural networks (GNNs); Graph processing; Graphical domains; Recursive neural networks","Biochemistry; Computer vision; Data processing; Image processing; Information management; Learning algorithms; Learning systems; Molecular biology; Pattern recognition; Recursive functions; Computational costs; Euclidean spaces; Generalization capabilities; Graph neural networks (GNNs); Graph processing; Graphical domains; Network methods; Network modelling; Recursive neural networks; Neural networks; algorithm; article; artificial intelligence; artificial neural network; automated pattern recognition; comparative study; factual database; Internet; nonlinear system; regression analysis; reproducibility; statistical model; Algorithms; Artificial Intelligence; Databases, Factual; Internet; Linear Models; Neural Networks (Computer); Nonlinear Dynamics; Pattern Recognition, Automated; Regression Analysis; Reproducibility of Results",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-58649113008
"Gödel K.","16546130700;","Über formal unentscheidbare Sätze der Principia Mathematica und verwandter Systeme I",1931,"Monatshefte für Mathematik und Physik","38","1",,"173","198",,1538,"10.1007/BF01700692","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34347193348&doi=10.1007%2fBF01700692&partnerID=40&md5=186d081aa2cca2e34d515397f843c8bd",[No abstract available],,,,,,,,Article,"Final","",Scopus,2-s2.0-34347193348
"Zoph B., Vasudevan V., Shlens J., Le Q.V.","57156654400;57202537142;8632398500;12140898300;","Learning Transferable Architectures for Scalable Image Recognition",2018,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition",,,"8579005","8697","8710",,1507,"10.1109/CVPR.2018.00907","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062864819&doi=10.1109%2fCVPR.2018.00907&partnerID=40&md5=9cc7779a086dc894416a7ace38dab4a3","Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the 'NASNet search space') which enables transferability. In our experiments, we search for the best convolutional layer (or 'cell') on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a 'NASNet architecture'. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset. © 2018 IEEE.",,"Cells; Computer vision; Convolution; Cytology; Image recognition; Large dataset; Network architecture; Object detection; Architectural buildings; Architecture engineering; Classification models; Computational costs; Computational demands; Computer vision problems; Model architecture; Regularization technique; Image classification",,"31st Meeting of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2018","18 June 2018 through 22 June 2018",,143811,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85062864819
"Mintz M., Bills S., Snow R., Jurafsky D.","","Distant supervision for relation extraction without labeled data",2009,"Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP",,,,"1003","1011",,1484,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-77957863090
"Tsochantaridis I., Joachims T., Hofmann T., Altun Y.","6508296014;6602804136;56735589800;14041321400;","Large margin methods for structured and interdependent output variables",2005,"Journal of Machine Learning Research","6",,,"","",32,1442,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-24944537843&partnerID=40&md5=67e4cdafba56b0d655b6e349d1872674","Learning general functional dependencies between arbitrary input and output spaces is one of the key challenges in computational intelligence. While recent progress in machine learning has mainly focused on designing flexible and powerful input representations, this paper addresses the complementary issue of designing classification algorithms that can deal with more complex outputs, such as trees, sequences, or sets. More generally, we consider problems involving multiple dependent output variables, structured output spaces, and classification problems with class attributes. In order to accomplish this, we propose to appropriately generalize the well-known notion of a separation margin and derive a corresponding maximum-margin formulation. While this leads to a quadratic program with a potentially prohibitive, i.e. exponential, number of constraints, we present a cutting plane algorithm that solves the optimization problem in polynomial time for a large class of problems. The proposed method has important applications in areas such as computational biology, natural language processing, information retrieval/extraction, and optical character recognition. Experiments from various domains involving different types of output spaces emphasize the breadth and generality of our approach.",,"Algorithms; Artificial intelligence; Classification (of information); Information retrieval; Natural language processing systems; Optical character recognition; Optimization; Polynomials; Problem solving; Set theory; Trees (mathematics); Classification algorithms; Computational biology; Interdependent output variables; Large margin methods; Learning systems",,,,,,Article,"Final","",Scopus,2-s2.0-24944537843
"Kingma D., Ba J.","","""Adam: A Method for Stochastic Optimization""",0000,"Adam: A Method for Stochastic Optimization",,,,"","",,1439,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84950982946
"Sennrich R., Haddow B., Birch A.","36474235300;8583975100;35263632000;","Neural machine translation of rare words with subword units",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers","3",,,"1715","1725",,1382,"10.18653/v1/p16-1162","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011827221&doi=10.18653%2fv1%2fp16-1162&partnerID=40&md5=535e3d4ab018357fbc86b60be458aeea","Neural machine translation (NMT) models typically operate with a fixed vocabulary, but translation is an open-vocabulary problem. Previous work addresses the translation of out-of-vocabulary words by backing off to a dictionary. In this paper, we introduce a simpler and more effective approach, making the NMT model capable of open-vocabulary translation by encoding rare and unknown words as sequences of subword units. This is based on the intuition that various word classes are translatable via smaller units than words, for instance names (via character copying or transliteration), compounds (via compositional translation), and cognates and loanwords (via phonological and morphological transformations). We discuss the suitability of different word segmentation techniques, including simple character ngram models and a segmentation based on the byte pair encoding compression algorithm, and empirically show that subword models improve over a back-off dictionary baseline for the WMT 15 translation tasks English→German and English→Russian by up to 1.1 and 1.3 Bleu, respectively. © 2016 Association for Computational Linguistics.",,"Computer aided language translation; Encoding (symbols); Signal encoding; Byte-pair encoding; Compositional translation; Compression algorithms; Effective approaches; Machine translations; Morphological transformations; Out of vocabulary words; Word segmentation; Computational linguistics","Amazon;Baidu;Bloomberg;et al.;Facebook;Google","54th Annual Meeting of the Association for Computational Linguistics, ACL 2016","7 August 2016 through 12 August 2016",,125782,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85011827221
"Pickering M.J., Garrod S.","7006461382;7004121632;","Toward a mechanistic psychology of dialogue",2004,"Behavioral and Brain Sciences","27","2",,"169","190",,1374,"10.1017/s0140525x04000056","https://www.scopus.com/inward/record.uri?eid=2-s2.0-12744262642&doi=10.1017%2fs0140525x04000056&partnerID=40&md5=17605729f2e1250166d57aa38d01dd4f","Traditional mechanistic accounts of language processing derive almost entirely from the study of monologue. Yet, the most natural and basic form of language use is dialogue. As a result, these accounts may only offer limited theories of the mechanisms that underlie language processing in general. We propose a mechanistic account of dialogue, the interactive alignment account, and use it to derive a number of predictions about basic language processes. The account assumes that, in dialogue, the linguistic representations employed by the interlocutors become aligned at many levels, as a result of a largely automatic process. This process greatly simplifies production and comprehension in dialogue. After considering the evidence for the interactive alignment model, we concentrate on three aspects of processing that follow from it. It makes use of a simple interactive inference mechanism, enables the development of local dialogue routines that greatly simplify language processing, and explains the origins of self-monitoring in production. We consider the need for a grammatical framework that is designed to deal with language in dialogue rather than monologue, and discuss a range of implications of the account. © 2004 Cambridge University Press.","Common ground; Dialogue; Dialogue routines; Language comprehension; Language production; Monitoring; Perception-behavior link","article; auditory discrimination; autonomic nervous system function; cognition; comprehension; conversation; coordination; experimental model; health status; interpersonal communication; language ability; maze test; mental function; mental health; nerve conduction; phonetics; prediction; reading; self monitoring; semantics; social interaction; speech articulation; speech discrimination; speech intelligibility; theory; human; human relation; language; linguistics; mental function; psychological theory; review; speech; systems theory; Humans; Interpersonal Relations; Language; Mental Processes; Psycholinguistics; Psychological Theory; Speech; Systems Theory",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-12744262642
"Rossi F., van Beek P., Walsh T.","",[No title available],2006,"Handbook of Constraint Programming",,,,"","",,1334,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-33748706709
"Mikolov T., Sutskever I., Chen K., Corrado G., Dean J.","","Distributed representations of words and phrases and their compositionality. In: Conference on Advances in Neural Information Processing Systems",2013,"Distributed Representations of Words and Phrases and Their Compositionality",,,,"3111","3119",,1323,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84902100258
"Pang B., Lee L.","8644537200;7404389769;","Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales",2005,"ACL-05 - 43rd Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",,,,"115","124",,1265,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859895244&partnerID=40&md5=ad0a999b1a746a5ebfdd039844757330","We address the rating-inference problem, wherein rather than simply decide whether a review is .thumbs up. or .thumbs down., as in previous sentiment analysis work, one must determine an author's evaluation with respect to a multi-point scale (e.g., one to five .stars.). This task represents an interesting twist on standard multi-class text categorization because there are several different degrees of similarity between class labels; for example, .three stars. is intuitively closer to .four stars. than to .one star.. We first evaluate human performance at the task. Then, we apply a metaalgorithm, based on a metric labeling formulation of the problem, that alters a given n-ary classifier's output in an explicit attempt to ensure that similar items receive similar labels. We show that the meta-algorithm can provide significant improvements over both multi-class and regression versions of SVMs when we employ a novel similarity measure appropriate to the problem. © 2005 Association for Computational Linguistics.",,"Class labels; Class relationship; Human performance; Metric labeling; Multi-class; Rating scale; Sentiment analysis; Similarity measure; Text categorization; Computational linguistics; Rating; Text processing; Stars",,"43rd Annual Meeting of the Association for Computational Linguistics, ACL-05","25 June 2005 through 30 June 2005","Ann Arbor, MI",89385,Conference Paper,"Final","",Scopus,2-s2.0-84859895244
"Luong M.-T., Pham H., Manning C.D.","","Effective approaches to attention-based neural machine translation",2015,"Effective Approaches to Attention-based Neural Machine Translation",,,,"","",,1257,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84994358876
"Jolliffe I.","",[No title available],2005,"Principal Component Analysis",,,,"","",,1255,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84887397359
"Go A., Bhayani R., Huang L.","","Twitter sentiment classification using distant supervision",2009,"Twitter Sentiment Classification Using Distant Supervision",,,,"","",,1254,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-79953762206
"Clark H.H., Wilkes-Gibbs D.","7202405686;6507473491;","Referring as a collaborative process",1986,"Cognition","22","1",,"1","39",,1242,"10.1016/0010-0277(86)90010-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0022671781&doi=10.1016%2f0010-0277%2886%2990010-7&partnerID=40&md5=1d1bf0a7b8357ecf8d49f22c06fa685e","In conversation, speakers and addressees work together in the making of a definite reference. In the model we propose, the speaker initiates the process by presenting or inviting a noun phrase. Before going on to the next contribution, the participants, if necessary, repair, expand on, or replace the noun phrase in an iterative process until they reach a version they mutually accept. In doing so they try to minimize their joint effort. The preferred procedure is for the speaker to present a simple noun phrase and for the addressee to accept it by allowing the next contribution to begin. We describe a communication task in which pairs of people conversed about arranging complex figures and show how the proposed model accounts for many features of the references they produced. The model follows, we suggest, from the mutual responsibility that participants in conversation bear toward the understanding of each utterance. © 1986.",,"article; human; human relation; interpersonal communication; linguistics; model; speech; Communication; Human; Interpersonal Relations; Linguistics; Models, Psychological; Speech; Support, Non-U.S. Gov't; Support, U.S. Gov't, Non-P.H.S.; Support, U.S. Gov't, P.H.S.",,,,,,Article,"Final","",Scopus,2-s2.0-0022671781
"Aho A.V., Ullman J.D.","",[No title available],1972,"The Theory of Parsing, Translation and Compiling","1",,,"","",,1230,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003791899
"Ijspeert A.J.","7003779012;","Central pattern generators for locomotion control in animals and robots: A review",2008,"Neural Networks","21","4",,"642","653",,1222,"10.1016/j.neunet.2008.03.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-40049092971&doi=10.1016%2fj.neunet.2008.03.014&partnerID=40&md5=0845bdcceeddb5ad2b068daf31730033","The problem of controlling locomotion is an area in which neuroscience and robotics can fruitfully interact. In this article, I will review research carried out on locomotor central pattern generators (CPGs), i.e. neural circuits capable of producing coordinated patterns of high-dimensional rhythmic output signals while receiving only simple, low-dimensional, input signals. The review will first cover neurobiological observations concerning locomotor CPGs and their numerical modelling, with a special focus on vertebrates. It will then cover how CPG models implemented as neural networks or systems of coupled oscillators can be used in robotics for controlling the locomotion of articulated robots. The review also presents how robots can be used as scientific tools to obtain a better understanding of the functioning of biological CPGs. Finally, various methods for designing CPGs to control specific modes of locomotion will be briefly reviewed. In this process, I will discuss different types of CPG models, the pros and cons of using CPGs with robots, and the pros and cons of using robots as scientific tools. Open research topics both in biology and in robotics will also be discussed. © 2008 Elsevier Ltd. All rights reserved.","Central pattern generators; Computational models; Dynamical systems; Locomotion; Neural networks; Robots; Systems of coupled oscillators","Animals; Architectural design; Artificial intelligence; Automation; Biomechanics; Biped locomotion; Computer networks; Metropolitan area networks; Network protocols; Neural networks; Numerical methods; Oscillators (electronic); Pigments; Robots; (2+1)-dimensional; (e ,3e) process; Articulated robots; Central Pattern Generators (CPG); Coordinated patterns; Coupled oscillators; Different types; Elsevier (CO); High-dimensional; input signals; Locomotion control; Modes of locomotion; Neural circuits; Numerical modelling; Output signals; Research topics; Robotics; article; generator; locomotion; mathematical model; neurobiology; nonhuman; oscillator; priority journal; robotics; signal processing; vertebrate; Animals; Biological Clocks; Central Nervous System; Humans; Locomotion; Movement; Nerve Net; Neural Networks (Computer); Neural Pathways; Neurosciences; Robotics",,,,,,Article,"Final","",Scopus,2-s2.0-40049092971
"Smolensky P.","","Information processing in dynamical systems: Foundations of harmony theory",1986,"Parallel Distributed Processing: Explorations in the Microstructure of Cognition","1",,,"194","281",,1212,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0000329993
"Medhat W., Hassan A., Korashy H.","56173896900;57217371978;56173070900;","Sentiment analysis algorithms and applications: A survey",2014,"Ain Shams Engineering Journal","5","4",,"1093","1113",,1192,"10.1016/j.asej.2014.04.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919491355&doi=10.1016%2fj.asej.2014.04.011&partnerID=40&md5=2634a461826a2d90229fe0e28b6a8280","Sentiment Analysis (SA) is an ongoing field of research in text mining field. SA is the computational treatment of opinions, sentiments and subjectivity of text. This survey paper tackles a comprehensive overview of the last update in this field. Many recently proposed algorithms' enhancements and various SA applications are investigated and presented briefly in this survey. These articles are categorized according to their contributions in the various SA techniques. The related fields to SA (transfer learning, emotion detection, and building resources) that attracted researchers recently are discussed. The main target of this survey is to give nearly full image of SA techniques and the related fields with brief details. The main contributions of this paper include the sophisticated categorizations of a large number of recent articles and the illustration of the recent trend of research in the sentiment analysis and its related areas. © 2014 Production and hosting by Elsevier B.V.","Building resources; Emotion detection; Feature selection; Sentiment analysis; Sentiment classification; Transfer learning","Feature extraction; Sentiment analysis; Analysis algorithms; Emotion detection; Recent trends; Sentiment classification; Text mining; Transfer learning; Surveys",,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-84919491355
"Derrida J.","",[No title available],1973,"Speech and Phenomena and Other Essays on Husserl's Theory of Signs",,,,"","",,1153,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003667368
"Zaremba W., Sutskever I., Vinyals O.","",[No title available],2014,"Recurrent Neural Network Regularization",,,,"","",,1152,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84944053926
"Douglas B.L.","57190036381;","CYC: A Large-Scale Investment in Knowledge Infrastructure",1995,"Communications of the ACM","38","11",,"33","38",,1143,"10.1145/219717.219745","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029410109&doi=10.1145%2f219717.219745&partnerID=40&md5=cb61873621300b5942fa1ec5aae462c9",[No abstract available],,"Artificial intelligence; Automation; Database systems; Information retrieval; Information technology; Knowledge based systems; Knowledge engineering; Knowledge representation; Large scale systems; Learning systems; Natural language processing systems; Word processing; Knowledge infrastructure; Natural language understanding; Reasoning; Expert systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0029410109
"Myers I.B.","",[No title available],1962,"Myers-Briggs Type Indicator",,,,"","",,1141,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003595680
"Sukhbaatar S., Szlam A., Weston J., Fergus R.","55669357500;11539079600;8865128200;14821791600;","End-to-end memory networks",2015,"Advances in Neural Information Processing Systems","2015-January",,,"2440","2448",,1139,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965143740&partnerID=40&md5=05de00797a59096d7a267804d925c93b","We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.",,"Information science; Natural language processing systems; Attention model; End to end; External memory; Language model; Memory network; Question Answering; Treebanks; Modeling languages","et al.;Google, Inc.;Ketchum Trading;Microsoft;Taobao (China) Software Co., Ltd. (Alibaba);Twitter","29th Annual Conference on Neural Information Processing Systems, NIPS 2015","7 December 2015 through 12 December 2015",,120037,Conference Paper,"Final","",Scopus,2-s2.0-84965143740
"Lin Y., Liu Z., Sun M., Liu Y., Zhu X.","57155321900;55714725800;7403180987;57196311263;56172608200;","Learning entity and relation embeddings for knowledge graph completion",2015,"Proceedings of the National Conference on Artificial Intelligence","3",,,"2181","2187",,1122,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959863917&partnerID=40&md5=4f601ea69c685e55a17ee25d2910db94","Knowledge graph completion aims to perform link prediction between entities. In this paper, we consider the approach of knowledge graph embeddings. Recently, models such as TransE and TransH build entity and relation embeddings by regarding a relation as translation from head entity to tail entity. We note that these models simply put both entities and relations within the same semantic space. In fact, an entity may have multiple aspects and various relations may focus on different aspects of entities, which makes a common space insufficient for modeling. In this paper, we propose TransR to build entity and relation embeddings in separate entity space and relation spaces. Afterwards, we learn embeddings by first projecting entities from entity space to corresponding relation space and then building translations between projected entities. In experiments, we evaluate our models on three tasks including link prediction, triple classification and relational fact extraction. Experimental results show significant and consistent improvements compared to state-of-the-art baselines including TransE and TransH. The source code of this paper can be obtained from https://github.com/mrlyk423/relation-extraction. © Copyright 2015, Association for the Advancement of Artificial Intelligence (www.aaa1.org). All rights reserved.",,"Semantics; Common spaces; Corresponding relations; Fact extraction; Knowledge graphs; Link prediction; Semantic Space; Source codes; State of the art; Artificial intelligence","AI Journal;Association for the Advancement of Artificial Intelligence (AAAI);Baidu;et al.;Infosys;National Science Foundation","29th AAAI Conference on Artificial Intelligence, AAAI 2015 and the 27th Innovative Applications of Artificial Intelligence Conference, IAAI 2015","25 January 2015 through 30 January 2015",,114360,Conference Paper,"Final","",Scopus,2-s2.0-84959863917
"Rajpurkar P., Zhang J., Lopyrev K., Liang P.","57056352800;57211160729;57211155731;56646712700;","SQuad: 100,000+ questions for machine comprehension of text",2016,"EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"2383","2392",,1085,"10.18653/v1/d16-1264","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071396128&doi=10.18653%2fv1%2fd16-1264&partnerID=40&md5=1f3910cbbfd2e7859f54a4bc41e9b8f9","We present the Stanford Question Answering Dataset (SQuAD), a new reading comprehension dataset consisting of 100,000+ questions posed by crowdworkers on a set of Wikipedia articles, where the answer to each question is a segment of text from the corresponding reading passage. We analyze the dataset to understand the types of reasoning required to answer the questions, leaning heavily on dependency and constituency trees. We build a strong logistic regression model, which achieves an F1 score of 51.0%, a significant improvement over a simple baseline (20%). However, human performance (86.8%) is much higher, indicating that the dataset presents a good challenge problem for future research. The dataset is freely available at https://stanford-qa.com. © 2016 Association for Computational Linguistics",,"Regression analysis; Challenge problems; F1 scores; Human performance; Logistic Regression modeling; Question Answering; Reading comprehension; Stanford; Wikipedia articles; Natural language processing systems","Amazon.com;Baidu;et al.;Google;Grammarly;Microsoft","2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016","1 November 2016 through 5 November 2016",,150070,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85071396128
"Bengio Y., Louradour J., Collobert R., Weston J.","7003958245;35612099600;14064641400;8865128200;","Curriculum learning",2009,"Proceedings of the 26th International Conference On Machine Learning, ICML 2009",,,,"41","48",,1075,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-71149116544&partnerID=40&md5=5a497924eb12b0202bcb80b713ca89a4","Humans and animals learn much better when the examples are not randomly presented but organized in a meaningful order which illustrates gradually more concepts, and gradually more complex ones. Here, we formalize such training strategies in the context of machine learning, and call them ""curriculum learning"". In the context of recent research studying the difficulty of training in the presence of non-convex training criteria (for deep deterministic and stochastic neural networks), we explore curriculum learning in various set-ups. The experiments show that significant improvements in generalization can be achieved. We hypothesize that curriculum learning has both an effect on the speed of convergence of the training process to a minimum and, in the case of non-convex criteria, on the quality of the local minima obtained: curriculum learning can be seen as a particular form of continuation method (a general strategy for global optimization of non-convex functions).",,"Continuation method; Local minimums; Machine-learning; Nonconvex functions; Set-ups; Speed of convergence; Stochastic neural network; Training process; Training strategy; Animals; Global optimization; Neural networks; Robot learning; Curricula",,"26th International Conference On Machine Learning, ICML 2009","14 June 2009 through 18 June 2009","Montreal, QC",78500,Conference Paper,"Final","",Scopus,2-s2.0-71149116544
"Godfrey J.J., Holliman E.C., McDaniel J.","7102768096;6506677514;57194279142;","SWITCHBOARD: Telephone speech corpus for research and development",1992,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","1",,"225858","517","520",,1063,"10.1109/ICASSP.1992.225858","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85016587886&doi=10.1109%2fICASSP.1992.225858&partnerID=40&md5=830dd451d31c9679e487017e398107f2","SWITCHBOARD is a large multispeaker corpus of conversational speech and text which should be of interest to researchers in speaker authentication and large vocabulary speech recognition. About 2500 conversations by 500 speakers from around the U.S. were collected automatically over T1 lines at Texas Instruments. Designed for training and testing of a variety of speech processing algorithms, especially in speaker verification, it has over an hour of speech from each of 50 speakers, and several minutes each from hundreds of others. A time-aligned word for word transcription accompanies each recording. © 1992 IEEE.",,"Audio signal processing; Character recognition; Electric switchboards; Signal processing; Speech; Speech communication; Speech processing; Conversational speech; Large vocabulary speech recognition; Processing algorithms; Research and development; Speaker verification; Telephone speech; Texas Instruments; Training and testing; Speech recognition","","1992 International Conference on Acoustics, Speech, and Signal Processing, ICASSP 1992","23 March 1992 through 26 March 1992",,127735,Conference Paper,"Final","",Scopus,2-s2.0-85016587886
"Niles I., Pease A.","6506475470;7004582448;","Towards a standard upper ontology",2001,"Formal Ontology in Information Systems: Collected Papers from the Second International Conference",,,,"2","9",,1056,"10.1145/505168.505170","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035789772&doi=10.1145%2f505168.505170&partnerID=40&md5=73b4f896fe221f38e67e7a4ff0b97625","The Suggested Upper Merged Ontology (SUMO) is an upper level ontology that has been proposed as a starter document for The Standard Upper Ontology Working Group, an IEEE-sanctioned working group of collaborators from the fields of engineering, philosophy, and information science. The SUMO provides definitions for general-purpose terms and acts as a foundation for more specific domain ontologies. In this paper we outline the strategy used to create the current version of the SUMO, discuss some of the challenges that we faced in constructing the ontology, and describe in detail its most general concepts and the relations between them.","Knowledge Interchange Format; Ontologies","Computer software; Knowledge based systems; Knowledge representation; Semantics; World Wide Web; Ontology; Artificial intelligence","ACM/SIGART","Formal Ontology in Information Systems: Collected Papers from the Second International Conference","17 October 2001 through 19 October 2001","Ogunquit, ME",60894,Conference Paper,"Final","",Scopus,2-s2.0-0035789772
"Wang Z., Zhang J., Feng J., Chen Z.","56393607000;55904683400;55468540400;55574827100;","Knowledge graph embedding by translating on hyperplanes",2014,"Proceedings of the National Conference on Artificial Intelligence","2",,,"1112","1119",,1050,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908213178&partnerID=40&md5=df83ba807091443a9ba70416dc458740","We deal with embedding a large scale knowledge graph composed of entities and relations into a continuous vector space. TransE is a promising method proposed recently, which is very efficient while achieving state-of-the-art predictive performance. We discuss some mapping properties of relations which should be considered in embedding, such as reflexive, one-to-many, many-to-one, and many-to-many. We note that TransE does not do well in dealing with these properties. Some complex models are capable of preserving these mapping properties but sacrifice efficiency in the process. To make a good trade-off between model capacity and efficiency, in this paper we propose TransH which models a relation as a hyperplanc together with a translation operation on it. In this way, we can well preserve the above mapping properties of relations with almost the same model complexity of TransE. Additionally, as a practical knowledge graph is often far from completed, how to construct negative examples to reduce false negative labels in training is very important. Utilizing the one-to-many/many-to-one mapping property of a relation, we propose a simple trick to reduce the possibility of false negative labeling. We conduct extensive experiments on link prediction, triplet classification and fact extraction on benchmark datasets like WordNet and Freebase. Experiments show TransH delivers significant improvements over TransE on predictive accuracy with comparable capability to scale up. Copyright © 2014, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Knowledge graphs","AI Journal;Association for the Advancement of Artificial Intelligence (AAAI);CRA Committee on the Status of Women in Computing Research/ Coalition to Diversify Computing;et al.;Microsoft Research;National Science Foundation","28th AAAI Conference on Artificial Intelligence, AAAI 2014, 26th Innovative Applications of Artificial Intelligence Conference, IAAI 2014 and the 5th Symposium on Educational Advances in Artificial Intelligence, EAAI 2014","27 July 2014 through 31 July 2014",,108517,Conference Paper,"Final","",Scopus,2-s2.0-84908213178
"Socher R., Chen D., Manning C.D., Ng A.Y.","24766896100;56121986100;35280197500;35410071600;","Reasoning with neural tensor networks for knowledge base completion",2013,"Advances in Neural Information Processing Systems",,,,"","",,1029,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898956227&partnerID=40&md5=944be75cf417e1b8dbc7225e7d3d87d8","Knowledge bases are an important resource for question answering and other tasks but often suffer from incompleteness and lack of ability to reason over their discrete entities and relationships. In this paper we introduce an expressive neural tensor network suitable for reasoning over relationships between two entities. Previous work represented entities as either discrete atomic units or with a single entity vector representation. We show that performance can be improved when entities are represented as an average of their constituting word vectors. This allows sharing of statistical strength between, for instance, facts involving the ""Sumatran tiger"" and ""Bengal tiger."" Lastly, we demonstrate that all models improve when these word vectors are initialized with vectors learned from unsupervised large corpora. We assess the model by considering the problem of predicting additional true relations between entities given a subset of the knowledge base. Our model outperforms previous models and can classify unseen relationships in WordNet and FreeBase with an accuracy of 86.2% and 90.0%, respectively.",,"Knowledge based systems; Tensors; Atomic units; Knowledge base; Knowledge basis; Large corpora; Question Answering; Statistical strength; Vector representations; Word vectors; Neural networks","Air Force Office of Scientific Research (AFOSR);Amazon.com;et al.;Facebook;Google;Microsoft Research","27th Annual Conference on Neural Information Processing Systems, NIPS 2013","5 December 2013 through 10 December 2013","Lake Tahoe, NV",104690,Conference Paper,"Final","",Scopus,2-s2.0-84898956227
"Lenat D.B., Guha R.V.","",[No title available],1990,"Building Large Knowledge-Based Systems",,,,"","",,1010,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004053085
"Strapparava C., Valitutti A.","6602773549;23390897000;","WordNet-Affect: An affective extension of WordNet",2004,"Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC 2004",,,,"1083","1086",,966,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029531951&partnerID=40&md5=144309ff73dea2643bfaaf622230abd7","In this paper we present a linguistic resource for the lexical representation of affective knowledge. This resource (named WordNetAffect) was developed starting from WordNet, through a selection and tagging of a subset of synsets representing the affective meanings.",,"Linguistic resources; Synsets; Wordnet; Ontology","Fundacao Calouste Gulbenkian;Fundacao para a Ciencia e a Tecnologia (FCT);IBM;Microsoft;Porto Editora - Dictionaries Experties;Priberam Informatica","4th International Conference on Language Resources and Evaluation, LREC 2004","26 May 2004 through 28 May 2004",,131721,Conference Paper,"Final","",Scopus,2-s2.0-85029531951
"See A., Liu P.J., Manning C.D.","57200334205;57200337917;35280197500;","Get to the point: Summarization with pointer-generator networks",2017,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)","1",,,"1073","1083",,951,"10.18653/v1/P17-1099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037342720&doi=10.18653%2fv1%2fP17-1099&partnerID=40&md5=9f80a19297f6003b23a8694380c755f8","Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points. © 2017 Association for Computational Linguistics.",,"Linguistics; Text processing; Keep track of; New approaches; Novel architecture; Sequence models; Source text; State of the art; Text summarization; Computational linguistics","Amazon;Apple;Baidu;et al.;Google;Tencent","55th Annual Meeting of the Association for Computational Linguistics, ACL 2017","30 July 2017 through 4 August 2017",,132950,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85037342720
"Lai S., Xu L., Liu K., Zhao J.","55522964500;55843856700;55729555700;57190004147;","Recurrent convolutional neural networks for text classification",2015,"Proceedings of the National Conference on Artificial Intelligence","3",,,"2267","2273",,944,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959872385&partnerID=40&md5=09f4688ba998a6c1b18b624f72671778","Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent convolutional neural network for text classification without human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to capture the key components in texts. We conduct experiments on four commonly used datasets. The experimental results show that the proposed method outperforms the state-of-the-art methods on several datasets, particularly on document-level datasets. © Copyright 2015, Association for the Advancement of Artificial Intelligence (www.aaa1.org). All rights reserved.",,"Artificial intelligence; Classification (of information); Convolution; Neural networks; Recurrent neural networks; Contextual information; Convolutional neural network; Knowledge basis; State-of-the-art methods; Text classification; Text classifiers; Window-based; Word representations; Text processing","AI Journal;Association for the Advancement of Artificial Intelligence (AAAI);Baidu;et al.;Infosys;National Science Foundation","29th AAAI Conference on Artificial Intelligence, AAAI 2015 and the 27th Innovative Applications of Artificial Intelligence Conference, IAAI 2015","25 January 2015 through 30 January 2015",,114360,Conference Paper,"Final","",Scopus,2-s2.0-84959872385
"Aggarwal C.C., Zhai C.","7006797289;35232046000;","A survey of text classification algorithms",2012,"Mining Text Data","9781461432234",,,"163","222",,941,"10.1007/978-1-4614-3223-4_6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873127711&doi=10.1007%2f978-1-4614-3223-4_6&partnerID=40&md5=d6b3369b50a6269cfaf6991dfce886f2","The problem of classification has been widely studied in the data mining, machine learning, database, and information retrieval communities with applications in a number of diverse domains, such as target marketing, medical diagnosis, news group filtering, and document organization. In this paper we will provide a survey of a wide variety of text classification algorithms. © 2012 Springer Science+Business Media, LLC. All rights reserved.","Text Classification","Classification (of information); Computer aided diagnosis; Data mining; Diagnosis; Information filtering; Information retrieval systems; Marketing; Medical computing; Surveys; Diverse domains; Document organization; Target marketing; Text classification; Text processing",,,,,,Book Chapter,"Final","All Open Access, Green",Scopus,2-s2.0-84873127711
"Firth J.R.","","A synopsis of linguistic theory 1930-1955",1957,"Studies in Linguistic Analysis",,,,"1","32",,933,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0007186571
"Ramos J.","","Using TF-IDF to determine word relevance in document queries",2003,"Using TF-IDF to Determine Word Relevance in Document Queries",,,,"","",,929,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-77950327510
"Wiebe J., Wilson T., Cardie C.","7005437314;55510852800;6602309687;","Annotating expressions of opinions and emotions in language",2005,"Language Resources and Evaluation","39","2-3",,"165","210",,926,"10.1007/s10579-005-7880-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644632271&doi=10.1007%2fs10579-005-7880-9&partnerID=40&md5=6163d4661dd62dffb93b1a41cbc65bbc","This paper describes a corpus annotation project to study issues in the manual annotation of opinions, emotions, sentiments, speculations, evaluations and other private states in language. The resulting corpus annotation scheme is described, as well as examples of its use. In addition, the manual annotation process and the results of an inter-annotator agreement study on a 10,000-sentence corpus of articles drawn from the world press are presented. © Springer 2006.","Affect; Attitudes; Corpus annotation; Emotion; Natural language processing; Opinions; Sentiment; Subjectivity",,,,,,,Review,"Final","",Scopus,2-s2.0-33644632271
"Romero C., Ventura S.","55865135900;8846948400;","Educational data mining: A survey from 1995 to 2005",2007,"Expert Systems with Applications","33","1",,"135","146",,923,"10.1016/j.eswa.2006.04.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33845663518&doi=10.1016%2fj.eswa.2006.04.005&partnerID=40&md5=364bd5cd311a0c50f82ebcb65468d322","Currently there is an increasing interest in data mining and educational systems, making educational data mining as a new growing research community. This paper surveys the application of data mining to traditional educational systems, particular web-based courses, well-known learning content management systems, and adaptive and intelligent web-based educational systems. Each of these systems has different data source and objectives for knowledge discovering. After preprocessing the available data in each case, data mining techniques can be applied: statistics and visualization; clustering, classification and outlier detection; association rule mining and pattern mining; and text mining. The success of the plentiful work needs much more specialized work in order for educational data mining to become a mature area. © 2006 Elsevier Ltd. All rights reserved.","Data mining; Educational systems; Web mining; Web-based educational systems","Data processing; Data reduction; Education; Text processing; World Wide Web; Association rule mining; Educational systems; Web mining; Web-based educational systems; Data mining",,,,,,Article,"Final","",Scopus,2-s2.0-33845663518
"Sejnowski T.J., Rosenberg C.R.","","Parallel networks that learn to pronounce English text",1987,"Complex Systems","1","1",,"145","168",,922,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0000383868
"Zeng D., Liu K., Lai S., Zhou G., Zhao J.","55953844000;55729555700;55522964500;54581936100;57190004147;","Relation classification via convolutional deep neural network",2014,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",,,,"2335","2344",,917,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959862537&partnerID=40&md5=941bf9f282e37f03bbb4a724f8abb056","The state-of-the-art methods used for relation classification are primarily based on statistical machine learning, and their performance strongly depends on the quality of the extracted features. The extracted features are often derived from the output of pre-existing natural language processing (NLP) systems, which leads to the propagation of the errors in the existing tools and hinders the performance of these systems. In this paper, we exploit a convolutional deep neural network (DNN) to extract lexical and sentence level features. Our method takes all of the word tokens as input without complicated pre-processing. First, the word tokens are transformed to vectors by looking up word embeddings1. Then, lexical level features are extracted according to the given nouns. Meanwhile, sentence level features are learned using a convolutional approach. These two level features are concatenated to form the final extracted feature vector. Finally, the features are fed into a softmax classifier to predict the relationship between two marked nouns. The experimental results demonstrate that our approach significantly outperforms the state-of-the-art methods.",,"Artificial intelligence; Classification (of information); Computational linguistics; Convolution; Data mining; Learning algorithms; Learning systems; Linguistics; Deep neural networks; Feature vectors; NAtural language processing; Pre-processing; Relation classifications; Sentence level; State-of-the-art methods; Statistical machine learning; Natural language processing systems","Baidu;eBay;Google;Microsoft;Symantec","25th International Conference on Computational Linguistics, COLING 2014","23 August 2014 through 29 August 2014",,116676,Conference Paper,"Final","",Scopus,2-s2.0-84959862537
"Tarski A.","",[No title available],1956,"Logic, Semantics, Metamathematics",,,,"","",,915,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004213353
"Bingham E., Mannila H.","7005213428;7005002288;","Random projection in dimensionality reduction: Applications to image and text data",2001,"Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"245","250",,910,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035789317&partnerID=40&md5=35ccc188601bf47f1c7e42b17850d580","Random projections have recently emerged as a powerful method for dimensionality reduction. Theoretical results indicate that the method preserves distances quite nicely; however, empirical results are sparse. We present experimental results on using random projection as a dimensionality reduction tool in a number of cases, where the high dimensionality of the data would otherwise lead to burdensome computations. Our application areas are the processing of both noisy and noiseless images, and information retrieval in text documents. We show that projecting the data onto a random lower-dimensional subspace yields resuits comparable to conventional dimensionality reduction methods such as principal component analysis: the similarity of data vectors is preserved well under random projection. However, using random projections is computationally significantly less expensive than using, e.g., principal component analysis. We also show experimentally that using a sparse random matrix gives additional computational savings in random projection.","Dimensionality reduction; High-dimensional data; Image data; Random projection; Text document data","Computational methods; Image analysis; Information retrieval; Matrix algebra; Vectors; Random projections; Data reduction",,"Proceedings of the Seventh ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD-2001)","26 August 2001 through 29 August 2001","San Francisco, CA",60361,Conference Paper,"Final","",Scopus,2-s2.0-0035789317
"Yang Z., Dai Z., Yang Y., Carbonell J., Salakhutdinov R., Le Q.V.","57192119391;57193232324;35231480000;35609950300;57203057355;12140898300;","XLNet: Generalized autoregressive pretraining for language understanding",2019,"Advances in Neural Information Processing Systems","32",,,"","",,875,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090172731&partnerID=40&md5=b10fa8fef6f3c459f8a552b8f2fc09c5","With the capability of modeling bidirectional contexts, denoising autoencoding based pretraining like BERT achieves better performance than pretraining approaches based on autoregressive language modeling. However, relying on corrupting the input with masks, BERT neglects dependency between the masked positions and suffers from a pretrain-finetune discrepancy. In light of these pros and cons, we propose XLNet, a generalized autoregressive pretraining method that (1) enables learning bidirectional contexts by maximizing the expected likelihood over all permutations of the factorization order and (2) overcomes the limitations of BERT thanks to its autoregressive formulation. Furthermore, XLNet integrates ideas from Transformer-XL, the state-of-the-art autoregressive model, into pretraining. Empirically, under comparable experiment setting, XLNet outperforms BERT on 20 tasks, often by a large margin, including question answering, natural language inference, sentiment analysis, and document ranking. © 2019 Neural information processing systems foundation. All rights reserved.",,"Search engines; Sentiment analysis; Auto regressive models; Bi-directional contexts; Document ranking; Expected likelihoods; Language understanding; Natural languages; Question Answering; State of the art; Modeling languages","Citadel;Doc.AI;et al.;Lambda;Lyft;Microsoft Research","33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019","8 December 2019 through 14 December 2019",,161263,Conference Paper,"Final","",Scopus,2-s2.0-85090172731
"Nickel M., Tresp V., Kriegel H.-P.","52264441000;6603805670;7005718994;","A three-way model for collective learning on multi-relational data",2011,"Proceedings of the 28th International Conference on Machine Learning, ICML 2011",,,,"809","816",,872,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053444720&partnerID=40&md5=377127db1d452199f8e2bd427b139284","Relational learning is becoming increasingly important in many areas of application. Here, we present a novel approach to relational learning based on the factorization of a three-way tensor. We show that unlike other tensor approaches, our method is able to perform collective learning via the latent components of the model and provide an efficient algorithm to compute the factorization. We substantiate our theoretical considerations regarding the collective learning capabilities of our model by the means of experiments on both a new dataset and a dataset commonly used in entity resolution. Furthermore, we show on common benchmark datasets that our approach achieves better or on-par results, if compared to current state-of-the-art relational learning solutions, while it is significantly faster to compute. Copyright 2011 by the author(s)/owner(s).",,"Benchmark datasets; Collective learning; Data sets; Efficient algorithm; Relational learning; Algorithms; Factorization; Learning systems; Tensors","amazon.com;NSF;Microsoft;Google;Yahoo! Labs","28th International Conference on Machine Learning, ICML 2011","28 June 2011 through 2 July 2011","Bellevue, WA",86752,Conference Paper,"Final","",Scopus,2-s2.0-80053444720
"Socher R., Huval B., Manning C.D., Ng A.Y.","24766896100;55697378500;35280197500;35410071600;","Semantic compositionality through recursive matrix-vector spaces",2012,"EMNLP-CoNLL 2012 - 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, Proceedings of the Conference",,,,"1201","1211",,870,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870715081&partnerID=40&md5=d40adb6b58b2b043bf9afb97445a9919","Single-word vector space models have been very successful at learning lexical information. However, they cannot capture the compositional meaning of longer phrases, preventing them from a deeper understanding of language. We introduce a recursive neural network (RNN) model that learns compositional vector representations for phrases and sentences of arbitrary syntactic type and length. Our model assigns a vector and a matrix to every node in a parse tree: the vector captures the inherent meaning of the constituent, while the matrix captures how it changes the meaning of neighboring words or phrases. This matrix-vector RNN can learn the meaning of operators in propositional logic and natural language. The model obtains state of the art performance on three different experiments: predicting fine-grained sentiment distributions of adverb-adjective pairs; classifying sentiment labels of movie reviews and classifying semantic relationships such as cause-effect or topic-message between nouns using the syntactic path between them. © 2012 Association for Computational Linguistics.",,"Lexical information; Natural languages; Propositional logic; Recursive neural networks; Semantic relationships; State-of-the-art performance; Vector representations; Vector space models; Formal logic; Neural networks; Semantics; Syntactics; Vector spaces; Natural language processing systems","Baidu;Google;Microsoft Research","2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL 2012","12 July 2012 through 14 July 2012","Jeju Island",98999,Conference Paper,"Final","",Scopus,2-s2.0-84870715081
"Edmundson H.P.","24495068100;","New Methods in Automatic Extracting",1969,"Journal of the ACM (JACM)","16","2",,"264","285",,861,"10.1145/321510.321519","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945708920&doi=10.1145%2f321510.321519&partnerID=40&md5=275823e143f44fe8ca5da72982774e1f","This paper describes new methods of automatically extracting documents for screening purposes, i.e. the computer selection of sentences having the greatest potential for conveying to the reader the substance of the document. While previous work has focused on one component of sentence significance, namely, the presence of high-frequency content words (key words), the methods described here also treat three additional components: pragmatic words (cue words); title and heading words; and structural indicators (sentence location).The research has resulted in an operating system and a research methodology. The extracting system is parameterized to control and vary the influence of the above four components. The research methodology includes procedures for the compilation of the required dictionaries, the setting of the control parameters, and the comparative evaluation of the automatic extracts with manually produced extracts. The results indicate that the three newly proposed components dominate the frequency component in the production of better extracts. © 1969, ACM. All rights reserved.",,,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84945708920
"Osgood C.E., May W.H., Miron M.S.","",[No title available],1975,"Cross-Cultural Universals of Affective Meaning",,,,"","",,858,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003474738
"Kleene S.C.","","Representation of events in nerve nets and finite automata",1956,"Automata Studies",,,,"3","41",,843,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003067817
"Achlioptas D.","7003284656;","Database-friendly random projections: Johnson-Lindenstrauss with binary coins",2003,"Journal of Computer and System Sciences","66","4",,"671","687",,820,"10.1016/S0022-0000(03)00025-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0038166193&doi=10.1016%2fS0022-0000%2803%2900025-4&partnerID=40&md5=4a09242baa2b2aec20547ad577ebffde","A classic result of Johnson and Lindenstrauss asserts that any set of n points in d-dimensional Euclidean space can be embedded into k-dimensional Euclidean space - where k is logarithmic in n and independent of d - so that all pairwise distances are maintained within an arbitrarily small factor. All known constructions of such embeddings involve projecting the n points onto a spherically random k-dimensional hyperplane through the origin. We give two constructions of such embeddings with the property that all elements of the projection matrix belong in {-1, 0, +1}. Such constructions are particularly well suited for database environments, as the computation of the embedding reduces to evaluating a single aggregate over k random partitions of the attributes. © 2003 Elsevier Science (USA). All rights reserved.",,"Computation theory; Matrix algebra; Projection systems; Vectors; Projection matrix; Database systems",,,,,,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-0038166193
"Socher R., Lin C.C.-Y., Ng A.Y., Manning C.D.","24766896100;35115314900;35410071600;35280197500;","Parsing natural scenes and natural language with recursive neural networks",2011,"Proceedings of the 28th International Conference on Machine Learning, ICML 2011",,,,"129","136",,819,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053438267&partnerID=40&md5=4bcc5c60ee41841026c5577527484c3d","Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%. Copyright 2011 by the author(s)/owner(s).",,"Alternative approach; Complex scenes; Data sets; Descriptors; Natural languages; Natural scene images; Natural scenes; Parse trees; Recursive neural networks; Recursive structure; Scene classification; Scene segmentation; Stanford; State-of-the-art performance; Structure prediction; Syntactic parsers; Treebanks; Algorithms; Learning systems; Semantics; Neural networks","amazon.com;NSF;Microsoft;Google;Yahoo! Labs","28th International Conference on Machine Learning, ICML 2011","28 June 2011 through 2 July 2011","Bellevue, WA",86752,Conference Paper,"Final","",Scopus,2-s2.0-80053438267
"Gilmer J., Schoenholz S.S., Riley P.F., Vinyals O., Dahl G.E.","","Neural message passing for quantum chemistry",2017,"Neural Message Passing for Quantum Chemistry",,,,"1263","1272",,814,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85038875959
"Haussler D.","","Convolution kernels on discrete structures",1999,"Technical Report",,,,"","",,805,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004019973
"Mikolov T., Chen K., Corrado G., Dean J.","","Efficient estimation of word representations in vector space",2013,"CoRR",,,,"","",,790,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84903761492
"Graves A., Wayne G., Danihelka I.","","Neural turing machines",2014,"Neural Turing Machines",,,,"","",,788,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84930616355
"Serban I.V., Sordoni A., Bengio Y., Courville A., Pineau J.","57191256019;35276255400;7003958245;6507291186;13404973100;","Building end-To-end dialogue systems using generative hierarchical neural network models",2016,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,,,"3776","3783",,786,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84980367197&partnerID=40&md5=16aeaefae2604a8bc4be780549788328","We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-The-Art neural language models and backoff n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger questionanswer pair corpus and from pretrained word embeddings. © Copyright 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Linguistics; Speech processing; Dialogue systems; Encoder-decoder; Generative model; Hierarchical neural networks; Language model; Question-answer pairs; State of the art; System response; Hierarchical systems","Artificial Intelligence;Baidu;et al.;IBM;Infosys;NSF","30th AAAI Conference on Artificial Intelligence, AAAI 2016","12 February 2016 through 17 February 2016",,124960,Conference Paper,"Final","",Scopus,2-s2.0-84980367197
"Bratko I.","",[No title available],1990,"Prolog Programming for Artificial Intelligence",,,,"","",,785,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003676303
"Dos Santos C.N., Gatti M.","55665418300;55319933300;","Deep convolutional neural networks for sentiment analysis of short texts",2014,"COLING 2014 - 25th International Conference on Computational Linguistics, Proceedings of COLING 2014: Technical Papers",,,,"69","78",,774,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84932166511&partnerID=40&md5=c9825887048817f46442e8d5306163a5","Sentiment analysis of short texts such as single sentences and Twitter messages is challenging because of the limited contextual information that they normally contain. Effectively solving this task requires strategies that combine the small text content with prior knowledge and use more than just bag-of-words. In this work we propose a new deep convolutional neural network that exploits from character- To sentence-level information to perform sentiment analysis of short texts. We apply our approach for two corpora of two different domains: The Stanford Sentiment Treebank (SSTb), which contains sentences from movie reviews; and the Stanford Twitter Sentiment corpus (STS), which contains Twitter messages. For the SSTb corpus, our approach achieves state-of-the-art results for single sentence sentiment prediction in both binary positive/negative classification, with 85.7% accuracy, and fine-grained classification, with 48.3% accuracy. For the STS corpus, our approach achieves a sentiment prediction accuracy of 86.4%.",,"Classification (of information); Convolution; Data mining; Linguistics; Neural networks; Social networking (online); Contextual information; Convolutional neural network; Different domains; Positive/negative classifications; Prediction accuracy; Prior knowledge; Sentiment analysis; State of the art; Computational linguistics","Baidu;eBay;Google;Microsoft;Symantec","25th International Conference on Computational Linguistics, COLING 2014","23 August 2014 through 29 August 2014",,116676,Conference Paper,"Final","",Scopus,2-s2.0-84932166511
"Collins M.","",[No title available],1999,"Head-Driven Statistical Models for Natural Language Parsing",,,,"","",,769,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0003984828
"Rush A.M., Chopra S., Weston J.","51665746500;56248489500;8865128200;","A neural attention model for sentence summarization",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",,,,"379","389",,754,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957571911&partnerID=40&md5=b1fd3c3201b1adc1e339b266585f7b15","Summarization based on text extraction is inherently limited, but generation-style abstractive methods have proven challenging to build. In this work, we propose a fully data-driven approach to abstractive sentence summarization. Our method utilizes a local attention-based model that generates each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. The model shows significant performance gains on the DUC-2004 shared task compared with several strong baselines. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Attention model; Data-driven approach; End to end; Large amounts; Performance Gain; Text extraction; Training data; Natural language processing systems","Baidu;Bloomberg;et al.;facebook;Google;Linkedin","Conference on Empirical Methods in Natural Language Processing, EMNLP 2015","17 September 2015 through 21 September 2015",,116677,Conference Paper,"Final","",Scopus,2-s2.0-84957571911
"Rudin C.","8850433600;","Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",2019,"Nature Machine Intelligence","1","5",,"206","215",,752,"10.1038/s42256-019-0048-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069492292&doi=10.1038%2fs42256-019-0048-x&partnerID=40&md5=be8fbc500efef0b2332fac4355bea0c3","Black box machine learning models are currently being used for high-stakes decision making throughout society, causing problems in healthcare, criminal justice and other domains. Some people hope that creating methods for explaining these black box models will alleviate some of the problems, but trying to explain black box models, rather than creating models that are interpretable in the first place, is likely to perpetuate bad practice and can potentially cause great harm to society. The way forward is to design models that are inherently interpretable. This Perspective clarifies the chasm between explaining black boxes and using inherently interpretable models, outlines several key reasons why explainable black boxes should be avoided in high-stakes decisions, identifies challenges to interpretable machine learning, and provides several example applications where interpretable models could potentially replace black box models in criminal justice, healthcare and computer vision. © 2019, Springer Nature Limited.",,"Behavioral research; Crime; Decision making; Health care; Medical computing; Bad practices; Black boxes; Black-box model; Criminal justice; Design models; Machine learning models; Machine learning",,,,,,Review,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85069492292
"Ishibuchi H., Tsukamoto N., Nojima Y.","7005630377;23098858800;8652791500;","Evolutionary many-objective optimization: A short review",2008,"2008 IEEE Congress on Evolutionary Computation, CEC 2008",,,"4631121","2419","2426",,742,"10.1109/CEC.2008.4631121","https://www.scopus.com/inward/record.uri?eid=2-s2.0-55749105514&doi=10.1109%2fCEC.2008.4631121&partnerID=40&md5=1d3cb8eb6c5821516030cf4d091a6a99","Whereas evolutionary multiobjective optimization (EMO) algorithms have successfully been used in a wide range of real-world application tasks, difficulties in their scalability to many-objective problems have also been reported. In this paper, first we demonstrate those difficulties through computational experiments. Then we review some approaches proposed in the literature for the scalability improvement of EMO algorithms. Finally we suggest future research directions in evolutionary many-objective optimization. © 2008 IEEE.",,"Optimization; Scalability; Application tasks; Computational experiments; EMO algorithms; Evolutionary multiobjective optimization algorithms; Future research directions; Objective optimizations; To many; Multiobjective optimization",,"2008 IEEE Congress on Evolutionary Computation, CEC 2008","1 June 2008 through 6 June 2008","Hong Kong",73863,Conference Paper,"Final","",Scopus,2-s2.0-55749105514
"Crescenzi V., Mecca G., Merialdo P.","7006793444;7003401550;6602539867;","RoadRunner: Towards automatic data extraction from large web sites",2001,"VLDB 2001 - Proceedings of 27th International Conference on Very Large Data Bases",,,,"109","118",,742,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944327150&partnerID=40&md5=70ecbd09617d898b2f25ae68bda812bd","The paper investigates techniques for extracting data from HTML sites through the use of automatically generated wrappers. To automate the wrapper generation and the data extraction process, the paper develops a novel technique to compare HTML pages and generate a wrapper based on their similarities and differences. Experimental results on real-life data-intensive Web sites confirm the feasibility of the approach.",,"Extraction; HTML; Websites; Automatically generated; Data extraction; HTML pages; Novel techniques; Real life data; Data mining","","27th International Conference on Very Large Data Bases, VLDB 2001","11 September 2001 through 14 September 2001",,114215,Conference Paper,"Final","",Scopus,2-s2.0-84944327150
"Shapiro S.S., Wilk M.B., Chen H.J.","7402734126;7005055568;56965115900;","A Comparative Study of Various Tests for Normality",1968,"Journal of the American Statistical Association","63","324",,"1343","1372",,732,"10.1080/01621459.1968.10480932","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84947367867&doi=10.1080%2f01621459.1968.10480932&partnerID=40&md5=b4c12e02b5571442cc262ece930c8b25","Results are given of an empirical sampling study of the sensitivities of nine statistical procedures for evaluating the normality of a complete sample. The nine statistics are W (Shapiro and Wilk, 1965), (standard third moment), b2 (standard fourth moment), KS (Kolmogorov-Smirnov), CM (Cramer-Von Mises), WCM (weighted CM), D (modified KS), CS (chi-squared) and u (Studentized range). Forty-five alternative distributions in twelve families and five sample sizes were studied. Results are included on the comparison of the statistical procedures in relation to groupings of the alternative distributions, on means and variances of the statistics under the various alternatives, on dependence of sensitivities on sample size, on approach to normality as measured by the W statistic within some classes of distribution, and on the effect of misspecification of parameters on the performance of the simple hypothesis test statistics. The general findings include: (i) The W statistic provides a generally superior omnibus measure of non-normality; (ii) the distance tests (KS, CM, WCM, D) are typically very insensitive; (iii) the u statistic is excellent against symmetric, especially short-tailed, distributions but has virtually no sensitivity to asymmetry; (iv) a combination of both and b2 usually provides a sensitive judgment but even their combined performance is usually dominated by W; (v) with sensitive procedures, good indication of extreme non-normality (e.g., the exponential distribution) can be achieved with samples of size less than 20. © Taylor &amp; Francis Group, LLC.",,,,,,,,Article,"Final","",Scopus,2-s2.0-84947367867
"Li X., Roth D.","","Learning question classifiers",2002,"Proceedings of the 19th International Conference on Computational Linguistics",,,,"556","562",,731,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-1542370072
"Haralick R.M., Elliott G.L.","35517072000;7202434316;","Increasing tree search efficiency for constraint satisfaction problems",1980,"Artificial Intelligence","14","3",,"263","313",,731,"10.1016/0004-3702(80)90051-X","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0019067870&doi=10.1016%2f0004-3702%2880%2990051-X&partnerID=40&md5=4d294ae1423a6fe18f148f3e97211b76","In this paper we explore the number of tree search operations required to solve binary constraint satisfaction problems. We show analytically and experimentally that the two principles of first trying the places most likely to fail and remembering what has been done to avoid repeating the same mistake twice improve the standard backtracking search. We experimentally show that a lookahead procedure called forward checking (to anticipate the future) which employs the most likely to fail principle performs better than standard backtracking, Ullman's, Waltz's, Mackworth's, and Haralick's discrete relaxation in all cases tested, and better than Gaschnig's backmarking in the larger problems. © 1980, All rights reserved.",,"Computer programming",,,,,,Article,"Final","",Scopus,2-s2.0-0019067870
"Langacker R.W.","",[No title available],1999,"Grammar and Conceptualization",,,,"","",,728,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0009708638
"Svozil D., Kvasnička V., Pospíchal J.","57222151633;55394488400;35617699700;","Introduction to multi-layer feed-forward neural networks",1997,"Chemometrics and Intelligent Laboratory Systems","39","1",,"43","62",,726,"10.1016/S0169-7439(97)00061-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0342871690&doi=10.1016%2fS0169-7439%2897%2900061-0&partnerID=40&md5=acca1a0620476a551141f643f16ed737","Basic definitions concerning the multi-layer feed-forward neural networks are given. The back-propagation training algorithm is explained. Partial derivatives of the objective function with respect to the weight and threshold coefficients are derived. These derivatives are valuable for an adaptation process of the considered neural network. Training and generalisation of multi-layer feed-forward neural networks are discussed. Improvements of the standard back-propagation algorithm are reviewed. Example of the use of multi-layer feed-forward neural networks for prediction of carbon-13 NMR chemical shifts of alkanes is given. Further applications of neural networks in chemistry are reviewed. Advantages and disadvantages of multilayer feed-forward neural networks are discussed.","Back-propagation network; Neural networks","algorithm; analytic method; artificial neural network; carbon nuclear magnetic resonance; conference paper; internet; priority journal; protein folding; quantitative structure activity relation; spectroscopy",,,,,,Conference Paper,"Final","",Scopus,2-s2.0-0342871690
"Conneau A., Kiela D., Schwenk H., Barrault L., Bordes A.","55115745600;56349687800;7005072756;14017765800;18933923000;","Supervised learning of universal sentence representations from natural language inference data",2017,"EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"670","680",,722,"10.18653/v1/d17-1070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068046401&doi=10.18653%2fv1%2fd17-1070&partnerID=40&md5=d4351cf6f12a52d34c70ab45049c6078","Many modern NLP systems rely on word embeddings, previously trained in an unsupervised manner on large corpora, as base features. Efforts to obtain embeddings for larger chunks of text, such as sentences, have however not been so successful. Several attempts at learning unsupervised representations of sentences have not reached satisfactory enough performance to be widely adopted. In this paper, we show how universal sentence representations trained using the supervised data of the Stanford Natural Language Inference datasets can consistently outperform unsupervised methods like SkipThought vectors (Kiros et al., 2015) on a wide range of transfer tasks. Much like how computer vision uses ImageNet to obtain features, which can then be transferred to other tasks, our work tends to indicate the suitability of natural language inference for transfer learning to other NLP tasks. Our encoder is publicly available1. © 2017 Association for Computational Linguistics.",,"Embeddings; Machine learning; Large corpora; Natural languages; NLP systems; Stanford; Transfer learning; Unsupervised method; Natural language processing systems","Amazon;Apple;Baidu;et al.;Facebook;Google","2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017","9 September 2017 through 11 September 2017",,150071,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85068046401
"Devlin J., Chang M.-W., Lee K., Toutanova K.","","BERT: Pre-training of deep bidirectional transformers for language understanding",2018,"CoRR",,,,"","",,660,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85063394559
"Battaglia P.W., Hamrick J.B., Bapst V., Sanchez-Gonzalez A., Zambaldi V., Malinowski M., Tacchetti A., Raposo D., Santoro A., Faulkner R.","","Relational inductive biases, deep learning, and graph networks",2018,"Relational Inductive Biases, Deep Learning, and Graph Networks",,,,"","",,660,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85058240608
"Mani I., Maybury M.T.","",[No title available],1999,"Advances in Automatic Text Summarization",,,,"","",,656,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004212367
"Wang Q., Mao Z., Wang B., Guo L.","57190702381;29467706600;55584805140;56564475400;","Knowledge graph embedding: A survey of approaches and applications",2017,"IEEE Transactions on Knowledge and Data Engineering","29","12","8047276","2724","2743",,649,"10.1109/TKDE.2017.2754499","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030627463&doi=10.1109%2fTKDE.2017.2754499&partnerID=40&md5=a27db194a2cc66996bacec79c674fa33","Knowledge graph (KG) embedding is to embed components of a KG including entities and relations into continuous vector spaces, so as to simplify the manipulation while preserving the inherent structure of the KG. It can benefit a variety of downstream tasks such as KG completion and relation extraction, and hence has quickly gained massive attention. In this article, we provide a systematic review of existing techniques, including not only the state-of-The-Arts but also those with latest trends. Particularly, we make the review based on the type of information used in the embedding task. Techniques that conduct embedding using only facts observed in the KG are first introduced. We describe the overall framework, specific model design, typical training procedures, as well as pros and cons of such techniques. After that, we discuss techniques that further incorporate additional information besides facts. We focus specifically on the use of entity types, relation paths, textual descriptions, and logical rules. Finally, we briefly introduce how KG embedding can be applied to and benefit a wide variety of downstream tasks such as KG completion, relation extraction, question answering, and so forth. © 1989-2012 IEEE.","Knowledge graph embedding; Latent factor models; Statistical relational learning; Tensor/matrix factorization models","Data mining; Extraction; Personnel training; Semantics; Tensile stress; Factorization model; Knowledge graphs; Latent factor models; Market researches; Matrix decomposition; Statistical relational learning; Systematics; Vector spaces",,,,,,Review,"Final","",Scopus,2-s2.0-85030627463
"Ji G., He S., Xu L., Liu K., Zhao J.","56898499600;56021680600;55843856700;55729555700;57190004147;","Knowledge graph embedding via dynamic mapping matrix",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference","1",,,"687","696",,649,"10.3115/v1/p15-1067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943792156&doi=10.3115%2fv1%2fp15-1067&partnerID=40&md5=601f4a29fe876438fd3ada2e11814eac","Knowledge graphs are useful resources for numerous AI applications, but they are far from completeness. Previous work such as TransE, TransH and TransR/CTransR regard a relation as translation from head entity to tail entity and the CTransR achieves state-of-The-Art performance. In this paper, we propose a more fine-grained model named TransD, which is an improvement of TransR/CTransR. In TransD, we use two vectors to represent a named symbol object (entity and relation). The first one represents the meaning of a(n) entity (relation), the other one is used to construct mapping matrix dynamically. Compared with TransR/CTransR, TransD not only considers the diversity of relations, but also entities. TransD has less parameters and has no matrix-vector multiplication operations, which makes it can be applied on large scale graphs. In Experiments, we evaluate our model on two typical tasks including triplets classification and link prediction. Evaluation results show that our approach outperforms stateof-the-Art methods. © 2015 Association for Computationl Linguisticss.",,"Classification (of information); Computational linguistics; Mapping; AI applications; Dynamic mapping; Evaluation results; Knowledge graphs; Link prediction; Matrix-vector multiplication operation; State-of-the-art methods; State-of-the-art performance; Natural language processing systems","Alibaba Group;Baidu;CreditEase;et al.;Samsung;Tencent","53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015","26 July 2015 through 31 July 2015",,114195,Conference Paper,"Final","",Scopus,2-s2.0-84943792156
"Elman J.L.","57225419564;","Distributed Representations, Simple Recurrent Networks, And Grammatical Structure",1991,"Machine Learning","7","2",,"195","225",,646,"10.1023/A:1022699029236","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001419757&doi=10.1023%2fA%3a1022699029236&partnerID=40&md5=fd7a7e7b32123359e34005122328c852","In this paper three problems for a connectionist account of language are considered: 1. What is the nature of linguistic representations? 2. How can complex structural relationships such as constituent structure be represented? 3. How can the apparently open-ended nature of language be accommodated by a fixed-resource system? Using a prediction task, a simple recurrent network (SRN) is trained on multiclausal sentences which contain multiply-embedded relative clauses. Principal component analysis of the hidden unit activation patterns reveals that the network solves the task by developing complex distributed representations which encode the relevant grammatical relations and hierarchical constituent structure. Differences between the SRN state representations and the more traditional pushdown store are discussed in the final section. © 1991, Kluwer Academic Publishers. All rights reserved.","Distributed representations; grammatical structure; simple recurrent networks",,,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-0001419757
"Schlichtkrull M., Kipf T.N., Bloem P., van den Berg R., Titov I., Welling M.","57204220447;57202470192;56414744500;57206172503;14039687400;55907170700;","Modeling Relational Data with Graph Convolutional Networks",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10843 LNCS",,,"593","607",,642,"10.1007/978-3-319-93417-4_38","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048485418&doi=10.1007%2f978-3-319-93417-4_38&partnerID=40&md5=ee4cdbdbd1539c2f24888dc14a7cc339","Knowledge graphs enable a wide variety of applications, including question answering and information retrieval. Despite the great effort invested in their creation and maintenance, even the largest (e.g., Yago, DBPedia or Wikidata) remain incomplete. We introduce Relational Graph Convolutional Networks (R-GCNs) and apply them to two standard knowledge base completion tasks: Link prediction (recovery of missing facts, i.e. subject-predicate-object triples) and entity classification (recovery of missing entity attributes). R-GCNs are related to a recent class of neural networks operating on graphs, and are developed specifically to handle the highly multi-relational data characteristic of realistic knowledge bases. We demonstrate the effectiveness of R-GCNs as a stand-alone model for entity classification. We further show that factorization models for link prediction such as DistMult can be significantly improved through the use of an R-GCN encoder model to accumulate evidence over multiple inference steps in the graph, demonstrating a large improvement of 29.8% on FB15k-237 over a decoder-only baseline. © 2018, Springer International Publishing AG, part of Springer Nature.",,"Computer system recovery; Convolution; Knowledge based systems; Convolutional networks; Factorization model; Knowledge basis; Knowledge graphs; Link prediction; Question Answering; Relational data; Relational graph; Semantic Web","","15th International Conference on Extended Semantic Web Conference, ESWC 2018","3 June 2018 through 7 June 2018",,214029,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85048485418
"Tannen D.","6603344278;","Talking voices: Repetition, dialogue, and imagery in conversational discourse",2007,"Talking Voices: Repetition, Dialogue, and Imagery in Conversational Discourse",,,,"1","233",,637,"10.1017/CBO9780511618987","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929849009&doi=10.1017%2fCBO9780511618987&partnerID=40&md5=7346dc00d05e1c34846e592e0f3a9eaa","Written in readable, vivid, non-technical prose, this book, first published in 2007, presents the highly respected scholarly research that forms the foundation for Deborah Tannen's best-selling books about the role of language in human relationships. It provides a clear framework for understanding how ordinary conversation works to create meaning and establish relationships. A significant theoretical and methodological contribution to both linguistic and literary analysis, it uses transcripts of tape-recorded conversation to demonstrate that everyday conversation is made of features that are associated with literary discourse: repetition, dialogue, and details that create imagery. This second edition features a new introduction in which the author shows the relationship between this groundbreaking work and the research that has appeared since its original publication in 1989. In particular, she shows its relevance to the contemporary topic 'intertextuality', and provides a useful summary of research on that topic. © Deborah Tannen 2007 and Cambridge University Press, 2010.",,,,,,,,Book,"Final","",Scopus,2-s2.0-84929849009
"Graves A., Wayne G., Reynolds M., Harley T., Danihelka I., Grabska-Barwińska A., Colmenarejo S.G., Grefenstette E., Ramalho T., Agapiou J., Badia A.P., Hermann K.M., Zwols Y., Ostrovski G., Cain A., King H., Summerfield C., Blunsom P., Kavukcuoglu K., Hassabis D.","56273511600;56080177300;57191031685;57191827750;56461353100;23396862600;57191829577;51664755300;57191834000;25824549100;57191830433;55667900000;34869018300;36996117900;57191822431;56535990600;6602640766;15043692000;25646533000;15839738700;","Hybrid computing using a neural network with dynamic external memory",2016,"Nature","538","7626",,"471","476",,635,"10.1038/nature20101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993949467&doi=10.1038%2fnature20101&partnerID=40&md5=948613dfbb4b65f1cbb288f957c3c6dd","Artificial neural networks are remarkably adept at sensory processing, sequence learning and reinforcement learning, but are limited in their ability to represent variables and data structures and to store data over long timescales, owing to the lack of an external memory. Here we introduce a machine learning model called a differentiable neural computer (DNC), which consists of a neural network that can read from and write to an external memory matrix, analogous to the random-access memory in a conventional computer. Like a conventional computer, it can use its memory to represent and manipulate complex data structures, but, like a neural network, it can learn to do so from data. When trained with supervised learning, we demonstrate that a DNC can successfully answer synthetic questions designed to emulate reasoning and inference problems in natural language. We show that it can learn tasks such as finding the shortest path between specified points and inferring the missing links in randomly generated graphs, and then generalize these tasks to specific graphs such as transport networks and family trees. When trained with reinforcement learning, a DNC can complete a moving blocks puzzle in which changing goals are specified by sequences of symbols. Taken together, our results demonstrate that DNCs have the capacity to solve complex, structured tasks that are inaccessible to neural networks without external read-write memory. © 2016 Macmillan Publishers Limited, part of Springer Nature. All rights reserved.",,"artificial neural network; computer; computer simulation; data acquisition; machine learning; timescale; Article; artificial neural network; computer memory; data processing; experiment; family; hybrid computer; machine learning; model; priority journal",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84993949467
"Jansen B.H., Rit V.G.","7202560605;6504692217;","Electroencephalogram and visual evoked potential generation in a mathematical model of coupled cortical columns",1995,"Biological Cybernetics","73","4",,"357","366",,634,"10.1007/BF00199471","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029374946&doi=10.1007%2fBF00199471&partnerID=40&md5=871f1f4c11fef3610b6684a44bfef1dc","This study deals with neurophysiologically based models simulating electrical brain activity (i.e., the electroencephalogram or EEG, and evoked potentials or EPs). A previously developed lumped-parameter model of a single cortical column was implemented using a more accurate computational procedure. Anatomically acceptable values for the various model parameters were determined, and a multi-dimensional exploration of the model parameter-space was conducted. It was found that the model could produce a large variety of EEG-like waveforms and rhythms. Coupling two models, with delays in the interconnections to simulate the synaptic connections within and between cortical areas, made it possible to replicate the spatial distribution of alpha and beta activity. EPs were simulated by presenting pulses to the input of the coupled models. In general, the responses were more realistic than those produced using a single model. Our simulations also suggest that the scalp-recorded EP is at least partially due to a phase reordering of the ongoing activity. © 1995 Springer-Verlag.",,"animal; article; biological model; brain cortex; cat; electroencephalography; evoked visual response; human; mathematics; nerve cell; physiology; scalp; Animal; Cats; Cerebral Cortex; Electroencephalography; Evoked Potentials, Visual; Human; Mathematics; Models, Neurological; Neurons; Scalp",,,,,,Article,"Final","",Scopus,2-s2.0-0029374946
"Thornton C., Hutter F., Hoos H.H., Leyton-Brown K.","56848796900;55931808800;6701843335;6602662604;","Auto-WEKA: Combined selection and hyperparameter optimization of classification algorithms",2013,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","Part F128815",,"2487629","847","855",,630,"10.1145/2487575.2487629","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018371540&doi=10.1145%2f2487575.2487629&partnerID=40&md5=d49eb38a8848ae4595d0b2edb12fd8d1","Many different machine learning algorithms exist; taking into account each algorithm's hyperparameters, there is a staggeringly large number of possible alternatives overall. We consider the problem of simultaneously selecting a learning algorithm and setting its hyperparameters, going beyond previous work that attacks these issues separately. We show that this problem can be addressed by a fully automated approach, leveraging recent innovations in Bayesian optimization. Specifically, we consider a wide range of feature selection techniques (combining 3 search and 8 evaluator methods) and all classification approaches implemented in WEKA's standard distribution, spanning 2 ensemble methods, 10 meta-methods, 27 base classifiers, and hyperparameter settings for each classifier. On each of 21 popular datasets from the UCI repository, the KDD Cup 09, variants of the MNIST dataset and CIFAR-10, we show classification performance often much better than using standard selection and hyperparameter optimization methods. We hope that our approach will help non-expert users to more effectively identify machine learning algorithms and hyperparameter settings appropriate to their applications, and hence to achieve improved performance. Copyright © 2013 ACM.","Hyperparameter optimization; Model selection; Weka","Artificial intelligence; Classification (of information); Data mining; Education; Learning systems; Optimization; Bayesian optimization; Classification algorithm; Classification approach; Classification performance; Hyper-parameter optimizations; Model Selection; Standard distributions; Weka; Learning algorithms","ACM KDD.org;ACM SIGMOD","19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2013","11 August 2013 through 14 August 2013",,128815,Conference Paper,"Final","",Scopus,2-s2.0-85018371540
"Anderson A.H., Bader M., Bard E.G., Boyle E., Doherty G., Garrod S., Isard S., Kowtko J., McAllister J., Miller J., Sotillo C., Thompson H.S., Weinert R.","7403369142;57189400019;57203628526;7103365240;7005557936;7004121632;7004158764;6507056159;24348575900;57206765710;6602265308;11044162100;6701589174;","The Hcrc Map Task Corpus",1991,"Language and Speech","34","4",,"351","366",,616,"10.1177/002383099103400404","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970235567&doi=10.1177%2f002383099103400404&partnerID=40&md5=362218e2361766a5206527be4f29b6a6","This paper describes a corpus of unscripted, task-oriented dialogues which has been designed, digitally recorded, and transcribed to support the study of spontaneous speech on many levels. The corpus uses the Map Task (Brown, Anderson, Yule, and Shillcock, 1983) in which speakers must collaborate verbally to reproduce on one participant's map a route printed on the other's. In all, the corpus includes four conversations from each of 64 young adults and manipulates the following variables: familiarity of speakers, eye contact between speakers, matching between landmarks on the participants’ maps, opportunities for contrastive stress, and phonological characteristics of landmark names. The motivations for the design are set out and basic corpus statistics are presented. © 1991, SAGE Publications. All rights reserved.","map task corpus; spontaneous speech",,,,,,,Article,"Final","",Scopus,2-s2.0-84970235567
"Chambers L.","",[No title available],1995,"Practical Handbook of Genetic Algorithms",,,,"","",,603,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003955441
"Donahue J., Hendricks L.A., Rohrbach M., Venugopalan S., Guadarrama S., Saenko K., Darrell T.","54956247200;57142063300;35108115100;56435854800;6506759870;8251043300;7003377605;","Long-Term Recurrent Convolutional Networks for Visual Recognition and Description",2017,"IEEE Transactions on Pattern Analysis and Machine Intelligence","39","4",,"677","691",,592,"10.1109/TPAMI.2016.2599174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020685307&doi=10.1109%2fTPAMI.2016.2599174&partnerID=40&md5=dd518198f347b7ada7a48378f46684bb","Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent are effective for tasks involving sequences, visual and otherwise. We describe a class of recurrent convolutional architectures which is end-to-end trainable and suitable for large-scale visual understanding tasks, and demonstrate the value of these models for activity recognition, image captioning, and video description. In contrast to previous models which assume a fixed visual representation or perform simple temporal averaging for sequential processing, recurrent convolutional models are 'doubly deep' in that they learn compositional representations in space and time. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Differentiable recurrent models are appealing in that they can directly map variable-length inputs (e.g., videos) to variable-length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent sequence models are directly connected to modern visual convolutional network models and can be jointly trained to learn temporal dynamics and convolutional perceptual representations. Our results show that such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined or optimized. © 1979-2012 IEEE.","Computer vision; convolutional nets; deep learning; transfer learning","Artificial intelligence; Computer vision; Deep learning; Compositional representation; convolutional nets; Convolutional networks; Long-term dependencies; Natural language text; Perceptual representations; Transfer learning; Visual representations; Convolution",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85020685307
"Fukushima K.","56199742700;","Neocognitron: A hierarchical neural network capable of visual pattern recognition",1988,"Neural Networks","1","2",,"119","130",,564,"10.1016/0893-6080(88)90014-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023846591&doi=10.1016%2f0893-6080%2888%2990014-7&partnerID=40&md5=a7cb3da7ecaa4e3a5b9126634c3d0552","A neural network model for visual pattern recognition, called the ""neocognitron,"" was previously proposed by the author. In this paper, we discuss the mechanism of the model in detail. In order to demonstrate the ability of the neocognitron, we also discuss a pattern-recognition system which works with the mechanism of the neocognitron. The system has been implemented on a minicomputer and has been trained to recognize handwritten numerals. The neocognitron is a hierarchical network consisting of many layers of cells, and has variable connections between the cells in adjoining layers. It can acquire the ability to recognize patterns by learning, and can be trained to recognize any set of patterns. After finishing the process of learning, pattern recognition is performed on the basis of similarity in shape between patterns, and is not affected by deformation, nor by changes in size, nor by shifts in the position of the input patterns. In the hierarchical network of the neocognitron, local features of the input pattern are extracted by the cells of a lower stage, and they are gradually integrated into more global features. Finally, each cell of the highest stage integrates all the information of the input pattern, and responds only to one specific pattern. Thus, the response of the cells of the highest stage shows the final result of the pattern-recognition of the network. During this process of extracting and integrating features, errors in the relative position of local features are gradually tolerated. The operation of tolerating positional error a little at a time at each stage, rather than all in one step, plays an important role in endowing the network with an ability to recognize even distorted patterns. © 1988.",,"PATTERN RECOGNITION; HIERARCHICAL NEURAL NETWORK; IMAGE PATTERN RECOGNITION; NEOCOGNITRON; VISUAL PATTERN RECOGNITION; SYSTEMS SCIENCE AND CYBERNETICS",,,,,,Article,"Final","",Scopus,2-s2.0-0023846591
"Suchanek F.M., Kasneci G., Weikum G.","14831857200;22834642800;56270327600;","YAGO: A Large Ontology from Wikipedia and WordNet",2008,"Web Semantics","6","3",,"203","217",,563,"10.1016/j.websem.2008.06.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-51449096194&doi=10.1016%2fj.websem.2008.06.001&partnerID=40&md5=d8f6da47555682cd3b4c184576b00999","This article presents YAGO, a large ontology with high coverage and precision. YAGO has been automatically derived from Wikipedia and WordNet. It comprises entities and relations, and currently contains more than 1.7 million entities and 15 million facts. These include the taxonomic Is-A hierarchy as well as semantic relations between entities. The facts for YAGO have been extracted from the category system and the infoboxes of Wikipedia and have been combined with taxonomic relations from WordNet. Type checking techniques help us keep YAGO's precision at 95%-as proven by an extensive evaluation study. YAGO is based on a clean logical model with a decidable consistency. Furthermore, it allows representing n-ary relations in a natural way while maintaining compatibility with RDFS. A powerful query model facilitates access to YAGO's data. © 2008.","Information extraction; Knowledge representation; Ontologies","Information extraction; Knowledge representation; Ontologies; Wikipedia; Word net; Ontology",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-51449096194
"Zeng D., Liu K., Chen Y., Zhao J.","55953844000;55729555700;55954392900;57190004147;","Distant supervision for relation extraction via Piecewise Convolutional Neural Networks",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",,,,"1753","1762",,558,"10.18653/v1/d15-1203","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959934618&doi=10.18653%2fv1%2fd15-1203&partnerID=40&md5=9217c1f66c77a1a443f102f42f02bef7","Two problems arise when using distant supervision for relation extraction. First, in this method, an already existing knowledge base is heuristically aligned to texts, and the alignment results are treated as labeled data. However, the heuristic alignment can fail, resulting in wrong label problem. In addition, in previous approaches, statistical models have typically been applied to ad hoc features. The noise that originates from the feature extraction process can cause poor performance. In this paper, we propose a novel model dubbed the Piecewise Convolutional Neural Networks (PCNNs) with multi-instance learning to address these two problems. To solve the first problem, distant supervised relation extraction is treated as a multi-instance problem in which the uncertainty of instance labels is taken into account. To address the latter problem, we avoid feature engineering and instead adopt convolutional architecture with piecewise max pooling to automatically learn relevant features. Experiments show that our method is effective and outperforms several competitive baseline methods. © 2015 Association for Computational Linguistics.",,"Convolution; Extraction; Knowledge based systems; Neural networks; Baseline methods; Convolutional neural network; Feature engineerings; Multi-instance learning; Multi-instance problems; Poor performance; Relation extraction; Relevant features; Natural language processing systems","Baidu;Bloomberg;et al.;facebook;Google;Linkedin","Conference on Empirical Methods in Natural Language Processing, EMNLP 2015","17 September 2015 through 21 September 2015",,116677,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84959934618
"Parikh A.P., Täckström O., Das D., Uszkoreit J.","36470807700;36028800000;16238076400;51666085000;","A decomposable attention model for natural language inference",2016,"EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"2249","2255",,552,"10.18653/v1/d16-1244","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072820995&doi=10.18653%2fv1%2fd16-1244&partnerID=40&md5=947656c50168e082af2110e48f97fd71","We propose a simple neural architecture for natural language inference. Our approach uses attention to decompose the problem into subproblems that can be solved separately, thus making it trivially parallelizable. On the Stanford Natural Language Inference (SNLI) dataset, we obtain state-of-the-art results with almost an order of magnitude fewer parameters than previous work and without relying on any word-order information. Adding intra-sentence attention that takes a minimum amount of order into account yields further improvements. © 2016 Association for Computational Linguistics",,"Attention model; Natural languages; Neural architectures; Stanford; State of the art; Sub-problems; Word orders; Natural language processing systems","Amazon.com;Baidu;et al.;Google;Grammarly;Microsoft","2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016","1 November 2016 through 5 November 2016",,150070,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85072820995
"Lin Y., Shen S., Liu Z., Luan H., Sun M.","57155321900;57155609300;55714725800;23035489400;7403180987;","Neural relation extraction with selective attention over instances",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers","4",,,"2124","2133",,552,"10.18653/v1/p16-1200","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011945255&doi=10.18653%2fv1%2fp16-1200&partnerID=40&md5=d3cdcfafef5e730d2ac361c0a39b7ab6","Distant supervised relation extraction has been widely used to find novel relational facts from text. However, distant supervision inevitably accompanies with the wrong labelling problem, and these noisy data will substantially hurt the performance of relation extraction. To alleviate this issue, we propose a sentence-level attention-based model for relation extraction. In this model, we employ convolutional neural networks to embed the semantics of sentences. Afterwards, we build sentence-level attention over multiple instances, which is expected to dynamically reduce the weights of those noisy instances. Experimental results on real-world datasets show that, our model can make full use of all informative sentences and effectively reduce the influence of wrong labelled instances. Our model achieves significant and consistent improvements on relation extraction as compared with baselines. The source code of this paper can be obtained from https://github.com/thunlp/NRE. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Neural networks; Semantics; Convolutional neural network; Multiple instances; Noisy data; Real-world datasets; Relation extraction; Selective attention; Sentence level; Source codes; Extraction","Amazon;Baidu;Bloomberg;et al.;Facebook;Google","54th Annual Meeting of the Association for Computational Linguistics, ACL 2016","7 August 2016 through 12 August 2016",,125782,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85011945255
"Goodfellow I.J., Warde-Farley D., Mirza M., Courville A., Bengio Y.","35956088800;24472176000;55431127900;6507291186;7003958245;","Maxout networks",2013,"30th International Conference on Machine Learning, ICML 2013",,"PART 3",,"2356","2364",,548,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897543523&partnerID=40&md5=a15ef95549ed9d8b6368de14644d34da","We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN. Copyright 2013 by the author(s).",,"Benchmarking; Classification (of information); Approximate model; Benchmark datasets; Classification performance; State of the art; Learning systems","","30th International Conference on Machine Learning, ICML 2013","16 June 2013 through 21 June 2013","Atlanta, GA",103423,Conference Paper,"Final","",Scopus,2-s2.0-84897543523
"Riedel S., Yao L., McCallum A.","36562438800;24475694900;7003773569;","Modeling relations and their mentions without labeled text",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6323 LNAI","PART 3",,"148","163",,544,"10.1007/978-3-642-15939-8_10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958036662&doi=10.1007%2f978-3-642-15939-8_10&partnerID=40&md5=075f9523fc462e46b6241e9394eaeff3","Several recent works on relation extraction have been applying the distant supervision paradigm: instead of relying on annotated text to learn how to predict relations, they employ existing knowledge bases (KBs) as source of supervision. Crucially, these approaches are trained based on the assumption that each sentence which mentions the two related entities is an expression of the given relation. Here we argue that this leads to noisy patterns that hurt precision, in particular if the knowledge base is not directly related to the text we are working with. We present a novel approach to distant supervision that can alleviate this problem based on the following two ideas: First, we use a factor graph to explicitly model the decision whether two entities are related, and the decision whether this relation is mentioned in a given sentence; second, we apply constraint-driven semi-supervision to train this model without any knowledge about which sentences express the relations in our training KB. We apply our approach to extract relations from the New York Times corpus and use Freebase as knowledge base. When compared to a state-of-the-art approach for relation extraction under distant supervision, we achieve 31% error reduction. © 2010 Springer-Verlag Berlin Heidelberg.",,"Error reduction; Factor graphs; Knowledge base; Knowledge basis; New york time; Noisy patterns; Problem-based; Relation extraction; State-of-the-art approach; Learning systems; Knowledge based systems","Fr. Natl. Inst. Res. Comput. Sci. Control (INRIA);Pascal2 European Network of Excellence;Nokia;Yahoo Labs;Google","European Conference on Machine Learning and Principles and Practice of Knowledge Discovery in Databases, ECML PKDD 2010","20 September 2010 through 24 September 2010","Barcelona",81973,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-77958036662
"Morin F., Bengio Y.","57197335675;7003958245;","Hierarchical probabilistic neural network language model",2005,"AISTATS 2005 - Proceedings of the 10th International Workshop on Artificial Intelligence and Statistics",,,,"246","252",,544,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547997987&partnerID=40&md5=2d1142b4b100a815ad5e3441e7072ea7","In recent years, variants of a neural network architecture for statistical language modeling have been proposed and successfully applied, e.g. in the language modeling component of speech recognizers. The main advantage of these architectures is that they learn an embedding for words (or other symbols) in a continuous space that helps to smooth the language model and provide good generalization even when the number of training examples is insufficient. However, these models are extremely slow in comparison to the more commonly used n-gram models, both for training and recognition. As an alternative to an importance sampling method proposed to speed-up training, we introduce a hierarchical decomposition of the conditional probabilities that yields a speed-up of about 200 both during training and recognition. The hierarchical decomposition is a binary hierarchical clustering constrained by the prior knowledge extracted from the WordNet semantic hierarchy.",,"Conditional probabilities; Continuous spaces; Hier-archical clustering; Hierarchical decompositions; Importance sampling method; Language model; Language modeling; N-gram models; Prior knowledge; Probabilistic neural networks; Speech recognizer; Statistical language modeling; Training example; WordNet semantics; Network architecture; Neural networks; Semantics; Computational linguistics","National ICT Australia;Microsoft;Pattern Anal., Stat. Model. Comput. Learn. (PASCAL)","10th International Workshop on Artificial Intelligence and Statistics, AISTATS 2005","6 January 2005 through 8 January 2005","Hastings, Christ Church",90668,Conference Paper,"Final","",Scopus,2-s2.0-34547997987
"Lopes da Silva F.H., Hoeks A., Smits H., Zetterberg L.H.","7102396397;7004437055;57198197742;7003326542;","Model of brain rhythmic activity - The alpha-rhythm of the thalamus",1974,"Kybernetik","15","1",,"27","37",,543,"10.1007/BF00270757","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0016138266&doi=10.1007%2fBF00270757&partnerID=40&md5=5dbf3f687787e2c512b3d90615b8b924","1. A model of a neuronal network has been set up in a digital computer based on histological and biophysical data experimentally obtained from the thalamus; the model includes two populations of neurons interconnected by means of negative feedback; in the model allowance is also made for other sort of interactions. 2. To test the hypothesis that the alpha-rhythm (8-13 Hz rhythmic activity characteristic of the EEG) is a filtered noise signal the simulated neuronal network was stimulated by random trains of pulses with a Poisson distribution. The density of pulses fired by the simulated neurons was computed as well as the oscillations of the mean membrane potential of the population of simulated neurons. The latter was found to be equivalent to the experimentally obtained alpha rhythms. 3. In order to test the hypothesis that several noise sources are responsible for thalamo-cortical coherences three simulated neuronal networks were coupled together using several noise sources as secondary inputs. It was shown that although all the networks produced simulated alpha signals with identical spectra they could have significantly different values of coherence depending on the relation between correlated and uncorrelated input signals. 4. The model was analysed by means of linear systems analysis after introducing the necessary simplifications and approximations. In this way it was possible to evaluate the influence of different physiological or histological parameters upon the statistical properties of the resulting rhythmic activity in an analytical form. 5. By changing the model parameters it was shown that a family of spectral curves could be obtained which simulated the development of the EEG as function of age from a predominantly low frequency to a clearly rhythmic type of signal. This was shown to depend mainly on the feedback coupling parameters. © 1974 Springer-Verlag.",,"alpha rhythm; animal; article; biological model; computer; cybernetics; dog; nerve cell network; physiology; thalamus; brain; brain depth recording; electroencephalography; mathematical model; model; nerve cell; nerve cell membrane potential; thalamus; theoretical study; data analysis; thalamocortical tract; Alpha Rhythm; Animal; Computers; Cybernetics; Dogs; Models, Neurological; Nerve Net; Thalamus",,,,,,Article,"Final","",Scopus,2-s2.0-0016138266
"Aizawa A.","6701312731;","An information-theoretic perspective of tf-idf measures",2003,"Information Processing and Management","39","1",,"45","65",,533,"10.1016/S0306-4573(02)00021-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037213089&doi=10.1016%2fS0306-4573%2802%2900021-3&partnerID=40&md5=4ba29085dec7d1bcf2a0060eb15351a4","This paper presents a mathematical definition of the ""probability-weighted amount of information"" (PWI), a measure of specificity of terms in documents that is based on an information-theoretic view of retrieval events. The proposed PWI is expressed as a product of the occurrence probabilities of terms and their amounts of information, and corresponds well with the conventional term frequency-inverse document frequency measures that are commonly used in today's information retrieval systems. The mathematical definition of the PWI is shown, together with some illustrative examples of the calculation. © 2002 Elsevier Science Ltd. All rights reserved.","Information theory; Term weighting theories; Text categorization; tf-idf","Computational methods; Information theory; Probability; Text processing; Term weighting theories; Information retrieval",,,,,,Article,"Final","",Scopus,2-s2.0-0037213089
"Prinz J.J.","","Furnishing the mind: Concepts and their perceptual basis",2002,"Furnishing the Mind: Concepts and Their Perceptual Basis",,,,"","",,527,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0242483374
"Hutter F., Kotthoff L., Vanschoren J.","","Automatic machine learning: Methods, systems, challenges",2019,"Challenges in Machine Learning",,,,"","",,508,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85065146640
"Wu H.C., Luk R.W.P., Wong K.F., Kwok K.L.","24448828300;6701545378;7404759311;56213741900;","Interpreting TF-IDF term weights as making relevance decisions",2008,"ACM Transactions on Information Systems","26","3","13","","",,507,"10.1145/1361684.1361686","https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249110298&doi=10.1145%2f1361684.1361686&partnerID=40&md5=4788f8cdc40fdd7b45ce9391279db0d5","A novel probabilistic retrieval model is presented. It forms a basis to interpret the TF-IDF term weights as making relevance decisions. It simulates the local relevance decision-making for every location of a document, and combines all of these ""local"" relevance decisions as the ""document-wide"" relevance decision for the document. The significance of interpreting TF-IDF in this way is the potential to: (1) establish a unifying perspective about information retrieval as relevance decision-making; and (2) develop advanced TF-IDF-related term weights for future elaborate retrieval models. Our novel retrieval model is simplified to a basic ranking formula that directly corresponds to the TF-IDF term weights. In general, we show that the term-frequency factor of the ranking formula can be rendered into different term-frequency factors of existing retrieval systems. In the basic ranking formula, the remaining quantity - log p(r̄t ∈ d) is interpreted as the probability of randomly picking a nonrelevant usage (denoted by ) of term t. Mathematically, we show that this quantity can be approximated by the inverse document-frequency (IDF). Empirically, we show that this quantity is related to IDF, using four reference TREC ad hoc retrieval data collections. © 2008 ACM.","Information retrieval; Relevance decision; Term weight","Ad Hoc retrieval; data collections; Frequency factors; General (CO); Probabilistic retrieval; Retrieval (MIR); retrieval models; retrieval systems; Boolean functions; Data structures; Image retrieval; Information analysis; Information retrieval; Information science; Information services; Natural language processing systems; Probability; Problem solving; Search engines; Decision making",,,,,,Article,"Final","",Scopus,2-s2.0-46249110298
"Dagan I., Glickman O., Magnini B.","6602573382;10241032500;22433254600;","The PASCAL Recognising Textual Entailment Challenge",2006,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","3944 LNAI",,,"177","190",,505,"10.1007/11736790_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33745838229&doi=10.1007%2f11736790_9&partnerID=40&md5=3f0b42cc0e07b6699f60e1a7a323cc9d","This paper describes the PASCAL Network of Excellence first Recognising Textual Entailment (RTE-1) Challenge benchmark1. The RTE task is defined as recognizing, given two text fragments, whether the meaning of one text can be inferred (entailed) from the other. This application-independent task is suggested as capturing major inferences about the variability of semantic expression which are commonly needed across multiple applications. The Challenge has raised noticeable attention in the research community, attracting 17 submissions from diverse groups, suggesting the generic relevance of the task. © Springer-Verlag Berlin Heidelberg 2006.",,"Artificial intelligence; Benchmarking; Data acquisition; Learning systems; Predictive control systems; Semantics; Generic relevance; Multiple applications; Recognising Textual Entailment; Semantic expression; Information retrieval","PASCAL","1st PASCAL Machine Learning Challenges Workshop, MLCW 2005","11 April 2005 through 13 April 2005","Southampton",67738,Conference Paper,"Final","",Scopus,2-s2.0-33745838229
"Santoro A., Raposo D., Barrett D.G.T., Malinowski M., Pascanu R., Battaglia P., Lillicrap T.","55793515200;57210602536;55699123900;56735775000;36464144900;7003855044;6503977179;","A simple neural network module for relational reasoning",2017,"Advances in Neural Information Processing Systems","2017-December",,,"4968","4977",,503,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047012589&partnerID=40&md5=95b32b8e25ed6a574ea7e03f14972d97","Relational reasoning is a central component of generally intelligent behavior, but has proven difficult for neural networks to learn. In this paper we describe how to use Relation Networks (RNs) as a simple plug-and-play module to solve problems that fundamentally hinge on relational reasoning. We tested RN-augmented networks on three tasks: visual question answering using a challenging dataset called CLEVR, on which we achieve state-of-the-art, super-human performance; text-based question answering using the bAbI suite of tasks; and complex reasoning about dynamic physical systems. Then, using a curated dataset called Sort-of-CLEVR we show that powerful convolutional networks do not have a general capacity to solve relational questions, but can gain this capacity when augmented with RNs. Thus, by simply augmenting convolutions, LSTMs, and MLPs with RNs, we can remove computational burden from network components that are not well-suited to handle relational reasoning, reduce overall network complexity, and gain a general ability to reason about the relations between entities and their properties. © 2017 Neural information processing systems foundation. All rights reserved.",,"Convolution; Central component; Computational burden; Convolutional networks; Human performance; Intelligent behavior; Physical systems; Question Answering; Relational reasoning; Complex networks","","31st Annual Conference on Neural Information Processing Systems, NIPS 2017","4 December 2017 through 9 December 2017",,136033,Conference Paper,"Final","",Scopus,2-s2.0-85047012589
"McCallum A.K., Nigam K., Rennie J., Seymore K.","7003773569;7006615971;7102446366;6506783443;","Automating the construction of internet portals with machine learning",2000,"Information Retrieval","3","2",,"127","163",,496,"10.1023/A:1009953814988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000806922&doi=10.1023%2fA%3a1009953814988&partnerID=40&md5=23f6882935fe8e0f6f4a2b21c40688f0","Domain-specific internet portals are growing in popularity because they gather content from the Web and organize it for easy access, retrieval and search. For example, www.campsearch.com allows complex queries by age, location, cost and specialty over summer camps. This functionality is not possible with general, Web-wide search engines. Unfortunately these portals are difficult and time-consuming to maintain. This paper advocates the use of machine learning techniques to greatly automate the creation and maintenance of domain-specific Internet portals. We describe new research in reinforcement learning, information extraction and text classification that enables efficient spidering, the identification of informative text segments, and the population of topic hierarchies. Using these techniques, we have built a demonstration system: a portal for computer science research papers. It already contains over 50,000 papers and is publicly available at www.cora.justresearch.com. These techniques are widely applicable to portal creation in other domains. © 2000 Kluwer Academic Publishers.","Crawling; Expectation-maximization; Hidden markov models; Information extraction; Naive bayes; Reinforcement learning; Spidering; Text classification; Unlabeled data",,,,,,,Article,"Final","",Scopus,2-s2.0-0000806922
"Speer R., Chin J., Havasi C.","","Conceptnet 5.5: An open multilingual graph of general knowledge",2017,"AAAI",,,,"4444","4451",,493,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85028996745
"Ravichandran D., Hovy E.","","Learning surface text patterns for a question answering system",2002,"Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",,,,"41","47",,491,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-1642397455
"Eliasmith C., Stewart T.C., Choo X., Bekolay T., DeWolf T., Tang C., Rasmussen D.","6603720957;15835430800;55505169400;38860956400;16303525900;55504996400;46161499000;","A large-scale model of the functioning brain",2012,"Science","338","6111",,"1202","1205",,490,"10.1126/science.1225266","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870209909&doi=10.1126%2fscience.1225266&partnerID=40&md5=ea566084703de11f9e48291507770d64","A central challenge for cognitive and systems neuroscience is to relate the incredibly complex behavior of animals to the equally complex activity of their brains. Recently described, large-scale neural models have not bridged this gap between neural activity and biological function. In this work, we present a 2.5-million-neuron model of the brain (called ""Spaun"") that bridges this gap by exhibiting many different behaviors. The model is presented only with visual image sequences, and it draws all of its responses with a physically modeled arm. Although simplified, the model captures many aspects of neuroanatomy, neurophysiology, and psychological behavior, which we demonstrate via eight diverse tasks.",,"neurotransmitter; anatomy; brain; cognition; neurology; numerical model; psychology; article; biological model; brain function; brain nerve cell; learning; memory; nerve cell network; neuroanatomy; neurophysiology; neuropsychology; neuroscience; priority journal; simulation; task performance",,,,,,Article,"Final","",Scopus,2-s2.0-84870209909
"Floridi L.","6603039594;","The Philosophy of Information",2011,"The Philosophy of Information",,,,"1","432",,485,"10.1093/acprof:oso/9780199232383.001.0001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84921941744&doi=10.1093%2facprof%3aoso%2f9780199232383.001.0001&partnerID=40&md5=b00c3d2a9c636a5f20fafb8cb1d02e2f","This book brings together the outcome of ten years of research. It is based on a simple project, which was begun towards the end of the 1990s: information is a crucial concept, which deserves a thorough philosophical investigation. So the book lays down the conceptual foundations of a new area of research: the philosophy of information. It does so systematically, by pursuing three goals. The first is metatheoretical. The book describes what the philosophy of information is, its problems, and its method of levels of abstraction. These are the topics of the first part, which comprises chapters one, two and three. The second goal is introductory. In chapters four and five, the book explores the complex and diverse nature of several informational concepts and phenomena. The third goal is constructive. In the remaining ten chapters, the book answers some classic philosophical questions in information-theoretical terms. As a result, the book provides the first, unified and coherent research programme for the philosophy of information, understood as a new, independent area of research, concerned with (1) the critical investigation of the conceptual nature and basic principles of information, including its dynamics, utilization, and sciences; and (2) the elaboration and application of information-theoretic and computational methodologies to philosophical problems. © Luciano Floridi 2011. All rights reserved.","Artificial agents; Computing; Information; Information technology; Information theory; Levels of abstraction; Modal logic; Philosophy of information; Semantic information",,,,,,,Book,"Final","",Scopus,2-s2.0-84921941744
"Friedman N., Getoor L., Koller D., Pfeffer A.","57203075240;6603498218;57203666468;7006987448;","Learning probabilistic relational models",1999,"IJCAI International Joint Conference on Artificial Intelligence","2",,,"1300","1307",,485,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880688943&partnerID=40&md5=7cde9fc6ce168f6de4328e638e34591d","A large portion of real-world data is stored in commercial relational database systems. In contrast, most statistical learning methods work only with ""flat"" data representations. Thus, to apply these methods, we are forced to convert our data into a flat form, thereby losing much of the relational structure present in our database. This paper builds on the recent work on probabilistic relational models (PRMs), and describes how to learn them from databases. PRMs allow the properties of an object to depend probabilistically both on other properties of that object and on properties of related objects. Although PRMs are significantly more expressive than standard models, such as Bayesian networks, we show how to extend well-known statistical methods for learning Bayesian networks to learn these models. We describe both parameter estimation and structure learning - the automatic induction of the dependency structure in a model. Moreover, we show how the learning procedure can exploit standard database retrieval techniques for efficient learning from large datasets. We present experimental results on both real and synthetic relational databases.",,"Data representations; Dependency structures; Learning Bayesian networks; Learning procedures; Probabilistic relational models; Relational Database; Relational structures; Statistical learning methods; Artificial intelligence; Data handling; Relational database systems; Bayesian networks","International Joint Conferences on Artificial;Intelligence, Inc. (IJCAII);Scandinavian AI Societies;Ericsson;Microsoft","16th International Joint Conference on Artificial Intelligence, IJCAI 1999","31 July 1999 through 6 August 1999","Stockholm",97869,Conference Paper,"Final","",Scopus,2-s2.0-84880688943
"Rose S., Engel D., Cramer N., Cowley W.","15751893000;55336548100;22333484900;14035186400;","Automatic Keyword Extraction from Individual Documents",2010,"Text Mining: Applications and Theory",,,,"1","20",,481,"10.1002/9780470689646.ch1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868695730&doi=10.1002%2f9780470689646.ch1&partnerID=40&md5=3ceb47c2aed742c66fffecadef499de7",[No abstract available],"Automatic information retrieval; Content analysis; Document analysis; Feature extraction; Index vocabulary; Information extraction; Keyword extraction methods; Stoplist generation; Text analysis; Text mining",,,,,,,Book Chapter,"Final","All Open Access, Green",Scopus,2-s2.0-84868695730
"De Marneffe M.-C., Manning C.D.","","Stanford typed dependencies manual",2008,"Stanford Typed Dependencies Manual",,,,"","",,480,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-77954188732
"Branigan H.P., Pickering M.J., Cleland A.A.","6603228182;7006461382;7006469212;","Syntactic co-ordination in dialogue",2000,"Cognition","75","2",,"B13","B25",,479,"10.1016/S0010-0277(99)00081-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034657936&doi=10.1016%2fS0010-0277%2899%2900081-5&partnerID=40&md5=73d106110808a23ec2526efec2ca4834","There is substantial evidence that speakers co-ordinate their contributions in dialogue. Until now, experimental studies of co-ordination have concentrated on the development of shared strategies for reference. We present an experiment that employed a novel confederate-scripting technique to investigate whether speakers also co-ordinate syntactic structure in dialogue. Pairs of speakers took it in turns to describe pictures to each other. One speaker was a confederate of the experimenter and produced scripted descriptions that systematically varied in syntactic structure. The syntactic structure of the confederate's description affected the syntactic structure of the other speaker's subsequent description. We suggest that these effects are instances of syntactic priming (Bock, 1986), and provide evidence for a shared level of representation in comprehension and production. We describe how these effects might be realized in a processing model of language production, and relate them to previous findings of linguistic co-ordination in dialogue. © 2000 Elsevier Science B.V.","Co-ordination; Dialogue; Language production; Syntactic priming; Syntax","article; automation; cognition; comprehension; coordination; human; human experiment; language; linguistics; normal human; priority journal; speech articulation; verbal communication; Humans; Language; Random Allocation; Speech; Verbal Behavior",,,,,,Article,"Final","",Scopus,2-s2.0-0034657936
"Kakas A.C., Kowalski R.A., Toni F.","6701880142;55218198200;6603756423;","Abductive logic programming",1992,"Journal of Logic and Computation","2","6",,"719","770",,473,"10.1093/logcom/2.6.719","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77957187070&doi=10.1093%2flogcom%2f2.6.719&partnerID=40&md5=7789cea618e0d60fd51d2dff978e174a","This paper is a survey and critical overview of recent work on the extension of logic programming to perform abductive reasoning (abductive logic programming). We outline the general framework of abduction and its applications to knowledge assimilation and default reasoning; and we introduce an argumentation-theoretic approach to the use of abduction as an interpretation for negation as failure. We also analyse the links between abduction and the extension of logic programming obtained by adding a form of explicit negation. Finally we discuss the relation between abduction and truth maintenance. © 1993 Oxford University Press.",,,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-77957187070
"Poria S., Cambria E., Bajpai R., Hussain A.","55316592700;56140547500;56875259000;19734290900;","A review of affective computing: From unimodal analysis to multimodal fusion",2017,"Information Fusion","37",,,"98","125",,468,"10.1016/j.inffus.2017.02.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011844403&doi=10.1016%2fj.inffus.2017.02.003&partnerID=40&md5=0b9d518723f057d6d0431c5d7b02cdd9","Affective computing is an emerging interdisciplinary research field bringing together researchers and practitioners from various fields, ranging from artificial intelligence, natural language processing, to cognitive and social sciences. With the proliferation of videos posted online (e.g., on YouTube, Facebook, Twitter) for product reviews, movie reviews, political views, and more, affective computing research has increasingly evolved from conventional unimodal analysis to more complex forms of multimodal analysis. This is the primary motivation behind our first of its kind, comprehensive literature review of the diverse field of affective computing. Furthermore, existing literature surveys lack a detailed discussion of state of the art in multimodal affect analysis frameworks, which this review aims to address. Multimodality is defined by the presence of more than one modality or channel, e.g., visual, audio, text, gestures, and eye gage. In this paper, we focus mainly on the use of audio, visual and text information for multimodal affect analysis, since around 90% of the relevant literature appears to cover these three modalities. Following an overview of different techniques for unimodal affect analysis, we outline existing methods for fusing information from different modalities. As part of this review, we carry out an extensive study of different categories of state-of-the-art fusion techniques, followed by a critical analysis of potential performance improvements with multimodal analysis compared to unimodal analysis. A comprehensive overview of these two complementary fields aims to form the building blocks for readers, to better understand this challenging and exciting research field. © 2017 Elsevier B.V.","Affective computing; Audio, visual and text information fusion; Multimodal affect analysis; Multimodal fusion; Sentiment analysis","Artificial intelligence; Sentiment analysis; Social networking (online); Social sciences computing; Affect analysis; Affective Computing; Fusion techniques; Interdisciplinary research; Literature reviews; Multi-modal fusion; Multimodal analysis; NAtural language processing; Modal analysis",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85011844403
"Garrod S., Anderson A.","7004121632;7403369817;","Saying what you mean in dialogue: A study in conceptual and semantic co-ordination",1987,"Cognition","27","2",,"181","218",,465,"10.1016/0010-0277(87)90018-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0023446165&doi=10.1016%2f0010-0277%2887%2990018-7&partnerID=40&md5=8b7b567a68145f4bd1d2de7dbe7924ae","This paper explores how conversants co-ordinate their use and interpretation of language in a restricted context. It revolves around the analysis of the spatial descriptions which emerge during the course of 56 dialogues, elicited in the laboratory using a specially designed computer maze game. Two types of analysis are reported. The first is a semantic analysis of the various types of description, which indicates how pairs of speakers develop different language schemes associated with different mental models of the maze configuration. The second analysis concerns how the communicants co-ordinate in developing their description schemes. The results from this study would suggest that language processing in dialogue may be governed by local principles of interaction which have received little attention in the psychological and linguistic literature to date. © 1987.",,"article; cooperation; depth perception; female; human; interpersonal communication; language; male; recreation; semantics; Communication; Cooperative Behavior; Female; Human; Language; Male; Play and Playthings; Semantics; Space Perception; Support, Non-U.S. Gov't",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0023446165
"Muggleton S.","7003491952;","Inductive logic programming",1991,"New Generation Computing","8","4",,"295","318",,464,"10.1007/BF03037089","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000640432&doi=10.1007%2fBF03037089&partnerID=40&md5=eca5117a0efba780a7b1b156fdc19036","A new research area, Inductive Logic Programming, is presently emerging. While inheriting various positive characteristics of the parent subjects of Logic Programming and Machine Learning, it is hoped that the new area will overcome many of the limitations of its forebears. The background to present developments within this area is discussed and various goals and aspirations for the increasing body of researchers are identified. Inductive Logic Programming needs to be based on sound principles from both Logic and Statistics. On the side of statistical justification of hypotheses we discuss the possible relationship between Algorithmic Complexity theory and Probably-Approximately-Correct (PAC) Learning. In terms of logic we provide a unifying framework for Muggleton and Buntine's Inverse Resolution (IR) and Plotkin's Relative Least General Generalisation (RLGG) by rederiving RLGG in terms of IR. This leads to a discussion of the feasibility of extending the RLGG framework to allow for the invention of new predicates, previously discussed only within the context of IR. © 1991 Ohmsha, Ltd. and Springer.","induction; information compression; inverse resolution; Learning; logic programming; predicate invention",,,,,,,Article,"Final","",Scopus,2-s2.0-0000640432
"Doddington G., Mitchell A., Przybocki M., Ramshaw L., Strassel S., Weischedel R.","6603911487;57198797776;7801553629;6602957870;6505697042;6603005682;","The automatic content extraction (ACE) program tasks, data, and evaluation",2004,"Proceedings of the 4th International Conference on Language Resources and Evaluation, LREC 2004",,,,"837","840",,461,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026954488&partnerID=40&md5=c525775d687e5d04a057cef50afc5823","The objective of the ACE program is to develop technology to automatically infer from human language data the entities being mentioned, the relations among these entities that are directly expressed, and the events in which these entities participate. Data sources include audio and image data in addition to pure text, and Arabic and Chinese in addition to English. The effort involves defining the research tasks in detail, collecting and annotating data needed for training, development, and evaluation, and supporting the research with evaluation tools and research workshops. This program began with a pilot study in 1999. The next evaluation is scheduled for September 2004.",,"Automatic content; Data-sources; Evaluation tool; Human language; Image data; Pilot studies","Fundacao Calouste Gulbenkian;Fundacao para a Ciencia e a Tecnologia (FCT);IBM;Microsoft;Porto Editora - Dictionaries Experties;Priberam Informatica","4th International Conference on Language Resources and Evaluation, LREC 2004","26 May 2004 through 28 May 2004",,131721,Conference Paper,"Final","",Scopus,2-s2.0-85026954488
"Dettmers T., Minervini P., Stenetorp P., Riedel S.","57205548652;36617521100;36663192600;36562438800;","Convolutional 2D knowledge graph embeddings",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"1811","1818",,459,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060490899&partnerID=40&md5=8cd9a0f3999efb9871d2248ef5fa5c07","Link prediction for knowledge graphs is the task of predicting missing relationships between entities. Previous work on link prediction has focused on shallow, fast models which can scale to large knowledge graphs. However, these models learn less expressive features than deep, multi-layer models - which potentially limits performance. In this work we introduce ConvE, a multi-layer convolutional network model for link prediction, and report state-of-the-art results for several established datasets. We also show that the model is highly parameter efficient, yielding the same performance as DistMult and R-GCN with 8x and 17x fewer parameters. Analysis of our model suggests that it is particularly effective at modelling nodes with high indegree - which are common in highly-connected, complex knowledge graphs such as Freebase and YAGO3. In addition, it has been noted that the WN18 and FB15k datasets suffer from test set leakage, due to inverse relations from the training set being present in the test set - however, the extent of this issue has so far not been quantified. We find this problem to be severe: a simple rule-based model can achieve state-of-the-art results on both WN18 and FB15k. To ensure that models are evaluated on datasets where simply exploiting inverse relations cannot yield competitive results, we investigate and validate several commonly used datasets - deriving robust variants where necessary. We then perform experiments on these robust datasets for our own and several previously proposed models, and find that ConvE achieves state-of-the-art Mean Reciprocal Rank across all datasets. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Convolution; Inverse problems; Network layers; Convolutional networks; Inverse relations; Knowledge graphs; Mean reciprocal ranks; Multilayer models; Relationships between entities; Rule-based models; State of the art; Forecasting","Association for the Advancement of Artificial Intelligence","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","2 February 2018 through 7 February 2018",,143510,Conference Paper,"Final","",Scopus,2-s2.0-85060490899
"Towell G.G., Shavlik J.W.","6506442915;7004146387;","Knowledge-based artificial neural networks",1994,"Artificial Intelligence","70","1-2",,"119","165",,459,"10.1016/0004-3702(94)90105-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028529307&doi=10.1016%2f0004-3702%2894%2990105-8&partnerID=40&md5=6d34230b2962e65559855c8da9f9becc","Hybrid learning methods use theoretical knowledge of a domain and a set of classified examples to develop a method for accurately classifying examples not seen during training. The challenge of hybrid learning systems is to use the information provided by one source of information to offset information missing from the other source. By so doing, a hybrid learning system should learn more effectively than systems that use only one of the information sources. KBANN (Knowledge-Based Artificial Neural Networks) is a hybrid learning system built on top of connectionist learning techniques. It maps problem-specific ""domain theories"", represented in propositional logic, into neural networks and then refines this reformulated knowledge using backpropagation. KBANN is evaluated by extensive empirical tests on two problems from molecular biology. Among other results, these tests show that the networks created by KBANN generalize better than a wide variety of learning systems, as well as several techniques proposed by biologists. © 1994.","Computational biology; Connectionism; Explanation-based learning; Hybrid algorithms; Machine learning; Theory refinement","Algorithms; Biology; Computational methods; Formal logic; Information use; Knowledge based systems; Learning systems; Man machine systems; Computational theory; Connectionism; Explanation based learning; Hybrid algorithms; Knowledge based artificial neural networks; Machine learning; Theory refinement; Neural networks",,,,,,Article,"Final","",Scopus,2-s2.0-0028529307
"Weston J., Chopra S., Bordes A.","",[No title available],2014,"Memory Networks",,,,"","",,447,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84930635225
"Wu Z., Pan S., Chen F., Long G., Zhang C., Yu P.S.","","A comprehensive survey on graph neural networks",2019,"A Comprehensive Survey on Graph Neural Networks",,,,"","",,445,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85066088228
"Gray F.","","Pulse code communications",1953,"Pulse Code Communication",,,,"","",,444,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0004086411
"Trouillon T., Welbl J., Riedel S., Gaussier É., Bouchard G.","","Complex embeddings for simple link prediction",2016,"Complex Embeddings for Simple Link Prediction",,,,"2071","2080",,435,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85019189005
"Zhang Y., Jin R., Zhou Z.-H.","57196199171;26643619900;55365262700;","Understanding bag-of-words model: A statistical framework",2010,"International Journal of Machine Learning and Cybernetics","1","1-4",,"43","52",,435,"10.1007/s13042-010-0001-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952313379&doi=10.1007%2fs13042-010-0001-0&partnerID=40&md5=148168c2bbba4f3ef1c023e33382a7dc","The bag-of-words model is one of the most popular representation methods for object categorization. The key idea is to quantize each extracted key point into one of visual words, and then represent each image by a histogram of the visual words. For this purpose, a clustering algorithm (e.g., K-means), is generally used for generating the visual words. Although a number of studies have shown encouraging results of the bag-of-words representation for object categorization, theoretical studies on properties of the bag-of-words model is almost untouched, possibly due to the difficulty introduced by using a heuristic clustering process. In this paper, we present a statistical framework which generalizes the bag-of-words representation. In this framework, the visual words are generated by a statistical process rather than using a clustering algorithm, while the empirical performance is competitive to clustering-based method. A theoretical analysis based on statistical consistency is presented for the proposed framework. Moreover, based on the framework we developed two algorithms which do not rely on clustering, while achieving competitive performance in object categorization when compared to clustering-based bag-ofwords representations. © Springer-Verlag 2010.","Bag of words model; Object recognition; Rademacher complexity","Bag of words; Bag of words model; Clustering process; Empirical performance; K-means; Keypoints; Object categorization; Rademacher complexity; Representation method; Statistical framework; Statistical process; Theoretical study; Visual word; Object recognition; Statistical methods; Clustering algorithms",,,,,,Article,"Final","",Scopus,2-s2.0-79952313379
"Zhang D., Lee W.S.","7405356099;55663394400;","Question Classification using Support Vector Machines",2003,"SIGIR Forum (ACM Special Interest Group on Information Retrieval)",,"SPEC. ISS.",,"26","32",,429,"10.1145/860435.860443","https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542377488&doi=10.1145%2f860435.860443&partnerID=40&md5=7c3e9491441acdac20668892d2f0697b","Question classification is very important for question answering. This paper presents our research work on automatic question classification through machine learning approaches. We have experimented with five machine learning algorithms: Nearest Neighbors (NN), Naïve Bayes (NB), Decision Tree (DT), Sparse Network of Winnows (SNoW), and Support Vector Machines (SVM) using two kinds of features: bag-of-words and bag-of-ngrams. The experiment results show that with only surface text features the SVM outperforms the other four methods for this task. Further, we propose to use a special kernel function called the tree kernel to enable the SVM to take advantage of the syntactic structures of questions. We describe how the tree kernel can be computed efficiently by dynamic programming. The performance of our approach is promising, when tested on the questions from the TREC QA track.","Kernel method; Machine learning; Question answering; Support vector machine; Text classification","Constraint theory; Data reduction; Dynamic programming; Learning algorithms; Learning systems; Search engines; Semantics; Text processing; Kernel methods; Question answering; Support vector machines; Text classification; Information retrieval","ACM/SIGIR","Proceedings of the Twenty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2003","28 July 2003 through 1 August 2003","Toronto, Ont.",62465,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-1542377488
"Smolensky P.","6602511111;","Tensor product variable binding and the representation of symbolic structures in connectionist systems",1990,"Artificial Intelligence","46","1-2",,"159","216",,427,"10.1016/0004-3702(90)90007-M","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025516779&doi=10.1016%2f0004-3702%2890%2990007-M&partnerID=40&md5=d49126e4189fa6e2b61bd47b97e06ec3","A general method, the tensor product representation, is defined for the connectionist representation of value/variable bindings. The technique is a formalization of the idea that a set of value/variable pairs can be represented by accumulating activity in a collection of units each of which computes the product of a feature of a variable and a feature of its value. The method allows the fully distributed representation of bindings and symbolic structures. Fully and partially localized special cases of the tensor product representation reduce to existing cases of connectionist representations of structured data. The representation rests on a principled analysis of structure; it saturates gracefully as larger structures are represented; it permits recursive construction of complex representations from simpler ones; it respects the independence of the capacities to generate and maintain multiple bindings in parallel; it extends naturally to continuous structures and continuous representational patterns; it permits values to also serve as variables; and it enables analysis of the interference of symbolic structures stored in associative memories. It has also served as the basis for working connectionist models of high-level cognitive tasks. © 1990.",,"Data Processing--Data Structures; Connectionist Networks; Symbolic Structures; Neural Networks",,,,,,Article,"Final","",Scopus,2-s2.0-0025516779
"Jozefowicz R., Vinyals O., Schuster M., Shazeer N., Wu Y.","","Exploring the limits of language modeling",2016,"Exploring the Limits of Language Modeling",,,,"","",,418,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84978840213
"Veličković P., Casanova A., Liò P., Cucurull G., Romero A., Bengio Y.","57190809820;57193434461;7004223170;57193324564;55437167800;7003958245;","Graph attention networks",2018,"6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings",,,,"","",,417,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083953221&partnerID=40&md5=9a7c0e7ee8c527a4a1b86038be0adf0c","We present graph attention networks (GATs), novel neural network architectures that operate on graph-structured data, leveraging masked self-attentional layers to address the shortcomings of prior methods based on graph convolutions or their approximations. By stacking layers in which nodes are able to attend over their neighborhoods’ features, we enable (implicitly) specifying different weights to different nodes in a neighborhood, without requiring any kind of computationally intensive matrix operation (such as inversion) or depending on knowing the graph structure upfront. In this way, we address several key challenges of spectral-based graph neural networks simultaneously, and make our model readily applicable to inductive as well as transductive problems. Our GAT models have achieved or matched state-of-the-art results across four established transductive and inductive graph benchmarks: the Cora, Citeseer and Pubmed citation network datasets, as well as a protein-protein interaction dataset (wherein test graphs remain unseen during training). © Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved.",,"Graphic methods; Network architecture; Proteins; Statistical tests; Citation networks; Graph neural networks; Graph structured data; Graph structures; Matrix operations; Novel neural network; Protein-protein interactions; State of the art; Multilayer neural networks",,"6th International Conference on Learning Representations, ICLR 2018","30 April 2018 through 3 May 2018",,149806,Conference Paper,"Final","",Scopus,2-s2.0-85083953221
"Guarino N., Welty C.","","An overview of OntoClean",2004,"Handbook on Ontologies",,,,"151","172",,415,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-22044458646
"Miwa M., Bansal M.","35208894900;16466939600;","End-to-end relation extraction using LSTMs on sequences and tree structures",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers","2",,,"1105","1116",,412,"10.18653/v1/p16-1105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011883671&doi=10.18653%2fv1%2fp16-1105&partnerID=40&md5=5799423b4bbcaeed9f45eb834acd29ef","We present a novel end-to-end neural model to extract entities and relations between them. Our recurrent neural network based model captures both word sequence and dependency tree substructure information by stacking bidirectional treestructured LSTM-RNNs on bidirectional sequential LSTM-RNNs. This allows our model to jointly represent both entities and relations with shared parameters in a single model. We further encourage detection of entities during training and use of entity information in relation extraction via entity pretraining and scheduled sampling. Our model improves over the stateof-the-art feature-based model on end-toend relation extraction, achieving 12.1% and 5.7% relative error reductions in F1-score on ACE2005 and ACE2004, respectively. We also show that our LSTM-RNN based model compares favorably to the state-of-the-art CNN based model (in F1-score) on nominal relation classification (SemEval-2010 Task 8). Finally, we present an extensive ablation analysis of several model components. © 2016 Association for Computational Linguistics.",,"Classification (of information); Computational linguistics; Extraction; Forestry; Trees (mathematics); Dependency trees; Feature based modeling; Model components; Neural modeling; Relation classifications; Relation extraction; Relative errors; State of the art; Long short-term memory","Amazon;Baidu;Bloomberg;et al.;Facebook;Google","54th Annual Meeting of the Association for Computational Linguistics, ACL 2016","7 August 2016 through 12 August 2016",,125782,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85011883671
"Hochreiter S., Schmidhuber J.","6602873810;7003514621;","LSTM can solve hard long time lag problems",1997,"Advances in Neural Information Processing Systems",,,,"473","479",,397,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000370416&partnerID=40&md5=18485fd9a5b6a5bfdb8ba0c73b8a9111","Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of.",,"Algorithms; Hard problems; Random searches; Random weight; Time lag; Problem solving","","10th Annual Conference on Neural Information Processing Systems, NIPS 1996","2 December 1996 through 5 December 1996","Denver, CO",104681,Conference Paper,"Final","",Scopus,2-s2.0-0000370416
"Garrod S., Pickering M.J.","7004121632;7006461382;","Why is conversation so easy?",2004,"Trends in Cognitive Sciences","8","1",,"8","11",,394,"10.1016/j.tics.2003.10.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0346362269&doi=10.1016%2fj.tics.2003.10.016&partnerID=40&md5=db9497afe72b646a9133ab3bec013fc5","Traditional accounts of language processing suggest that monologue - presenting and listening to speeches - should be more straightforward than dialogue - holding a conversation. This is clearly not the case. We argue that conversation is easy because of an interactive processing mechanism that leads to the alignment of linguistic representations between partners. Interactive alignment occurs via automatic alignment channels that are functionally similar to the automatic links between perception and behaviour (the so-called perception-behaviour expressway) proposed in recent accounts of social interaction. We conclude that humans are 'designed' for dialogue rather than monologue.",,"Social aspects; Speech processing; Speech recognition; Linguistic representations; Cognitive systems; behavior; conversation; human; language; language ability; linguistics; processing; review; social interaction; speech; speech articulation; speech perception",,,,,,Article,"Final","",Scopus,2-s2.0-0346362269
"Dietterich T.G.","","Ensemble methods in machine learning",2000,"First International Workshop on Multiple Classifier Systems",,,,"1","15",,391,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0002900451
"Lindsay R.K., Buchanan B.G., Feigenbaum E.A., Lederberg J.","",[No title available],1980,"Applications of Artificial Intelligence for Organic Chemistry: The DENDRAL Project",,,,"","",,391,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003659236
"Siegelmann H.T., Sontag E.D.","57189345629;7102557112;","On the computational power of neural nets",1995,"Journal of Computer and System Sciences","50","1",,"132","150",,389,"10.1006/jcss.1995.1013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029255891&doi=10.1006%2fjcss.1995.1013&partnerID=40&md5=f3dfa4f75875085c3c0d64addb7d244b","This paper deals with finite size networks which consist of interconnections of synchronously evolving processors. Each processor updates its state by applying a ""sigmoidal"" function to a linear combination of the previous states of all units. We prove that one may simulate all Turing machines by such nets. In particular, one can simulate any multi-stack Turing machine in real time, and there is a net made up of 886 processors which computes a universal partial-recursive function. Products (high order nets) are not required, contrary to what had been stated in the literature. Non-deterministic Turing machines can be simulated by non-deterministic rational nets, also in real time. The simulation result has many consequences regarding the decidability, or more generally the complexity, of questions about recursive nets. © 1995 by Academic Press, Inc.",,"Computability and decidability; Computational complexity; Computer simulation; Finite automata; Graph theory; Interconnection networks; Real time systems; Recursive functions; Synchronization; Turing machines; Non deterministic rational nets; Processor nets; Rational valued coefficient; Saturated linear function; Sigmoidal functions; Neural networks",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-0029255891
"Alom M.Z., Taha T.M., Yakopcic C., Westberg S., Sidike P., Nasrin M.S., Hasan M., Van Essen B.C., Awwal A.A.S., Asari V.K.","35566178100;23013518500;36698572000;57205398380;56585990900;24779959000;57210374202;18038810700;7006641028;6701420692;","A state-of-the-art survey on deep learning theory and architectures",2019,"Electronics (Switzerland)","8","3","292","","",,385,"10.3390/electronics8030292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064323901&doi=10.3390%2felectronics8030292&partnerID=40&md5=86662b134866afe0a888b8ac356b7c8f","In recent years, deep learning has garnered tremendous success in a variety of application domains. This new field of machine learning has been growing rapidly and has been applied to most traditional application domains, as well as some new areas that present more opportunities. Different methods have been proposed based on different categories of learning, including supervised, semi-supervised, and un-supervised learning. Experimental results show state-of-the-art performance using deep learning when compared to traditional machine learning approaches in the fields of image processing, computer vision, speech recognition, machine translation, art, medical imaging, medical information processing, robotics and control, bioinformatics, natural language processing, cybersecurity, and many others. This survey presents a brief survey on the advances that have occurred in the area of Deep Learning (DL), starting with the Deep Neural Network (DNN). The survey goes on to cover Convolutional Neural Network (CNN), Recurrent Neural Network (RNN), including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), Auto-Encoder (AE), Deep Belief Network (DBN), Generative Adversarial Network (GAN), and Deep Reinforcement Learning (DRL). Additionally, we have discussed recent developments, such as advanced variant DL techniques based on these DL approaches. This work considers most of the papers published after 2012 from when the history of deep learning began. Furthermore, DL approaches that have been explored and evaluated in different application domains are also included in this survey. We also included recently developed frameworks, SDKs, and benchmark datasets that are used for implementing and evaluating deep learning approaches. There are some surveys that have been published on DL using neural networks and a survey on Reinforcement Learning (RL). However, those papers have not discussed individual advanced techniques for training large-scale deep learning models and the recently developed method of generative models. © 2019 by the authors. Licensee MDPI, Basel, Switzerland.","Auto-encoder (AE); Convolutional neural network (CNN); Deep belief network (DBN); Deep learning; Deep reinforcement learning (DRL); Generative adversarial network (GAN); Recurrent neural network (RNN); Restricted Boltzmann machine (RBM); Transfer learning",,,,,,,Review,"Final","All Open Access, Gold",Scopus,2-s2.0-85064323901
"Bates A.W.T.","16490306300;","Technology, e-learning and distance education",2005,"Technology, e-learning and Distance Education",,,,"1","246",,383,"10.4324/9780203463772","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091860497&doi=10.4324%2f9780203463772&partnerID=40&md5=267dad77be2b67ecb68ff9423f1b91d1","Award-winning in its first edition, this book is an essential guide to the use of technology in flexible and distance learning, weighing up the pros and cons of different media. Fully updated, this second edition: examines criteria and guidelines for the design and delivery of effective teaching, using modern learning technologies focuses on the use of the Internet for distance and flexible education considers the design and use of emerging technologies such as web-based video-conferencing and speech recognition places emphasis on organisational and management issues and how these influence the effective use of technology gives attention to the integration of online teaching with campus-based face-to-face teaching. With a focus on basic principles and general guidelines this guidance applies to existing and emerging technologies. It is essential reading not just for those specialising in flexible and distance learning and distance education, but anyone concerned with the integration of technology with teaching. © 1995, 2005 A. W. (Tony) Bates.",,,,,,,,Book,"Final","",Scopus,2-s2.0-85091860497
"Kumar A., Irsoy O., Su J., Bradbury J., English R., Pierce B., Ondruska P., Gulrajani I., Socher R.","","Ask me anything: Dynamic memory networks for natural language processing",2015,"Ask Me Anything: Dynamic Memory Networks for Natural Language Processing",,,,"1378","1387",,382,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84978835300
"Piaget J.","",[No title available],1947,"La Psychologie de l'Intelligence",,,,"","",,382,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0003464618
"Severyn A., Moschitti A.","36562482900;6507876429;","Twitter Sentiment Analysis with deep convolutional neural networks",2015,"SIGIR 2015 - Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval",,,,"959","962",,381,"10.1145/2766462.2767830","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953807567&doi=10.1145%2f2766462.2767830&partnerID=40&md5=becb657d91c9741e0532f0f8a4cb3ecb","This paper describes our deep learning system for sentiment analysis of tweets. The main contribution of this work is a new model for initializing the parameter weights of the convolutional neural network, which is crucial to train an accurate model while avoiding the need to inject any additional features. Briefly, we use an unsupervised neural language model to train initial word embeddings that are further tuned by our deep learning model on a distant supervised corpus. At a final stage, the pre-trained parameters of the network are used to initialize the model. We train the latter on the supervised training data recently made available by the official system evaluation campaign on Twitter Sentiment Analysis organized by Semeval-2015. A comparison between the results of our approach and the systems participating in the challenge on the official test sets, suggests that our model could be ranked in the first two positions in both the phrase-level subtask A (among 11 teams) and on the message-level subtask B (among 40 teams). This is an important evidence on the practical value of our solution. © 2015 ACM.","Convolutional neural networks; Twitter Sentiment Analysis","Convolution; Information retrieval; Neural networks; Social networking (online); Accurate modeling; Convolutional neural network; Deep learning; Embeddings; Language model; Sentiment analysis; Supervised trainings; System evaluation; Data mining","ACM Special Interest Group on Information Retrieval (SIGIR)","38th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2015","9 August 2015 through 13 August 2015",,117001,Conference Paper,"Final","",Scopus,2-s2.0-84953807567
"Carreras X., Márquez L.","18933684800;7006837086;","Introduction to the CoNLL-2005 shared task: Semantic role labeling",2005,"CoNLL 2005 - Proceedings of the Ninth Conference on Computational Natural Language Learning",,,,"152","164",,379,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862288194&partnerID=40&md5=b7cf68acc542ef488bbd2889f9a3590f","In this paper we describe the CoNLL- 2005 shared task on Semantic Role Labeling. We introduce the specification and goals of the task, describe the data sets and evaluation methods, and present a general overview of the 19 systems that have contributed to the task, providing a comparative description and results. © 2005 Association for Computational Linguistics.",,"Data sets; Evaluation Method; Semantic role labeling; Semantics",,"9th Conference on Computational Natural Language Learning, CoNLL 2005","29 June 2005 through 30 June 2005","Ann Arbor, MI",89983,Conference Paper,"Final","",Scopus,2-s2.0-84862288194
"Osterhout L., Holcomb P.J., Swinney D.A.","6603802666;7003842688;7004552791;","Brain Potentials Elicited by Garden-Path Sentences: Evidence of the Application of Verb Information During Parsing",1994,"Journal of Experimental Psychology: Learning, Memory, and Cognition","20","4",,"786","803",,376,"10.1037/0278-7393.20.4.786","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028471458&doi=10.1037%2f0278-7393.20.4.786&partnerID=40&md5=5fe9b587320ecb5d3499608d8eba4695","Event-related potentials were recorded from 13 scalp locations while participants read sentences containing a syntactic ambiguity. In Experiment 1, syntactically disambiguating words that were inconsistent with the ""favored"" syntactic analysis elicited a positive-going brain potential (P600). Experiment 2 examined whether syntactic ambiguities are resolved by application of a phrase-structure-based minimal attachment principle or by word-specific subcategorization information. P600 amplitude was a function of subcategorization biases rather than syntactic complexity. These findings indicate that such biases exist and can influence the parser under certain conditions and that P600 amplitude is a function of the perceived syntactic well-formedness of the sentence.",,"adolescent; adult; article; brain; comparative study; electroencephalography; evoked response; female; human; language test; male; physiology; semantics; verbal behavior; Adolescent; Adult; Brain; Comparative Study; Electroencephalography; Evoked Potentials; Female; Human; Language Tests; Male; Semantics; Support, U.S. Gov't, P.H.S.; Verbal Behavior",,,,,,Article,"Final","",Scopus,2-s2.0-0028471458
"Gardner M., Grus J., Neumann M., Tafjord O., Dasigi P., Liu N.F., Peters M., Schmitz M., Zettlemoyer L.S.","","AllenNLP: A deep semantic natural language processing platform",2017,"Allennlp: A Deep Semantic Natural Language Processing Platform",,,,"1","6",,371,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85060462232
"Petrov S., Klein D.","51665451200;23009040500;","Improved inference for unlexicalized parsing",2007,"NAACL HLT 2007 - Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference",,,,"404","411",,370,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858380058&partnerID=40&md5=5ad2282c01fac799d05b3b07a1658f8a","We present several improvements to unlexicalized parsing with hierarchically state-split PCFGs. First, we present a novel coarse-to-fine method in which a grammar's own hierarchical projections are used for incremental pruning, including a method for efficiently computing projections of a grammar without a treebank. In our experiments, hierarchical pruning greatly accelerates parsing with no loss in empirical accuracy. Second, we compare various inference procedures for state-split PCFGs from the standpoint of risk minimization, paying particular attention to their practical tradeoffs. Finally, we present multilingual experiments which show that parsing with hierarchical state-splitting is fast and accurate in multiple languages and domains, even without any language-specific tuning. © 2007 Association for Computational Linguistics.",,"Coarse-to-fine; Multiple languages; Risk minimization; Treebanks; Computational linguistics; Experiments","Eastman Kodak Company;Microsoft Research;Powerset - Natural Language Search;Thomson;Association For Machine Translation in the Americas (AMTA)","Human Language Technologies 2007: The Conference of the North American Chapter of the Association for Computational Linguistics, NAACL HLT 2007","22 April 2007 through 27 April 2007","Rochester, NY",88927,Conference Paper,"Final","",Scopus,2-s2.0-84858380058
"Shen W., Wang J., Han J.","55574196474;57208873256;24325399900;","Entity linking with a knowledge base: Issues, techniques, and solutions",2015,"IEEE Transactions on Knowledge and Data Engineering","27","2","6823700","443","460",,368,"10.1109/TKDE.2014.2327028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84920115585&doi=10.1109%2fTKDE.2014.2327028&partnerID=40&md5=23de600770eeee4170406ebdd3a6d299","The large number of potential applications from bridging web data with knowledge bases have led to an increase in the entity linking research. Entity linking is the task to link entity mentions in text with their corresponding entities in a knowledge base. Potential applications include information extraction, information retrieval, and knowledge base population. However, this task is challenging due to name variations and entity ambiguity. In this survey, we present a thorough overview and analysis of the main approaches to entity linking, and discuss various applications, the evaluation of entity linking systems, and future directions. © 2014 IEEE.","entity disambiguation; Entity linking; knowledge base","Knowledge based systems; Entity disambiguation; Entity linking; Knowledge base; Knowledge basis; Web data; Data mining",,,,,,Article,"Final","",Scopus,2-s2.0-84920115585
"Goller Christoph, Kuechler Andreas","6602845062;6701866843;","Learning task-dependent distributed representations by backpropagation through structure",1996,"IEEE International Conference on Neural Networks - Conference Proceedings","1",,,"347","352",,358,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029727454&partnerID=40&md5=321bc61f27fb0f3563651dce0eff5edd","While neural networks are very successfully applied to the processing of fixed-length vectors and variable-length sequences, the current state of the art does not allow the efficient processing of structured objects of arbitrary shape (like logical terms, trees or graphs). We present a connectionist architecture together with a novel supervised learning scheme which is capable of solving inductive inference tasks on complex symbolic structures of arbitrary size. The most general structures that can be handled are labeled directed acyclic graphs. The major difference of our approach compared to others is that the structure-representations are exclusively tuned for the intended inference task. Our method is applied to tasks consisting in the classification of logical terms. These range from the detection of a certain subterm to the satisfaction of a specific unification pattern. Compared to previously known approaches we got superior results on that domain.",,"Associative storage; Backpropagation; Data structures; Distributed computer systems; Learning algorithms; Pattern recognition; Recursive functions; Complex symbolic structures; Labeled directed acyclic graphs; Recursive autoassociative memory; Supervised learning; Neural networks","IEEE","Proceedings of the 1996 IEEE International Conference on Neural Networks, ICNN. Part 1 (of 4)","3 June 1996 through 6 June 1996","Washington, DC, USA",45420,Conference Paper,"Final","",Scopus,2-s2.0-0029727454
"Sarlós T.","22942213400;","Improved approximation algorithms for large matrices via random projections",2006,"Proceedings - Annual IEEE Symposium on Foundations of Computer Science, FOCS",,,"4031351","143","152",,356,"10.1109/FOCS.2006.37","https://www.scopus.com/inward/record.uri?eid=2-s2.0-35348901208&doi=10.1109%2fFOCS.2006.37&partnerID=40&md5=963f7ab5da67e6a5e99421d9de73f960","Recently several results appeared that show significant reduction in time for matrix multiplication, singular value decomposition as well as linear (ℓ2) regression, all based on data dependent random sampling. Our key idea is that low dimensional embeddings can be used to eliminate data dependence and provide more versatile, linear time pass efficient matrix computation. Our main contribution is summarized as follows. Independent of the recent results of Har-Peled and of Deshpande and Vempala, one of the first-and to the best of our knowledge the most efficient - relative error (1 + ε) ∥ A - A k∥F approximation algorithms for the singular value decomposition of an m × n matrix A with M non-zero entries that requires 2 passes over the data and runs in time O ((M k/∈ + k log k) + (n + m) (k/∈ + k log k)2) log i/δ). The first o(nd2) time (1 + ∈) relative error approximation algorithm for n × d linear (ℓ2) regression. A matrix multiplication and norm approximation algorithm that easily applies to implicitly given matrices and can be used as a black box probability boosting tool. © 2006 IEEE.",,"Data reduction; Random processes; Regression analysis; Singular value decomposition; Low dimensional embeddings; Matrix multiplication; Non-zero entries; Approximation algorithms",,"47th Annual IEEE Symposium on Foundations of Computer Science, FOCS 2006","21 October 2006 through 24 October 2006","Berkeley, CA",71261,Conference Paper,"Final","",Scopus,2-s2.0-35348901208
"Andreas J., Rohrbach M., Darrell T., Klein D.","55667847200;35108115100;7003377605;23009040500;","Neural module networks",2016,"Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition","2016-December",,"7780381","39","48",,355,"10.1109/CVPR.2016.12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986272553&doi=10.1109%2fCVPR.2016.12&partnerID=40&md5=0ff1cd20b6203b026a60c0a7839d861e","Visual question answering is fundamentally compositional in nature - a question like where is the dog? shares substructure with questions like what color is the dog? and where is the cat? This paper seeks to simultaneously exploit the representational capacity of deep networks and the compositional linguistic structure of questions. We describe a procedure for constructing and learning neural module networks, which compose collections of jointly-trained neural 'modules' into deep networks for question answering. Our approach decomposes questions into their linguistic substructures, and uses these structures to dynamically instantiate modular networks (with reusable components for recognizing dogs, classifying colors, etc.). The resulting compound networks are jointly trained. We evaluate our approach on two challenging datasets for visual question answering, achieving state-of-the-art results on both the VQA natural image dataset and a new dataset of complex questions about abstract shapes. © 2016 IEEE.",,"Computer vision; Linguistics; Natural language processing systems; Pattern recognition; Complex questions; Linguistic structure; Modular network; Module networks; Natural images; Question Answering; Reusable components; State of the art; Complex networks","","29th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2016","26 June 2016 through 1 July 2016",,125363,Conference Paper,"Final","",Scopus,2-s2.0-84986272553
"Reimers N., Gurevych I.","57028066100;24474583400;","Sentence-BERT: Sentence embeddings using siamese BERT-networks",2020,"EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference",,,,"3982","3992",,352,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084192855&partnerID=40&md5=b39fed2a6b4235a7d5956be31e03083b","BERT (Devlin et al., 2018) and RoBERTa (Liu et al., 2019) has set a new state-of-the-art performance on sentence-pair regression tasks like semantic textual similarity (STS). However, it requires that both sentences are fed into the network, which causes a massive computational overhead: Finding the most similar pair in a collection of 10,000 sentences requires about 50 million inference computations (~65 hours) with BERT. The construction of BERT makes it unsuitable for semantic similarity search as well as for unsupervised tasks like clustering. In this publication, we present Sentence-BERT (SBERT), a modification of the pretrained BERT network that use siamese and triplet network structures to derive semantically meaningful sentence embeddings that can be compared using cosine-similarity. This reduces the effort for finding the most similar pair from 65 hours with BERT / RoBERTa to about 5 seconds with SBERT, while maintaining the accuracy from BERT. We evaluate SBERT and SRoBERTa on common STS tasks and transfer learning tasks, where it outperforms other state-of-the-art sentence embeddings methods. © 2019 Association for Computational Linguistics",,"Embeddings; Semantics; Transfer learning; Computational overheads; Cosine similarity; Network structures; Pair regression; Semantic similarity; State of the art; State-of-the-art performance; Textual similarities; Natural language processing systems","Apple;ASAPP;et al.;Facebook;Google;salesforce","2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019","3 November 2019 through 7 November 2019",,159367,Conference Paper,"Final","",Scopus,2-s2.0-85084192855
"Deng L.","36071490500;","A tutorial survey of architectures, algorithms, and applications for deep learning",2014,"APSIPA Transactions on Signal and Information Processing","3",,"e2","","",,349,"10.1017/ATSIP.2013.99","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906883759&doi=10.1017%2fATSIP.2013.99&partnerID=40&md5=2000d9b3dc71a2c6667c201f94830fcf","In this invited paper, my overview material on the same topic as presented in the plenary overview session of APSIPA-2011 and the tutorial material presented in the same conference [1] are expanded and updated to include more recent developments in deep learning. The previous and the updatedmaterials cover both theory and applications, and analyze its future directions. The goal of this tutorial survey is to introduce the emerging area of deep learning or hierarchical learning to the APSIPA community. Deep learning refers to a class of machine learning techniques, developed largely since 2006, where many stages of non-linear information processing in hierarchical architectures are exploited for pattern classification and for feature learning. In the more recent literature, it is also connected to representation learning, which involves a hierarchy of features or concepts where higherlevel concepts are defined from lower-level ones and where the same lower-level concepts help to define higher-level ones. In this tutorial survey, a brief history of deep learning research is discussed first. Then, a classificatory scheme is developed to analyze and summarize major work reported in the recent deep learning literature. Using this scheme, I provide a taxonomy-oriented survey on the existing deep architectures and algorithms in the literature, and categorize them into three classes: generative, discriminative, and hybrid. Three representative deep architectures - deep autoencoders, deep stacking networks with their generalization to the temporal domain (recurrent networks), and deep neural networks (pretrained with deep belief networks) - one in each of the three classes, are presented in more detail. Next, selected applications of deep learning are reviewed in broad areas of signal and information processing including audio/speech, image/vision, multimodality, language modeling, natural language processing, and information retrieval. Finally, future directions of deep learning are discussed and analyzed. © The Authors, 2014.","Algorithms; Deep learning; Information processing","Algorithms; Audio signal processing; Classification (of information); Data processing; Deep neural networks; Learning algorithms; Learning systems; Modeling languages; Natural language processing systems; Network architecture; Recurrent neural networks; Surveys; Deep architectures; Deep belief networks; Feature learning; Hierarchical architectures; Hierarchical learning; Machine learning techniques; Recurrent networks; Signal and information processing; Deep learning",,,,,,Review,"Final","",Scopus,2-s2.0-84906883759
"Shik M.L., Severin F.V., Orlovskii G.N.","7004314873;7004087690;7003793580;","Control of walking and running by means of electrical stimulation of the mid-brain",1966,"Biophysics","11","4",,"756","765",,348,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001761874&partnerID=40&md5=0a974fb4347337de8f405ce9b2cb761a","1. (1) On electrical stimulation of a definite region of the mid-brain in the mesencephalic cat locomotor activity appears. 2. (2) By regulating the strength of stimulation it is possible to change the speed of running and the gait. 3. (3) The basic characteristics of walking and running of the mesencephalic animal are similar to the corresponding characteristics in the intact animal. 4. (4) It is assumed that the control of walking and running in the intact animal is brought about through change in the excitability of a definite region of the mid-brain. © 1967.",,,,,,,,Article,"Final","",Scopus,2-s2.0-0001761874
"Tang D., Qin B., Liu T.","57204328071;57204316195;57199476645;","Aspect level sentiment classification with deep memory network",2016,"EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"214","224",,347,"10.18653/v1/d16-1021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072835413&doi=10.18653%2fv1%2fd16-1021&partnerID=40&md5=a8cea20f5671f5d348bee6568f09f839","We introduce a deep memory network for aspect level sentiment classification. Unlike feature-based SVM and sequential neural models such as LSTM, this approach explicitly captures the importance of each context word when inferring the sentiment polarity of an aspect. Such importance degree and text representation are calculated with multiple computational layers, each of which is a neural attention model over an external memory. Experiments on laptop and restaurant datasets demonstrate that our approach performs comparable to state-of-art feature based SVM system, and substantially better than LSTM and attention-based LSTM architectures. On both datasets we show that multiple computational layers could improve the performance. Moreover, our approach is also fast. The deep memory network with 9 layers is 15 times faster than LSTM with a CPU implementation. © 2016 Association for Computational Linguistics",,"Arts computing; Natural language processing systems; Network layers; Support vector machines; Attention model; Context-word; External memory; Feature-based; Importance degrees; Neural models; Sentiment classification; Text representation; Long short-term memory","Amazon.com;Baidu;et al.;Google;Grammarly;Microsoft","2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016","1 November 2016 through 5 November 2016",,150070,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85072835413
"Tsuda I.","7005519176;","Toward an interpretation of dynamic neural activity in terms of chaotic dynamical systems",2001,"Behavioral and Brain Sciences","24","5",,"793","810",,347,"10.1017/s0140525x01000097","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035495017&doi=10.1017%2fs0140525x01000097&partnerID=40&md5=324608e024c3222ee4d91c00994a8474","Using the concepts of chaotic dynamical systems, we present an interpretation of dynamic neural activity found in cortical and subcortical areas. The discovery of chaotic itinerancy in high-dimensional dynamical systems with and without a noise term has motivated a new interpretation of this dynamic neural activity, cast in terms of the high-dimensional transitory dynamics among ""exotic"" attractors. This interpretation is quite different from the conventional one, cast in terms of simple behavior on low-dimensional attractors. Skarda and Freeman (1987) presented evidence in support of the conclusion that animals cannot memorize odor without chaotic activity of neuron populations. Following their work, we study the role of chaotic dynamics in biological information processing, perception, and memory. We propose a new coding scheme of information in chaos-driven contracting systems we refer to as Cantor coding. Since these systems are found in the hippocampal formation and also in the olfactory system, the proposed coding scheme should be of biological significance. Based on these intensive studies, a hypothesis regarding the formation of episodic memory is given.","Cantor coding; Chaotic itinerancy; Dynamic aspects of the brain; Dynamic associative memory; Episodic memory; High-dimensional dynamical systems; SCND attractors","article; basal ganglion; brain cortex; chaotic dynamics; coding; controlled study; hippocampus; information processing; memory; nerve cell; nervous system; odor; olfactory system; perception",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0035495017
"Xu Y., Mou L., Li G., Chen Y., Peng H., Jin Z.","57155376600;55614839500;55901136600;56153036200;57217268613;8961795500;","Classifying relations via long short term memory networks along shortest dependency paths",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",,,,"1785","1794",,343,"10.18653/v1/d15-1206","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959865227&doi=10.18653%2fv1%2fd15-1206&partnerID=40&md5=43792c025c87aed0b8d370ad65718565","Relation classification is an important research arena in the field of natural language processing (NLP). In this paper, we present SDP-LSTM, a novel neural network to classify the relation of two entities in a sentence. Our neural architecture leverages the shortest dependency path (SDP) between two entities; multichannel recurrent neural networks, with long short term memory (LSTM) units, pick up heterogeneous information along the SDP. Our proposed model has several distinct features: (1) The shortest dependency paths retain most relevant information (to relation classification), while eliminating irrelevant words in the sentence. (2) The multichannel LSTM networks allow effective information integration from heterogeneous sources over the dependency paths. (3) A customized dropout strategy regularizes the neural network to alleviate overfitting. We test our model on the SemEval 2010 relation classification task, and achieve an F1-score of 83.7%, higher than competing methods in the literature. © 2015 Association for Computational Linguistics.",,"Brain; Classification (of information); Natural language processing systems; Heterogeneous information; Heterogeneous sources; Information integration; NAtural language processing; Neural architectures; Novel neural network; Relation classifications; Short term memory; Long short-term memory","Baidu;Bloomberg;et al.;facebook;Google;Linkedin","Conference on Empirical Methods in Natural Language Processing, EMNLP 2015","17 September 2015 through 21 September 2015",,116677,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84959865227
"Marcus G.","","Deep learning: A critical appraisal",2018,"Deep Learning: A Critical Appraisal",,,,"","",,341,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85048034820
"Zou W.Y., Socher R., Cer D., Manning C.D.","57219819448;24766896100;14017606400;35280197500;","Bilingual word embeddings for phrase-based machine translation",2013,"EMNLP 2013 - 2013 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,,"1393","1398",,341,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926285904&partnerID=40&md5=7096a09aecb9cdb0491a5746817802cb","We introduce bilingual word embeddings: semantic embeddings associated across two languages in the context of neural language models. We propose a method to learn bilingual embeddings from a large unlabeled corpus, while utilizing MT word alignments to constrain translational equivalence. The new embeddings significantly out-perform baselines in word semantic similarity. A single semantic similarity feature induced with bilingual embeddings adds near half a BLEU point to the results of NIST08 Chinese-English machine translation task. © 2013 Association for Computational Linguistics.",,"Computer aided language translation; Natural language processing systems; Semantics; Translation (languages); Embeddings; Language model; Machine translations; Phrase-based machine translations; Semantic similarity; Translational equivalence; Word alignment; Computational linguistics","Allen Institute for Artificial Intelligence;Amazon;et al.;Google;Microsoft;Nuance","2013 Conference on Empirical Methods in Natural Language Processing, EMNLP 2013","18 October 2013 through 21 October 2013",,111413,Conference Paper,"Final","",Scopus,2-s2.0-84926285904
"Lao N., Mitchell T., Cohen W.W.","6602831084;7402175019;7202924370;","Random walk inference and learning in a large scale knowledge base",2011,"EMNLP 2011 - Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,,"529","539",,340,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053232762&partnerID=40&md5=16eb5d963f1cf5ba2a8cf8762d5498dd","We consider the problem of performing learning and inference in a large scale knowledge base containing imperfect knowledge with incomplete coverage. We show that a soft inference procedure based on a combination of constrained, weighted, random walks through the knowledge base graph can be used to reliably infer new beliefs for the knowledge base. More specifically, we show that the system can learn to infer different target relations by tuning the weights associated with random walks that follow different paths through the graph, using a version of the Path Ranking Algorithm (Lao and Cohen, 2010b). We apply this approach to a knowledge base of approximately 500,000 beliefs extracted imperfectly from the web by NELL, a never-ending language learner (Carlson et al., 2010). This new system improves significantly over NELL's earlier Horn-clause learning and inference method: it obtains nearly double the precision at rank 100, and the new learning method is also applicable to many more inference tasks. © 2011 Association for Computational Linguistics.",,"Carlson; Inference methods; Knowledge base; Learning methods; Path ranking; Random Walk; Computational linguistics; Knowledge based systems; Random processes; User interfaces; Natural language processing systems","Google;Yahoo;Textkernel","Conference on Empirical Methods in Natural Language Processing, EMNLP 2011","27 July 2011 through 31 July 2011","Edinburgh",86713,Conference Paper,"Final","",Scopus,2-s2.0-80053232762
"Riedel S., Yao L., McCallum A., Marlin B.M.","36562438800;24475694900;7003773569;6506955008;","Relation extraction with matrix factorization and universal schemas",2013,"NAACL HLT 2013 - 2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Main Conference",,,,"74","84",,339,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84926180274&partnerID=40&md5=91e94b22ee57cbbaa27e98f40f3462d7","Traditional relation extraction predicts relations within some fixed and finite target schema. Machine learning approaches to this task require either manual annotation or, in the case of distant supervision, existing structured sources of the same schema. The need for existing datasets can be avoided by using a universal schema: the union of all involved schemas (surface form predicates as in OpenIE, and relations in the schemas of preexisting databases). This schema has an almost unlimited set of relations (due to surface forms), and supports integration with existing structured data (through the relation types of existing databases). To populate a database of such schema we present matrix factorization models that learn latent feature vectors for entity tuples and relations. We show that such latent models achieve substantially higher accuracy than a traditional classification approach. More importantly, by operating simultaneously on relations observed in text and in pre-existing structured DBs such as Freebase, we are able to reason about unstructured and structured data in mutually-supporting ways. By doing so our approach outperforms stateof- the-Art distant supervision. © 2013 Association for Computational Linguistics.",,"Artificial intelligence; Classification (of information); Computational linguistics; Database systems; Extraction; Factorization; Learning systems; Classification approach; Feature vectors; Machine learning approaches; Manual annotation; Matrix factorizations; Relation extraction; State of the art; Structured data; Matrix algebra","Appen ButlerHill;et al.;ETS;Google;Microsoft Research;Rakuten","2013 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2013","9 June 2013 through 14 June 2013",,111457,Conference Paper,"Final","",Scopus,2-s2.0-84926180274
"Etzioni O., Banko M., Soderland S., Weld D.S.","7004312379;15055390300;6603651046;7003334103;","Open information extraction from the web",2008,"Communications of the ACM","51","12",,"68","74",,330,"10.1145/1409360.1409378","https://www.scopus.com/inward/record.uri?eid=2-s2.0-59449097589&doi=10.1145%2f1409360.1409378&partnerID=40&md5=a9931b5d725e325014023a0c3e245251","Open Information Extraction (IE), where the identities of the relations to be extracted are unknown and the billions of documents found on the web necessitate highly scalable processing, is a reliable way of extracting information from the Internet. The first IE systems relied on some form of pattern-matching rules that were manually crafted for each domain. Modern IE automatically learns an extractor from a training set in which domain-specific examples are tagged. The development of suitable training data for IE requires substantial effort and expertise. The Know-ItAll web IE system automates IE by learning to label its own training examples using only a small set of domain-independent extraction patterns. TextRunner is a fully implemented Open IE system that utilizes the two-phase architecture. It's first phase uses a general model of language, which trains a graphical model called a conditional random field (CRF). Open IE also supports aggregating, fusing information across a large number of web pages.",,"Conditional random fields; Domain specifics; Extracting informations; Extraction patterns; General models; Graphical models; Information extractions; Pattern-matching; Training datum; Training examples; Training sets; Web pages; Graphic methods; Image segmentation; Information analysis; Websites",,,,,,Review,"Final","All Open Access, Green",Scopus,2-s2.0-59449097589
"Bordes A., Glorot X., Weston J., Bengio Y.","18933923000;49861305800;8865128200;7003958245;","A semantic matching energy function for learning with multi-relational data: Application to word-sense disambiguation",2014,"Machine Learning","94","2",,"233","259",,327,"10.1007/s10994-013-5363-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84894904920&doi=10.1007%2fs10994-013-5363-6&partnerID=40&md5=38f6b5149bdc255e2ceace65cb1b25d6","Large-scale relational learning becomes crucial for handling the huge amounts of structured data generated daily in many application domains ranging from computational biology or information retrieval, to natural language processing. In this paper, we present a new neural network architecture designed to embed multi-relational graphs into a flexible continuous vector space in which the original data is kept and enhanced. The network is trained to encode the semantics of these graphs in order to assign high probabilities to plausible components. We empirically show that it reaches competitive performance in link prediction on standard datasets from the literature as well as on data from a real-world knowledge base (WordNet). In addition, we present how our method can be applied to perform word-sense disambiguation in a context of open-text semantic parsing, where the goal is to learn to assign a structured meaning representation to almost any sentence of free text, demonstrating that it can scale up to tens of thousands of nodes and thousands of types of relation. © 2013 The Author(s).","Multi-relational data; Neural networks; Word-sense disambiguation",,,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-84894904920
"Fischer A., Igel C.","57190948135;6602116076;","An introduction to restricted Boltzmann machines",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7441 LNCS",,,"14","36",,327,"10.1007/978-3-642-33275-3_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865595751&doi=10.1007%2f978-3-642-33275-3_2&partnerID=40&md5=0d5fbb67b814ca81a017b0d9b8260268","Restricted Boltzmann machines (RBMs) are probabilistic graphical models that can be interpreted as stochastic neural networks. The increase in computational power and the development of faster learning algorithms have made them applicable to relevant machine learning problems. They attracted much attention recently after being proposed as building blocks of multi-layer learning systems called deep belief networks. This tutorial introduces RBMs as undirected graphical models. The basic concepts of graphical models are introduced first, however, basic knowledge in statistics is presumed. Different learning algorithms for RBMs are discussed. As most of them are based on Markov chain Monte Carlo (MCMC) methods, an introduction to Markov chains and the required MCMC techniques is provided. © 2012 Springer-Verlag.",,"Basic concepts; Building blockes; Computational power; Deep belief networks; GraphicaL model; Machine learning problem; Markov chain Monte Carlo method; Probabilistic graphical models; Restricted boltzmann machine; Stochastic neural network; Graphic methods; Image analysis; Learning algorithms; Learning systems; Markov processes; Neural networks; Computer vision",,"17th Iberoamerican Congress on Progress in Pattern Recognition, Image Analysis, Computer Vision, and Applications, CIARP 2012","3 September 2012 through 6 September 2012","Buenos Aires",92323,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-84865595751
"Nielsen F.A.","8053310300;","A new ANEW: Evaluation of a word list for sentiment analysis in microblogs",2011,"CEUR Workshop Proceedings","718",,,"93","98",,322,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84874354132&partnerID=40&md5=48b297427613f6fcf939799391779b10","Sentiment analysis of microblogs such as Twitter has recently gained a fair amount of attention. One of the simplest sentiment analysis approaches compares the words of a posting against a labeled word list, where each word has been scored for valence, - A ""sentiment lexicon"" or ""affective word lists"". There exist several affective word lists, e.g., ANEW (Affective Norms for English Words) developed before the advent of microblogging and sentiment analysis. I wanted to examine how well ANEW and other word lists performs for the detection of sentiment strength in microblog posts in comparison with a new word list specifically constructed for microblogs. I used manually labeled postings from Twitter scored for sentiment. Using a simple word matching I show that the new word list may perform better than ANEW, though not as good as the more elaborate approach found in SentiStrength.",,"Affective words; English word; Micro-blog; Microblogging; Microblogs; Sentiment analysis; Sentiment lexicons; Word matching; Social networking (online); Data mining",,"1st Workshop on Making Sense of Microposts 2011: Big Things Come in Small Packages, #MSM 2011 - Co-located with the 8th Extended Semantic Web Conference, ESWC 2011","30 May 2011 through 30 May 2011","Heraklion, Crete",101828,Conference Paper,"Final","",Scopus,2-s2.0-84874354132
"Elliott C.","","The affective reasoner: A process model of emotions in a multi-agent system",1992,"The Affective Reasoner: A Process Model of Emotions in a Multi-agent System",,,,"","",,322,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004059161
"Collins S.H., Ruina A.","57209232819;13310372800;","A bipedal walking robot with efficient and human-like gait",2005,"Proceedings - IEEE International Conference on Robotics and Automation","2005",,"1570404","1983","1988",,320,"10.1109/ROBOT.2005.1570404","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33645797164&doi=10.1109%2fROBOT.2005.1570404&partnerID=40&md5=2991f062067e97a9ecd473388f36a164","Here we present the design of a passive-dynamics based, fully autonomous, 3-D, bipedal walking robot that uses simple control, consumes little energy, and has human-like morphology and gait. Design aspects covered here include the freely rotating hip joint with angle bisecting mechanism; freely rotating knee joints with latches; direct actuation of the ankles with a spring, release mechanism, and reset motor; wide feet that are shaped to aid lateral stability; and the simple control algorithm. The biomechanics context of this robot is discussed in more detail in [1], and movies of the robot walking are available at Science On- line and http://www.tam.cornell.edu/~ruina/powerwalk.html. This robot adds evidence to the idea that passive-dynamic approaches might help design walking robots that are simpler, more efficient and easier to control. ©2005 IEEE.","Biped; Efficiency; Locomotion; Passive- Dynamic","Angle bisecting mechanisms; Walking robots; Biomechanics; Energy utilization; Gait analysis; Joints (anatomy); Motors; Position control; Mobile robots",,"2005 IEEE International Conference on Robotics and Automation","18 April 2005 through 22 April 2005","Barcelona",68921,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-33645797164
"Xie R., Liu Z., Jia J., Luan H., Sun M.","57155801500;55714725800;35200619700;23035489400;7403180987;","Representation learning of knowledge graphs with entity descriptions",2016,"30th AAAI Conference on Artificial Intelligence, AAAI 2016",,,,"2659","2665",,315,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007148114&partnerID=40&md5=9c731b496fb1dba06c6a6a55575a0e51","Representation learning (RL) of knowledge graphs aims to project both entities and relations into a continuous lowdimensional space. Most methods concentrate on learning representations with knowledge triples indicating relations between entities. In fact, in most knowledge graphs there are usually concise descriptions for entities, which cannot be well utilized by existing methods. In this paper, we propose a novel RL method for knowledge graphs taking advantages of entity descriptions. More specifically, we explore two encoders, including continuous bag-of-words and deep convolutional neural models to encode semantics of entity descriptions. We further learn knowledge representations with both triples and descriptions.We evaluate our method on two tasks, including knowledge graph completion and entity classification. Experimental results on real-world datasets show that, our method outperforms other baselines on the two tasks, especially under the zero-shot setting, which indicates that our method is capable of building representations for novel entities according to their descriptions. The source code of this paper can be obtained from https://github.com/xrb92/DKRL. © 2016, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Knowledge representation; Semantics; Bag of words; Knowledge graphs; Low-dimensional spaces; Neural models; Real-world datasets; Source codes; Graphic methods","Artificial Intelligence;Baidu;et al.;IBM;Infosys;NSF","30th AAAI Conference on Artificial Intelligence, AAAI 2016","12 February 2016 through 17 February 2016",,124960,Conference Paper,"Final","",Scopus,2-s2.0-85007148114
"Poggio T., Girosi F.","","A theory of networks for approximation and learning",1989,"A Theory of Networks for Approximation and Learning",,,,"","",,315,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0004030839
"Jean S., Cho K., Memisevic R., Bengio Y.","56006080600;55722769200;8970465000;7003958245;","On using very large target vocabulary for neural machine translation",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference","1",,,"1","10",,309,"10.3115/v1/p15-1001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943744936&doi=10.3115%2fv1%2fp15-1001&partnerID=40&md5=2d056180447bb8ff11eb6fc1d45153e6","Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrasebased statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to match, and in some cases outperform, the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use an ensemble of a few models with very large target vocabularies, we achieve performance comparable to the state of the art (measured by BLEU) on both the English!German and English!French translation tasks of WMT'14. © 2015 Association for Computational Linguistics.",,"Complex networks; Computational linguistics; Computer aided language translation; Decoding; Importance sampling; Natural language processing systems; Baseline models; Decoding complexity; Machine translation models; Machine translations; Phrase-based statistical machine translation; State of the art; Target words; Training complexity; Long short-term memory","Alibaba Group;Baidu;CreditEase;et al.;Samsung;Tencent","53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015","26 July 2015 through 31 July 2015",,114195,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84943744936
"Wang X., Ji H., Cui P., Yu P., Shi C., Wang B., Ye Y.","56454481700;57203507149;34568700100;7402366049;55447999200;7405918429;23037530700;","Heterogeneous graph attention network",2019,"The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019",,,,"2022","2032",,306,"10.1145/3308558.3313562","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066907835&doi=10.1145%2f3308558.3313562&partnerID=40&md5=4c18519566b48acbb4dcfee1cf0a8909","Graph neural network, as a powerful graph representation technique based on deep learning, has shown superior performance and attracted considerable research interest. However, it has not been fully considered in graph neural network for heterogeneous graph which contains different types of nodes and links. The heterogeneity and rich semantic information bring great challenges for designing a graph neural network for heterogeneous graph. Recently, one of the most exciting advancements in deep learning is the attention mechanism, whose great potential has been well demonstrated in various areas. In this paper, we first propose a novel heterogeneous graph neural network based on the hierarchical attention, including node-level and semantic-level attentions. Specifically, the node-level attention aims to learn the importance between a node and its meta-path based neighbors, while the semantic-level attention is able to learn the importance of different meta-paths. With the learned importance from both node-level and semantic-level attention, the importance of node and meta-path can be fully considered. Then the proposed model can generate node embedding by aggregating features from meta-path based neighbors in a hierarchical manner. Extensive experimental results on three real-world heterogeneous graphs not only show the superior performance of our proposed model over the state-of-the-arts, but also demonstrate its potentially good interpretability for graph analysis. © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.","Graph Analysis; Neural Network; Social Network","Deep learning; Neural networks; Semantics; Social networking (online); World Wide Web; Attention mechanisms; Graph analysis; Graph neural networks; Graph representation; Heterogeneous graph; Interpretability; Research interests; Semantic information; Graph theory","Amazon;Bloomberg;Criteo AI Lab;et al.;Google;Microsoft","2019 World Wide Web Conference, WWW 2019","13 May 2019 through 17 May 2019",,147966,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85066907835
"Chang C.-H., Lui S.-C.","37092331000;7102379152;","IEPAD: Information extraction based on pattern discovery",2001,"Proceedings of the 10th International Conference on World Wide Web, WWW 2001",,,,"681","688",,305,"10.1145/371920.372182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042021254&doi=10.1145%2f371920.372182&partnerID=40&md5=97f8243c2f605041fbb07231dbbf6b23","The research in information extraction (IE) regards the generation of wrappers that can extract particular information from semistructured Web documents. Similar to compiler generation, the extractor is actually a driver program, which is accompanied with the generated extraction rule. Previous work in this field aims to learn extraction rules from users' training example. In this paper, we propose IEPAD, a system that automatically discovers extraction rules from Web pages. The system can automatically identify record boundary by repeated pattern mining and multiple sequence alignment. The discovery of repeated patterns are realized through a data structure call PAT trees. Additionally, repeated patterns are further extended by pattern alignment to comprehend all record instances. This new track to IE involves no human effort and content-dependent heuristics. Experimental results show that the constructed extraction rules can achieve 97 percent extraction over fourteen popular search engines. © 2001 ACM.","Extraction rule; Information extraction; Multiple string alignment; PAT tree","Forestry; Program compilers; Search engines; Trees (mathematics); Websites; Compiler generation; Content dependent; Extraction rule; Information extraction based on pattern discoveries; Multiple sequence alignments; PAT tree; Pattern alignment; String alignment; Information retrieval","ACM Special Interest Group on Hypertext, Hypermedia, and Web (SIGWEB);Hypertext, Hypermedia, and Web (SIGLINK);International World Wide Web Conference Committee (IW3C2)","10th International Conference on World Wide Web, WWW 2001","1 May 2001 through 5 May 2001",,129813,Conference Paper,"Final","",Scopus,2-s2.0-85042021254
"Krstajic D., Buturovic L.J., Leahy D.E., Thomas S.","12753879800;23481165400;7004941924;57214059984;","Cross-validation pitfalls when selecting and assessing regression and classification models",2014,"Journal of Cheminformatics","6","1","10","","",,300,"10.1186/1758-2946-6-10","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899084283&doi=10.1186%2f1758-2946-6-10&partnerID=40&md5=9de626d7b63a6688d0c414263286b839","Background: We address the problem of selecting and assessing classification and regression models using cross-validation. Current state-of-the-art methods can yield models with high variance, rendering them unsuitable for a number of practical applications including QSAR. In this paper we describe and evaluate best practices which improve reliability and increase confidence in selected models. A key operational component of the proposed methods is cloud computing which enables routine use of previously infeasible approaches. Methods. We describe in detail an algorithm for repeated grid-search V-fold cross-validation for parameter tuning in classification and regression, and we define a repeated nested cross-validation algorithm for model assessment. As regards variable selection and parameter tuning we define two algorithms (repeated grid-search cross-validation and double cross-validation), and provide arguments for using the repeated grid-search in the general case. Results: We show results of our algorithms on seven QSAR datasets. The variation of the prediction performance, which is the result of choosing different splits of the dataset in V-fold cross-validation, needs to be taken into account when selecting and assessing classification and regression models. Conclusions: We demonstrate the importance of repeating cross-validation when selecting an optimal model, as well as the importance of repeating nested cross-validation when assessing a prediction error. © 2014 Krstajic et al.; licensee Chemistry Central Ltd.",,,,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-84899084283
"Davis M.D., Sigal R., Weyuker E.J.","",[No title available],1994,"Computability, Complexity, and Languages",,,,"","",,298,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0003648431
"Palmerini T., Benedetto U., Bacchi-Reggiani L., Riva D.D., Biondi-Zoccai G., Feres F., Abizaid A., Hong M.-K., Kim B.-K., Jang Y., Kim H.-S., Park K.W., Genereux P., Bhatt D.L., Orlandi C., De Servi S., Petrou M., Rapezzi C., Stone G.W.","6601968223;13906087500;56962741700;57196637172;57209103657;7003453206;36122299200;7402687966;35189204900;55429942500;33567809200;35300576900;26022992000;7102974175;56548411200;7005676518;7102903975;7005883289;7202761439;","Mortality in patients treated with extended duration dual antiplatelet therapy after drug-eluting stent implantation: A pairwise and Bayesian network meta-analysis of randomised trials",2015,"The Lancet","385","9985",,"2371","2382",,294,"10.1016/S0140-6736(15)60263-X","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930923550&doi=10.1016%2fS0140-6736%2815%2960263-X&partnerID=40&md5=48a1229d2eb60b0626e98f9ef68820a1","Background Despite recent studies, the optimum duration of dual antiplatelet therapy (DAPT) after coronary drug-eluting stent placement remains uncertain. We performed a meta-analysis with several analytical approaches to investigate mortality and other clinical outcomes with different DAPT strategies. Methods We searched Medline, Embase, Cochrane databases, and proceedings of international meetings on Nov 20, 2014, for randomised controlled trials comparing different DAPT durations after drug-eluting stent implantation. We extracted study design, inclusion and exclusion criteria, sample characteristics, and clinical outcomes. DAPT duration was categorised in each study as shorter versus longer, and as 6 months or shorter versus 1 year versus longer than 1 year. Analyses were done by both frequentist and Bayesian approaches. Findings We identified ten trials published between Dec 16, 2011, and Nov 16, 2014, including 31 666 randomly assigned patients. By frequentist pairwise meta-analysis, shorter DAPT was associated with significantly lower all-cause mortality compared with longer DAPT (HR 0·82, 95% CI 0·69-0·98; p=0·02; number needed to treat [NNT]=325), with no significant heterogeneity apparent across trials. The reduced mortality with shorter compared with longer DAPT was attributable to lower non-cardiac mortality (0·67, 0·51-0·89; p=0·006; NNT=347), with similar cardiac mortality (0·93, 0·73-1·17; p=0.52). Shorter DAPT was also associated with a lower risk of major bleeding, but a higher risk of myocardial infarction and stent thrombosis. We noted similar results in a Bayesian framework with non-informative priors. By network meta-analysis, patients treated with 6-month or shorter DAPT and 1-year DAPT had higher risk of myocardial infarction and stent thrombosis but lower risk of mortality compared with patients treated with DAPT for longer than 1 year. Patients treated with DAPT for 6 months or shorter had similar rates of mortality, myocardial infarction, and stent thrombosis, but lower rates of major bleeding than did patients treated with 1-year DAPT. Interpretation Although treatment with DAPT beyond 1 year after drug-eluting stent implantation reduces myocardial infarction and stent thrombosis, it is associated with increased mortality because of an increased risk of non-cardiovascular mortality not offset by a reduction in cardiac mortality. Funding None. © 2015 Elsevier Ltd.",,"acetylsalicylic acid; antithrombocytic agent; clopidogrel; thienopyridine derivative; antithrombocytic agent; Article; bleeding; cerebrovascular accident; drug eluting stent; heart death; heart infarction; human; meta analysis; mortality; priority journal; randomized controlled trial (topic); stent thrombosis; treatment duration; treatment outcome; coronary artery disease; drug combination; Hemorrhage; Myocardial Infarction; thrombosis; time; Coronary Artery Disease; Drug Therapy, Combination; Drug-Eluting Stents; Hemorrhage; Humans; Myocardial Infarction; Platelet Aggregation Inhibitors; Randomized Controlled Trials as Topic; Thrombosis; Time Factors",,,,,,Article,"Final","",Scopus,2-s2.0-84930923550
"Smith J.C., Abdala A.P.L., Koizumi H., Rybak I.A., Paton J.F.R.","7410174798;6603735500;7202497218;57191881039;55486090800;","Spatial and functional architecture of the mammalian brain stem respiratory network: A hierarchy of three oscillatory mechanisms",2007,"Journal of Neurophysiology","98","6",,"3370","3387",,294,"10.1152/jn.00985.2007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37549055001&doi=10.1152%2fjn.00985.2007&partnerID=40&md5=90181f15f5962d10c9096dc37b1c794d","Mammalian central pattern generators (CPGs) producing rhythmic movements exhibit extremely robust and flexible behavior. Network architectures that enable these features are not well understood. Here we studied organization of the brain stem respiratory CPG. By sequential rostral to caudal transections through the pontine-medullary respiratory network within an in situ perfused rat brain stem-spinal cord preparation, we showed that network dynamics reorganized and new rhythmogenic mechanisms emerged. The normal three-phase respiratory rhythm transformed to a two-phase and then to a one-phase rhythm as the network was reduced. Expression of the three-phase rhythm required the presence of the pons, generation of the two-phase rhythm depended on the integrity of Bötzinger and pre-Bötzinger complexes and interactions between them, and the one-phase rhythm was generated within the pre-Bötzinger complex. Transformation from the three-phase to a two-phase pattern also occurred in intact preparations when chloride-mediated synaptic inhibition was reduced. In contrast to the three-phase and two-phase rhythms, the one-phase rhythm was abolished by blockade of persistent sodium current (INaP). A model of the respiratory network was developed to reproduce and explain these observations. The model incorporated interacting populations of respiratory neurons within spatially organized brain stem compartments. Our simulations reproduced the respiratory patterns recorded from intact and sequentially reduced preparations. Our results suggest that the three-phase and two-phase rhythms involve inhibitory network interactions, whereas the one-phase rhythm depends on INaP. We conclude that the respiratory network has rhythmogenic capabilities at multiple levels of network organization, allowing expression of motor patterns specific for various physiological and pathophysiological respiratory behaviors.",,"animal experiment; animal tissue; article; brain stem; electrophysiology; histology; male; mathematical model; medulla oblongata; nerve cell network; nonhuman; pons; priority journal; rat; respiratory nerve cell; sodium current; spinal cord; synaptic inhibition; synaptic transmission; Algorithms; Animals; Brain Stem; Chlorides; Cyanides; Data Interpretation, Statistical; Electrophysiology; Image Processing, Computer-Assisted; Male; Medulla Oblongata; Models, Neurological; Nerve Net; Neurons; Pons; Rats; Rats, Wistar; Respiratory Mechanics; Respiratory Muscles; Sodium; Spinal Cord; Stimulation, Chemical",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-37549055001
"Hsu Chun-Nan, Dung Ming-Tzung","22933960800;7801536574;","Generating finite-state transducers for semi-structured data extraction from the Web",1998,"Information Systems","23","8",,"521","538",,294,"10.1016/S0306-4379(98)00027-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032309862&doi=10.1016%2fS0306-4379%2898%2900027-1&partnerID=40&md5=79c58cc1c5b85c2c36531bfb60d1eb60","Integrating a large number of Web information sources may significantly increase the utility of the World-Wide Web. A promising solution to the integration is through the use of a Web Information mediator that provides seamless, transparent access for the clients. Information mediators need wrappers to access a Web source as a structured database, but building wrappers by hand is impractical. Previous work on wrapper induction is too restrictive to handle a large number of Web pages that contain tuples with missing attributes, multiple values, variant attribute permutations, exceptions and typos. This paper presents SoftMealy, a novel wrapper representation formalism. This representation is based on a finite-state transducer (FST) and contextual rules. This approach can wrap a wide range of semistructured Web pages because FSTs can encode each different attribute permutation as a path. A SoftMealy wrapper can be induced from a handful of labeled examples using our generalization algorithm. We have implemented this approach into a prototype system and tested it on real Web pages. The performance statistics shows that the sizes of the induced wrappers as well as the required training effort are linear with regard to the structural variance of the test pages. Our experiment also shows that the induced wrappers can generalize over unseen pages.",,"Algorithms; Computer systems programming; Data acquisition; Data recording; Data reduction; Data structures; Formal logic; Response time (computer systems); Statistics; Finite-state transducers (FST); Information extraction; Wrapper induction; World Wide Web",,,,,,Article,"Final","",Scopus,2-s2.0-0032309862
"Dredze M., Crammer K., Pereira F.","14041686400;57207502492;56002780800;","Confidence-weighted linear classification",2008,"Proceedings of the 25th International Conference on Machine Learning",,,,"264","271",,292,"10.1145/1390156.1390190","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56449101965&doi=10.1145%2f1390156.1390190&partnerID=40&md5=0d2b6f6511f6031ae9863b22aae5e24f","We introduce confidence-weighted linear classifiers, which add parameter confidence information to linear classifiers. Online learners in this setting update both classifier parameters and the estimate of their confidence. The particular online algorithms we study here maintain a Gaussian distribution over parameter vectors and update the mean and eovarianee of the distribution with each instance. Empirical evaluation on a range of NLP tasks show that our algorithm improves over other state of the art online and batch methods, learns faster in the online setting, and lends itself to better classifier combination after parallel training. Copyright 2008 by the author(s)/owner(s).",,"Machine learning; Classifiers; Parallel algorithms; Robot learning; Classifier combination; Confidence information; Empirical evaluations; Linear classification; Linear classifiers; On-line algorithms; Parallel training; Parameter vectors; Classification (of information); Learning systems; Batch methods; Classifier combinations; Confidence informations; Empirical evaluations; Gaussian; Linear classifications; Linear classifiers; On-line algorithms; On-line settings; Parallel trainings; Parameter vectors; State-of-the arts","et al.;Federation of Finnish Learned Societies;Helsinki Institute for Information Technology;Intel Corporation;Machine Learning Journal/Springer;University of Helsinki","25th International Conference on Machine Learning","5 July 2008 through 9 July 2008","Helsinki",74109,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-56449101965
"Gentner D., Goldin-Meadow S.","",[No title available],2003,"Language in Mind: Advances in the Study of Language and Thought",,,,"","",,292,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0006127966
"Melacci S., Belkin M.","23397653000;57205523518;","Laplacian support vector machines trained in the primal",2011,"Journal of Machine Learning Research","12",,,"1149","1184",,291,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79955855934&partnerID=40&md5=b8d6a4c9cb96bc8b325d8af8d807288d","In the last few years, due to the growing ubiquity of unlabeled data, much effort has been spent by the machine learning community to develop better understanding and improve the quality of classifiers exploiting unlabeled data. Following the manifold regularization approach, Laplacian Support Vector Machines (LapSVMs) have shown the state of the art performance in semi-supervised classification. In this paper we present two strategies to solve the primal LapSVM problem, in order to overcome some issues of the original dual formulation. In particular, training a LapSVM in the primal can be efficiently performed with preconditioned conjugate gradient. We speed up training by using an early stopping strategy based on the prediction on unlabeled data or, if available, on labeled validation examples. This allows the algorithm to quickly compute approximate solutions with roughly the same classification accuracy as the optimal ones, considerably reducing the training time. The computational complexity of the training algorithm is reduced from 0(n3) to 0(kn2), where n is the combined number of labeled and unlabeled examples and k is empirically evaluated to be significantly smaller than n. Due to its simplicity, training LapSVM in the primal can be the starting point for additional enhancements of the original LapSVM formulation, such as those for dealing with large data sets. We present an extensive experimental evaluation on real world data showing the benefits of the proposed approach. © 2011 Stefano Melacci and Mikhail Belkin.","Classification; Laplacian support vector machines; Manifold regularization; Optimization; Semi-supervised learning","Approximate solution; Classification; Classification accuracy; Dual formulations; Early stopping; Experimental evaluation; Laplacians; Large datasets; Machine learning communities; Manifold regularization; Preconditioned conjugate gradient; Real world data; Regularization approach; Semi-supervised classification; Semi-supervised learning; Speed-ups; State-of-the-art performance; Training algorithms; Training time; Unlabeled data; Computational complexity; Conjugate gradient method; Learning algorithms; Optimization; Supervised learning; Support vector machines; Laplace transforms",,,,,,Article,"Final","",Scopus,2-s2.0-79955855934
"Kowsari K., Meimandi K.J., Heidarysafa M., Mendu S., Barnes L., Brown D.","56404571200;57202981342;57202468239;57204426452;7103077338;57202998030;","Text classification algorithms: A survey",2019,"Information (Switzerland)","10","4","150","","",,287,"10.3390/info10040150","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065859140&doi=10.3390%2finfo10040150&partnerID=40&md5=33e982e90faf19e1da6e85b075722a7b","In recent years, there has been an exponential growth in the number of complex documents and texts that require a deeper understanding of machine learning methods to be able to accurately classify texts in many applications. Many machine learning approaches have achieved surpassing results in natural language processing. The success of these learning algorithms relies on their capacity to understand complex models and non-linear relationships within data. However, finding suitable structures, architectures, and techniques for text classification is a challenge for researchers. In this paper, a brief overview of text classification algorithms is discussed. This overview covers different text feature extractions, dimensionality reduction methods, existing algorithms and techniques, and evaluations methods. Finally, the limitations of each technique and their application in real-world problems are discussed. © 2019 by the authors.","Document classification; Text analysis; Text categorization; Text classification; Text mining; Text representation","Classification (of information); Data mining; Information retrieval systems; Learning algorithms; Machine learning; Natural language processing systems; Document Classification; Text analysis; Text categorization; Text classification; Text mining; Text representation; Text processing",,,,,,Review,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85065859140
"Nickel M., Kiela D.","52264441000;56349687800;","Poincaré embeddings for learning hierarchical representations",2017,"Advances in Neural Information Processing Systems","2017-December",,,"6339","6348",,281,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047019893&partnerID=40&md5=08ad048a3120c82d83a510dba6eae8fe","Representation learning has become an invaluable approach for learning from symbolic data such as text and graphs. However, state-of-the-art embedding methods typically do not account for latent hierarchical structures which are characteristic for many complex symbolic datasets. In this work, we introduce a new approach for learning hierarchical representations of symbolic data by embedding them into hyperbolic space - or more precisely into an n-dimensional Poincaré ball. Due to the underlying hyperbolic geometry, this allows us to learn parsimonious representations of symbolic data by simultaneously capturing hierarchy and similarity. We present an efficient algorithm to learn the embeddings based on Riemannian optimization and show experimentally that Poincaré embeddings can outperform Euclidean embeddings significantly on data with latent hierarchies, both in terms of representation capacity and in terms of generalization ability. © 2017 Neural information processing systems foundation. All rights reserved.",,"Embedding method; Generalization ability; Hierarchical representation; Hierarchical structures; Hyperbolic geometry; Hyperbolic spaces; Riemannian optimizations; State of the art","","31st Annual Conference on Neural Information Processing Systems, NIPS 2017","4 December 2017 through 9 December 2017",,136033,Conference Paper,"Final","",Scopus,2-s2.0-85047019893
"Golubitsky M., Stewart I., Buono P.-L., Collins J.J.","7003736392;35547513000;6701741008;35478063700;","Symmetry in locomotor central pattern generators and animal gaits",1999,"Nature","401","6754",,"693","695",,281,"10.1038/44416","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033554720&doi=10.1038%2f44416&partnerID=40&md5=9bc8a230d511e3e586f4546b4ae8b2f6","Animal locomotion is controlled, in part, by a central pattern generator (CPG), which is an intraspinal network of neurons capable of generating a rhythmic output. The spatio-temporal symmetries of the quadrupedal gaits walk, trot and pace lead to plausible assumptions about the symmetries of locomotor CPGs. These assumptions imply that the CPG of a quadruped should consist of eight nominally identical subcircuits, arranged in an essentially unique matter. Here we apply analogous arguments to myriapod CPGs. Analyses based on symmetry applied to these networks lead to testable predictions, including a distinction between primary and secondary gaits, the existence of a new primary gait called 'jump', and the occurrence of half-integer wave numbers in myriapod gaits. For bipeds, our analysis also predicts two gaits with the out-of-phase symmetry of the walk and two gaits with the in-phase symmetry of the hop. We present data that support each of these predictions. This work suggests that symmetry can be used to infer a plausible class of CPG network architectures from observed patterns of animal gaits.",,"article; gait; locomotion; nerve cell network; prediction; priority journal; Animals; Gait; Locomotion; Models, Biological; Nerve Net; Neurons; Spinal Cord",,,,,,Article,"Final","",Scopus,2-s2.0-0033554720
"Williams P.M.","","Bayesian regularization and pruning using a Laplace prior",1995,"Neural Computation","7","1",,"117","143",,281,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0000673452
"Wang X., He X., Cao Y., Liu M., Chua T.-S.","57191904438;56285637300;57015851100;56571781700;7101702977;","KGAT: Knowledge graph attention network for recommendation",2019,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"950","958",,280,"10.1145/3292500.3330989","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071195192&doi=10.1145%2f3292500.3330989&partnerID=40&md5=75b7ca64bcc92f7172bc0327251cb161","To provide more accurate, diverse, and explainable recommendation, it is compulsory to go beyond modeling user-item interactions and take side information into account. Traditional methods like factorization machine (FM) cast it as a supervised learning problem, which assumes each interaction as an independent instance with side information encoded. Due to the overlook of the relations among instances or items (e.g., the director of a movie is also an actor of another movie), these methods are insufficient to distill the collaborative signal from the collective behaviors of users. In this work, we investigate the utility of knowledge graph (KG), which breaks down the independent interaction assumption by linking items with their attributes. We argue that in such a hybrid structure of KG and user-item graph, high-order relations - which connect two items with one or multiple linked attributes - are an essential factor for successful recommendation. We propose a new method named Knowledge Graph Attention Network (KGAT) which explicitly models the high-order connectivities in KG in an end-to-end fashion. It recursively propagates the embeddings from a node's neighbors (which can be users, items, or attributes) to refine the node's embedding, and employs an attention mechanism to discriminate the importance of the neighbors. Our KGAT is conceptually advantageous to existing KG-based recommendation methods, which either exploit high-order relations by extracting paths or implicitly modeling them with regularization. Empirical results on three public benchmarks show that KGAT significantly outperforms state-of-the-art methods like Neural FM [11] and RippleNet [29]. Further studies verify the efficacy of embedding propagation for high-order relation modeling and the interpretability benefits brought by the attention mechanism. We release the codes and datasets at https://github.com/xiangwang1223/knowledge_graph_attention_network. © 2019 Association for Computing Machinery.","Collaborative Filtering; Embedding Propagation; Graph Neural Network; Higher-order Connectivity; Knowledge Graph; Recommendation","Backpropagation; Collaborative filtering; Embeddings; Factorization machines; Graph neural networks; Higher-order; Knowledge graphs; Recommendation; Recommendation methods; State-of-the-art methods; Supervised learning problems; Data mining","ACM SIGKDD;ACM SIGMOD","25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2019","4 August 2019 through 8 August 2019",,149966,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85071195192
"He Y., Lin J., Liu Z., Wang H., Li L.-J., Han S.","","AMC: AutoML for model compression and acceleration on mobile devices",2018,"Proc. Eur. Conf. Comput. Vis. (ECCV)",,,,"784","800",,274,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85058359193
"Yang B., Yih W.-T., He X., Gao J., Deng L.","","Embedding entities and relations for learning and inference in knowledge bases",2014,"Embedding Entities and Relations for Learning and Inference in Knowledge Bases",,,,"","",,273,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84952335118
"Smolensky P., Legendre G.","",[No title available],2006,"The Harmonic Mind: From Neural Computation to Optimality-Theoretic Grammar",,,,"","",,273,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-33748699965
"Golomb S.W., Baumert L.D.","7003852806;7801511005;","Backtrack Programming",1965,"Journal of the ACM (JACM)","12","4",,"516","524",,273,"10.1145/321296.321300","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000260998&doi=10.1145%2f321296.321300&partnerID=40&md5=910fdcebeda7902482a77bd73a42741e",[No abstract available],,,,,,,,Article,"Final","",Scopus,2-s2.0-0000260998
"Xu K., Hu W., Leskovec J., Jegelka S.","","How powerful are graph neural networks?",2018,"How Powerful Are Graph Neural Networks?",,,,"","",,270,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85066945478
"Lee H., Grosse R., Ranganath R., Ng A.Y.","15056237200;34875103900;34875603200;35410071600;","Unsupervised learning of hierarchical representations with convolutional deep belief networks",2011,"Communications of the ACM","54","10",,"95","103",,268,"10.1145/2001269.2001295","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053540444&doi=10.1145%2f2001269.2001295&partnerID=40&md5=1a544b75fe3fc207558973f2688057df","There has been much interest in unsupervised learning of hierarchical generative models such as deep belief networks (DBNs); however, scaling such models to full-sized, highdimensional images remains a difficult problem. To address this problem, we present the convolutional deep belief network, a hierarchical generative model that scales to realistic image sizes. This model is translation-invariant and supports efficient bottom-up and top-down probabilistic inference. Key to our approach is probabilistic max-pooling, a novel technique that shrinks the representations of higher layers in a probabilistically sound way. Our experiments show that the algorithm learns useful high-level visual features, such as object parts, from unlabeled images of objects and natural scenes. We demonstrate excellent performance on several visual recognition tasks and show that our model can perform hierarchical (bottom-up and top-down) inference over full-sized images. © 2011 ACM.",,"Bottom-up and top-down; Excellent performance; Generative model; Hierarchical representation; High-dimensional images; Natural scenes; Novel techniques; Probabilistic inference; Realistic images; Translation invariants; Visual feature; Visual recognition; Convolution; Unsupervised learning; Bayesian networks",,,,,,Article,"Final","",Scopus,2-s2.0-80053540444
"Saif H., He Y., Fernandez M., Alani H.","55330697500;22985368400;8928406600;8892548000;","Contextual semantics for sentiment analysis of Twitter",2016,"Information Processing and Management","52","1",,"5","19",,264,"10.1016/j.ipm.2015.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84924110504&doi=10.1016%2fj.ipm.2015.01.005&partnerID=40&md5=d72b980718806a294daba9ae70d3ecba","Sentiment analysis on Twitter has attracted much attention recently due to its wide applications in both, commercial and public sectors. In this paper we present SentiCircles, a lexicon-based approach for sentiment analysis on Twitter. Different from typical lexicon-based approaches, which offer a fixed and static prior sentiment polarities of words regardless of their context, SentiCircles takes into account the co-occurrence patterns of words in different contexts in tweets to capture their semantics and update their pre-assigned strength and polarity in sentiment lexicons accordingly. Our approach allows for the detection of sentiment at both entity-level and tweet-level. We evaluate our proposed approach on three Twitter datasets using three different sentiment lexicons to derive word prior sentiments. Results show that our approach significantly outperforms the baselines in accuracy and F-measure for entity-level subjectivity (neutral vs. polar) and polarity (positive vs. negative) detections. For tweet-level sentiment detection, our approach performs better than the state-of-the-art SentiStrength by 4-5% in accuracy in two datasets, but falls marginally behind by 1% in F-measure in the third dataset. © 2015 Elsevier Ltd. All rights reserved.","Contextual semantics; Sentiment analysis; Twitter","Data mining; Semantics; Co-occurrence pattern; Contextual semantics; Lexicon-based; Public sector; Sentiment analysis; Sentiment lexicons; State of the art; Twitter; Social networking (online)",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84924110504
"Kotthoff L., Thornton C., Hoos H.H., Hutter F., Leyton-Brown K.","36447466900;56848796900;6701843335;55931808800;6602662604;","Auto-WEKA 2.0: Automatic model selection and hyperparameter optimization in WEKA",2017,"Journal of Machine Learning Research","18",,,"1","5",,259,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020694807&partnerID=40&md5=dd6083e90d10da4f132d774534d32dc9","WEKA is a widely used, open-source machine learning platform. Due to its intuitive interface, it is particularly popular with novice users. However, such users often find it hard to identify the best approach for their particular dataset among the many available. We describe the new version of Auto-WEKA, a system designed to help such users by automatically searching through the joint space of WEKA’s learning algorithms and their respective hyperparameter settings to maximize performance, using a state-of-the-art Bayesian optimization method. Our new package is tightly integrated with WEKA, making it just as accessible to end users as any other learning algorithm. © 2017 Lars Kotthoff, Chris Thornton, Frank Hutter, Holger H. Hoos, and Kevin Leyton-Brown.","Feature Selection; Hyperparameter Optimization; Model Selection","Feature extraction; Learning systems; Optimization; Automatic model selection; Bayesian optimization; Hyper-parameter; Hyper-parameter optimizations; Intuitive interfaces; Model Selection; Open sources; State of the art; Learning algorithms",,,,,,Article,"Final","",Scopus,2-s2.0-85020694807
"Ye J., Ni J., Yi Y.","57195622247;13106118300;7202372806;","Deep Learning Hierarchical Representations for Image Steganalysis",2017,"IEEE Transactions on Information Forensics and Security","12","11","7937836","2545","2557",,253,"10.1109/TIFS.2017.2710946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029281151&doi=10.1109%2fTIFS.2017.2710946&partnerID=40&md5=b95095608e86d6493307bf74a74f56c7","Nowadays, the prevailing detectors of steganographic communication in digital images mainly consist of three steps, i.e., residual computation, feature extraction, and binary classification. In this paper, we present an alternative approach to steganalysis of digital images based on convolutional neural network (CNN), which is shown to be able to well replicate and optimize these key steps in a unified framework and learn hierarchical representations directly from raw images. The proposed CNN has a quite different structure from the ones used in conventional computer vision tasks. Rather than a random strategy, the weights in the first layer of the proposed CNN are initialized with the basic high-pass filter set used in the calculation of residual maps in a spatial rich model (SRM), which acts as a regularizer to suppress the image content effectively. To better capture the structure of embedding signals, which usually have extremely low SNR (stego signal to image content), a new activation function called a truncated linear unit is adopted in our CNN model. Finally, we further boost the performance of the proposed CNN-based steganalyzer by incorporating the knowledge of selection channel. Three state-of-the-art steganographic algorithms in spatial domain, e.g., WOW, S-UNIWARD, and HILL, are used to evaluate the effectiveness of our model. Compared to SRM and its selection-channel-aware variant maxSRMd2, our model achieves superior performance across all tested algorithms for a wide variety of payloads. © 2005-2012 IEEE.","convolutional neural networks; feature learning; Steganalysis","Convolution; Deep learning; High pass filters; Image processing; Neural networks; Signal to noise ratio; Steganography; Binary classification; Conventional computers; Convolutional neural network; Feature learning; Hierarchical representation; New activation functions; Steganalysis; Steganographic algorithms; Feature extraction",,,,,,Article,"Final","",Scopus,2-s2.0-85029281151
"Liu P., Qiu X., Huang X.","","Recurrent neural network for text classification with multi-task learning",2016,"Recurrent Neural Network for Text Classification with Multi-Task Learning",,,,"","",,252,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85007376457
"Andreas J., Rohrbach M., Darrell T., Klein D.","55667847200;35108115100;7003377605;23009040500;","Learning to compose neural networks for question answering",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",,,,"1545","1554",,250,"10.18653/v1/n16-1181","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84993660571&doi=10.18653%2fv1%2fn16-1181&partnerID=40&md5=48ea7ed32cedd04f1b7d1b57b5775b54","We describe a question answering model that applies to both images and structured knowledge bases. The model uses natural language strings to automatically assemble neural networks from a collection of composable modules. Parameters for these modules are learned jointly with network-assembly parameters via reinforcement learning, with only (world, question, answer) triples as supervision. Our approach, which we term a dynamic neural module network, achieves state-of-theart results on benchmark datasets in both visual and structured domains. ©2016 Association for Computational Linguistics.",,"Computational linguistics; Natural language processing systems; Assembly parameters; Benchmark datasets; Composable; Model use; Module networks; Natural languages; Question Answering; Structured knowledge; Reinforcement learning","Amazon;Baidu;Bloomberg;eBay;et al.;Google","15th Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016","12 June 2016 through 17 June 2016",,124044,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84993660571
"Muslea Ion, Minton Steve, Knoblock Craig","6602746183;7005757440;7003507107;","Hierarchical approach to wrapper induction",1999,"Proceedings of the International Conference on Autonomous Agents",,,,"190","197",,250,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032684968&partnerID=40&md5=3446126db4a536146ce791c05d154b5c","With the tremendous amount of information that becomes available on the Web on a daily basis, the ability to quickly develop information agents has become a crucial problem. A vital component of any Web-based information agent is a set of wrappers that can extract the relevant data from semistructured information sources. Our novel approach to wrapper induction is based on the idea of hierarchical information extraction, which turns the hard problem of extracting data from an arbitrarily complex document into a series of easier extraction tasks. We introduce an inductive algorithm, STALKER, that generates high accuracy extraction rules based on user-labeled training examples. Labeling the training data represents the major bottleneck in using wrapper induction techniques, and our experimental results show that STALKER does significantly better then other approaches; on one hand, STALKER requires up to two orders of magnitude fewer examples than other algorithms, while on the other hand it can handle information sources that could not be wrapped by existing techniques.",,"Inductive algorithms; Software package STALKER; Web-based information agents; Wrapper inductions; Artificial intelligence; Data structures; Feature extraction; Hierarchical systems; Learning algorithms; World Wide Web; Neural networks","ACM SIGART","Proceedings of the 1999 3rd International Conference on Autonomous Agents","1 May 1999 through 5 May 1999","Seattle, WA, USA",55456,Conference Paper,"Final","",Scopus,2-s2.0-0032684968
"Poria S., Chaturvedi I., Cambria E., Hussain A.","55316592700;12646151300;56140547500;19734290900;","Convolutional MKL based multimodal emotion recognition and sentiment analysis",2017,"Proceedings - IEEE International Conference on Data Mining, ICDM",,,"7837868","439","448",,248,"10.1109/ICDM.2016.178","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014552954&doi=10.1109%2fICDM.2016.178&partnerID=40&md5=181d4fa5a93d931d619afe3a3d2593ea","Technology has enabled anyone with an Internet connection to easily create and share their ideas, opinions and content with millions of other people around the world. Much of the content being posted and consumed online is multimodal. With billions of phones, tablets and PCs shipping today with built-in cameras and a host of new video-equipped wearables like Google Glass on the horizon, the amount of video on the Internet will only continue to increase. It has become increasingly difficult for researchers to keep up with this deluge of multimodal content, let alone organize or make sense of it. Mining useful knowledge from video is a critical need that will grow exponentially, in pace with the global growth of content. This is particularly important in sentiment analysis, as both service and product reviews are gradually shifting from unimodal to multimodal. We present a novel method to extract features from visual and textual modalities using deep convolutional neural networks. By feeding such features to a multiple kernel learning classifier, we significantly outperform the state of the art of multimodal emotion recognition and sentiment analysis on different datasets. © 2016 IEEE.","Convolutional neural networks; Deep learning; Multimodal sentiment analysis; Multiple kernel learning","Classification (of information); Convolution; Data mining; Deep learning; Deep neural networks; Neural networks; Speech recognition; Convolutional neural network; Internet connection; Internet wills; Multimodal emotion recognition; Multiple Kernel Learning; Product reviews; Sentiment analysis; State of the art; Modal analysis","IEEE Computer Society","16th IEEE International Conference on Data Mining, ICDM 2016","12 December 2016 through 15 December 2016",,126383,Conference Paper,"Final","",Scopus,2-s2.0-85014552954
"Paltoglou G., Thelwall M.","23502436700;55396590500;","A study of Information Retrieval weighting schemes for sentiment analysis",2010,"ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",,,,"1386","1395",,247,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79958278126&partnerID=40&md5=033a2d055d70343eb483b04cafa6407d","Most sentiment analysis approaches use as baseline a support vector machines (SVM) classifier with binary unigram weights. In this paper, we explore whether more sophisticated feature weighting schemes from Information Retrieval can enhance classification accuracy. We show that variants of the classic tf.idf scheme adapted to sentiment analysis provide significant increases in accuracy, especially when using a sublinear function for term frequency weights and document frequency smoothing. The techniques are tested on a wide selection of data sets and produce the best accuracy to our knowledge. © 2010 Association for Computational Linguistics.",,"Classification accuracy; Data sets; Document frequency; Feature weighting; Sentiment analysis; Sublinear; Support vector machine (SVM); Term Frequency; Weighting scheme; Wide selection; Computational linguistics; Information retrieval; Support vector machines; Data mining",,"48th Annual Meeting of the Association for Computational Linguistics, ACL 2010","11 July 2010 through 16 July 2010","Uppsala",89384,Conference Paper,"Final","",Scopus,2-s2.0-79958278126
"Gupta M.M., Qi J.","35447607100;7201522031;","Theory of T-norms and fuzzy inference methods",1991,"Fuzzy Sets and Systems","40","3",,"431","450",,245,"10.1016/0165-0114(91)90171-L","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000652239&doi=10.1016%2f0165-0114%2891%2990171-L&partnerID=40&md5=b8e7eacb3444d124f7688e766bab1164","In this paper, the theory of T-norm and T-conorm is reviewed and the T-norm, T-conorm and negation function are defined as a set of T-operators. Some typical T-operators and their mathematical properties are presented. Finally, the T-operators are extended to the conventional fuzzy reasoning methods which are based on the min and max operators. This extended fuzzy reasoning provides both a general and a flexible method for the design of fuzzy logic controllers and, more generally, for the modelling of any decision-making process. © 1991.","fuzzy inference; fuzzy logic controller; T-conorms; T-norms; T-operators",,,,,,,Article,"Final","",Scopus,2-s2.0-0000652239
"Majumder N., Poria S., Gelbukh A., Cambria E.","57203239752;55316592700;24604968400;56140547500;","Deep Learning-Based Document Modeling for Personality Detection from Text",2017,"IEEE Intelligent Systems","32","2","7887639","74","79",,244,"10.1109/MIS.2017.23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017176110&doi=10.1109%2fMIS.2017.23&partnerID=40&md5=f664ffa9ceca5aecc55f0de31b087fd8","This article presents a deep learning based method for determining the author's personality type from text: given a text, the presence or absence of the Big Five traits is detected in the author's psychological profile. For each of the five traits, the authors train a separate binary classifier, with identical architecture, based on a novel document modeling technique. Namely, the classifier is implemented as a specially designed deep convolutional neural network, with injection of the document-level Mairesse features, extracted directly from the text, into an inner layer. The first layers of the network treat each sentence of the text separately; then the sentences are aggregated into the document vector. Filtering out emotionally neutral input sentences improved the performance. This method outperformed the state of the art for all five traits, and the implementation is freely available for research purposes. © 2017 IEEE.","artificial intelligence; convolutional neural network; distributional semantics; intelligent systems; natural language processing; neural-based document modeling; personality","Artificial intelligence; Convolution; Deep learning; Deep neural networks; Intelligent systems; Natural language processing systems; Network layers; Neural networks; Semantics; Convolutional neural network; Distributional semantics; Document model; NAtural language processing; personality; Modeling languages",,,,,,Article,"Final","",Scopus,2-s2.0-85017176110
"Floridi L.","",[No title available],2013,"The Ethics of Information",,,,"","",,244,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84875931041
"Nguyen T.H., Grishman R.","","Relation extraction: Perspective from convolutional neural networks",2015,"Proceedings of NAACL-HLT",,,,"39","48",,243,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84991022526
"Marblestone A.H., Wayne G., Kording K.P.","26645104000;56080177300;6603812799;","Toward an integration of deep learning and neuroscience",2016,"Frontiers in Computational Neuroscience","10","SEP","94","","",,241,"10.3389/fncom.2016.00094","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84989345063&doi=10.3389%2ffncom.2016.00094&partnerID=40&md5=9e9b81f64da9d6c31bf88b06d7b1f182","Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses. © 2016 Marblestone, Wayne and Kording.","Cognitive architecture; Cost functions; Neural networks; Neuroscience","Costs; Deep learning; Digital storage; Electrophysiology; Network architecture; Neural networks; Neurology; Cognitive architectures; Computational problem; Credit assignment; Dedicated systems; Neuroscience; Specialized systems; Specific problems; Training procedures; Cost functions; attention; behavior; brain; learning; long term memory; nerve cell; neuroscience; storage",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-84989345063
"Merity S., Keskar N.S., Socher R.","","Regularizing and optimizing LSTM language models",2017,"Regularizing and Optimizing LSTM Language Models",,,,"","",,240,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85048703118
"Toutanova K., Chen D., Pantel P., Poon H., Choudhury P., Gamon M.","6506107920;56121986100;11139999100;15056546800;56367722200;14018010600;","Representing text for joint embedding of text and knowledge bases",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",,,,"1499","1509",,240,"10.18653/v1/d15-1174","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959903485&doi=10.18653%2fv1%2fd15-1174&partnerID=40&md5=d8a408379243e5e8421d4c31197065dd","Models that learn to represent textual and knowledge base relations in the same continuous latent space are able to perform joint inferences among the two kinds of relations and obtain high accuracy on knowledge base completion (Riedel et al., 2013). In this paper we propose a model that captures the compositional structure of textual relations, and jointly optimizes entity knowledge base, and textual relation representations. The proposed model significantly improves performance over a model that does not share parameters among textual relations with common sub-structure. © 2015 Association for Computational Linguistics.",,"Knowledge based systems; Compositional structure; High-accuracy; Knowledge base; Knowledge basis; Sub-structures; Natural language processing systems","Baidu;Bloomberg;et al.;facebook;Google;Linkedin","Conference on Empirical Methods in Natural Language Processing, EMNLP 2015","17 September 2015 through 21 September 2015",,116677,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84959903485
"Zhang M., Chen Y.","57191868624;57196271259;","Link prediction based on graph neural networks",2018,"Advances in Neural Information Processing Systems","2018-December",,,"5165","5175",,237,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064835122&partnerID=40&md5=e49d0d63acd21faf4b87d7d2be89b383","Link prediction is a key problem for network-structured data. Link prediction heuristics use some score functions, such as common neighbors and Katz index, to measure the likelihood of links. They have obtained wide practical uses due to their simplicity, interpretability, and for some of them, scalability. However, every heuristic has a strong assumption on when two nodes are likely to link, which limits their effectiveness on networks where these assumptions fail. In this regard, a more reasonable way should be learning a suitable heuristic from a given network instead of using predefined ones. By extracting a local subgraph around each target link, we aim to learn a function mapping the subgraph patterns to link existence, thus automatically learning a “heuristic” that suits the current network. In this paper, we study this heuristic learning paradigm for link prediction. First, we develop a novel -decaying heuristic theory. The theory unifies a wide range of heuristics in a single framework, and proves that all these heuristics can be well approximated from local subgraphs. Our results show that local subgraphs reserve rich information related to link existence. Second, based on the -decaying theory, we propose a new method to learn heuristics from local subgraphs using a graph neural network (GNN). Its experimental results show unprecedented performance, working consistently well on a wide range of problems. © 2018 Curran Associates Inc..All rights reserved.",,"Forecasting; Heuristic methods; Function mapping; Graph neural networks; Heuristic learning; Interpretability; Link prediction; Practical use; Score function; Structured data; Graph theory",,"32nd Conference on Neural Information Processing Systems, NeurIPS 2018","2 December 2018 through 8 December 2018",,147412,Conference Paper,"Final","",Scopus,2-s2.0-85064835122
"Manning C.D., Raghavan P., Schutze H.","","Scoring, term weighting and the vector space model",2008,"Introduction to Information Retrieval",,,,"109","133",,237,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-78650343482
"Schöner G., Jiang W.Y., Kelso J.A.S.","7004911210;16185442400;7101702561;","A synergetic theory of quadrupedal gaits and gait transitions",1990,"Journal of Theoretical Biology","142","3",,"359","391",,237,"10.1016/S0022-5193(05)80558-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025064144&doi=10.1016%2fS0022-5193%2805%2980558-2&partnerID=40&md5=4a1dff1d6b3c0a00ad688457cf5b84e2","We present a theoretical analysis of the patterns of interlimb co-ordination in the gaits of quadrupedal locomotion. Introducing as collective variables a set of relative phases that describe the co-ordination patterns, we classify gaits by their symmetry properties, which can be expressed as invariances under groups of transformations. We define dynamics of the collective variables, on which we impose symmetry restrictions. The stable observable gait patterns correspond to atractors of these dynamics. A non-trivial consequence of this theoretical viewpoint is that gait transitions can take the form of non-equilibrium phase transitions that are accompanied by loss of stability. We show how various types of such phase transitions involving hysteresis, slowing down and fluctuation enhancement can occur. Also the difference between smooth and abrupt transitions is given theoretical foundation. While existing experimental evidence is consistent with the theory developed here, we propose new experimental measures that can serve to test the present theoretical framework. Finally, the influence of underlying symmetries of the dynamics on the nature of the gait patterns and their stability is analyzed. For example, breaking of a front-hind symmetry can lead to a change from absolute to relative co-ordination in the sense of von Holst (1939, Ergebnisse der Physiologie 42, 228). Also, differential stability of straight and reverse gaits results from thus lowering the symmetry. © 1990 Academic Press Limited.",,"article; gait; horse; locomotion; nonhuman; priority journal; theoretical study; Gait; Models, Biological; Support, U.S. Gov't, Non-P.H.S.; Support, U.S. Gov't, P.H.S.; Equus caballus",,,,,,Article,"Final","",Scopus,2-s2.0-0025064144
"Speriosu M., Sudan N., Upadhyay S., Baldridge J.","","Twitter polarity classification with label propagation over lexical links and the follower graph",2011,"Proceedings of the First Workshop on Unsupervised Learning in NLP",,,,"53","63",,233,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84868586679
"Lee K., He L., Lewis M., Zettlemoyer L.","56349980800;57155529800;57200329926;57204370628;","End-to-end neural coreference resolution",2017,"EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"188","197",,231,"10.18653/v1/d17-1018","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073172227&doi=10.18653%2fv1%2fd17-1018&partnerID=40&md5=7f02a6252bbbb78204687b756ee135c9","We introduce the first end-to-end coreference resolution model and show that it significantly outperforms all previous work without using a syntactic parser or hand-engineered mention detector. The key idea is to directly consider all spans in a document as potential mentions and learn distributions over possible antecedents for each. The model computes span embeddings that combine context-dependent boundary representations with a head-finding attention mechanism. It is trained to maximize the marginal likelihood of gold antecedent spans from coreference clusters and is factored to enable aggressive pruning of potential mentions. Experiments demonstrate state-of-the-art performance, with a gain of 1.5 F1 on the OntoNotes benchmark and by 3.1 F1 using a 5-model ensemble, despite the fact that this is the first approach to be successfully trained with no external resources. © 2017 Association for Computational Linguistics.",,"Benchmarking; Syntactics; Attention mechanisms; Boundary representations; Co-reference resolutions; Context dependent; External resources; Marginal likelihood; State-of-the-art performance; Syntactic parsers; Natural language processing systems","Amazon;Apple;Baidu;et al.;Facebook;Google","2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017","9 September 2017 through 11 September 2017",,150071,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85073172227
"Bhatia K., Jain H., Kar P., Varma M., Jain P.","57189097357;57197553406;34976937400;24823171100;57209869316;","Sparse local embeddings for extreme multi-label classification",2015,"Advances in Neural Information Processing Systems","2015-January",,,"730","738",,230,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965155847&partnerID=40&md5=d921424e2d46381d93b9e0e0bf28645c","The objective in extreme multi-label learning is to train a classifier that can automatically tag a novel data point with the most relevant subset of labels from an extremely large label set. Embedding based approaches attempt to make training and prediction tractable by assuming that the training label matrix is low-rank and reducing the effective number of labels by projecting the high dimensional label vectors onto a low dimensional linear subspace. Still, leading embedding approaches have been unable to deliver high prediction accuracies, or scale to large problems as the low rank assumption is violated in most real world applications. In this paper we develop the SLEEC classifier to address both limitations. The main technical contribution in SLEEC is a formulation for learning a small ensemble of local distance preserving embeddings which can accurately predict infrequently occurring (tail) labels. This allows SLEEC to break free of the traditional low-rank assumption and boost classification accuracy by learning embeddings which preserve pairwise distances between only the nearest label vectors. We conducted extensive experiments on several real-world, as well as benchmark data sets and compared our method against state-of-the-art methods for extreme multi-label classification. Experiments reveal that SLEEC can make significantly more accurate predictions then the state-of-the-art methods including both embedding-based (by as much as 35%) as well as tree-based (by as much as 6%) methods. SLEEC can also scale efficiently to data sets with a million labels which are beyond the pale of leading embedding methods.",,"Forecasting; Information science; Accurate prediction; Classification accuracy; Multi label classification; Multi-label learning; Pairwise distances; Prediction accuracy; State-of-the-art methods; Technical contribution; Classification (of information)","et al.;Google, Inc.;Ketchum Trading;Microsoft;Taobao (China) Software Co., Ltd. (Alibaba);Twitter","29th Annual Conference on Neural Information Processing Systems, NIPS 2015","7 December 2015 through 12 December 2015",,120037,Conference Paper,"Final","",Scopus,2-s2.0-84965155847
"Xiong C., Merity S., Socher R.","","Dynamic memory networks for visual and textual question answering",2016,"Dynamic Memory Networks for Visual and Textual Question Answering",,,,"2397","2406",,229,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84980035974
"Moore C.","35610564000;","Unpredictability and undecidability in dynamical systems",1990,"Physical Review Letters","64","20",,"2354","2357",,229,"10.1103/PhysRevLett.64.2354","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001102743&doi=10.1103%2fPhysRevLett.64.2354&partnerID=40&md5=00d92b66c7ef16c81fd4d5429e65a079","We show that motion with as few as three degrees of freedom (for instance, a particle moving in a three-dimensional potential) can be equivalent to a Turing machine, and so be capable of universal computation. Such systems possess a type of unpredictability qualitatively stronger than that which has been previously discussed in the study of low-dimensional chaos: Even if the initial conditions are known exactly, virtually any question about their long-term dynamics is undecidable. © 1990 The American Physical Society.",,,,,,,,Article,"Final","",Scopus,2-s2.0-0001102743
"Barzilay R., McKeown K.R.","23007765100;56269175400;","Sentence fusion for multidocument news summarization",2005,"Computational Linguistics","31","3",,"297","327",,225,"10.1162/089120105774321091","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646391652&doi=10.1162%2f089120105774321091&partnerID=40&md5=f1376de513d07a980f72ff04a33af080","A system that can produce informative summaries, highlighting common information found in many online documents, will help Web users to pinpoint information that they need without extensive reading. In this article, we introduce sentence fusion, a novel text-to-text generation technique for synthesizing common information across documents. Sentence fusion involves bottom-up local multisequence alignment to identify phrases conveying similar information and statistical generation to combine common phrases into a sentence. Sentence fusion moves the summarization field from the use of purely extractive methods to the generation of abstracts that contain sentences not found in any of the input documents and can synthesize information across sources. © 2005 Association for Computational Linguistics.",,"Bottom up; Multi-document; News summarization; On-line documents; Sentence fusions; Text generations; Web users",,,,,,Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-33646391652
"Liu J., Chang W.-C., Wu Y., Yang Y.","57195630920;57195631837;57195629718;35231480000;","Deep learning for extreme multi-label text classification",2017,"SIGIR 2017 - Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval",,,,"115","124",,222,"10.1145/3077136.3080834","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029387475&doi=10.1145%2f3077136.3080834&partnerID=40&md5=a051abb8887affb803391328cccc5f45","Extreme multi-label text classification (XMTC) refers to the problem of assigning to each document its most relevant subset of class labels from an extremely large label collection, where the number of labels could reach hundreds of thousands or millions. The huge label space raises research challenges such as data sparsity and scalability. Significant progress has been made in recent years by the development of new machine learning methods, such as tree induction with large-margin partitions of the instance spaces and label-vector embedding in the target space. However, deep learning has not been explored for XMTC, despite its big successes in other related areas. This paper presents the first attempt at applying deep learning to XMTC, with a family of new Convolutional Neural Network (CNN) models which are tailored for multi-label classification in particular. With a comparative evaluation of 7 state-of-The-Art methods on 6 benchmark datasets where the number of labels is up to 670,000, we show that the proposed CNN approach successfully scaled to the largest datasets, and consistently produced the best or the second best results on all the datasets. On the Wikipedia dataset with over 2 million documents and 500,000 labels in particular, it outperformed the second best method by 11:7% ∼ 15:3% in precision@K and by 11:5% ∼ 11:7% in NDCG@K for K = 1,3,5. © 2017 Copyright held by the owner/author(s).",,"Classification (of information); Deep learning; Information retrieval; Information retrieval systems; Learning systems; Neural networks; Vector spaces; Benchmark datasets; Comparative evaluations; Convolutional neural network; Machine learning methods; Multi label classification; Multi-label text classification; Research challenges; State-of-the-art methods; Text processing","Association for Computing Machinery Special Interest Group on Information Retrieval (ACM SIGIR)","40th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2017","7 August 2017 through 11 August 2017",,129692,Conference Paper,"Final","",Scopus,2-s2.0-85029387475
"Choi E., Bahadori M.T., Song L., Stewart W.F., Sun J.","57188811144;55376715600;55587150100;57203078025;9737233900;","GRAM: Graph-based attention model for healthcare representation learning",2017,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","Part F129685",,,"787","795",,220,"10.1145/3097983.3098126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029024456&doi=10.1145%2f3097983.3098126&partnerID=40&md5=dafd9d48230f47683c3d20430c87fa1d","Deep learning methods exhibit promising performance for predictive modeling in healthcare, but two important challenges remain: • Data insufficiency: Often in healthcare predictive modeling, the sample size is insufficient for deep learning methods to achieve satisfactory results. • Interpretation: The representations learned by deep learning methods should align with medical knowledge. To address these challenges, we propose GRaph-based Attention Model (GRAM) that supplements electronic health records (EHR) with hierarchical information inherent to medical ontologies. Based on the data volume and the ontology structure, GRAM represents a medical concept as a combination of its ancestors in the ontology via an attention mechanism. We compared predictive performance (i.e. accuracy, data needs, interpretability) of GRAM to various methods including the recurrent neural network (RNN) in two sequential diagnoses prediction tasks and one heart failure prediction task. Compared to the basic RNN, GRAM achieved 10% higher accuracy for predicting diseases rarely observed in the training data and 3% improved area under the ROC curve for predicting heart failure using an order of magnitude less training data. Additionally, unlike other methods, the medical concept representations learned by GRAM are well aligned with the medical ontology. Finally, GRAM exhibits intuitive attention behaviors by adaptively generalizing to higher level concepts when facing data insufficiency at the lower level concepts. © 2017 ACM.","Attention model; Electronic health records; Graph; Predictive healthcare","Cardiology; Data mining; Deep learning; Forecasting; Graphic methods; Health care; Learning systems; Ontology; Records management; Recurrent neural networks; Attention mechanisms; Attention model; Electronic health record; Graph; Hierarchical information; Predictive performance; Recurrent neural network (RNN); Sequential diagnosis; Diagnosis","ACM SIGKDD;ACM SIGMOD","23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2017","13 August 2017 through 17 August 2017",,129685,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85029024456
"Xie M., Yin H., Wang H., Xu F., Chen W., Wang S.","57188960120;55007318200;36663130400;8552693700;56477550000;55919171700;","Learning graph-based poi embedding for location-based recommendation",2016,"International Conference on Information and Knowledge Management, Proceedings","24-28-October-2016",,,"15","24",,220,"10.1145/2983323.2983711","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84996503921&doi=10.1145%2f2983323.2983711&partnerID=40&md5=60b76ff3f491dfc58f86d7cf4920482a","With the rapid prevalence of smart mobile devices and the dramatic proliferation of location-based social networks (LB-SNs), location-based recommendation has become an important means to help people discover attractive and interesting points of interest (POIs). However, extreme sparsity of user-POI matrix and cold-start issue create severe challenges, causing CF-based methods to degrade significantly in their recommendation performance. Moreover, location-based recommendation requires spatiotemporal context awareness and dynamic tracking of the user's latest preferences in a real-time manner. To address these challenges, we stand on recent advances in embedding learning techniques and propose a generic graph-based embedding model, called GE, in this paper. GE jointly captures the sequential effect, geographical influence, temporal cyclic effect and semantic effect in a unified way by embedding the four corresponding relational graphs (POI-POI, POI-Region, POI-Time and POI-Word) into a shared low dimensional space. Then, to support real-time recommendation, we develop a novel time-decay method to dynamically compute the user's latest preferences based on the embedding of his/her checked-in POIs learnt in the latent s-pace. We conduct extensive experiments to evaluate the performance of our model on two real large-scale datasets, and the experimental results show its superiority over other competitors, especially in recommending cold-start POIs. Besides, we study the contribution of each factor to improve location-based recommendation, and find that both sequential effect and temporal cyclic effect play more important roles than geographical influence and semantic effect. © 2016 ACM.",,"Knowledge management; Location; Semantics; Context- awareness; Interesting points; Large-scale datasets; Learning techniques; Location-based social networks; Low-dimensional spaces; Recommendation performance; Sequential effects; Graphic methods","ACM SIGIR;ACM SIGWEB","25th ACM International Conference on Information and Knowledge Management, CIKM 2016","24 October 2016 through 28 October 2016",,124616,Conference Paper,"Final","",Scopus,2-s2.0-84996503921
"Smith J.C., Abdala A.P.L., Borgmann A., Rybak I.A., Paton J.F.R.","7410174798;6603735500;21741870600;57191881039;55486090800;","Brainstem respiratory networks: Building blocks and microcircuits",2013,"Trends in Neurosciences","36","3",,"152","162",,220,"10.1016/j.tins.2012.11.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84875261242&doi=10.1016%2fj.tins.2012.11.004&partnerID=40&md5=14e6813b040a28583f144f4ffd4959c7","Breathing movements in mammals are driven by rhythmic neural activity generated within spatially and functionally organized brainstem neural circuits comprising the respiratory central pattern generator (CPG). This rhythmic activity provides homeostatic regulation of gases in blood and tissues and integrates breathing with other motor acts. We review new insights into the spatial-functional organization of key neural microcircuits of this CPG from recent multidisciplinary experimental and computational studies. The emerging view is that the microcircuit organization within the CPG allows the generation of multiple rhythmic breathing patterns and adaptive switching between them, depending on physiological or pathophysiological conditions. These insights open the possibility for site- and mechanism-specific interventions to treat various disorders of the neural control of breathing. © 2012.","Brainstem; Breathing; Breathing disorders; Respiratory central pattern generator","brain stem; breathing pattern; central pattern generator; congenital central hypoventilation syndrome; human; nonhuman; pathophysiology; pontine nucleus; priority journal; respiration depression; Rett syndrome; review; sleep disordered breathing; solitary tract nucleus; sudden infant death syndrome; synaptic inhibition; Afferent Pathways; Animals; Biological Clocks; Brain Stem; Chemoreceptor Cells; Exhalation; Homeostasis; Humans; Inhalation; Models, Neurological; Nerve Net; Pons; Raphe Nuclei; Respiration Disorders; Respiratory Center; Respiratory Physiological Phenomena; Solitary Nucleus; Spinal Cord; Structure-Activity Relationship",,,,,,Review,"Final","All Open Access, Green",Scopus,2-s2.0-84875261242
"Rabinovich M.I., Huerta R., Varona P., Afraimovich V.S.","35548393700;7005773603;55886441700;7004340229;","Transient cognitive dynamics, metastability, and decision making",2008,"PLoS Computational Biology","4","5","e1000072","","",,219,"10.1371/journal.pcbi.1000072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-44349183017&doi=10.1371%2fjournal.pcbi.1000072&partnerID=40&md5=6f38af983428621b9c269c28b0326622","The idea that cognitive activity can be understood using nonlinear dynamics has been intensively discussed at length for the last 15 years. One of the popular points of view is that metastable states play a key role in the execution of cognitive functions. Experimental and modeling studies suggest that most of these functions are the result of transient activity of large-scale brain networks in the presence of noise. Such transients may consist of a sequential switching between different metastable cognitive states. The main problem faced when using dynamical theory to describe transient cognitive processes is the fundamental contradiction between reproducibility and flexibility of transient behavior. In this paper, we propose a theoretical description of transient cognitive dynamics based on the interaction of functionally dependent metastable cognitive states. The mathematical image of such transient activity is a stable heteroclinic channel, i.e., a set of trajectories in the vicinity of a heteroclinic skeleton that consists of saddles and unstable separatrices that connect their surroundings. We suggest a basic mathematical model, a strongly dissipative dynamical system, and formulate the conditions for the robustness and reproducibility of cognitive transients that satisfy the competing requirements for stability and flexibility. Based on this approach, we describe here an effective solution for the problem of sequential decision making, represented as a fixed time game: a player takes sequential actions in a changing noisy environment so as to maximize a cumulative reward. As we predict and verify in computer simulations, noise plays an important role in optimizing the gain. © 2008 Rabinovich et al.",,"article; brain function; cognition; computer simulation; decision making; mathematical model; metastability; nonlinear system; reproducibility; animal; biological model; game; human; physiology; Animals; Cognition; Computer Simulation; Decision Making; Game Theory; Humans; Models, Biological",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-44349183017
"Džeroski S., De Raedt L., Driessens K.","7006806328;55760010700;8593708100;","Relational reinforcement learning",2001,"Machine Learning","43","1-2",,"7","52",,214,"10.1023/A:1007694015589","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035312760&doi=10.1023%2fA%3a1007694015589&partnerID=40&md5=5f9df2c71d2e6fee022bd739df3cfbd0","Relational reinforcement learning is presented, a learning technique that combines reinforcement learning with relational learning or inductive logic programming. Due to the use of a more expressive representation language to represent states, actions and Q-functions, relational reinforcement learning can be potentially applied to a new range of learning tasks. One such task that we investigate is planning in the blocks world, where it is assumed that the effects of the actions are unknown to the agent and the agent has to learn a policy. Within this simple domain we show that relational reinforcement learning solves some existing problems with reinforcement learning. In particular, relational reinforcement learning allows us to employ structural representations, to abstract from specific goals pursued and to exploit the results of previous learning phases when addressing new (more complex) situations.","Inductive logic programming; Planning; Reinforcement learning","Computer programming languages; Functions; Knowledge representation; Logic programming; Problem solving; Inductive logic programming; Q functions; Relational reinforcement learning; Representation language; Structural representation; Learning systems",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-0035312760
"Poria S., Mazumder N., Cambria E., Hazarika D., Morency L.-P., Zadeh A.","55316592700;57203239752;56140547500;57200336259;6603047400;57144043100;","Context-dependent sentiment analysis in user-generated videos",2017,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)","1",,,"873","883",,209,"10.18653/v1/P17-1081","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039788178&doi=10.18653%2fv1%2fP17-1081&partnerID=40&md5=6b849ef47c3394272415904b7bb2c78b","Multimodal sentiment analysis is a developing area of research, which involves the identification of sentiments in videos. Current research considers utterances as independent entities, i.e., ignores the inter-dependencies and relations among the utterances of a video. In this paper, we propose a LSTM-based model that enables utterances to capture contextual information from their surroundings in the same video, thus aiding the classification process. Our method shows 5-10% performance improvement over the state of the art and high robustness to generalizability. © 2017 Association for Computational Linguistics.",,"Computational linguistics; Data mining; Linguistics; Long short-term memory; Classification process; Context dependent; Contextual information; High robustness; Inter-dependencies; Sentiment analysis; State of the art; User-generated video; Classification (of information)","Amazon;Apple;Baidu;et al.;Google;Tencent","55th Annual Meeting of the Association for Computational Linguistics, ACL 2017","30 July 2017 through 4 August 2017",,132950,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85039788178
"Wan X., Xiao J.","7202533498;35520554800;","Single document keyphrase extraction using neighborhood knowledge",2008,"Proceedings of the National Conference on Artificial Intelligence","2",,,"855","860",,207,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749198218&partnerID=40&md5=976fdcfe23d169035dbb314d62c3a43b","Existing methods for single document keyphrase extraction usually make use of only the information contained in the specified document. This paper proposes to use a small number of nearest neighbor documents to provide more knowledge to improve single document keyphrase extraction. A specified document is expanded to a small document set by adding a few neighbor documents close to the document, and the graph-based ranking algorithm is then applied on the expanded document set to make use of both the local information in the specified document and the global information in the neighbor documents. Experimental results demonstrate the good effectiveness and robustness of our proposed approach. Copyright © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Bionics; Document sets; Existing methods; Global informations; Keyphrase extractions; Local informations; Nearest neighbors; Ranking algorithms; Artificial intelligence","Cornell Univ. Intelligent Information Systems Inst.;Toyota Motor Engineering and Manufacturing North America Inc.;ACM/SIGART;Boeing","23rd AAAI Conference on Artificial Intelligence and the 20th Innovative Applications of Artificial Intelligence Conference, AAAI-08/IAAI-08","13 July 2008 through 17 July 2008","Chicago, IL",74777,Conference Paper,"Final","",Scopus,2-s2.0-57749198218
"Bowman S.R., Angeli G., Potts C., Manning C.D.","","A large annotated corpus for learning natural language inference",2015,"Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing (EMNLP)",,,,"","",,206,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84994092258
"Weston J., Chopra S., Bordes A.","8865128200;56248489500;18933923000;","Memory networks",2015,"3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings",,,,"","",,206,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083951616&partnerID=40&md5=8194f9dd15bd92b57e7f801973200231","We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs. © 2015 International Conference on Learning Representations, ICLR. All rights reserved.",,"Knowledge base; Learning models; Long term memory; Memory network; Question Answering; Knowledge based systems",,"3rd International Conference on Learning Representations, ICLR 2015","7 May 2015 through 9 May 2015",,149801,Conference Paper,"Final","",Scopus,2-s2.0-85083951616
"Jaeger H.","","The echo state approach to analysing and training recurrent neural networks-with an erratum note",2001,"Bonn, Germany: German National Research Center for Information Technology GMD Technical Report","148","34",,"","",,204,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84956927934
"Yilmaz E., Kanoulas E., Aslam J.A.","57203056765;55888206200;7004337816;","A simple and efficient sampling method for estimating AP and NDCG",2008,"ACM SIGIR 2008 - 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Proceedings",,,,"603","610",,203,"10.1145/1390334.1390437","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349107098&doi=10.1145%2f1390334.1390437&partnerID=40&md5=597dc47d7a266baaab64a3cc8e17dff4","We consider the problem of large scale retrieval evaluation. Recently two methods based on random sampling were proposed as a solution to the extensive effort required to judge tens of thousands of documents. While the first method proposed by Aslam et al. [1] is quite accurate and efficient, it is overly complex, making it difficult to be used by the community, and while the second method proposed by Yilmaz et al., infAP [14], is relatively simple, it is less efficient than the former since it employs uniform random sampling from the set of complete judgments. Further, none of these methods provide confidence intervals on the estimated values. The contribution of this paper is threefold: (1) we derive confidence intervals for infAP, (2) we extend infAP to incorporate nonrandom relevance judgments by employing stratified random sampling, hence combining the efficiency of stratification with the simplicity of random sampling, (3) we describe how this approach can be utilized to estimate nDCG from incomplete judgments. We validate the proposed methods using TREC data and demonstrate that these new methods can be used to incorporate nonrandom samples, as were available in TREC Terabyte track '06. Copyright 2008 ACM.","Average precision; Evaluation; Incomplete judgments; InfAP; nDCG; Sampling","Average precision; Evaluation; Incomplete judgments; InfAP; nDCG; Information retrieval; Information retrieval systems; Information services; Research and development management; Random processes",,"31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, ACM SIGIR 2008","20 July 2008 through 24 July 2008","Singapore",74470,Conference Paper,"Final","",Scopus,2-s2.0-57349107098
"Christiansen M.H., Chater N.","7102525296;7007185366;","Toward a connectionist model of recursion in human linguistic performance",1999,"Cognitive Science","23","2",,"157","205",,203,"10.1207/s15516709cog2302_2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000046043&doi=10.1207%2fs15516709cog2302_2&partnerID=40&md5=14a8f11a9ff5740b4c23149e847accb1","Naturally occurring speech contains only a limited amount of complex recursive structure, and this is reflected in the empirically documented difficulties that people experience when processing such structures. We present a connectionist model of human performance in processing recursive language structures. The model is trained on simple artificial languages. We find that the qualitative performance profile of the model matches human behavior, both on the relative difficulty of center-embedding and cross-dependency, and between the processing of these complex recursive structures and right-branching recursive constructions. We analyze how these differences in performance are reflected in the internal representations of the model by performing discriminant analyses on these representations both before and after training. Furthermore, we show how a network trained to process recursive structures can also generate such structures in a probabilistic fashion. This work suggests a novel explanation of people's limited recursive performance, without assuming the existence of a mentally represented competence grammar allowing unbounded recursion. © 1999 Cognitive Science Society, Inc.",,,,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-0000046043
"Cambria E., Poria S., Hazarika D., Kwok K.","56140547500;55316592700;57200336259;57226032645;","SenticNet 5: Discovering conceptual primitives for sentiment analysis by means of context embeddings",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"1795","1802",,202,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039765367&partnerID=40&md5=20cafce6422fa49d88f5ce3fe49c2040","With the recent development of deep learning, research in AI has gained new vigor and prominence. While machine learning has succeeded in revitalizing many research fields, such as computer vision, speech recognition, and medical diagnosis, we are yet to witness impressive progress in natural language understanding. One of the reasons behind this unmatched expectation is that, while a bottom-up approach is feasible for pattern recognition, reasoning and understanding often require a top-down approach. In this work, we couple sub-symbolic and symbolic AI to automatically discover conceptual primitives from text and link them to commonsense concepts and named entities in a new three-level knowledge representation for sentiment analysis. In particular, we employ recurrent neural networks to infer primitives by lexical substitution and use them for grounding common and commonsense knowledge by means of multi-dimensional scaling. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Data mining; Deep learning; Diagnosis; Knowledge representation; Recurrent neural networks; Sentiment analysis; Speech recognition; Bottom up approach; Commonsense knowledge; Multi-dimensional scaling; Named entities; Natural language understanding; Research fields; Sub-symbolic; Top down approaches; Natural language processing systems","Association for the Advancement of Artificial Intelligence","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","2 February 2018 through 7 February 2018",,143510,Conference Paper,"Final","",Scopus,2-s2.0-85039765367
"Dredze M., Mcnamee P., Rao D., Gerber A., Finin T.","14041686400;35230438500;56209934500;52163345200;7003679538;","Entity disambiguation for knowledge base population",2010,"Coling 2010 - 23rd International Conference on Computational Linguistics, Proceedings of the Conference","2",,,"277","285",,202,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053221437&partnerID=40&md5=f56c6df6f068b9f3cf9c03bdd25dc2ce","The integration of facts derived from information extraction systems into existing knowledge bases requires a system to disambiguate entity mentions in the text. This is challenging due to issues such as non-uniform variations in entity names, mention ambiguity, and entities absent from a knowledge base. We present a state of the art system for entity disambiguation that not only addresses these challenges but also scales to knowledge bases with several million entries using very little resources. Further, our approach achieves performance of up to 95% on entities mentioned from newswire and 80% on a public test set that was designed to include challenging queries.",,"Entity disambiguation; Information extraction systems; Knowledge base; Knowledge basis; State-of-the-art system; Test sets; Computational linguistics; Information retrieval systems; Knowledge based systems","National Natural Science Foundation of China;Dep. Lang. Inf. Adm., Minist. Educ.;BaiDu;Google;Fujitsu R and D Center CO., LTD.","23rd International Conference on Computational Linguistics, Coling 2010","23 August 2010 through 27 August 2010","Beijing",86715,Conference Paper,"Final","",Scopus,2-s2.0-80053221437
"Siegelmann H.T., Sontag E.D.","57189345629;7102557112;","Turing computability with neural nets",1991,"Applied Mathematics Letters","4","6",,"77","80",,200,"10.1016/0893-9659(91)90080-F","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002663413&doi=10.1016%2f0893-9659%2891%2990080-F&partnerID=40&md5=e2f3a7d669ea5e91b062e40bf13762f0","This paper shows the existence of a finite neural network, made up of sigmoidal neurons, which simulates a universal Turing machine. It is composed of less than 105 synchronously evolving processors, interconnected linearly. High-order connections are not required. © 1991.",,,,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-0002663413
"Cambria E., Poria S., Bajpai R., Schuller B.","56140547500;55316592700;56875259000;6603767415;","SenticNet 4: A semantic resource for sentiment analysis based on conceptual primitives",2016,"COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers",,,,"2666","2677",,197,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046586891&partnerID=40&md5=24955d2efda5f17971a03215a990ee82","An important difference between traditional AI systems and human intelligence is the human ability to harness commonsense knowledge gleaned from a lifetime of learning and experience to make informed decisions. This allows humans to adapt easily to novel situations where AI fails catastrophically due to a lack of situation-specific rules and generalization capabilities. Commonsense knowledge also provides background information that enables humans to successfully operate in social situations where such knowledge is typically assumed. Since commonsense consists of information that humans take for granted, gathering it is an extremely difficult task. Previous versions of SenticNet were focused on collecting this kind of knowledge for sentiment analysis but they were heavily limited by their inability to generalize. SenticNet 4 overcomes such limitations by leveraging on conceptual primitives automatically generated by means of hierarchical clustering and dimensionality reduction. © 1963-2018 ACL.",,"Computational linguistics; Semantics; Sentiment analysis; Automatically generated; Background information; Commonsense knowledge; Dimensionality reduction; Generalization capability; Hier-archical clustering; Human intelligence; Semantic resources; Data mining",,"26th International Conference on Computational Linguistics, COLING 2016","11 December 2016 through 16 December 2016",,136517,Conference Paper,"Final","",Scopus,2-s2.0-85046586891
"Vieira S.M., Mendonça L.F., Farinha G.J., Sousa J.M.C.","8542158100;6701792779;55368146000;35547129100;","Modified binary PSO for feature selection using SVM applied to mortality prediction of septic patients",2013,"Applied Soft Computing Journal","13","8",,"3494","3504",,197,"10.1016/j.asoc.2013.03.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878154666&doi=10.1016%2fj.asoc.2013.03.021&partnerID=40&md5=6be1e794e2b916a12f5a62da84dfee3d","This paper proposes a modified binary particle swarm optimization (MBPSO) method for feature selection with the simultaneous optimization of SVM kernel parameter setting, applied to mortality prediction in septic patients. An enhanced version of binary particle swarm optimization, designed to cope with premature convergence of the BPSO algorithm is proposed. MBPSO control the swarm variability using the velocity and the similarity between best swarm solutions. This paper uses support vector machines in a wrapper approach, where the kernel parameters are optimized at the same time. The approach is applied to predict the outcome (survived or deceased) of patients with septic shock. Further, MBPSO is tested in several benchmark datasets and is compared with other PSO based algorithms and genetic algorithms (GA). The experimental results showed that the proposed approach can correctly select the discriminating input features and also achieve high classification accuracy, specially when compared to other PSO based algorithms. When compared to GA, MBPSO is similar in terms of accuracy, but the subset solutions have less selected features. © 2013 Elsevier B.V. All rights reserved.","Feature selection; Particle swarm optimization; Premature convergence; Sepsis; Support vector machines; Wrapper methods","Benchmark datasets; Binary particle swarm optimization; Classification accuracy; Kernel parameter; Pre-mature convergences; Sepsis; Simultaneous optimization; Wrapper methods; Feature extraction; Forecasting; Genetic algorithms; Particle swarm optimization (PSO); Support vector machines",,,,,,Article,"Final","",Scopus,2-s2.0-84878154666
"Morency L.-P., Mihalcea R., Doshi P.","6603047400;8619220500;54683665100;","Towards multimodal sentiment analysis: Harvesting opinions from the web",2011,"ICMI'11 - Proceedings of the 2011 ACM International Conference on Multimodal Interaction",,,,"169","176",,197,"10.1145/2070481.2070509","https://www.scopus.com/inward/record.uri?eid=2-s2.0-83455176765&doi=10.1145%2f2070481.2070509&partnerID=40&md5=2c91e12d1ed6a64dca1cacde4a12b934","With more than 10,000 new videos posted online every day on social websites such as YouTube and Facebook, the internet is becoming an almost infinite source of information. One crucial challenge for the coming decade is to be able to harvest relevant information from this constant flow of multimodal data. This paper addresses the task of multimodal sentiment analysis, and conducts proof-of-concept experiments that demonstrate that a joint model that integrates visual, audio, and textual features can be effectively used to identify sentiment in Web videos. This paper makes three important contributions. First, it addresses for the first time the task of tri-modal sentiment analysis, and shows that it is a feasible task that can benefit from the joint exploitation of visual, audio and textual modalities. Second, it identifies a subset of audio-visual features relevant to sentiment analysis and present guidelines on how to integrate these features. Finally, it introduces a new dataset consisting of real online data, which will be useful for future research in this area. © 2011 ACM.","audio-visual integration; multimodal signal processing; subjectivity and sentiment analysis; YouTube videos","Audio-visual features; Audio-visual integration; Constant flow; Data sets; Facebook; Joint models; Multi-modal; Multi-modal data; multimodal signal processing; Online data; Proof of concept; Sentiment analysis; Web video; YouTube; Data processing; Interactive computer systems; Signal processing; Websites; Data mining","Special Interest Group on Computer-Human Interaction (ACM SIGCHI)","2011 ACM International Conference on Multimodal Interaction, ICMI'11","14 November 2011 through 18 November 2011","Alicante",87685,Conference Paper,"Final","",Scopus,2-s2.0-83455176765
"Chen H., Sun M., Tu C., Lin Y., Liu Z.","57211736069;7403180987;56403932800;57155321900;57191691341;","Neural sentiment classification with user and product attention",2016,"EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"1650","1659",,196,"10.18653/v1/d16-1171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045912242&doi=10.18653%2fv1%2fd16-1171&partnerID=40&md5=a1ffbe47368916225c9b2dbcddd57e25","Document-level sentiment classification aims to predict user's overall sentiment in a document about a product. However, most of existing methods only focus on local text information and ignore the global user preference and product characteristics. Even though some works take such information into account, they usually suffer from high model complexity and only consider word-level preference rather than semantic levels. To address this issue, we propose a hierarchical neural network to incorporate global user and product information into sentiment classification. Our model first builds a hierarchical LSTM model to generate sentence and document representations. Afterwards, user and product information is considered via attentions over different semantic levels due to its ability of capturing crucial semantic components. The experimental results show that our model achieves significant and consistent improvements compared to all state-of-the-art methods. The source code of this paper can be obtained from https://github.com/thunlp/NSC. © 2016 Association for Computational Linguistics",,"Classification (of information); Information retrieval systems; Natural language processing systems; Semantics; Document Representation; Hierarchical neural networks; Product characteristics; Product information; Semantic components; Sentiment classification; State-of-the-art methods; Text information; Long short-term memory","Amazon.com;Baidu;et al.;Google;Grammarly;Microsoft","2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016","1 November 2016 through 5 November 2016",,150070,Conference Paper,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85045912242
"Kok S., Domingos P.","8425432600;7003565655;","Learning the structure of markov logic networks",2005,"ICML 2005 - Proceedings of the 22nd International Conference on Machine Learning",,,,"441","448",,196,"10.1145/1102351.1102407","https://www.scopus.com/inward/record.uri?eid=2-s2.0-31844432693&doi=10.1145%2f1102351.1102407&partnerID=40&md5=0398dfe7d24e0e2179eb4886e75bff9a","Markov logic networks (MLNs) combine logic and probability by attaching weights to first-order clauses, and viewing these as templates for features of Markov networks. In this paper we develop an algorithm for learning the structure of MLNs from relational databases, combining ideas from inductive logic programming (ILP) and feature induction in Markov networks. The algorithm performs a beam or shortestfirst search of the space of clauses, guided by a weighted pseudo-likelihood measure. This requires computing the optimal weights for each candidate structure, but we show how this can be done efficiently. The algorithm can be used to learn an MLN from scratch, or to refine an existing knowledge base. We have applied it in two real-world domains, and found that it outperforms using off-the-shelf ILP systems to learn the MLN structure, as well as pure ILP, purely probabilistic and purely knowledge-based approaches.",,"Algorithms; Formal logic; Markov processes; Mathematical programming; Probability; Inductive logic programming (ILP); Markov logic networks; Real-world domains; Learning systems","Yahoo! Inc;Microsoft Corp.;Daimler Chrysler;Boeing Co;National ICT Australia;et al","ICML 2005: 22nd International Conference on Machine Learning","7 August 2005 through 11 August 2005","Bonn",66483,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-31844432693
"Bloomfield L.","","A set of postulates for the science of language",1926,"Language","2","3",,"153","164",,194,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0040153743
"Cambria E., Poria S., Gelbukh A., Thelwall M.","56140547500;55316592700;24604968400;55396590500;","Sentiment Analysis Is a Big Suitcase",2017,"IEEE Intelligent Systems","32","6","8267597","74","80",,193,"10.1109/MIS.2017.4531228","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038582534&doi=10.1109%2fMIS.2017.4531228&partnerID=40&md5=a1a4abcf7cd5bd1c41b4e88e7e3d64f7","Although most works approach it as a simple categorization problem, sentiment analysis is actually a suitcase research problem that requires tackling many natural language processing (NLP) tasks. The expression 'sentiment analysis' itself is a big suitcase (like many others related to affective computing, such as emotion recognition or opinion mining) that all of us use to encapsulate our jumbled idea about how our minds convey emotions and opinions through natural language. The authors address the composite nature of the problem via a three-layer structure inspired by the 'jumping NLP curves' paradigm. In particular, they argue that there are (at least) 15 NLP problems that need to be solved to achieve human-like performance in sentiment analysis. © 2017 IEEE.","artificial intelligence; computational linguistics; intelligent systems; knowledge representation and reasoning; machine learning; natural language processing; sentiment analysis","Artificial intelligence; Computational linguistics; Data mining; Intelligent systems; Knowledge representation; Learning algorithms; Learning systems; Affective Computing; Emotion recognition; Knowledge representation and reasoning; Natural languages; Opinion mining; Research problems; Sentiment analysis; Three-layer structures; Natural language processing systems",,,,,,Article,"Final","",Scopus,2-s2.0-85038582534
"Prabhu Y., Varma M.","56017115600;24823171100;","FastXML: A fast, accurate and stable tree-classifier for extreme multi-label learning",2014,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"263","272",,192,"10.1145/2623330.2623651","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907027193&doi=10.1145%2f2623330.2623651&partnerID=40&md5=908d068a3533a000403c6340651b4313","The objective in extreme multi-label classification is to learn a classifier that can automatically tag a data point with the most relevant subset of labels from a large label set. Extreme multi-label classification is an important research problem since not only does it enable the tackling of applications with many labels but it also allows the reformulation of ranking problems with certain advantages over existing formulations. Our objective, in this paper, is to develop an extreme multi-label classifier that is faster to train and more accurate at prediction than the state-of-the-art Multi-label Random Forest (MLRF) algorithm [2] and the Label Partitioning for Sub-linear Ranking (LPSR) algorithm [35]. MLRF and LPSR learn a hierarchy to deal with the large number of labels but optimize task independent measures, such as the Gini index or clustering error, in order to learn the hierarchy. Our proposed FastXML algorithm achieves significantly higher accuracies by directly optimizing an nDCG based ranking loss function. We also develop an alternating minimization algorithm for efficiently optimizing the proposed formulation. Experiments reveal that FastXML can be trained on problems with more than a million labels on a standard desktop in eight hours using a single core and in an hour using multiple cores. © 2014 ACM.","extreme classification; multi-label learning; ranking",,"ACM SIGKDD;ACM SIGMOD","20th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2014","24 August 2014 through 27 August 2014","New York, NY",107474,Conference Paper,"Final","",Scopus,2-s2.0-84907027193
"Kohonen T., Somervuo P.","7005640110;6602669606;","Self-organizing maps of symbol strings",1998,"Neurocomputing","21","1-3",,"19","30",,192,"10.1016/S0925-2312(98)00031-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0344972931&doi=10.1016%2fS0925-2312%2898%2900031-9&partnerID=40&md5=0dc6bd35d6e219ea893b26b62b75a929","Unsupervised self-organizing maps (SOMs), as well as supervised learning by Learning Vector Quantization (LVQ) can be defined for string variables, too. Their computing becomes possible when the SOM and the LVQ algorithms are expressed as batch versions, and when the average over a list of symbol strings is defined to be the string that has the smallest sum of generalized distance functions from all the other strings.Unsupervised self-organizing maps (SOMs), as well as supervised learning by Learning Vector Quantization (LVQ) can be defined for string variables, too. Their computing becomes possible when the SOM and the LVQ algorithms are expressed as batch versions, and when the average over a list of symbol strings is defined to be the string that has the smallest sum of generalized distance functions from all the other strings.","Learning vector quantization; Self-organizing map; String clustering","Algorithms; Codes (symbols); Learning systems; Vector quantization; Learning vector quantization (LVQ); Self-organizing map (SOM); String clustering; Symbol strings; Neural networks; algorithm; article; associative memory; clinical article; cluster analysis; human; learning; mathematical computing; nerve cell network; phoneme; priority journal; symbolism; system analysis; word recognition",,,,,,Article,"Final","",Scopus,2-s2.0-0344972931
"Radev D.R., McKeown K.R.","7006578526;56269175400;","Generating Natural Language Summaries from Multiple On-Line Sources",1998,"Computational Linguistics","24","3",,"468","500",,192,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0000670441&partnerID=40&md5=38dbe0d72bcd0c8d9812b74c0ec54f58","We present a methodology for summarization of news about current events in the form of briefings that include appropriate background (historical) information. The system that we developed, SUMMONS, uses the output of systems developed for the DARPA Message Understanding Conferences to generate summaries of multiple documents on the same or related events, presenting similarities and differences, contradictions, and generalizations among sources of information. We describe the various components of the system, showing how information from multiple articles is combined, organized into a paragraph, and finally, realized as English sentences. A feature of our work is the extraction of descriptions of entities such as people and places for reuse to enhance a briefing.",,"Current events; English sentences; Line sources; Message understanding conferences; Multiple documents; Natural languages; Sources of informations; Natural language processing systems",,,,,,Article,"Final","",Scopus,2-s2.0-0000670441
"Zhang H.-Y., Wang J.-Q., Chen X.-H.","36601805500;56132836000;56888182800;","Interval neutrosophic sets and their application in multicriteria decision making problems",2014,"The Scientific World Journal","2014",,"645953","","",,191,"10.1155/2014/645953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896337799&doi=10.1155%2f2014%2f645953&partnerID=40&md5=b9e601352cd1e2c81911fce5f02425c4","As a generalization of fuzzy sets and intuitionistic fuzzy sets, neutrosophic sets have been developed to represent uncertain, imprecise, incomplete, and inconsistent information existing in the real world. And interval neutrosophic sets (INSs) have been proposed exactly to address issues with a set of numbers in the real unit interval, not just a specific number. However, there are fewer reliable operations for INSs, as well as the INS aggregation operators and decision making method. For this purpose, the operations for INSs are defined and a comparison approach is put forward based on the related research of interval valued intuitionistic fuzzy sets (IVIFSs) in this paper. On the basis of the operations and comparison approach, two interval neutrosophic number aggregation operators are developed. Then, a method for multicriteria decision making problems is explored applying the aggregation operators. In addition, an example is provided to illustrate the application of the proposed method. © 2014 Hong-yu Zhang et al.",,"article; controlled study; decision making; fuzzy logic; fuzzy system; interval neutrosophic set; interval valued intuitionistic fuzzy set; decision support system; theoretical model; Decision Support Techniques; Models, Theoretical",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-84896337799
"Socher R., Manning C.D., Ng A.Y.","","Learning continuous phrase representations and syntactic parsing with recursive neural networks",2010,"Proceedings of the NIPS-2010 Deep Learning and Unsupervised Feature Learning Workshop",,,,"1","9",,191,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-80053234189
"Tellex S., Katz B., Fernandes A., Marton G.","7801643708;7203039606;7201782144;36766604500;","Quantitative Evaluation of Passage Retrieval Algorithms for Question Answering",2003,"SIGIR Forum (ACM Special Interest Group on Information Retrieval)",,"SPEC. ISS.",,"41","47",,190,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-1542377556&partnerID=40&md5=f3b1eed00fff1732634e8da47cb1f68e","Passage retrieval is an important component common to many question answering systems. Because most evaluations of question answering systems focus on end-to-end performance, comparison of common components becomes difficult. To address this shortcoming, we present a quantitative evaluation of various passage retrieval algorithms for question answering, implemented in a framework called Pauchok. We present three important findings: Boolean querying schemes perform well in the question answering task. The performance differences between various passage retrieval algorithms vary with the choice of document retriever, which suggests significant interactions between document retrieval and passage retrieval. The best algorithms in our evaluation employ density-based measures for scoring query terms. Our results reveal future directions for passage retrieval and question answering.","Passage retrieval; Question answering","Algorithms; Computer software; Data storage equipment; Large scale systems; Natural language processing systems; Problem solving; Query languages; Search engines; Passage retrieval; Question answering (QA); Information retrieval","ACM/SIGIR","Proceedings of the Twenty-Sixth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2003","28 July 2003 through 1 August 2003","Toronto, Ont.",62465,Conference Paper,"Final","",Scopus,2-s2.0-1542377556
"Niu F., Ré C., Doan A., Shavlik J.","35786676400;10739281400;6701481293;7004146387;","Tuffy: Scaling up statistical inference in markov logic networks using an RDBMS",2011,"Proceedings of the VLDB Endowment","4","6",,"373","384",,188,"10.14778/1978665.1978669","https://www.scopus.com/inward/record.uri?eid=2-s2.0-83055176154&doi=10.14778%2f1978665.1978669&partnerID=40&md5=d935b8d072ecd56d90d77948f94d15d9","Markov Logic Networks (MLNs) have emerged as a powerful framework that combines statistical and logical reasoning; they have been applied to many data intensive problems in-cluding information extraction, entity resolution, and text mining. Current implementations of MLNs do not scale to large real-world data sets, which is preventing their wide-spread adoption. We present Tuffy that achieves scalabil-ity via three novel contributions: (1) a bottom-up approach to grounding that allows us to leverage the full power of the relational optimizer, (2) a novel hybrid architecture that al-lows us to perform AI-style local search efficiently using an RDBMS, and (3) a theoretical insight that shows when one can (exponentially) improve the efficiency of stochastic local search. We leverage (3) to build novel partitioning, loading, and parallel algorithms. We show that our approach outper-forms state-of-the-art implementations in both quality and speed on several publicly available datasets. © 2011 VLDB Endowment.",,"Computer circuits; Local search (optimization); Markov processes; Probabilistic logics; Stochastic systems; Virtual reality; Bottom up approach; Entity resolutions; Hybrid architectures; Logical reasoning; Markov logic networks; State of the art; Statistical inference; Stochastic local searches; Data mining",,,,,,Article,"Final","",Scopus,2-s2.0-83055176154
"Neville J., Jensen D.","7006145328;7402549436;","Relational dependency networks",2007,"Journal of Machine Learning Research","8",,,"653","692",,188,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33947664999&partnerID=40&md5=ad993916378590d338cb4f4253450485","Recent work on graphical models for relational data has demonstrated significant improvements in classification and inference when models represent the dependencies among instances. Despite its use in conventional statistical models, the assumption of instance independence is contradicted by most relational data sets. For example, in citation data there are dependencies among the topics of a paper's references, and in genomic data there are dependencies among the functions of interacting proteins. In this paper, we present relational dependency networks (RDNs), graphical models that are capable of expressing and reasoning with such dependencies in a relational setting. We discuss RDNs in the context of relational Bayes networks and relational Markov networks and outline the relative strengths of RDNs - namely, the ability to represent cyclic dependencies, simple methods for parameter estimation, and efficient structure learning techniques. The strengths of RDNs are due to the use of pseudolikelihood learning techniques, which estimate an efficient approximation of the full joint distribution. We present learned RDNs for a number of real-world data sets and evaluate the models in a prediction context, showing that RDNs identify and exploit cyclic relational dependencies to achieve significant performance gains over conventional conditional models. In addition, we use synthetic data to explore model performance under various relational data characteristics, showing that RDN learning and inference techniques are accurate over a wide range of conditions.","Dependency networks; Graphical models; Knowledge discovery; Probabilistic relational models; Pseudolikelihood estimation; Relational learning","Data mining; Function evaluation; Graphic methods; Markov processes; Mathematical models; Proteins; Dependency networks; Graphical models; Probabilistic relational models; Pseudolikelihood estimation; Relational learning; Learning systems",,,,,,Article,"Final","",Scopus,2-s2.0-33947664999
"Clarke J., Lapata M.","57198946864;55910108500;","Global inference for sentence compression an integer linear programming approach",2008,"Journal of Artificial Intelligence Research","31",,,"399","429",,187,"10.1613/jair.2433","https://www.scopus.com/inward/record.uri?eid=2-s2.0-44449150380&doi=10.1613%2fjair.2433&partnerID=40&md5=67f2379450d3c6f9dfbfd3a1f5638e2a","Sentence compression holds promise for many applications ranging from summarization to subtitle generation. Our work views sentence compression as an optimization problem and uses integer linear programming (ILP) to infer globally optimal compressions in the presence of linguistically motivated constraints. We show how previous formulations of sentence compression can be recast as ILPs and extend these models with novel global constraints. Experimental results on written and spoken texts demonstrate improvements over state-of-the-art models. © 2008 AI Access Foundation. All rights reserved.",,"Computational linguistics; Global optimization; Inference engines; Integer programming; Linear programming; Global inference; Integer linear programming (ILP); Sentence compression; Data compression",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-44449150380
"Li G., Kou G., Peng Y.","56822488900;57068290300;36022039700;","A Group Decision Making Model for Integrating Heterogeneous Information",2018,"IEEE Transactions on Systems, Man, and Cybernetics: Systems","48","6",,"982","992",,186,"10.1109/TSMC.2016.2627050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034042880&doi=10.1109%2fTSMC.2016.2627050&partnerID=40&md5=3d1a6776c154d61c88f46c4e428778d3","This paper proposes a group decision-making (GDM) method for integrating heterogeneous information. To avoid information loss, instead of transforming heterogeneous information into a single form, the proposed method integrates heterogeneous information using a weighted-power average operator. The consensus degree between the individual-decision matrix and the group-decision matrix is then calculated based on the deviation degree. In addition, the feedback mechanism with the iterative algorithm is used to adjust the individual decision matrix, which does not reach the consensus. Furthermore, a ranking formula with heterogeneous technique for order preference by similarity to an ideal solution is adopted to select the best alternative. A numerical example of supplier selection is introduced to validate the proposed model and compare it with other similar GDM models. The results illustrate that the proposed method cannot only avoid information loss, but also effectively integrate heterogeneous information in heterogeneous GDM environment. © 2013 IEEE.","Group consensus; group-decision making (GDM); heterogeneous information; information fusion","Information fusion; Iterative methods; Decision matrices; Feedback mechanisms; Group consensus; Group Decision Making; Group decision-making models; Heterogeneous information; Iterative algorithm; Supplier selection; Decision making",,,,,,Article,"Final","",Scopus,2-s2.0-85034042880
"Cavallari S., Zheng V.W., Cai H., Chang K.C.-C., Cambria E.","57195682394;23037782800;55210319000;7407034942;56140547500;","Learning community embedding with community detection and node embedding on graphs",2017,"International Conference on Information and Knowledge Management, Proceedings","Part F131841",,,"377","386",,186,"10.1145/3132847.3132925","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85037326952&doi=10.1145%2f3132847.3132925&partnerID=40&md5=81b0dab24602fff974ad8072c8fba99b","In this paper, we study an important yet largely under-explored setting of graph embedding, i.e., embedding communities instead of each individual nodes. We find that community embedding is not only useful for community-level applications such as graph visualization, but also beneficial to both community detection and node classification. To learn such embedding, our insight hinges upon a closed loop among community embedding, community detection and node embedding. On the one hand, node embedding can help improve community detection, which outputs good communities for fitting better community embedding. On the other hand, community embedding can be used to optimize the node embedding by introducing a community-aware high-order proximity. Guided by this insight, we propose a novel community embedding framework that jointly solves the three tasks together. We evaluate such a framework on multiple real-world datasets, and show that it improves graph visualization and outperforms state-of-the-art baselines in various application tasks, e.g., community detection and node classification. © 2017 Association for Computing Machinery.","Community embedding; Graph embedding","Classification (of information); Graphic methods; Knowledge management; Population dynamics; Visualization; Application tasks; Community detection; Community embedding; Graph embeddings; Graph visualization; Learning community; Real-world datasets; State of the art; Graph theory","ACM SIGWEB;Special Interest Group on Information Retrieval (ACM SIGIR)","26th ACM International Conference on Information and Knowledge Management, CIKM 2017","6 November 2017 through 10 November 2017",,131841,Conference Paper,"Final","",Scopus,2-s2.0-85037326952
"Zhang S., Dinan E., Urbanek J., Szlam A., Kiela D., Weston J.","57190950221;57204527802;57207857852;11539079600;56349687800;8865128200;","Personalizing dialogue agents: I have a dog, do you have pets too?",2018,"ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)","1",,,"2204","2213",,185,"10.18653/v1/p18-1205","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063091043&doi=10.18653%2fv1%2fp18-1205&partnerID=40&md5=3e9180852450143eb3c3214066a374ff","Chit-chat models are known to have several problems: they lack specificity, do not display a consistent personality and are often not very captivating. In this work we present the task of making chit-chat more engaging by conditioning on profile information. We collect data and train models to (i) condition on their given profile information; and (ii) information about the person they are talking to, resulting in improved dialogues, as measured by next utterance prediction. Since (ii) is initially unknown, our model is trained to engage its partner with personal topics, and we show the resulting dialogue can be used to predict profile information about the interlocutors. © 2018 Association for Computational Linguistics",,"Train model; Computational linguistics","Apple;ByteDance;et al.;Facebook;Google;Samsung Research","56th Annual Meeting of the Association for Computational Linguistics, ACL 2018","15 July 2018 through 20 July 2018",,145927,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85063091043
"Kwok C.C.T., Etzioni O., Weld D.S.","7102029172;7004312379;7003334103;","Scaling question answering to the web",2001,"Proceedings of the 10th International Conference on World Wide Web, WWW 2001",,,,"150","161",,184,"10.1145/371920.371973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944091179&doi=10.1145%2f371920.371973&partnerID=40&md5=1f30b8759b806800b708d8849b008eb9","The wealth of information on the web makes it an at-tractive resource for seeking quick answers to simple, factual questions such as ""who was the first American in space?"" or ""what is the second tallest mountain in the world?"" Yet today's most advanced web search services (e.g., Google and AskJeeves) make it surprisingly te-dious to locate answers to such questions. In this paper, we extend question-answering techniques, first studied in the information retrieval literature, to the web and experimentally evaluate their performance. First we introduce Mulder, which we believe to be the first general-purpose, fully-automated question- A nswering system available on the web. Second, we de-scribe Mulder's architecture, which relies on multiple search-engine queries, natural-language parsing, and a novel voting procedure to yield reliable answers coupled with high recall. Finally, we compare Mulder's per-formance to that of Google and AskJeeves on questions drawn from the TREC-8 question track. We find that Mulder's recall is more than a factor of three higher than that of AskJeeves. In addition, we find that Google requires 6.6 times as much user effort to achieve the same level of recall as Mulder. © 2001 ACM.",,"Natural language processing systems; Search engines; World Wide Web; Fully automated; Multiple search; Natural language parsing; Question Answering; Wealth of information; Web searches; Information retrieval","ACM Special Interest Group on Hypertext, Hypermedia, and Web (SIGWEB);Hypertext, Hypermedia, and Web (SIGLINK);International World Wide Web Conference Committee (IW3C2)","10th International Conference on World Wide Web, WWW 2001","1 May 2001 through 5 May 2001",,129813,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84944091179
"Hendrickx I., Kim S.N., Kozareva Z., Nakov P., Séaghdha D.O., Padó S., Pennacchiotti M., Romano L., Szpakowicz S.","23091086500;13005694000;8921219900;15043153900;50562197900;10242453100;8932220500;23135601200;6701668280;","SemEval-2010 task 8: Multi-way classification of semantic relations between pairs of nominals",2010,"ACL 2010 - SemEval 2010 - 5th International Workshop on Semantic Evaluation, Proceedings",,,,"33","38",,183,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85006003503&partnerID=40&md5=da5ef27d6ecd86fc7fe90f70966e402a","SemEval-2 Task 8 focuses on Multi-way classification of semantic relations between pairs of nominals. The task was designed to compare different approaches to semantic relation classification and to provide a standard testbed for future research. This paper defines the task, describes the training and test data and the process of their creation, lists the participating systems (10 teams, 28 runs), and discusses their results. © 2010 Association for Computational Linguistics.",,"Multi-way classification; Participating systems; Semantic relations; Test data; Semantics","","5th International Workshop on Semantic Evaluation, SemEval 2010","15 July 2010 through 16 July 2010",,131703,Conference Paper,"Final","",Scopus,2-s2.0-85006003503
"Stanley K.O., Clune J., Lehman J., Miikkulainen R.","7102875151;23388416900;36439865800;7003434895;","Designing neural networks through neuroevolution",2019,"Nature Machine Intelligence","1","1",,"24","35",,181,"10.1038/s42256-018-0006-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064166007&doi=10.1038%2fs42256-018-0006-z&partnerID=40&md5=c1eac8ccf3921746a22422ce0e9c233e","Much of recent machine learning has focused on deep learning, in which neural network weights are trained through variants of stochastic gradient descent. An alternative approach comes from the field of neuroevolution, which harnesses evolutionary algorithms to optimize neural networks, inspired by the fact that natural brains themselves are the products of an evolutionary process. Neuroevolution enables important capabilities that are typically unavailable to gradient-based approaches, including learning neural network building blocks (for example activation functions), hyperparameters, architectures and even the algorithms for learning themselves. Neuroevolution also differs from deep learning (and deep reinforcement learning) by maintaining a population of solutions during search, enabling extreme exploration and massive parallelization. Finally, because neuroevolution research has (until recently) developed largely in isolation from gradient-based neural network research, it has developed many unique and effective techniques that should be effective in other machine learning areas too. This Review looks at several key aspects of modern neuroevolution, including large-scale computing, the benefits of novelty and diversity, the power of indirect encoding, and the field’s contributions to meta-learning and architecture search. Our hope is to inspire renewed interest in the field as it meets the potential of the increasing computation available today, to highlight how many of its ideas can provide an exciting resource for inspiration and hybridization to the deep learning, deep reinforcement learning and machine learning communities, and to explain how neuroevolution could prove to be a critical tool in the long-term pursuit of artificial general intelligence. © 2019, Springer Nature Limited.",,"Evolutionary algorithms; Gradient methods; Learning algorithms; Machine learning; Network architecture; Reinforcement learning; Stochastic systems; Activation functions; Artificial general intelligences; Evolutionary process; Gradient-based neural networks; Large-scale computing; Learning neural networks; Machine learning communities; Stochastic gradient descent; Deep learning",,,,,,Review,"Final","",Scopus,2-s2.0-85064166007
"Ferreira R., De Souza Cabral L., Lins R.D., Pereira E Silva G., Freitas F., Cavalcanti G.D.C., Lima R., Simske S.J., Favaro L.","56405263500;55860420500;56266354100;23135741200;25640711800;7004509486;36175808200;7003506391;55751314600;","Assessing sentence scoring techniques for extractive text summarization",2013,"Expert Systems with Applications","40","14",,"5755","5764",,180,"10.1016/j.eswa.2013.04.023","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84878347084&doi=10.1016%2fj.eswa.2013.04.023&partnerID=40&md5=ac7f627f1bd4998844597b9515bae961","Text summarization is the process of automatically creating a shorter version of one or more text documents. It is an important way of finding relevant information in large text libraries or in the Internet. Essentially, text summarization techniques are classified as Extractive and Abstractive. Extractive techniques perform text summarization by selecting sentences of documents according to some criteria. Abstractive summaries attempt to improve the coherence among sentences by eliminating redundancies and clarifying the contest of sentences. In terms of extractive summarization, sentence scoring is the technique most used for extractive text summarization. This paper describes and performs a quantitative and qualitative assessment of 15 algorithms for sentence scoring available in the literature. Three different datasets (News, Blogs and Article contexts) were evaluated. In addition, directions to improve the sentence extraction results obtained are suggested. © 2013 Elsevier Ltd. All rights reserved.","Extractive summarization; Sentence scoring methods; Summarization evaluation","Extractive summarizations; Quantitative and qualitative assessments; Relevant informations; Sentence extraction; Sentence scoring; Summarization evaluation; Text document; Text summarization; Internet; Text processing",,,,,,Review,"Final","",Scopus,2-s2.0-84878347084
"Spohn W.","8578928300;","The Laws of Belief: Ranking Theory and Its Philosophical Applications",2012,"The Laws of Belief: Ranking Theory and Its Philosophical Applications",,,,"1","625",,180,"10.1093/acprof:oso/9780199697502.001.0001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84863188620&doi=10.1093%2facprof%3aoso%2f9780199697502.001.0001&partnerID=40&md5=dd8d2f39b00d9b8db47d699b35c28812","Ranking theory will be claimed to deliver the first full account of the dynamics of belief and to be the legitimate sister of probability theory. This entails its deep significance for fundamental issues in epistemology and the philosophy of science. Therefore this book motivates and introduces the basic notion of a ranking function, which provides a new kind of degrees ob belief that at same time accounts for belief simpliciter. It develops ranking theory in ample detail, up to algorithms of inductive reasoning. It provides a measurement theory for ranking functions. It accounts for auto-epistemology in ranking-theoretic terms. It explicates the basic notion of a (deductive or non-deductive) reason, which is the entry to its rich philosophical applications. Among these are: a new account of lawlikeness, an account of ceteris paribus laws, a new perspective on dispositions, a rich and detailed theory of deterministic causation, an understanding of natural modalities as an objectification of epistemic modalities, an account of the experiential basis of belief, and thus a restructuring of the debate on foundationalism and coherentism (and externalism and contextualism), and finally a revival of fundamental a priori principles of reason fathoming the basics of empiricism and the relation between reason and truth and concluding in a proof of a weak principle of causality. All this is accompanied by thorough-going comparative discussions, on a general level as well as within each topic, and in particular with respect to probability theory that proves to be the major guideline of this book. © Wolfgang Spohn 2012. All rights reserved.","Apriority; Belief; Belief change; Causation; Ceteris paribus laws; Coherentism; Disposition; Foundationalism; Lawlikeness; Probability theory; Ranking theory; Reason",,,,,,,Book,"Final","",Scopus,2-s2.0-84863188620
"Dehmer M.","13404645900;","Information processing in complex networks: Graph entropy and information functionals",2008,"Applied Mathematics and Computation","201","1-2",,"82","94",,179,"10.1016/j.amc.2007.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-44749091859&doi=10.1016%2fj.amc.2007.12.010&partnerID=40&md5=d466d3891e9729e8583e02de12db378d","This paper introduces a general framework for defining the entropy of a graph. Our definition is based on a local information graph and on information functionals derived from the topological structure of a given graph. More precisely, an information functional quantifies structural information of a graph based on a derived probability distribution. Such a probability distribution leads directly to an entropy of a graph. Then, the structural information content of a graph will be is interpreted and defined as the derived graph entropy. Another major contribution of this paper is the investigation of relationships between graph entropies. In addition to this, we provide numerical results demonstrating not only the feasibility of our method, which has polynomial time complexity, but also its usefulness with regard to practical applications aiming to an understanding of information processing in complex networks. © 2007 Elsevier Inc. All rights reserved.","Applied graph theory; Graph entropy; Graphs; Information theory; Structural information content","Computational complexity; Data processing; Graph theory; Polynomial approximation; Applied graph theory; Graph entropy; Structural information content; Information theory",,,,,,Article,"Final","",Scopus,2-s2.0-44749091859
"Peng N., Poon H., Quirk C., Toutanova K., Yih W.-T.","","Cross-sentence n-ary relation extraction with graph lstms",2017,"Transactions of the Association for Computational Linguistics","5","1",,"101","115",,174,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85037343681
"Giatsoglou M., Vozalis M.G., Diamantaras K., Vakali A., Sarigiannidis G., Chatzisavvas K.C.","36634454300;9633863800;7003525351;6701925503;57188364100;9337855400;","Sentiment analysis leveraging emotions and word embeddings",2017,"Expert Systems with Applications","69",,,"214","224",,174,"10.1016/j.eswa.2016.10.043","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994061017&doi=10.1016%2fj.eswa.2016.10.043&partnerID=40&md5=844623e749d2227ed4f590400a48f877","Sentiment analysis and opinion mining are valuable for extraction of useful subjective information out of text documents. These tasks have become of great importance, especially for business and marketing professionals, since online posted products and services reviews impact markets and consumers shifts. This work is motivated by the fact that automating retrieval and detection of sentiments expressed for certain products and services embeds complex processes and pose research challenges, due to the textual phenomena and the language specific expression variations. This paper proposes a fast, flexible, generic methodology for sentiment detection out of textual snippets which express people's opinions in different languages. The proposed methodology adopts a machine learning approach with which textual documents are represented by vectors and are used for training a polarity classification model. Several documents’ vector representation approaches have been studied, including lexicon-based, word embedding-based and hybrid vectorizations. The competence of these feature representations for the sentiment classification task is assessed through experiments on four datasets containing online user reviews in both Greek and English languages, in order to represent high and weak inflection language groups. The proposed methodology requires minimal computational resources, thus, it might have impact in real world scenarios where limited resources is the case. © 2016 Elsevier Ltd","Hybrid vectorization; Machine learning; Multilingual sentiment analysis; Online user reviews; Text analysis; Vector representation","Artificial intelligence; Classification (of information); Commerce; Data mining; Learning systems; Natural language processing systems; Online users; Sentiment analysis; Text analysis; Vector representations; Vectorization; Information retrieval systems",,,,,,Article,"Final","",Scopus,2-s2.0-84994061017
"Collins J.J., Richmond S.A.","35478063700;7005460923;","Hard-wired central pattern generators for quadrupedal locomotion",1994,"Biological Cybernetics","71","5",,"375","385",,174,"10.1007/BF00198915","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028715081&doi=10.1007%2fBF00198915&partnerID=40&md5=ba56a5977ec95a6f05fdefb676983269","Animal locomotion is generated and controlled, in part, by a central pattern generator (CPG), which is an intraspinal network of neurons capable of producing rhythmic output. In the present work, it is demonstrated that a hard-wired CPG model, made up of four coupled nonlinear oscillators, can produce multiple phase-locked oscillation patterns that correspond to three common quadrupedal gaits - the walk, trot, and bound. Transitions between the different gaits are generated by varying the network's driving signal and/or by altering internal oscillator parameters. The above in numero results are obtained without changing the relative strengths or the polarities of the system's synaptic interconnections, i.e., the network maintains an invariant coupling architecture. It is also shown that the ability of the hard-wired CPG network to produce and switch between multiple gait patterns is a model-independent phenomenon, i.e., it does not depend upon the detailed dynamics of the component oscillators and/or the nature of the inter-oscillator coupling. Three different neuronal oscillator models - the Stein neuronal model, the Van der Pol oscillator, and the FitzHugh-Nagumo model -and two different coupling schemes are incorporated into the network without impeding its ability to produce the three quadrupedal gaits and the aforementioned gait transitions. © 1994 Springer-Verlag.",,"Biocontrol; Brain models; Gait analysis; Mathematical models; Oscillations; FitzHugh Nagumo model; Hard wired central pattern generator; Phase locked oscillation patterns; Quadrupedal locomotion; Stein neuronal model; Van der Pol oscillator; Biomechanics",,,,,,Article,"Final","",Scopus,2-s2.0-0028715081
"Kushmerick N.","","Wrapper induction for information extraction",1997,"Wrapper Induction for Information Extraction",,,,"","",,172,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0003614560
"Chen Q., Zhu X., Ling Z., Wei S., Jiang H., Inkpen D.","","Enhanced LSTM for natural language inference",2017,"ACL",,,,"1657","1668",,170,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85046971548
"Hu Z., Ma X., Liu Z., Hovy E., Xing E.P.","56272615700;57155457400;57191847261;6602910705;57203079532;","Harnessing deep neural networks with logic rules",2016,"54th Annual Meeting of the Association for Computational Linguistics, ACL 2016 - Long Papers","4",,,"2410","2420",,170,"10.18653/v1/p16-1228","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012027098&doi=10.18653%2fv1%2fp16-1228&partnerID=40&md5=21fb9e8745cd37660e1e73e86fc3ef68","Combining deep neural networks with structured logic rules is desirable to harness flexibility and reduce uninterpretability of the neural models. We propose a general framework capable of enhancing various types of neural networks (e.g., CNNs and RNNs) with declarative first-order logic rules. Specifically, we develop an iterative distillation method that transfers the structured information of logic rules into the weights of neural networks. We deploy the framework on a CNN for sentiment analysis, and an RNN for named entity recognition. With a few highly intuitive rules, we obtain substantial improvements and achieve state-of-the-art or comparable results to previous best-performing systems. © 2016 Association for Computational Linguistics.",,"Computational linguistics; Computer circuits; Distillation; Formal logic; Iterative methods; Sentiment analysis; Distillation method; First order logic; Logic rules; Named entity recognition; Neural models; State of the art; Structured information; Deep neural networks","Amazon;Baidu;Bloomberg;et al.;Facebook;Google","54th Annual Meeting of the Association for Computational Linguistics, ACL 2016","7 August 2016 through 12 August 2016",,125782,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85012027098
"Chiu B., Crichton G., Korhonen A., Pyysalo S.","","How to train good word embeddings for biomedical NLP",2016,"Proceedings of the 15th Workshop on Biomedical Natural Language Processing",,,,"166","174",,169,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85021655548
"Ibarz B., Casado J.M., Sanjuán M.A.F.","12804546100;56273983500;7005152862;","Map-based models in neuronal dynamics",2011,"Physics Reports","501","1-2",,"1","74",,169,"10.1016/j.physrep.2010.12.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79953039734&doi=10.1016%2fj.physrep.2010.12.003&partnerID=40&md5=bae69a77ce9cd380fd9c3c514a699296","Ever since the pioneering work of Hodgkin and Huxley, biological neuron models have consisted of ODEs representing the evolution of the transmembrane voltage and the dynamics of ionic conductances. It is only recently that discrete dynamical systems-also known as maps-have begun to receive attention as valid phenomenological neuron models. The present review tries to provide a coherent perspective of map-based biological neuron models, describing their dynamical properties; stressing the similarities and differences, both among them and in relation to continuous-time models; exploring their behavior in networks; and examining their wide-ranging possibilities of application in computational neuroscience. © 2010 Elsevier B.V.","Discrete-time; Map-based; Neural networks; Neuron models; Nonlinear dynamics",,,,,,,Review,"Final","",Scopus,2-s2.0-79953039734
"Filliat D.","55976331500;","A visual bag of words method for interactive qualitative localization and mapping",2007,"Proceedings - IEEE International Conference on Robotics and Automation",,,"4209698","3921","3926",,167,"10.1109/ROBOT.2007.364080","https://www.scopus.com/inward/record.uri?eid=2-s2.0-36348999309&doi=10.1109%2fROBOT.2007.364080&partnerID=40&md5=c563ec3e9406b19170de8d8d5ea13710","Localization for low cost humanoid or animal-like personal robots has to rely on cheap sensors and has to be robust to user manipulations of the robot. We present a visual localization and map-learning system that relies on vision only and that is able to incrementally learn to recognize the different rooms of an apartment from any robot position. This system is inspired by visual categorization algorithms called bag of words methods that we modified to make fully incremental and to allow a user-interactive training. Our system is able to reliably recognize the room in which the robot is after a short training time and is stable for long term use. Empirical validation on a real robot and on an image database acquired in real environments are presented. © 2007 IEEE.",,"Algorithms; Database systems; Image analysis; Learning systems; Manipulators; User interfaces; Image databases; Interactive qualitative localization; Real robots; Visual localization; Anthropomorphic robots","University of Illinois/Urbana, USA","2007 IEEE International Conference on Robotics and Automation, ICRA'07","10 April 2007 through 14 April 2007","Rome",70639,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-36348999309
"Mueller E.T.","7201904293;","Commonsense Reasoning",2006,"Commonsense Reasoning",,,,"","",404,167,"10.1016/B978-0-12-369388-4.X5054-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013875233&doi=10.1016%2fB978-0-12-369388-4.X5054-1&partnerID=40&md5=980809b5f4c796bf2e983e8ebdc5634b","To endow computers with common sense is one of the major long-term goals of Artificial Intelligence research. One approach to this problem is to formalize commonsense reasoning using mathematical logic. Commonsense Reasoning is a detailed, high-level reference on logic-based commonsense reasoning. It uses the event calculus, a highly powerful and usable tool for commonsense reasoning, which Erik T. Mueller demonstrates as the most effective tool for the broadest range of applications. He provides an up-to-date work promoting the use of the event calculus for commonsense reasoning, and bringing into one place information scattered across many books and papers. Mueller shares the knowledge gained in using the event calculus and extends the literature with detailed event calculus solutions to problems that span many areas of the commonsense world. © 2006 Elsevier Inc. All rights reserved.",,,,,,,,Book,"Final","",Scopus,2-s2.0-85013875233
"Cambria E., Livingstone A., Hussain A.","56140547500;15822174100;19734290900;","The hourglass of emotions",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7403 LNCS",,,"144","157",,166,"10.1007/978-3-642-34584-5_11","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870364413&doi=10.1007%2f978-3-642-34584-5_11&partnerID=40&md5=71c6332ae38830215c178f206b86490d","Human emotions and their modelling are increasingly understood to be a crucial aspect in the development of intelligent systems. Over the past years, in fact, the adoption of psychological models of emotions has become a common trend among researchers and engineers working in the sphere of affective computing. Because of the elusive nature of emotions and the ambiguity of natural language, however, psychologists have developed many different affect models, which often are not suitable for the design of applications in fields such as affective HCI, social data mining, and sentiment analysis. To this end, we propose a novel biologically-inspired and psychologically-motivated emotion categorisation model that goes beyond mere categorical and dimensional approaches. Such model represents affective states both through labels and through four independent but concomitant affective dimensions, which can potentially describe the full range of emotional experiences that are rooted in any of us. © 2012 Springer-Verlag.","Affective HCI; Cognitive and Affective Modelling; NLP","Affective Computing; Affective state; Human emotion; In-field; Natural languages; NLP; Psychological model; Sentiment analysis; Social data mining; Human computer interaction; Intelligent systems; Cognitive systems","Eur. COST Action 2102 Cross-Modal Anal. VerbalNonverbal Commun.;SSPnet: European Network on Social Signal Processing;EUCogII: 2nd Eur. Netw. Adv. Artif. Cogn. Syst., Interact. Rob.;University of Glaskow, School of Computing Science;Second University of Naples, Department of Psychology","International Training School on Cognitive Behavioural Systems, COST 2102","21 February 2011 through 26 February 2011","Dresden",94196,Conference Paper,"Final","",Scopus,2-s2.0-84870364413
"Chandra Pandey A., Singh Rajpoot D., Saraswat M.","56669195200;57194049965;35956779900;","Twitter sentiment analysis using hybrid cuckoo search method",2017,"Information Processing and Management","53","4",,"764","779",,164,"10.1016/j.ipm.2017.02.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85014785827&doi=10.1016%2fj.ipm.2017.02.004&partnerID=40&md5=e0a7984ed1603b85b2f2d73913c6a45d","Sentiment analysis is one of the prominent fields of data mining that deals with the identification and analysis of sentimental contents generally available at social media. Twitter is one of such social medias used by many users about some topics in the form of tweets. These tweets can be analyzed to find the viewpoints and sentiments of the users by using clustering-based methods. However, due to the subjective nature of the Twitter datasets, metaheuristic-based clustering methods outperforms the traditional methods for sentiment analysis. Therefore, this paper proposes a novel metaheuristic method (CSK) which is based on K-means and cuckoo search. The proposed method has been used to find the optimum cluster-heads from the sentimental contents of Twitter dataset. The efficacy of proposed method has been tested on different Twitter datasets and compared with particle swarm optimization, differential evolution, cuckoo search, improved cuckoo search, gauss-based cuckoo search, and two n-grams methods. Experimental results and statistical analysis validate that the proposed method outperforms the existing methods. The proposed method has theoretical implications for the future research to analyze the data generated through social networks/medias. This method has also very generalized practical implications for designing a system that can provide conclusive reviews on any social issues. © 2017 Elsevier Ltd","Cuckoo search; Data preprocessing; K-means; Sentiment analysis; Twitter","Data mining; Evolutionary algorithms; Particle swarm optimization (PSO); Social networking (online); Cuckoo searches; Data preprocessing; K-means; Sentiment analysis; Twitter; Optimization",,,,,,Article,"Final","",Scopus,2-s2.0-85014785827
"Zhou H., Young T., Huang M., Zhao H., Xu J., Zhu X.","57200416124;57203267719;7404260571;57204475096;57198896401;7406185137;","Commonsense knowledge aware conversation generation with graph attention",2018,"IJCAI International Joint Conference on Artificial Intelligence","2018-July",,,"4623","4629",,163,"10.24963/ijcai.2018/643","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055701005&doi=10.24963%2fijcai.2018%2f643&partnerID=40&md5=30fb7d91d04da226c2891179047f5e29","Commonsense knowledge is vital to many natural language processing tasks. In this paper, we present a novel open-domain conversation generation model to demonstrate how large-scale commonsense knowledge can facilitate language understanding and generation. Given a user post, the model retrieves relevant knowledge graphs from a knowledge base and then encodes the graphs with a static graph attention mechanism, which augments the semantic information of the post and thus supports better understanding of the post. Then, during word generation, the model attentively reads the retrieved knowledge graphs and the knowledge triples within each graph to facilitate better generation through a dynamic graph attention mechanism. This is the first attempt that uses large-scale commonsense knowledge in conversation generation. Furthermore, unlike existing models that use knowledge triples (entities) separately and independently, our model treats each knowledge graph as a whole, which encodes more structured, connected semantic information in the graphs. Experiments show that the proposed model can generate more appropriate and informative responses than state-of-the-art baselines. © 2018 International Joint Conferences on Artificial Intelligence.All right reserved.",,"Artificial intelligence; Encoding (symbols); Knowledge based systems; Natural language processing systems; Semantics; Attention mechanisms; Commonsense knowledge; Knowledge base; Knowledge graphs; Language understanding; NAtural language processing; Semantic information; State of the art; Graphic methods","International Joint Conferences on Artifical Intelligence (IJCAI)","27th International Joint Conference on Artificial Intelligence, IJCAI 2018","13 July 2018 through 19 July 2018",,140653,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85055701005
"Golubitsky M., Stewart I., Buono P.-L., Collins J.J.","7003736392;35547513000;6701741008;35478063700;","A modular network for legged locomotion",1998,"Physica D: Nonlinear Phenomena","115","1-2",,"56","72",,163,"10.1016/S0167-2789(97)00222-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002659841&doi=10.1016%2fS0167-2789%2897%2900222-4&partnerID=40&md5=1cec99b2905ecdbd9744e78bfba2c144","In this paper we use symmetry methods to study networks of coupled cells, which are models for central pattern generators (CPGs). In these models the cells obey identical systems of differential equations and the network specifies how cells are coupled. Previously, Collins and Stewart showed that the phase relations of many of the standard gaits of quadrupeds and hexapods can be obtained naturally via Hopf bifurcation in small networks. For example, the networks they used to study quadrupeds all had four cells, with the understanding that each cell determined the phase of the motion of one leg. However, in their work it seemed necessary to employ several different four-oscillator networks to obtain all of the standard quadrupedal gaits. We show that this difficulty with four-oscillator networks is unavoidable, but that the problems can be overcome by using a larger network. Specifically, we show that the standard gaits of a quadruped, including walk, trot and pace, cannot all be realized by a single four-cell network without introducing unwanted conjugacies between trot and pace - conjugacies that imply a dynamic equivalence between these gaits that seems inconsistent with observations. In this sense a single network with four cells cannot model the CPG of a quadruped. We also introduce a single eight-cell network that can model all of the primary gaits of quadrupeds without these unwanted conjugacies. Moreover, this network is modular in that it naturally generalizes to provide models of gaits in hexapods, centipedes, and millipedes. The analysis of models for many-legged animals shows that wave-like motions, similar to those obtained by Kopell and Ermentrout, can be expected. However, our network leads to a prediction that the wavelength of the wave motion will divide twice the length of the animal. Indeed, we reproduce illustrations of wave-like motions in centipedes where the animal is approximately one-and-a-half wavelength long - motions that are consistent with this prediction. We discuss the implications of these results for the development of modular control networks for adaptive legged robots. Copyright © 1998 Elsevier Science B.V.","Control pattern generator; Gaits; Hopf bifurcation; Symmetry",,,,,,,Article,"Final","",Scopus,2-s2.0-0002659841
"Pasupat P., Liang P.","55756364000;56646712700;","Compositional semantic parsing on semi-structured tables",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference","1",,,"1470","1480",,162,"10.3115/v1/p15-1142","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943788139&doi=10.3115%2fv1%2fp15-1142&partnerID=40&md5=f161280b7f35aa86aec2c3bfab6de996","Two important aspects of semantic parsing for question answering are the breadth of the knowledge source and the depth of logical compositionality. While existing work trades off one aspect for another, this paper simultaneously makes progress on both fronts through a new task: Answering complex questions on semi-structured tables using question-Answer pairs as supervision. The central challenge arises from two compounding factors: The broader domain results in an open-ended set of relations, and the deeper compositionality results in a combinatorial explosion in the space of logical forms. We propose a logical-form driven parsing algorithm guided by strong typing constraints and show that it obtains significant improvements over natural baselines. For evaluation, we created a new dataset of 22,033 complex questions on Wikipedia tables, which is made publicly available. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Semantics; Combinatorial explosion; Complex questions; Compositional semantics; Knowledge sources; Parsing algorithm; Question Answering; Question-answer pairs; Semantic parsing; Natural language processing systems","Alibaba Group;Baidu;CreditEase;et al.;Samsung;Tencent","53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015","26 July 2015 through 31 July 2015",,114195,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84943788139
"Bougouin A., Boudin F., Daille B.","","TopicRank: Graph-based topic ranking for keyphrase extraction",2013,"Proc of the 6th International Joint Conference on Natural Language Processing",,,,"543","551",,162,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84905122756
"Bordes A., Weston J., Collobert R., Bengio Y.","","Learning structured embeddings of knowledge bases",2011,"AAAI",,,,"","",,162,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84872513656
"Conneau A., Schwenk H., Barrault L., Le Cun Y.","",[No title available],2016,"Very Deep Convolutional Networks for Text Classification",,,,"","",,161,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85020494288
"Li X., Chen Y.-N., Li L., Gao J., Celikyilmaz A.","","End-to-end task-completion neural dialogue systems",2017,"Proceedings of IJCNLP",,,,"733","743",,160,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85036635914
"Lloret E., Palomar M.","25929448300;55664073900;","Text summarisation in progress: A literature review",2012,"Artificial Intelligence Review","37","1",,"1","41",,160,"10.1007/s10462-011-9216-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855347506&doi=10.1007%2fs10462-011-9216-z&partnerID=40&md5=7cbac7f7404013c1380784d346e2a5a2","This paper contains a large literature review in the research field of Text Summarisation (TS) based on Human Language Technologies (HLT). TS helps users manage the vast amount of information available, by condensing documents' content and extracting the most relevant facts or topics included in them. The rapid development of emerging technologies poses new challenges to this research field, which still need to be solved. Therefore, it is essential to analyse its progress over the years, and provide an overview of the past, present and future directions, highlighting the main advances achieved and outlining remaining limitations. With this purpose, several important aspects are addressed within the scope of this survey. On the one hand, the paper aims at giving a general perspective on the state-of-the-art, describing the main concepts, as well as different summarisation approaches, and relevant international forums. Furthermore, it is important to stress upon the fact that the birth of new requirements and scenarios has led to new types of summaries with specific purposes (e.g. sentiment-based summaries), and novel domains within which TS has proven to be also suitable for (e.g. blogs). In addition, TS is successfully combined with a number of intelligent systems based on HLT (e.g. information retrieval, question answering, and text classification). On the other hand, a deep study of the evaluation of summaries is also conducted in this paper, where the existing methodologies and systems are explained, as well as new research that has emerged concerning the automatic evaluation of summaries' quality. Finally, some thoughts about TS in general and its future will encourage the reader to think of novel approaches, applications and lines to conduct research in the next years. The analysis of these issues allows the reader to have a wide and useful background on the main important aspects of this research field. © 2011 Springer Science+Business Media B.V.","Human language technologies; Intelligent systems; Text summarisation","Amount of information; Automatic evaluation; Emerging technologies; Future directions; Human language technologies; Literature reviews; Novel domain; Question Answering; Rapid development; Research fields; Text classification; Text summarisation; Intelligent systems; Quality control; Research; Search engines; Text processing; Information retrieval",,,,,,Review,"Final","",Scopus,2-s2.0-84855347506
"Floridi L.","6603039594;","The method of levels of abstraction",2008,"Minds and Machines","18","3",,"303","329",,160,"10.1007/s11023-008-9113-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-50249130726&doi=10.1007%2fs11023-008-9113-7&partnerID=40&md5=4afab987ecd7d2f6a191fdd5abb2ec3e","The use of ""levels of abstraction"" in philosophical analysis (levelism) has recently come under attack. In this paper, I argue that a refined version of epistemological levelism should be retained as a fundamental method, called the method of levels of abstraction. After a brief introduction, in section ""Some Definitions and Preliminary Examples"" the nature and applicability of the epistemological method of levels of abstraction is clarified. In section ""A Classic Application of the Method of Abstraction"", the philosophical fruitfulness of the new method is shown by using Kant's classic discussion of the ""antinomies of pure reason"" as an example. In section ""The Philosophy of the Method of Abstraction"", the method is further specified and supported by distinguishing it from three other forms of ""levelism"": (i) levels of organisation; (ii) levels of explanation and (iii) conceptual schemes. In that context, the problems of relativism and antirealism are also briefly addressed. The conclusion discusses some of the work that lies ahead, two potential limitations of the method and some results that have already been obtained by applying the method to some long-standing philosophical problems. © 2008 Springer Science+Business Media B.V.","Abstraction; Gradient of abstraction; Level of abstraction; Levelism; Observable; Stance","Clarification; Philosophical aspects; Abstraction; Gradient of abstraction; Level of abstraction; Levelism; Levels of abstraction; Observable; Stance; Abstracting",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-50249130726
"Freitag D.","7005046856;","Machine learning for information extraction in informal domains",2000,"Machine Learning","39","2",,"169","202",,160,"10.1023/a:1007601113994","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033907729&doi=10.1023%2fa%3a1007601113994&partnerID=40&md5=7e8fe4a34431a98794d5df88f5f19c68","We consider the problem of learning to perform information extraction in domains where linguistic processing is problematic, such as Usenet posts, email, and finger plan files. In place of syntactic and semantic information, other sources of information can be used, such as term frequency, typography, formatting, and mark-up. We describe four learning approaches to this problem, each drawn from a different paradigm: a rote learner, a term-space learner based on Naive Bayes, an approach using grammatical induction, and a relational rule learner. Experiments on 14 information extraction problems defined over four diverse document collections demonstrate the effectiveness of these approaches. Finally, we describe a multistrategy approach which combines these learners and yields performance competitive with or better than the best of them. This technique is modular and flexible, and could find application in other machine learning problems.",,"Computational linguistics; Electronic mail; Information retrieval; Learning algorithms; Text processing; Grammatical induction; Information extraction; Multistrategy learning; Naive bayes; Semantic information; Learning systems",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-0033907729
"Bordes A., Lan Boureau Y., Weston J.","18933923000;22233842500;8865128200;","Learning end-to-end goal-oriented dialog",2017,"5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings",,,,"","",,159,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086405484&partnerID=40&md5=74e84623bf37bfdf71b649c1e5b0b0a0","Traditional dialog systems used in goal-oriented applications require a lot of domain-specific handcrafting, which hinders scaling up to new domains. End-to-end dialog systems, in which all components are trained from the dialogs themselves, escape this limitation. But the encouraging success recently obtained in chit-chat dialog may not carry over to goal-oriented settings. This paper proposes a testbed to break down the strengths and shortcomings of end-to-end dialog systems in goal-oriented applications. Set in the context of restaurant reservation, our tasks require manipulating sentences and symbols in order to properly conduct conversations, issue API calls and use the outputs of such calls. We show that an end-to-end dialog system based on Memory Networks can reach promising, yet imperfect, performance and learn to perform non-trivial operations. We confirm those results by comparing our system to a hand-crafted slot-filling baseline on data from the second Dialog State Tracking Challenge (Henderson et al., 2014a). We show similar result patterns on data extracted from an online concierge service. © ICLR 2019 - Conference Track Proceedings. All rights reserved.",,"Break down; Dialog systems; Domain specific; End to end; Goal-oriented; Memory network; Non-trivial; State tracking",,"5th International Conference on Learning Representations, ICLR 2017","24 April 2017 through 26 April 2017",,149804,Conference Paper,"Final","",Scopus,2-s2.0-85086405484
"Brewka G.","",[No title available],1991,"Nonmonotonic Reasoning: Logical Foundations of Commonsense",,,,"","",,159,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0003546756
"Olson R.S., Moore J.H.","","TPOT: A tree-based pipeline optimization tool for automating machine learning",2016,"Workshop on Automatic Machine Learning",,,,"66","74",,157,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85040290232
"Elman J.L.","","Language as a dynamical system",1995,"Mind as Motion: Explorations in the Dynamics of Cognition",,,,"195","225",,157,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0000894759
"Ghazvininejad M., Brockett C., Chang M.-W., Dolan B., Gao J., Yih W.-T., Galley M.","53871373500;12445867900;25925685700;35423406000;55702627000;23010913500;15055739800;","A knowledge-grounded neural conversation model",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"5110","5117",,156,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051552811&partnerID=40&md5=503fcfc55835095277a01bcb91982696","Neural network models are capable of generating extremely natural sounding conversational interactions. However, these models have been mostly applied to casual scenarios (e.g., as “chatbots”) and have yet to demonstrate they can serve in more useful conversational applications. This paper presents a novel, fully data-driven, and knowledge-grounded neural conversation model aimed at producing more contentful responses. We generalize the widely-used Sequence-to-Sequence (SEQ2SEQ) approach by conditioning responses on both conversation history and external “facts”, allowing the model to be versatile and applicable in an open-domain setting. Our approach yields significant improvements over a competitive SEQ2SEQ baseline. Human judges found that our outputs are significantly more informative. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Chatbots; Conversational interaction; Data driven; Neural network model; Artificial intelligence","Association for the Advancement of Artificial Intelligence","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","2 February 2018 through 7 February 2018",,143510,Conference Paper,"Final","",Scopus,2-s2.0-85051552811
"Shute V., Towle B.","6602162640;36440184700;","Adaptive e-learning",2003,"Educational Psychologist","38","2",,"105","114",,156,"10.1207/S15326985EP3802_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037903187&doi=10.1207%2fS15326985EP3802_5&partnerID=40&md5=e17a0d50fdc1618b22c92d8a48778b2c","It has long been known that differences among individuals have an effect on learning. Dick Snow's research on aptitude-treatment interactions (ATIs) was designed to investigate and quantify these effects, and more recent research in this vein has clearly established that these effects can be quantified and predicted. Technology has now reached a point where we have the opportunity to capitalize on these effects to the benefit of learners. In this article, we review some of the demonstrated effects of ATIs, describe how ATI research naturally leads to adaptive e-learning, and describe one way in which an adaptive e-learning system might be implemented to take advantage of these effects.",,,,,,,,Review,"Final","",Scopus,2-s2.0-0037903187
"Cambria E., Das D., Bandyopadhyay S., Feraco A.","","A Practical Guide to Sentiment Analysis",2017,"A Practical Guide to Sentiment Analysis",,,,"","",,153,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85037568228
"Greene D., Cunningham P.","13405696600;7201579809;","Practical solutions to the problem of diagonal dominance in kernel document clustering",2006,"ACM International Conference Proceeding Series","148",,,"377","384",,153,"10.1145/1143844.1143892","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34250762673&doi=10.1145%2f1143844.1143892&partnerID=40&md5=3717cd4d1a3d20c928e9c085cfe27f1a","In supervised kernel methods, it has been observed that the performance of the SVM classifier is poor in cases where the diagonal entries of the Gram matrix are large relative to the off-diagonal entries. This problem, referred to as diagonal dominance, often occurs when certain kernel functions are applied to sparse high-dimensional data, such as text corpora. In this paper we investigate the implications of diagonal dominance for unsupervised kernel methods, specifically in the task of document clustering. We propose a selection of strategies for addressing this issue, and evaluate their effectiveness in producing more accurate and stable clusterings.",,"Classification (of information); Clustering algorithms; Matrix algebra; Problem solving; Diagonal dominance; Document clustering; Gram matrix; High-dimensional data; Kernel functions; SVM classifier; Text corpora; Support vector machines","Carnegie Mellon;National Science Foundation;Microsoft;Google, Inc.;The Boeing Company","23rd International Conference on Machine Learning, ICML 2006","25 June 2006 through 29 June 2006","Pittsburgh, PA",69847,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-34250762673
"De Raedt L., Frasconi P., Kersting K., Muggleton S.","","Probabilistic Induc-tive Logic Programming-Theory and Applications",2008,"Probabilistic Inductive Logic Programming",,,,"","",,152,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-52449101351
"Grillner S., Zangger P.","7103319256;7003895381;","How detailed is the central pattern generation for locomotion?",1975,"Brain Research","88","2",,"367","371",,152,"10.1016/0006-8993(75)90401-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0016770333&doi=10.1016%2f0006-8993%2875%2990401-1&partnerID=40&md5=c22e5bcf4ef693e03de637cbaad248c7",[No abstract available],,"cat; cuneiform nucleus; electromyography; extensor muscle; flexor muscle; hindlimb; locomotion; theoretical study; Animal; Cats; Denervation; Electric Stimulation; Electromyography; Ganglia, Spinal; Hindlimb; Locomotion; Neural Pathways; Neurons, Afferent; Spinal Cord; Spinal Nerves",,,,,,Article,"Final","",Scopus,2-s2.0-0016770333
"Williams A., Nangia N., Bowman S.R.","","A broad-coverage challenge corpus for sentence understanding through inference",2017,"A Broad-Coverage Challenge Corpus for Sentence Understanding through Inference",,,,"","",,151,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85046966647
"Li J., Sun Y., Johnson R.J., Sciaky D., Wei C.-H., Leaman R., Davis A.P., Mattingly C.J., Wiegers T.C., Lu Z.","55904685100;57213516768;57199372664;55652139600;35192504800;23972756600;7404296854;7003640709;6701361518;23474115300;","BioCreative V CDR task corpus: a resource for chemical disease relation extraction",2016,"Database : the journal of biological databases and curation","2016",,,"","",,151,"10.1093/database/baw068","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013335148&doi=10.1093%2fdatabase%2fbaw068&partnerID=40&md5=f376441901d6bd75e641e37ebf734253","Community-run, formal evaluations and manually annotated text corpora are critically important for advancing biomedical text-mining research. Recently in BioCreative V, a new challenge was organized for the tasks of disease named entity recognition (DNER) and chemical-induced disease (CID) relation extraction. Given the nature of both tasks, a test collection is required to contain both disease/chemical annotations and relation annotations in the same set of articles. Despite previous efforts in biomedical corpus construction, none was found to be sufficient for the task. Thus, we developed our own corpus called BC5CDR during the challenge by inviting a team of Medical Subject Headings (MeSH) indexers for disease/chemical entity annotation and Comparative Toxicogenomics Database (CTD) curators for CID relation annotation. To ensure high annotation quality and productivity, detailed annotation guidelines and automatic annotation tools were provided. The resulting BC5CDR corpus consists of 1500 PubMed articles with 4409 annotated chemicals, 5818 diseases and 3116 chemical-disease interactions. Each entity annotation includes both the mention text spans and normalized concept identifiers, using MeSH as the controlled vocabulary. To ensure accuracy, the entities were first captured independently by two annotators followed by a consensus annotation: The average inter-annotator agreement (IAA) scores were 87.49% and 96.05% for the disease and chemicals, respectively, in the test set according to the Jaccard similarity coefficient. Our corpus was successfully used for the BioCreative V challenge tasks and should serve as a valuable resource for the text-mining research community.Database URL: http://www.biocreative.org/tasks/biocreative-v/track-3-cdr/. Published by Oxford University Press 2016. This work is written by US Government employees and is in the public domain in the United States.",,"biology; data mining; diseases; factual database; human; procedures; toxicogenetics; Computational Biology; Data Mining; Databases, Factual; Disease; Humans; Toxicogenetics",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85013335148
"Steil J.J.","6701553064;","Backpropagation-Decorrelation: Online recurrent learning with O(N) complexity",2004,"IEEE International Conference on Neural Networks - Conference Proceedings","2",,,"843","848",,148,"10.1109/IJCNN.2004.1380039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-10944225085&doi=10.1109%2fIJCNN.2004.1380039&partnerID=40&md5=32b74cfc9d2fef8b0dd8351f04703fca","We introduce a new learning rule for fully recurrent neural networks which we call Backpropagation-Decorrelation rule (BPDC). It combines important principles: one-step backpropagation of errors and the usage of temporal memory in the network dynamics by means of decorrelation of activations. The BPDC rule is derived and theoretically justified from regarding learning as a constraint optimization problem and applies uniformly in discrete and continuous time. It is very easy to implement, and has a minimal complexity of 2N multiplications per time-step in the single output case. Nevertheless we obtain fast tracking and excellent performance in some benchmark problems including the Mackey-Glass time-series.",,"Network dynamics; Optimization problem; Recurrent networks; Time constants; Algorithms; Backpropagation; Error analysis; Learning systems; Optimization; Problem solving; Time series analysis; Neural networks","IEEE Neural Networks Society, INNS","2004 IEEE International Joint Conference on Neural Networks - Proceedings","25 July 2004 through 29 July 2004","Budapest",64098,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-10944225085
"Velikovich L., Blair-Goldensohn S., Hannan K., McDonald R.","6507171769;6507943623;55894904400;12772969000;","The viability of web-derived polarity lexicons",2010,"NAACL HLT 2010 - Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference",,,,"777","785",,147,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858422546&partnerID=40&md5=f5a3520fa530a3ba229aeefff43114d3","We examine the viability of building large polarity lexicons semi-automatically from the web. We begin by describing a graph propagation framework inspired by previous work on constructing polarity lexicons from lexical graphs (Kim and Hovy, 2004; Hu and Liu, 2004; Esuli and Sabastiani, 2009; Blair-Goldensohn et al., 2008; Rao and Ravichandran, 2009). We then apply this technique to build an English lexicon that is significantly larger than those previously studied. Crucially, this web-derived lexicon does not require WordNet, part-of-speech taggers, or other language-dependent resources typical of sentiment analysis systems. As a result, the lexicon is not limited to specific word classes - e.g., adjectives that occur in WordNet - and in fact contains slang, misspellings, multiword expressions, etc. We evaluate a lexicon derived from English documents, both qualitatively and quantitatively, and show that it provides superior performance to previously studied lexicons, including one derived from WordNet. © 2010 Association for Computational Linguistics.",,"Multiword expressions; Part-of-speech tagger; Sentiment analysis; Wordnet; Computational linguistics; Ontology","AT and T Interactive;Microsoft Research;USC Information Sciences Institute;Google;J.D. Power and Associates","2010 Human Language Technologies Conference ofthe North American Chapter of the Association for Computational Linguistics, NAACL HLT 2010","2 June 2010 through 4 June 2010","Los Angeles, CA",88929,Conference Paper,"Final","",Scopus,2-s2.0-84858422546
"Krupa M.","7003885195;","Robust Heteroclinic Cycles",1997,"Journal of Nonlinear Science","7","2",,"129","176",,147,"10.1007/BF02677976","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0001225644&doi=10.1007%2fBF02677976&partnerID=40&md5=1eb05553ff7df206dadc3e3ff9891723","One phenomenon in the dynamics of differential equations which does not typically occur in systems without symmetry is heteroclinic cycles. In symmetric systems, cycles can be robust for symmetry-preserving perturbations and stable. Cycles have been observed in a number of simulations and experiments, for example in rotating convection between two plates and for turbulent flows in a boundary layer. Theoretically the existence of robust cycles has been proved in the unfoldings of some low codimension bifurcations and in the context of forced symmetry breaking from a larger to a smaller symmetry group. In this article we review the theoretical and the applied research on robust cycles.","Bifurcation; Experiment; Heteroclinic cycles; Robust; Simulation; Stability; Symmetry",,,,,,,Article,"Final","",Scopus,2-s2.0-0001225644
"Cambria E., Hussain A.","","Sentic Computing. A Common-Sense-Based Framework for Concept-Level Sentiment Analysis",2015,"Sentic Computing: A Common-Sense-Based Framework for Concept-Level Sentiment Analysis",,,,"","",,146,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84937631625
"Moldovan D., Paşca M., Harabagiu S., Surdeanu M.","7004716430;56370539400;6601915885;23398535300;","Performance issues and error analysis in an open-domain question answering system",2003,"ACM Transactions on Information Systems","21","2",,"133","154",,145,"10.1145/763693.763694","https://www.scopus.com/inward/record.uri?eid=2-s2.0-2442589044&doi=10.1145%2f763693.763694&partnerID=40&md5=ac7609571104103031cae0757dd1235c","This paper presents an in-depth analysis of a state-of-the-art Question Answering system. Several scenarios are examined: (1) the performance of each module in a serial baseline system, (2) the impact of feedbacks and the insertion of a logic prover, and (3) the impact of various retrieval strategies and lexical resources. The main conclusion is that the overall performance depends on the depth of natural language processing resources and the tools used for answer finding.","Natural language applications; Performance analysis; Question answering; Text retrieval","Algorithms; Error analysis; Information retrieval; Systems analysis; Text processing; Natural language applications; Performance analysis; Question answering; Text retrieval; Natural language processing systems",,,,,,Article,"Final","",Scopus,2-s2.0-2442589044
"Huang Z., Thint M., Qin Z.","55494527100;6602381335;8935532300;","Question classification using head words and their hypernyms",2008,"EMNLP 2008 - 2008 Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference: A Meeting of SIGDAT, a Special Interest Group of the ACL",,,,"927","936",,142,"10.3115/1613715.1613835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956056624&doi=10.3115%2f1613715.1613835&partnerID=40&md5=0c22f8a4fed91e3bee599f0443f744ee","Question classification plays an important role in question answering. Features are the key to obtain an accurate question classifier. In contrast to Li and Roth (2002)'s approach which makes use of very rich feature space, we propose a compact yet effective feature set. In particular, we propose head word feature and present two approaches to augment semantic features of such head words using WordNet. In addition, Lesk's word sense disambiguation (WSD) algorithm is adapted and the depth of hypernym feature is optimized. With further augment of other standard features such as unigrams, our linear SVM and Maximum Entropy (ME) models reach the accuracy of 89.2%and 89.0%respectively over a standard benchmark dataset, which outperform the best previously reported accuracy of 86.2%. © 2008 Association for Computational Linguistics.",,"Semantics; Support vector machines; Benchmark datasets; Feature sets; Maximum entropy models; Question Answering; Question classification; Rich features; Semantic features; Word-sense disambiguation; Natural language processing systems",,"2008 Conference on Empirical Methods in Natural Language Processing, EMNLP 2008, Co-located with AMTA 2008 and the International Workshop on Spoken Language Translation","25 October 2008 through 27 October 2008","Honolulu, HI",86730,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-77956056624
"Joulin A., Mikolov T.","36466607100;34969425500;","Inferring algorithmic patterns with stack-augmented recurrent nets",2015,"Advances in Neural Information Processing Systems","2015-January",,,"190","198",,141,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965117324&partnerID=40&md5=7ab3098174a5f98bdb7a0a28f81448df","Despite the recent achievements in machine learning, we are still very far from achieving real artificial intelligence. In this paper, we discuss the limitations of standard deep learning approaches and show that some of these limitations can be overcome by learning how to grow the complexity of a model in a structured way. Specifically, we study the simplest sequence prediction problems that are beyond the scope of what is learnable with standard recurrent networks, algorithmically generated sequences which can only be learned by models which have the capacity to count and to memorize sequences. We show that some basic algorithms can be learned from sequential data using a recurrent network associated with a trainable memory.",,"Algorithms; Artificial intelligence; Information science; Learning systems; Deep learning; Recurrent networks; Sequence prediction; Sequential data; Complex networks","et al.;Google, Inc.;Ketchum Trading;Microsoft;Taobao (China) Software Co., Ltd. (Alibaba);Twitter","29th Annual Conference on Neural Information Processing Systems, NIPS 2015","7 December 2015 through 12 December 2015",,120037,Conference Paper,"Final","",Scopus,2-s2.0-84965117324
"Agrawal R., Gupta A., Prabhu Y., Varma M.","57212907262;57198676842;56017115600;24823171100;","Multi-label learning with millions of labels: Recommending advertiser bid phrases for web pages",2013,"WWW 2013 - Proceedings of the 22nd International Conference on World Wide Web",,,,"13","23",,140,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893108239&partnerID=40&md5=bf10ee7f252b44ad4a1e1de309e31273","Recommending phrases from web pages for advertisers to bid on against search engine queries is an important research problem with direct commercial impact. Most approaches have found it infeasible to determine the relevance of all possible queries to a given ad landing page and have focussed on making recommendations from a small set of phrases extracted (and expanded) from the page using NLP and ranking based techniques. In this paper, we eschew this paradigm, and demonstrate that it is possible to efficiently predict the relevant subset of queries from a large set of monetizable ones by posing the problem as a multi-label learning task with each query being represented by a separate label. We develop Multi-label Random Forests to tackle problems with millions of labels. Our proposed classifier has prediction costs that are logarithmic in the number of labels and can make predictions in a few milliseconds using 10 Gb of RAM. We demonstrate that it is possible to generate training data for our classifier automatically from click logs without any human annotation or intervention. We train our classifier on tens of millions of labels, features and training points in less than two days on a thousand node cluster. We develop a sparse semi-supervised multi-label learning formulation to deal with training set biases and noisy labels harvested automatically from the click logs. This formulation is used to infer a belief in the state of each label for each training ad and the random forest classifier is extended to train on these beliefs rather than the given labels. Experiments reveal significant gains over ranking and NLP based techniques on a large test set of 5 million ads using multiple metrics. Copyright is held by the International World Wide Web Conference Committee (IW3C2).","Bid phrase recommendation; Large scale learning; Multi-label learning; Random forests; Semi-supervised learning","Bid phrase recommendation; Large-scale learning; Multi-label learning; Random forests; Semi-supervised learning; Decision trees; Natural language processing systems; Search engines; Supervised learning; Websites; Classification (of information)","Comite Gestor da Internet no Brazil (CGI.BR);Nucleo de Informatcao e Coordenacao do Ponto BR (NIC.BR);BR PETROBRAS;Banco do Brasil;Microsoft","22nd International Conference on World Wide Web, WWW 2013","13 May 2013 through 17 May 2013","Rio de Janeiro",102216,Conference Paper,"Final","",Scopus,2-s2.0-84893108239
"D'Avila Garcez A.S., Broda K., Gabbay D.M.","6603397393;9133273600;7003522670;","Symbolic knowledge extraction from trained neural networks: A sound approach",2001,"Artificial Intelligence","125","1-2",,"155","207",,139,"10.1016/S0004-3702(00)00077-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035127989&doi=10.1016%2fS0004-3702%2800%2900077-1&partnerID=40&md5=b02247a37a4c87e7b1535642753bfee0","Some of the main problems of knowledge extraction methods were discussed. The results of an empirical analysis of the extraction system, using traditional examples and real-world application problems were presented. The results have shown that a very high fidelity between the extracted set of rules and the network can be achieved.",,"Decision theory; Learning algorithms; Learning systems; Neural networks; Neural-symbolic integration; Nonmonotonic reasoning; Knowledge acquisition",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-0035127989
"Qiu J., Tang J., Ma H., Dong Y., Wang K., Tang J.","56381858800;55713998400;23012636500;49963255100;7501399246;26667745200;","DeepInf: Social influence prediction with deep learning",2018,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"2110","2119",,137,"10.1145/3219819.3220077","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051486121&doi=10.1145%2f3219819.3220077&partnerID=40&md5=206698431871729a18482ce7b6c00e94","Social and information networking activities such as on Facebook, Twitter, WeChat, and Weibo have become an indispensable part of our everyday life, where we can easily access friends' behaviors and are in turn influenced by them. Consequently, an effective social influence prediction for each user is critical for a variety of applications such as online recommendation and advertising. Conventional social influence prediction approaches typically design various hand-crafted rules to extract user- and network-specific features. However, their effectiveness heavily relies on the knowledge of domain experts. As a result, it is usually difficult to generalize them into different domains. Inspired by the recent success of deep neural networks in a wide range of computing applications, we design an end-to-end framework, DeepInf1, to learn users' latent feature representation for predicting social influence. In general, DeepInf takes a user's local network as the input to a graph neural network for learning her latent social representation. We design strategies to incorporate both network structures and user-specific features into convolutional neural and attention networks. Extensive experiments on Open Academic Graph, Twitter, Weibo, and Digg, representing different types of social and information networks, demonstrate that the proposed end-to-end model, DeepInf, significantly outperforms traditional feature engineering-based approaches, suggesting the effectiveness of representation learning for social applications. © 2018 Association for Computing Machinery.","Graph Attention; Graph Convolution; Network Embedding; Representation Learning; Social Influence; Social Networks","Convolution; Data mining; Deep neural networks; Forecasting; Information services; Social networking (online); Computing applications; Feature representation; Graph Attention; Information networking; Network embedding; Representation Learning; Social influence; Social representations; Economic and social effects","ACM SIGKDD;ACM SIGMOD","24th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2018","19 August 2018 through 23 August 2018",,138322,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85051486121
"Peng H., Li J., He Y., Liu Y., Bao M., Wang L., Song Y., Yang Q.","57190014707;55720560100;57194429723;57194433649;57194435374;57007714500;57216616056;57201354715;","Large-scale hierarchical text classification with recursively regularized deep graph-CNN",2018,"The Web Conference 2018 - Proceedings of the World Wide Web Conference, WWW 2018",,,,"1063","1072",,137,"10.1145/3178876.3186005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060280062&doi=10.1145%2f3178876.3186005&partnerID=40&md5=330d78cdc79acddee2c8ff3a2af092f7","Text classification to a hierarchical taxonomy of topics is a common and practical problem. Traditional approaches simply use bag-of-words and have achieved good results. However, when there are a lot of labels with different topical granularities, bag-of-words representation may not be enough. Deep learning models have been proven to be effective to automatically learn different levels of representations for image data. It is interesting to study what is the best way to represent texts. In this paper, we propose a graph-CNN based deep learning model to first convert texts to graph-of-words, and then use graph convolution operations to convolve the word graph. Graph-of-words representation of texts has the advantage of capturing non-consecutive and long-distance semantics. CNN models have the advantage of learning different level of semantics. To further leverage the hierarchy of labels, we regularize the deep architecture with the dependency among labels. Our results on both RCV1 and NYTimes datasets show that we can significantly improve large-scale hierarchical text classification over traditional hierarchical text classification and existing deep models. © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License.","Convolutional neural networks; Deep learning; Graph-of-words; Hierarchical text classification; Recursive regularization","Classification (of information); Deep learning; Large dataset; Learning systems; Semantics; World Wide Web; Bag of words; Deep architectures; Hierarchical taxonomy; Learning models; Practical problems; Text classification; Traditional approaches; Word graphs; Text processing","Amazon;Baidu;et al.;Google;IDEX Lyon;Inria","27th International World Wide Web, WWW 2018","23 April 2018 through 27 April 2018",,159681,Conference Paper,"Final","",Scopus,2-s2.0-85060280062
"Chen K., Zhang Z., Long J., Zhang H.","57191200276;15041087700;35756419000;57189602052;","Turning from TF-IDF to TF-IGM for term weighting in text classification",2016,"Expert Systems with Applications","66",,,"1339","1351",,137,"10.1016/j.eswa.2016.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987860660&doi=10.1016%2fj.eswa.2016.09.009&partnerID=40&md5=4fae16e434215c2413c656c50ebc82d6","Massive textual data management and mining usually rely on automatic text classification technology. Term weighting is a basic problem in text classification and directly affects the classification accuracy. Since the traditional TF-IDF (term frequency & inverse document frequency) is not fully effective for text classification, various alternatives have been proposed by researchers. In this paper we make comparative studies on different term weighting schemes and propose a new term weighting scheme, TF-IGM (term frequency & inverse gravity moment), as well as its variants. TF-IGM incorporates a new statistical model to precisely measure the class distinguishing power of a term. Particularly, it makes full use of the fine-grained term distribution across different classes of text. The effectiveness of TF-IGM is validated by extensive experiments of text classification using SVM (support vector machine) and kNN (k nearest neighbors) classifiers on three commonly used corpora. The experimental results show that TF-IGM outperforms the famous TF-IDF and the state-of-the-art supervised term weighting schemes. In addition, some new findings different from previous studies are obtained and analyzed in depth in the paper. © 2016 Elsevier Ltd","Class distinguishing power; Classifier; Inverse gravity moment (IGM); Term weighting; Text classification","Classification (of information); Classifiers; Information management; Information retrieval systems; Nearest neighbor search; Support vector machines; Automatic text classification; Class distinguishing power; Classification accuracy; Inverse Document Frequency; Inverse gravity moment (IGM); SVM(support vector machine); Term weighting; Text classification; Text processing",,,,,,Article,"Final","",Scopus,2-s2.0-84987860660
"Shin J., Wu S., Wang F., De Sa C., Zhang C., Ré C.","57045113500;34868994500;55811018500;57045695500;55703849700;10739281400;","Incremental knowledge base construction using deepdive",2015,"Proceedings of the VLDB Endowment","8","11",,"1310","1321",,136,"10.14778/2809974.2809991","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84953875555&doi=10.14778%2f2809974.2809991&partnerID=40&md5=868d6f883199286e8b8c35131b2c4d3e","Populating a database with unstructured information is a long-standing problem in industry and research that encompasses problems of extraction, cleaning, and integration. Recent names used for this problem include dealing with dark data and knowledge base construction (KBC). In this work, we describe DeepDive, a system that combines database and machine learning ideas to help develop KBC systems, and we present techniques to make the KBC process more efficient. We observe that the KBC process is iterative, and we develop techniques to incrementally produce inference results for KBC systems. We propose two methods for incremental inference, based respectively on sampling and variational techniques. We also study the tradeoff space of these methods and develop a simple rule-based optimizer. DeepDive includes all of these contributions, and we evaluate Deep- Dive on five KBC systems, showing that it can speed up KBC inference tasks by up to two orders of magnitude with negligible impact on quality. © 2015 VLDB Endowment 21508097/15/07.",,"Artificial intelligence; Database systems; Knowledge based systems; Learning systems; Variational techniques; Deep dives; Knowledge-base construction; Optimizers; Orders of magnitude; Rule based; Speed up; Standing problems; Iterative methods","Geospatial Information Technology Co. Ltd.","3rd Workshop on Spatio-Temporal Database Management, STDBM 2006, Co-located with the 32nd International Conference on Very Large Data Bases, VLDB 2006","11 September 2006 through 11 September 2006","Seoul",114946,Book Chapter,"Final","All Open Access, Green",Scopus,2-s2.0-84953875555
"Tropp J.A.","7003918957;","Improved analysis of the subsampled randomized hadamard transform",2011,"Advances in Adaptive Data Analysis","3","1-2",,"115","126",,136,"10.1142/S1793536911000787","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80052645998&doi=10.1142%2fS1793536911000787&partnerID=40&md5=6358378994d2367cca236bfaa3ed42c8","This paper presents an improved analysis of a structured dimension-reduction map called the subsampled randomized Hadamard transform. This argument demonstrates that the map preserves the Euclidean geometry of an entire subspace of vectors. The new proof is much simpler than previous approaches, and it offers for the first time optimal constants in the estimate on the number of dimensions required for the embedding. © 2011 World Scientific Publishing Company.","Dimension reduction; numerical linear algebra; random matrix; WalshHadamard matrix","Dimension reduction; Euclidean Geometry; Hadamard transforms; Numerical Linear Algebra; random matrix; Time-optimal; Walsh-Hadamard; Vectors; Mathematical transformations",,,,,,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-80052645998
"Tran S.D., Davis L.S.","14036494700;57203425293;","Event modeling and recognition using Markov logic networks",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5303 LNCS","PART 2",,"610","623",,135,"10.1007/978-3-540-88688-4_45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56749176261&doi=10.1007%2f978-3-540-88688-4_45&partnerID=40&md5=5dce90071b59e83bd9398e62500d04e1","We address the problem of visual event recognition in surveillance where noise and missing observations are serious problems. Common sense domain knowledge is exploited to overcome them. The knowledge is represented as first-order logic production rules with associated weights to indicate their confidence. These rules are used in combination with a relaxed deduction algorithm to construct a network of grounded atoms, the Markov Logic Network. The network is used to perform probabilistic inference for input queries about events of interest. The system's performance is demonstrated on a number of videos from a parking lot domain that contains complex interactions of people and vehicles. © 2008 Springer Berlin Heidelberg.",,"Computer vision; Markov processes; Probabilistic logics; Domain knowledge; Event recognition; First order logic; Markov logic networks; Missing observations; Probabilistic inference; Production rules; System's performance; Computer circuits","Deutsche Telekom Laboratories;EADS;et al.;INRIA;Microsoft Research;Ville de Marseille","10th European Conference on Computer Vision, ECCV 2008","12 October 2008 through 18 October 2008","Marseille",74339,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-56749176261
"Wegner P.","7005517088;","Interactive foundations of computing",1998,"Theoretical Computer Science","192","2",,"315","351",,134,"10.1016/S0304-3975(97)00154-0","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002973118&doi=10.1016%2fS0304-3975%2897%2900154-0&partnerID=40&md5=48dc7ff2a0364a22eea2f921b61a6578","The claim that interactive systems have richer behavior than algorithms is surprisingly easy to prove. Turing machines cannot model interaction machines (which extend Turing machines with interactive input/output) because interaction is not expressible by a finite initial input string. Interaction machines extend the Chomsky hierarchy, are modeled by interaction grammars, and precisely capture fuzzy concepts like open systems and empirical computer science. Computable functions cannot model real-world behavior because functions are too strong an abstraction, sacrificing the ability to model time and other real-world properties to realize formal tractability. Part I of this paper examines extensions to interactive models for algorithms, machines, grammars, and semantics, while Part II considers the expressiveness of different forms of interaction. Interactive identity machines are already more powerful than Turing machines, while noninteractive parallelism and distribution are algorithmic. The extension of Turing to interaction machines parallels that of the lambda to the pi calculus. Asynchronous and nonserializable interaction are shown to be more expressive than sequential interaction (multiple streams are more expressive than a single stream). In Part III. it is shown that interaction machines cannot be described by sound and complete first-order logics (a form of Godel incompleteness), and that incompleteness is inherently necessary to realize greater expressiveness. In the final section the robustness of interactive models in expressing open systems, programming in the large, graphical user interfaces, and agent-oriented artificial intelligence is compared to the robustness of Turing machines. Less technical discussion of these ideas may be found in [25-27]. Applications of interactive models to coordination, objects and components, patterns and frameworks, software engineering, and AI are examined elsewhere [28,29]. The propositions P1-P36 embody the principal claims, while observations O1 through O40 provide additional insights.","Constraints; Coordination; Emergent behavior; Empirical computer science; Games; Grammars; Incompleteness; Interaction; Logic; Models; On-line algorithms; Process models; Time; Turing machines",,,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-0002973118
"Wubben S., Van Den Bosch A., Krahmer E.","55082793200;7003450744;6602935272;","Sentence simplification by monolingual machine translation",2012,"50th Annual Meeting of the Association for Computational Linguistics, ACL 2012 - Proceedings of the Conference","1",,,"1015","1024",,131,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876797872&partnerID=40&md5=2b49aa1ffecfb60a20716e2ed8034e44","In this paper we describe a method for simplifying sentences using Phrase Based Machine Translation, augmented with a re-ranking heuristic based on dissimilarity, and trained on a monolingual parallel corpus. We compare our system to a word-substitution baseline and two state-of-the-art systems, all trained and tested on paired sentences from the English part of Wikipedia and Simple Wikipedia. Human test subjects judge the output of the different systems. Analysing the judgements shows that by relatively careful phrase-based paraphrasing our model achieves similar simplification results to state-of-the-art systems, while generating better formed output. We also argue that text readability metrics such as the Flesch-Kincaid grade level should be used with caution when evaluating the output of simplification systems. © 2012 Association for Computational Linguistics.",,"Human tests; Machine translations; Parallel corpora; Phrase-based machine translations; Re-ranking; State-of-the-art system; Wikipedia; Computational linguistics; Heuristic methods; Websites; Computer aided language translation","Baidu;Google;Elsevier;Microsoft Research;Korea Advanced Institute of Science and Technology (KAIST)","50th Annual Meeting of the Association for Computational Linguistics, ACL 2012","8 July 2012 through 14 July 2012","Jeju Island",97156,Conference Paper,"Final","",Scopus,2-s2.0-84876797872
"Tabor W., Juliano C., Tanenhaus M.K.","6603814753;6603926717;7003537050;","Parsing in a Dynamical System: An Attractor-based Account of the Interaction of Lexical and Structural Constraints in Sentence Processing",1997,"Language and Cognitive Processes","12","2-3",,"211","271",,130,"10.1080/016909697386853","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0031486856&doi=10.1080%2f016909697386853&partnerID=40&md5=eba8a8e20299ab614f47fdca5b0d35da","A dynamical systems approach to parsing is proposed in which syntactic hypotheses are associated with attractors in a metric space. These attractors have many of the properties of traditional syntactic categories, while at the same time encoding context-dependent, lexically specific distinctions. Hypotheses motivated by the dynamical system theory were tested in four reading time experiments examining the interaction of simple lexical frequencies, frequencies that are contingent on an environment defined by syntactic categories, and frequencies contingent on verb argument structure. The experiments documented a variety of contingent frequency effects that cut across traditional linguistic grains, each of which was predicted by the dynamical systems model. These effects were simulated in an implementation of the theory, employing a recurrent network trained from a corpus to construct metric representations and an algorithm implementing a gravitational dynamical system to model reading time as time to gravitate to an attractor.",,,,,,,,Article,"Final","",Scopus,2-s2.0-0031486856
"Evans R., Grefenstette E.","55713782300;51664755300;","Learning explanatory rules from noisy data",2018,"Journal of Artificial Intelligence Research","61",,,"65","170",,128,"10.1613/jair.5477","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041111935&doi=10.1613%2fjair.5477&partnerID=40&md5=98673491cbeae2b0215d194bab839689","Artificial Neural Networks are powerful function approximators capable of modelling solutions to a wide variety of problems, both supervised and unsupervised. As their size and expressivity increases, so too does the variance of the model, yielding a nearly ubiquitous overfitting problem. Although mitigated by a variety of model regularisation methods, the common cure is to seek large amounts of training data—which is not necessarily easily obtained—that sufficiently approximates the data distribution of the domain we wish to test on. In contrast, logic programming methods such as Inductive Logic Programming offer an extremely data-efficient process by which models can be trained to reason on symbolic domains. However, these methods are unable to deal with the variety of domains neural networks can be applied to: they are not robust to noise in or mislabelling of inputs, and perhaps more importantly, cannot be applied to non-symbolic domains where the data is ambiguous, such as operating on raw pixels. In this paper, we propose a Differentiable Inductive Logic framework, which can not only solve tasks which traditional ILP systems are suited for, but shows a robustness to noise and error in the training data which ILP cannot cope with. Furthermore, as it is trained by backpropagation against a likelihood objective, it can be hybridised by connecting it with neural networks over ambiguous data in order to be applied to domains which ILP cannot address, while providing data efficiency and generalisation beyond what neural networks on their own can achieve. © 2018 AI Access Foundation. All rights reserved.",,"Computer circuits; Logic programming; Neural networks; Ambiguous Data; Data distribution; Efficient process; Inductive logic; Over fitting problem; Powerful functions; Regularisation; Robustness to noise; Inductive logic programming (ILP)",,,,,,Review,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85041111935
"Lowd D., Domingos P.","12140683600;7003565655;","Efficient weight learning for Markov logic networks",2007,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","4702 LNAI",,,"200","211",,128,"10.1007/978-3-540-74976-9_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-38049174896&doi=10.1007%2f978-3-540-74976-9_21&partnerID=40&md5=f543e05638bd2d14c5a344ce63f8894e","Markov logic networks (MLNs) combine Markov networks and first-order logic, and are a powerful and increasingly popular representation for statistical relational learning. The state-of-the-art method for discriminative learning of MLN weights is the voted perceptron algorithm, which is essentially gradient descent with an MPE approximation to the expected sufficient statistics (true clause counts). Unfortunately, these can vary widely between clauses, causing the learning problem to be highly ill-conditioned, and making gradient descent very slow. In this paper, we explore several alternatives, from per-weight learning rates to second-order methods. In particular, we focus on two approaches that avoid computing the partition function: diagonal Newton and scaled conjugate gradient. In experiments on standard SRL datasets, we obtain order-of-magnitude speedups, or more accurate models given comparable learning times. © Springer-Verlag Berlin Heidelberg 2007.",,"Approximation algorithms; Database systems; Learning algorithms; Markov processes; Problem solving; Statistical methods; Efficient weight learning; First-order logic; Markov logic networks (MLN); Statistical relational learning; Computer networks",,"11th European Conference on Principles and Practice of Knowledge Discovery in Databases, PKDD 2007","17 September 2007 through 21 September 2007","Warsaw",71124,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-38049174896
"El-Beltagy S.R., Rafea A.","6506716427;55920312200;","KP-Miner: A keyphrase extraction system for English and Arabic documents",2009,"Information Systems","34","1",,"132","144",,127,"10.1016/j.is.2008.05.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-55549106775&doi=10.1016%2fj.is.2008.05.002&partnerID=40&md5=9ca27f83c9575a3caf9e501004b0c369","Automatic keyphrase extraction has many important applications including but not limited to summarization, cataloging/indexing, feature extraction for clustering and classification, and data mining. This paper presents the KP-Miner system, and demonstrates through experimentation and comparison with widely used systems that it is effective and efficient in extracting keyphrases from both English and Arabic documents of varied length. Unlike other existing keyphrase extraction systems, the KP-Miner system does not need to be trained on a particular document set in order to achieve its task. It also has the advantage of being configurable as the rules and heuristics adopted by the system are related to the general nature of documents and keyphrases. This implies that the users of this system can use their understanding of the document(s) being input into the system to fine-tune it to their particular needs. © 2008 Elsevier B.V. All rights reserved.","Automatic indexing; Heuristic rules; Keyphrase extraction","Automatic indexing; Decision support systems; Heuristic methods; Information management; Miners; Configurable; Document sets; General natures; Heuristic rules; Keyphrase extraction; Keyphrase extractions; Feature extraction",,,,,,Article,"Final","",Scopus,2-s2.0-55549106775
"Mirończuk M.M., Protasiewicz J.","15077100400;56732713300;","A recent overview of the state-of-the-art elements of text classification",2018,"Expert Systems with Applications","106",,,"36","54",,126,"10.1016/j.eswa.2018.03.058","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045127865&doi=10.1016%2fj.eswa.2018.03.058&partnerID=40&md5=e342b251fcd522137509f756eb61d76b","The aim of this study is to provide an overview the state-of-the-art elements of text classification. For this purpose, we first select and investigate the primary and recent studies and objectives in this field. Next, we examine the state-of-the-art elements of text classification. In the following steps, we qualitatively and quantitatively analyse the related works. Herein, we describe six baseline elements of text classification including data collection, data analysis for labelling, feature construction and weighing, feature selection and projection, training of a classification model, and solution evaluation. This study will help readers acquire the necessary information about these elements and their associated techniques. Thus, we believe that this study will assist other researchers and professionals to propose new studies in the field of text classification. © 2018 The Authors","Document classification; Document classification overview; Text classification; Text classification overview","Classification (of information); Information retrieval systems; Classification models; Data collection; Document Classification; Feature construction; New study; Related works; State of the art; Text classification; Text processing",,,,,,Review,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85045127865
"Amari S.-i.","35553922600;","A method of statistical neurodynamics",1974,"Kybernetik","14","4",,"201","215",,126,"10.1007/BF00274806","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0016375547&doi=10.1007%2fBF00274806&partnerID=40&md5=2fbebce612460a417f560cec13ba1021","A method of statistical neurodynamics is presented for treating ensembles of nets of randomly connected neuron-like elements. The concept of a macrostate plays a fundamental role in statistical neurodynamics and a criterion is given for ascertaining that given macroscopic quantities together constitute a macrostate. The activity of a nerve net is shown to be a macrostate and the equation of the dynamics of the activity is elucidated for various ensembles of random nerve nets. It is shown that the distance between two microstates can also be treated as a macrostate in a generalized sense. The equation of its dynamics represents how the distance between two states changes in the course of state transitions. The dynamics of distance reveals interesting microscopic properties of random nerve nets, such as the stability of state-transition, the transient lengths, etc. © 1974 Springer-Verlag.",,"anatomy; hemodynamics; mathematical model; model; nerve cell; statistics; theoretical study; Animal; Human; Mathematics; Models, Neurological; Neurons; Statistics",,,,,,Article,"Final","",Scopus,2-s2.0-0016375547
"Jing H., McKeown K.","","Cut and paste based text summarization",2000,"Proceedings of the 1st North American Chapter of the Association for Computational Linguistics Conference",,,,"178","185",,125,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0039141162
"Dunne R.A., Campbell N.A.","","On the pairing of the softmax activation and cross-entropy penalty functions and the derivation of the softmax activation function",1997,"Proceedings of the Eighth Australasian Conference on Neural Networks",,,,"181","185",,125,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-27544483995
"Al-Smadi M., Qawasmeh O., Al-Ayyoub M., Jararweh Y., Gupta B.","6602910969;57128645700;25926888700;25825095600;34770593700;","Deep Recurrent neural network vs. support vector machine for aspect-based sentiment analysis of Arabic hotels’ reviews",2018,"Journal of Computational Science","27",,,"386","393",,124,"10.1016/j.jocs.2017.11.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85034042570&doi=10.1016%2fj.jocs.2017.11.006&partnerID=40&md5=48b557101f2683f06388943e4f2582d5","In this research, state-of-the-art approaches based on supervised machine learning are presented to address the challenges of aspect-based sentiment analysis (ABSA) of Arabic Hotels’ reviews. Two approaches of deep recurrent neural network (RNN) and support vector machine (SVM) are implemented and trained along with lexical, word, syntactic, morphological, and semantic features. The proposed approaches are evaluated using a reference dataset of Arabic Hotels’ reviews. Evaluation results show that the SVM approach outperforms the other deep RNN approach in the research investigated tasks (T1: aspect category identification, T2: aspect opinion target expression (OTE) extraction, and T3: aspect sentiment polarity identification). Whereas, when focusing on the execution time required for training and testing the models, the deep RNN execution time was faster, especially for the second task. © 2017 Elsevier B.V.","Arabic reviews; Aspect-based sentiment analysis; Deep learning; Supervised machine learning","Data mining; Deep learning; Hotels; Recurrent neural networks; Semantics; Sentiment analysis; Supervised learning; Support vector machines; Evaluation results; Execution time; Opinion targets; Recurrent neural network (RNN); Semantic features; State-of-the-art approach; Supervised machine learning; Training and testing; Deep neural networks",,,,,,Article,"Final","",Scopus,2-s2.0-85034042570
"Yang F., Yang Z., Cohen W.W.","57198992330;57192119391;7202924370;","Differentiable learning of logical rules for knowledge base reasoning",2017,"Advances in Neural Information Processing Systems","2017-December",,,"2320","2329",,124,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047020078&partnerID=40&md5=cba21d8907da1bbed1394de06d06dc37","We study the problem of learning probabilistic first-order logical rules for knowledge base reasoning. This learning problem is difficult because it requires learning the parameters in a continuous space as well as the structure in a discrete space. We propose a framework, Neural Logic Programming, that combines the parameter and structure learning of first-order logical rules in an end-to-end differentiable model. This approach is inspired by a recently-developed differentiable logic called TensorLog [5], where inference tasks can be compiled into sequences of differentiable operations. We design a neural controller system that learns to compose these operations. Empirically, our method outperforms prior work on multiple knowledge base benchmark datasets, including Freebase and WikiMovies. © 2017 Neural information processing systems foundation. All rights reserved.",,"Computer circuits; Formal logic; Logic programming; Benchmark datasets; Continuous spaces; Discrete spaces; Knowledge base; Learning problem; Logical rules; Neural controller; Structure-learning; Knowledge based systems","","31st Annual Conference on Neural Information Processing Systems, NIPS 2017","4 December 2017 through 9 December 2017",,136033,Conference Paper,"Final","",Scopus,2-s2.0-85047020078
"Poveda-Villalón M., Suárez-Figueroa M.C., Gómez-Pérez A.","55236543100;56005389600;6603934482;","Validating ontologies with OOPS!",2012,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7603 LNAI",,,"267","281",,124,"10.1007/978-3-642-33876-2_24","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84867666599&doi=10.1007%2f978-3-642-33876-2_24&partnerID=40&md5=75b80c0ab66b09a2cb7fbff8f76387bc","Ontology quality can be affected by the difficulties involved in ontology modelling which may imply the appearance of anomalies in ontologies. This situation leads to the need of validating ontologies, that is, assessing their quality and correctness. Ontology validation is a key activity in different ontology engineering scenarios such as development and selection. This paper contributes to the ontology validation activity by proposing a web-based tool, called OOPS!, independent of any ontology development environment, for detecting anomalies in ontologies. This tool will help developers to improve ontology quality by automatically detecting potential errors. © 2012 Springer-Verlag.","ontology; ontology evaluation; ontology validation; pitfalls","Ontology development; Ontology engineering; Ontology modelling; pitfalls; Potential errors; Web-based tools; Knowledge engineering; Ontology; Knowledge management","Elsevier;Fluid Operations;Interaction-Design.org;STI International;Digital Enterprise Research Institute (DERI)","18th International Conference on Knowledge Engineering and Knowledge Management, EKAW 2012","8 October 2012 through 12 October 2012","Galway City",93240,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84867666599
"Zampieri M., Malmasi S., Nakov P., Rosenthal S., Farra N., Kumar R.","8948587300;56578472800;15043153900;56266449200;36986781300;55492176800;","Predicting the type and target of offensive posts in social media",2019,"NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference","1",,,"1415","1420",,123,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072852138&partnerID=40&md5=9971b931e0f927b4f1143f26a60fe502","As offensive content has become pervasive in social media, there has been much research in identifying potentially offensive messages. However, previous work on this topic did not consider the problem as a whole, but rather focused on detecting very specific types of offensive content, e.g., hate speech, cyberbulling, or cyber-aggression. In contrast, here we target several different kinds of offensive content. In particular, we model the task hierarchically, identifying the type and the target of offensive messages in social media. For this purpose, we complied the Offensive Language Identification Dataset (OLID), a new dataset with tweets annotated for offensive content using a fine-grained three-layer annotation scheme, which we make publicly available. We discuss the main similarities and differences between OLID and pre-existing datasets for hate speech identification, aggression detection, and similar tasks. We further experiment with and we compare the performance of different machine learning models on OLID. © 2019 Association for Computational Linguistics",,"Computational linguistics; Social networking (online); Aggression detections; Annotation scheme; Fine grained; Machine learning models; Offensive languages; Offensive messages; Social media; Speech identification; Speech recognition","Amazon;ASAPP;Bloomberg Engineering;et al.;facebook;Google","2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019","2 June 2019 through 7 June 2019",,159851,Conference Paper,"Final","",Scopus,2-s2.0-85072852138
"Pei W., Ge T., Chang B.","56017460100;55953962900;55837183200;","Max-margin tensor neural network for Chinese word segmentation",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference","1",,,"293","303",,123,"10.3115/v1/p14-1028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906927532&doi=10.3115%2fv1%2fp14-1028&partnerID=40&md5=aa82856ce6a7f7c0878918ac187c4b86","Recently, neural network models for natural language processing tasks have been increasingly focused on for their ability to alleviate the burden of manual feature engineering. In this paper, we propose a novel neural network model for Chinese word segmentation called Max-Margin Tensor Neural Network (MMTNN). By exploiting tag embeddings and tensorbased transformation, MMTNN has the ability to model complicated interactions between tags and context characters. Furthermore, a new tensor factorization approach is proposed to speed up the model and avoid overfitting. Experiments on the benchmark dataset show that our model achieves better performances than previous neural network models and that our model can achieve a competitive performance with minimal feature engineering. Despite Chinese word segmentation being a specific case, MMTNN can be easily generalized and applied to other sequence labeling tasks. © 2014 Association for Computational Linguistics.",,"Benchmarking; Natural language processing systems; Neural networks; Tensors; Benchmark datasets; Chinese word segmentation; Competitive performance; Feature engineerings; NAtural language processing; Neural network model; Novel neural network; Tensor factorization; Computational linguistics","amazon.com;Baidu;Bloomberg;et al.;Google;IBM Watson","52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014","22 June 2014 through 27 June 2014","Baltimore, MD",107373,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84906927532
"Kou G., Yang P., Peng Y., Xiao F., Chen Y., Alsaadi F.E.","57068290300;57211921159;36022039700;57211923575;43960920900;57204294532;","Evaluation of feature selection methods for text classification with small datasets using multiple criteria decision-making methods",2020,"Applied Soft Computing Journal","86",,"105836","","",,121,"10.1016/j.asoc.2019.105836","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075378577&doi=10.1016%2fj.asoc.2019.105836&partnerID=40&md5=c17224413691aaf0aa9837bb5fd3f31d","The evaluation of feature selection methods for text classification with small sample datasets must consider classification performance, stability, and efficiency. It is, thus, a multiple criteria decision-making (MCDM) problem. Yet there has been few research in feature selection evaluation using MCDM methods which considering multiple criteria. Therefore, we use MCDM-based methods for evaluating feature selection methods for text classification with small sample datasets. An experimental study is designed to compare five MCDM methods to validate the proposed approach with 10 feature selection methods, nine evaluation measures for binary classification, seven evaluation measures for multi-class classification, and three classifiers with 10 small datasets. Based on the ranked results of the five MCDM methods, we make recommendations concerning feature selection methods. The results demonstrate the effectiveness of the used MCDM-based method in evaluating feature selection methods. © 2019 The Author(s)","Feature selection; MCDM; Small sample dataset; Text classification","Decision making; Feature extraction; Text processing; Classification performance; Feature selection methods; MCDM; Multi-class classification; Multiple criteria decision making; Multiple criteria decision-making problems; Small samples; Text classification; Classification (of information)",,,,,,Article,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85075378577
"Stevenson R.A., Mikels J.A., James T.W.","16242505400;6507399574;57212072372;","Characterization of the Affective Norms for English Words by discrete emotional categories",2007,"Behavior Research Methods","39","4",,"1020","1024",,120,"10.3758/BF03192999","https://www.scopus.com/inward/record.uri?eid=2-s2.0-37349084229&doi=10.3758%2fBF03192999&partnerID=40&md5=788c0a8cfcc3d5ae89f8eaa374f97c72","The Affective Norms for English Words (ANEW) are a commonly used set of 1,034 words characterized on the affective dimensions of valence, arousal, and dominance. Traditionally, studies of affect have used stimuli characterized along either affective dimensions or discrete emotional categories, but much current research draws on both of these perspectives. As such, stimuli that have been thoroughly characterized according to both of these approaches are exceptionally useful. In an effort to provide researchers with such a characterization of stimuli, we have collected descriptive data on the ANEW to identify which discrete emotions are elicited by each word in the set. Our data, coupled with previous characterizations of the dimensional aspects of these words, will allow researchers to control for or manipulate stimulus properties in accordance with both dimensional and discrete emotional views, and provide an avenue for further integration of these two perspectives. Our data have been archived at www.psychonomic.org/archive/. Copyright 2007 Psychonomic Society, Inc.",,,,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-37349084229
"Dong Z., Dong Q.","52163429200;57201600773;","HowNet - A hybrid language and knowledge resource",2003,"NLP-KE 2003 - 2003 International Conference on Natural Language Processing and Knowledge Engineering, Proceedings",,,"1276017","820","824",,119,"10.1109/NLPKE.2003.1276017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-26844553261&doi=10.1109%2fNLPKE.2003.1276017&partnerID=40&md5=aaf60293dce3f6fd4ea83da038419978","HowNet is an on-line common-sense knowledge base unveiling inter-conceptual relations and inter-attribute relations of concepts as connoting in Chinese and English bilingual lexicons. Since it was released in 1999. HowNet has become more and more popular and been applied in various areas of NLP, related to both Chinese and English. This paper shows a general picture of HowNet. discusses its theory and peculiarities. © 2003 IEEE.","HowNet; Knowledge base; Lexicon; Ontology; Semantics","Computational linguistics; Knowledge based systems; Knowledge engineering; Ontology; Semantics; Bilingual lexicons; Commonsense knowledge; Conceptual relations; HowNet; Hybrid languages; Knowledge base; Knowledge resource; Lexicon; Natural language processing systems","Beijing Network and Multimedia Lab;Chinese Association for Artificial Intelligence;Chinese Information Processing Society of China;Chinese Institute of Electronics;et al.;Natural Science Foundation of China","International Conference on Natural Language Processing and Knowledge Engineering, NLP-KE 2003","26 October 2003 through 29 October 2003",,114460,Conference Paper,"Final","",Scopus,2-s2.0-26844553261
"Xing F.Z., Cambria E., Welsch R.E.","57196019442;56140547500;8214812500;","Natural language based financial forecasting: a survey",2018,"Artificial Intelligence Review","50","1",,"49","73",,117,"10.1007/s10462-017-9588-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032466713&doi=10.1007%2fs10462-017-9588-9&partnerID=40&md5=804a3970c8530b4c69eaad7cabc2a9ea","Natural language processing (NLP), or the pragmatic research perspective of computational linguistics, has become increasingly powerful due to data availability and various techniques developed in the past decade. This increasing capability makes it possible to capture sentiments more accurately and semantics in a more nuanced way. Naturally, many applications are starting to seek improvements by adopting cutting-edge NLP techniques. Financial forecasting is no exception. As a result, articles that leverage NLP techniques to predict financial markets are fast accumulating, gradually establishing the research field of natural language based financial forecasting (NLFF), or from the application perspective, stock market prediction. This review article clarifies the scope of NLFF research by ordering and structuring techniques and applications from related work. The survey also aims to increase the understanding of progress and hotspots in NLFF, and bring about discussions across many different disciplines. © 2017, Springer Science+Business Media B.V.","Computational finance; Financial forecasting; Knowledge engineering; Natural language processing; Predictive analytics; Text mining","Commerce; Data mining; Engineering research; Finance; Financial markets; Forecasting; Knowledge engineering; Predictive analytics; Semantics; Surveys; Computational finance; Data availability; Financial forecasting; Natural languages; Nlp techniques; Research fields; Stock market prediction; Text mining; Natural language processing systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85032466713
"Appel O., Chiclana F., Carter J., Fujita H.","56664354200;9134178600;55454576600;35611951900;","A hybrid approach to the sentiment analysis problem at the sentence level",2016,"Knowledge-Based Systems","108",,,"110","124",,117,"10.1016/j.knosys.2016.05.040","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84970016111&doi=10.1016%2fj.knosys.2016.05.040&partnerID=40&md5=8632b9a4442b004cb0811df10aeabb1c","The objective of this article is to present a hybrid approach to the Sentiment Analysis problem at the sentence level. This new method uses natural language processing (NLP) essential techniques, a sentiment lexicon enhanced with the assistance of SentiWordNet, and fuzzy sets to estimate the semantic orientation polarity and its intensity for sentences, which provides a foundation for computing with sentiments. The proposed hybrid method is applied to three different data-sets and the results achieved are compared to those obtained using Naïve Bayes and Maximum Entropy techniques. It is demonstrated that the presented hybrid approach is more accurate and precise than both Naïve Bayes and Maximum Entropy techniques, when the latter are utilised in isolation. In addition, it is shown that when applied to datasets containing snippets, the proposed method performs similarly to state of the art techniques. © 2016","Computing with sentiments; Fuzzy sets; Maximum entropy; Naïve Bayes; Semantic rules; Sentiment analysis; SentiWordNet; Unsupervised machine learning","Artificial intelligence; Data mining; Entropy; Fuzzy sets; Learning algorithms; Learning systems; Natural language processing systems; Semantics; Sodium; Computing with sentiments; Semantic rules; Sentiment analysis; SentiWordNet; Unsupervised machine learning; Maximum entropy methods",,,,,,Article,"Final","",Scopus,2-s2.0-84970016111
"Komer B., Bergstra J., Eliasmith C.","","Hyperopt-sklearn: Automatic hyperparameter configuration for scikit-learn",2014,"ICML Workshop on AutoML",,,,"2825","2830",,117,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84908279482
"Honnibal Matthew, Montani Ines, Van Landeghem Sofie, Boyd Adriane","","spaCy: Industrial-strength Natural Language Processing in Python",2020,"spaCy: Industrial-strength Natural Language Processing in Python",,,,"","",,116,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85100566793
"Shen Y., Huang P.-S., Gao J., Chen W.","56729408300;47061231500;55702627000;23007589000;","ReasoNet: Learning to stop reading in machine comprehension",2017,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining","Part F129685",,,"1047","1055",,115,"10.1145/3097983.3098177","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029117561&doi=10.1145%2f3097983.3098177&partnerID=40&md5=9288a8761ecb5d18ebe3c055804cfa25","Teaching a computer to read and answer general questions pertaining to a document is a challenging yet unsolved problem. In this paper, we describe a novel neural network architecture called the Reasoning Network (ReasoNet) for machine comprehension tasks. ReasoNets make use of multiple turns to effectively exploit and then reason over the relation among queries, documents, and answers. Different from previous approaches using a fixed number of turns during inference, ReasoNets introduce a termination state to relax this constraint on the reasoning depth. With the use of reinforcement learning, ReasoNets can dynamically determine whether to continue the comprehension process after digesting intermediate results, or to terminate reading when it concludes that existing information is adequate to produce an answer. ReasoNets achieve superior performance in machine comprehension datasets, including unstructured CNN and Daily Mail datasets, the Stanford SQuAD dataset, and a structured Graph Reachability dataset. © 2017 ACM.","Deep reinforcement learning; Machine reading comprehension; ReasoNet","Data mining; Network architecture; Neural networks; Comprehension process; Comprehension tasks; Intermediate results; Novel neural network; Reading comprehension; ReasoNet; Structured graphs; Unsolved problems; Reinforcement learning","ACM SIGKDD;ACM SIGMOD","23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2017","13 August 2017 through 17 August 2017",,129685,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85029117561
"Xiong W., Hoang T., Wang W.Y.","57204282857;57211254190;35726254300;","DeepPath: A reinforcement learning method for knowledge graph reasoning",2017,"EMNLP 2017 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"564","573",,115,"10.18653/v1/d17-1060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073145268&doi=10.18653%2fv1%2fd17-1060&partnerID=40&md5=e49f8273a70d50d792ebba80c80bbc3f","We study the problem of learning to reason in large scale knowledge graphs (KGs). More specifically, we describe a novel reinforcement learning framework for learning multi-hop relational paths: we use a policy-based agent with continuous states based on knowledge graph embeddings, which reasons in a KG vector space by sampling the most promising relation to extend its path. In contrast to prior work, our approach includes a reward function that takes the accuracy, diversity, and efficiency into consideration. Experimentally, we show that our proposed method outperforms a path-ranking based algorithm and knowledge graph embedding methods on Freebase and Never-Ending Language Learning datasets.1 © 2017 Association for Computational Linguistics.",,"Embeddings; Graph theory; Machine learning; Natural language processing systems; Vector spaces; Continuous state; Knowledge graphs; Language learning; Multihop; Path ranking; Policy-based; Reinforcement learning method; Reward function; Reinforcement learning","Amazon;Apple;Baidu;et al.;Facebook;Google","2017 Conference on Empirical Methods in Natural Language Processing, EMNLP 2017","9 September 2017 through 11 September 2017",,150071,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85073145268
"Walker C., Strassel S., Medero J., Maeda K.","","Ace 2005 multilingual training corpus. Linguistic data consortium",2006,"ACE 2005 Multilingual Training Corpus",,,,"","",,115,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-80052410254
"Lauer F., Bloch G.","22333888400;7005864326;","Incorporating prior knowledge in support vector machines for classification: A review",2008,"Neurocomputing","71","7-9",,"1578","1594",,114,"10.1016/j.neucom.2007.04.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-40649086418&doi=10.1016%2fj.neucom.2007.04.010&partnerID=40&md5=5e5a81ece046db2982a18510a68ff011","For classification, support vector machines (SVMs) have recently been introduced and quickly became the state of the art. Now, the incorporation of prior knowledge into SVMs is the key element that allows to increase the performance in many applications. This paper gives a review of the current state of research regarding the incorporation of two general types of prior knowledge into SVMs for classification. The particular forms of prior knowledge considered here are presented in two main groups: class-invariance and knowledge on the data. The first one includes invariances to transformations, to permutations and in domains of input space, whereas the second one contains knowledge on unlabeled data, the imbalance of the training set or the quality of the data. The methods are then described and classified into the three categories that have been used in literature: sample methods based on the modification of the training data, kernel methods based on the modification of the kernel and optimization methods based on the modification of the problem formulation. A recent method, developed for support vector regression, considers prior knowledge on arbitrary regions of the input space. It is exposed here when applied to the classification case. A discussion is then conducted to regroup sample and optimization methods under a regularization framework. © 2007 Elsevier B.V. All rights reserved.","Classification; Invariance; Pattern recognition; Prior knowledge; Support vector machine (SVM)","Invariance; Optimization; Pattern recognition; Regression analysis; Class- invariance; Input space; Kernel methods; Permutations; Prior knowledge; Problem formulation; Training data; Support vector machines; article; artificial neural network; classification; computer program; information processing; kernel method; knowledge; machine learning; mathematical analysis; medical research; pattern recognition; priority journal; process optimization; quality control; scientific literature; support vector machine",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-40649086418
"Neelakantan A., Roth B., McCallum A.","57159776800;36165172500;7003773569;","Compositional vector space models for knowledge base completion",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference","1",,,"156","166",,113,"10.3115/v1/p15-1016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943805009&doi=10.3115%2fv1%2fp15-1016&partnerID=40&md5=879ba44126de07b93886fc12eeeeeb40","Knowledge base (KB) completion adds new facts to a KB by making inferences from existing facts, for example by inferring with high likelihood nationality(X,Y) from bornIn(X,Y). Most previous methods infer simple one-hop relational synonyms like this, or use as evidence a multi-hop relational path treated as an atomic feature, like bornIn(X,Z)!containedIn(Z,Y). This paper presents an approach that reasons about conjunctions of multi-hop relations non-Atomically, composing the implications of a path using a recurrent neural network (RNN) that takes as inputs vector embeddings of the binary relation in the path. Not only does this allow us to generalize to paths unseen at training time, but also, with a single high-capacity RNN, to predict new relation types not seen when the compositional model was trained (zero-shot learning). We assemble a new dataset of over 52M relational triples, and show that our method improves over a traditional classifier by 11%, and a method leveraging pre-trained embeddings by 7%. © 2015 Association for Computational Linguistics.",,"Classification (of information); Computational linguistics; Embeddings; Knowledge based systems; Recurrent neural networks; Vector spaces; Binary relation; Compositional modeling; High capacity; Knowledge base; Multihop; Recurrent neural network (RNN); Training time; Vector space models; Natural language processing systems","Alibaba Group;Baidu;CreditEase;et al.;Samsung;Tencent","53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015","26 July 2015 through 31 July 2015",,114195,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84943805009
"Cao Y., Wang X., He X., Hu Z., Chua T.-S.","57015851100;57191904438;56285637300;57209225981;7101702977;","Unifying knowledge graph learning and recommendation: Towards a better understanding of user preferences",2019,"The Web Conference 2019 - Proceedings of the World Wide Web Conference, WWW 2019",,,,"151","161",,112,"10.1145/3308558.3313705","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066889141&doi=10.1145%2f3308558.3313705&partnerID=40&md5=aac8e65993870975b9cabb1976f0469b","Incorporating knowledge graph (KG) into recommender system is promising in improving the recommendation accuracy and explainability. However, existing methods largely assume that a KG is complete and simply transfer the ""knowledge"" in KG at the shallow level of entity raw data or embeddings. This may lead to suboptimal performance, since a practical KG can hardly be complete, and it is common that a KG has missing facts, relations, and entities. Thus, we argue that it is crucial to consider the incomplete nature of KG when incorporating it into recommender system. In this paper, we jointly learn the model of recommendation and knowledge graph completion. Distinct from previous KG-based recommendation methods, we transfer the relation information in KG, so as to understand the reasons that a user likes an item. As an example, if a user has watched several movies directed by (relation) the same person (entity), we can infer that the director relation plays a critical role when the user makes the decision, thus help to understand the user's preference at a finer granularity. Technically, we contribute a new translation-based recommendation model, which specially accounts for various preferences in translating a user to an item, and then jointly train it with a KG completion model by combining several transfer schemes. Extensive experiments on two benchmark datasets show that our method outperforms state-of-the-art KG-based recommendation methods. Further analysis verifies the positive effect of joint training on both tasks of recommendation and KG completion, and the advantage of our model in understanding user preference. We publish our project at https://github.com/TaoMiner/joint-kg-recommender. © 2019 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY 4.0 License.","Embedding; Item Recommendation; Joint Model; Knowledge Graph","Embeddings; HTTP; World Wide Web; Embedding; Item Recommendation; Joint modeling; Knowledge graphs; Recommendation accuracy; Recommendation methods; Relation information; Sub-optimal performance; Recommender systems","Amazon;Bloomberg;Criteo AI Lab;et al.;Google;Microsoft","2019 World Wide Web Conference, WWW 2019","13 May 2019 through 17 May 2019",,147966,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85066889141
"Frank S.L., Otten L.J., Galli G., Vigliocco G.","7202570349;7005408598;56817076900;55919485500;","The ERP response to the amount of information conveyed by words in sentences",2015,"Brain and Language","140",,,"1","11",,111,"10.1016/j.bandl.2014.10.006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84910611051&doi=10.1016%2fj.bandl.2014.10.006&partnerID=40&md5=ca70b7c72152502369859c97348ed2f7","Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence's hierarchical structure for generating expectations about the upcoming word. © 2014 The Authors.","Entropy; Event-related potentials; Information theory; Reading; Sentence comprehension; Surprisal","adult; amplitude modulation; Article; artificial neural network; controlled study; entropy; event related potential; female; grammar; human; human experiment; language processing; male; measurement accuracy; normal human; reading; signal transduction; word recognition; evoked response; language; linguistics; physiology; probability; reading; Adult; Evoked Potentials; Female; Humans; Language; Linguistics; Male; Probability; Reading",,,,,,Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84910611051
"Zhou D., Schölkopf B.","8887012600;7004460308;","Regularization on discrete spaces",2005,"Lecture Notes in Computer Science","3663",,,"361","368",,111,"10.1007/11550518_45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-27244449175&doi=10.1007%2f11550518_45&partnerID=40&md5=0cf6f93b5134ec090c9693f8ba89609c","We consider the classification problem on a finite set of objects. Some of them are labeled, and the task is to predict the labels of the remaining unlabeled ones. Such an estimation problem is generally referred to as transductive inference. It is well-known that many meaningful inductive or supervised methods can be derived from a regularization framework, which minimizes a loss function plus a regularization term. In the same spirit, we propose a general discrete regularization framework defined on finite object sets, which can be thought of as discrete analogue of classical regularization theory. A family of transductive inference schemes is then systemically derived from the framework, including our earlier algorithm for transductive inference, with which we obtained encouraging results on many practical classification problems. The discrete regularization framework is built on discrete analysis and geometry developed by ourselves, in which a number of discrete differential operators of various orders are constructed, which can be thought of as discrete analogues of their counterparts in the continuous case. © Springer-Verlag Berlin Heidelberg 2005.",,"Algorithms; Classification (of information); Decision making; Finite difference method; Mathematical operators; Parameter estimation; Problem solving; Classical regularization theory; Discrete spaces; Finite set of objects; Regularization framework; Transductive inference; Pattern recognition",,"27th DAGM (German Association for Pattern Recognition) Symposium, DAGM 2005","31 August 2005 through 2 September 2005","Vienna",65878,Conference Paper,"Final","",Scopus,2-s2.0-27244449175
"Moore C.","35610564000;","Generalized shifts: Unpredictability and undecidability in dynamical systems",1991,"Nonlinearity","4","2","002","199","230",,111,"10.1088/0951-7715/4/2/002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-30244452047&doi=10.1088%2f0951-7715%2f4%2f2%2f002&partnerID=40&md5=8fd718f52b157692890a6c65c945af91","A class of shift-like dynamical systems is presented that displays a wide variety of behaviours. Three examples are presented along with some general definitions and results. A correspondence with Turing machines allows one to discuss issues of predictability and complexity. These systems possess a type of unpredictability qualitatively stronger than that which has been previously discussed in the study of low-dimensional chaos, and many simple questions about their dynamics are undecidable. The author discusses the complexity of various sets they generate, including periodic points, basins of attraction, and time series. Finally, he shows that they can be embedded in smooth maps in R 2, or smooth flows in R3.",,,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-30244452047
"Bozkurt A., Akgun-Ozbek E., Yilmazel S., Erdogdu E., Ucar H., Guler E., Sezgin S., Karadeniz A., Sen-Ersoy N., Goksel-Canbek N., Dincer G.D., Ari S., Aydin C.H.","56566181600;56566774300;56566706000;6505803551;56566358400;56566276800;6602821861;54953963000;56566739100;56566725600;36992088300;56094736200;36723258400;","Trends in distance education research: A content analysis of journals 2009-2013",2015,"International Review of Research in Open and Distance Learning","16","1",,"330","363",,110,"10.19173/irrodl.v16i1.1953","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84925351385&doi=10.19173%2firrodl.v16i1.1953&partnerID=40&md5=52c90bb2a1fc98fdab149e8f41c43768","This study intends to explore the current trends in the field of distance education research during the period of 2009-2013. The trends were identified by an extensive review of seven peer reviewed scholarly journals: The American Journal of Distance Education (AJDE), Distance Education (DE), The European Journal of Open, Distance and e-Learning (EURODL), The Journal of Distance Education (JDE), The Journal of Online Learning and Technology (JOLT), Open Learning: The Journal of Open, Distance and e-Learning (OL) and The International Review of Research in Open and Distributed Learning (IRRODL). A total of 861 research articles was reviewed. Mainly content analysis was employed to be able to analyze the current research. Also, a social network analysis (SNA) was used to interpret the interrelationship between keywords indicated in these articles. Themes were developed and the content of the articles in the selected journals were coded according to categories derived from earlier studies. The results were interpreted using descriptive analysis (frequencies) and social network analysis. The reporting of the results were organized into the following categories: research areas, theoretical and conceptual frameworks, variables, methods, models, strategies, data collection and analysis methods, and the participants. The study also identified the most commonly used keywords, and the most frequently cited authors and studies in distance education. The findings obtained in this study may be useful in the exploration of potential research areas and identification of neglected areas in the field of distance education. © Bozkurt, Akgun-Ozbek, Yilmazel, Erdogdu, Ucar, Guler, Sezgin, Karadeniz, Sen-Ersoy, Goksel-Canbek, Dincer, Ari, and Aydin.","Content analysis; Distance education issues; Distance education trends; Research evaluation",,,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-84925351385
"McGhee R.B.","7004639231;","Some finite state aspects of legged locomotion",1968,"Mathematical Biosciences","2","1-2",,"67","84",,110,"10.1016/0025-5564(68)90007-2","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34848851673&doi=10.1016%2f0025-5564%2868%2990007-2&partnerID=40&md5=3905399bcd9e3479944b17ea7e32758b","Animal locomotion systems making use of legs as the basic component for support and propulsion can be studied from the point of view of finite state machine theory by regarding each leg as an elementary two-state sequential machine. The two states are simply the state of being in contact with the supporting surface and the state of being raised above it. This idealization permits the construction of a general theory of locomotion equally applicable to animals and legged locomotion machines. Such a theory can be made sufficiently complete to permit the synthesis of finite control algorithms capable of coordinating limb movements in either animals or machines. The validity of the finite state approach has been established by the construction and testing of an artificial quadruped based entirely upon finite state principles. © 1968.",,,,,,,,Article,"Final","",Scopus,2-s2.0-34848851673
"Cambria E., Fu J., Bisio F., Poria S.","56140547500;36620988300;55547779800;55316592700;","AffectiveSpace 2: Enabling affective intuition for concept-level sentiment analysis",2015,"Proceedings of the National Conference on Artificial Intelligence","1",,,"508","514",,109,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959482068&partnerID=40&md5=609838314ab306caace8dc0edf9281f2","Predicting the affective valence of unknown multi-word expressions is key for concept-level sentiment analysis. AffectiveSpace 2 is a vector space model, built by means of random projection, that allows for reasoning by analogy on natural language concepts. By reducing the dimensionality of affective common-sense knowledge, the model allows semantic features associated with concepts to be generalized and, hence, allows concepts to be intuitively clustered according to their semantic and affective relatedness. Such an affective intuition (so called because it does not rely on explicit features, but rather on implicit analogies) enables the inference of emotions and polarity conveyed by multiword expressions, thus achieving efficient concept-level sentiment analysis. Copyright © 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Data mining; Semantics; Vector spaces; Commonsense knowledge; Multi-word expressions; Natural languages; Random projections; Reasoning by analogies; Semantic features; Sentiment analysis; Vector space models; Natural language processing systems","AI Journal;Association for the Advancement of Artificial Intelligence (AAAI);Baidu;et al.;Infosys;National Science Foundation","29th AAAI Conference on Artificial Intelligence, AAAI 2015 and the 27th Innovative Applications of Artificial Intelligence Conference, IAAI 2015","25 January 2015 through 30 January 2015",,114360,Conference Paper,"Final","",Scopus,2-s2.0-84959482068
"Grefenstette E., Hermann K.M., Suleyman M., Blunsom P.","51664755300;55667900000;57189098021;15043692000;","Learning to transduce with unbounded memory",2015,"Advances in Neural Information Processing Systems","2015-January",,,"1828","1836",,109,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965153738&partnerID=40&md5=e2023caa056e3df36b936659464bdcec","Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.",,"Bacteriophages; Information science; Continuously differentiable; Generalisation; Machine translations; Natural languages; Recurrent networks; Unbounded memory; Recurrent neural networks","et al.;Google, Inc.;Ketchum Trading;Microsoft;Taobao (China) Software Co., Ltd. (Alibaba);Twitter","29th Annual Conference on Neural Information Processing Systems, NIPS 2015","7 December 2015 through 12 December 2015",,120037,Conference Paper,"Final","",Scopus,2-s2.0-84965153738
"Pérez-Rosas V., Mihalcea R., Morency L.-P.","38961879200;8619220500;6603047400;","Utterance-level multimodal sentiment analysis",2013,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference","1",,,"973","982",,109,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907316816&partnerID=40&md5=30d1c69dcbe2ffd38ff834cf6d43953c","During real-life interactions, people are naturally gesturing and modulating their voice to emphasize specific points or to express their emotions. With the recent growth of social websites such as YouTube, Facebook, and Amazon, video reviews are emerging as a new source of multimodal and natural opinions that has been left almost untapped by automatic opinion analysis techniques. This paper presents a method for multimodal sentiment classification, which can identify the sentiment expressed in utterance-level visual datastreams. Using a new multimodal dataset consisting of sentiment annotated utterances extracted from video reviews, we show that multimodal sentiment analysis can be effectively performed, and that the joint use of visual, acoustic, and linguistic modalities can lead to error rate reductions of up to 10.5% as compared to the best performing individual modality. © 2013 Association for Computational Linguistics.",,"Computational linguistics; Speech recognition; Error rate reduction; Facebook; Multi-modal; Multi-modal dataset; New sources; Opinion analysis; Sentiment analysis; Sentiment classification; Data mining","Baidu;et al.;Google;Microsoft Research;ontotext;Qatar Computing Research Institute (QCRI)","51st Annual Meeting of the Association for Computational Linguistics, ACL 2013","4 August 2013 through 9 August 2013","Sofia",107371,Conference Paper,"Final","",Scopus,2-s2.0-84907316816
"Coster W., Kauchak D.","55145880500;12140512500;","Simple English Wikipedia: A new text simplification task",2011,"ACL-HLT 2011 - Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies","2",,,"665","669",,109,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859060885&partnerID=40&md5=359d61950ac1d2a24fc8f8f2f89cdfcd","In this paper we examine the task of sentence simplification which aims to reduce the reading complexity of a sentence by incorporating more accessible vocabulary and sentence structure. We introduce a new data set that pairs English Wikipedia with Simple English Wikipedia and is orders of magnitude larger than any previously examined for sentence simplification. The data contains the full range of simplification operations including rewording, reordering, insertion and deletion. We provide an analysis of this corpus as well as preliminary results using a phrase-based translation approach for simplification. © 2011 Association for Computational Linguistics.",,"Data sets; Orders of magnitude; Sentence structures; Wikipedia; Computational linguistics; Websites","Google;Baidu;Microsoft Research;Pacific Northwest National Laboratory;Yahoo","49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, ACL-HLT 2011","19 June 2011 through 24 June 2011","Portland, OR",89192,Conference Paper,"Final","",Scopus,2-s2.0-84859060885
"Liu F., Flanigan J., Thomson S., Sadeh N., Smith N.A.","55717322200;55149217900;56350022300;6603834332;37044239600;","Toward abstractive summarization using semantic representations",2015,"NAACL HLT 2015 - 2015 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Proceedings of the Conference",,,,"1077","1086",,106,"10.3115/v1/n15-1114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957578529&doi=10.3115%2fv1%2fn15-1114&partnerID=40&md5=1885dd3a2a7e2414b3b0fbefbe9a6e06","We present a novel abstractive summarization framework that draws on the recent development of a treebank for the Abstract Meaning Representation (AMR). In this framework, the source text is parsed to a set of AMR graphs, the graphs are transformed into a summary graph, and then text is generated from the summary graph. We focus on the graph-to-graph transformation that reduces the source semantic graph into a summary graph, making use of an existing AMR parser and assuming the eventual availability of an AMR-to-text generator. The framework is data-driven, trainable, and not specifically designed for a particular domain. Experiments on gold-standard AMR annotations and system parses show promising results. Code is available at: https://github.com/summarization. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Natural language processing systems; Semantics; Data driven; Gold standards; Graph Transformation; Semantic graphs; Semantic representation; Source text; Text generators; Treebanks; Abstracting","A9;Bloomberg;et al.;Goldman Sachs;Google;IBM Watson","Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2015","31 May 2015 through 5 June 2015",,116756,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84957578529
"Rao D., McNamee P., Dredze M.","","Entity linking: Finding extracted entities in a knowledge base",2013,"Multi-source, Multilingual Information Extraction and Summarization",,,,"93","115",,106,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84901365491
"Metzler D., Croft W.B.","57204248523;7006788293;","Analysis of statistical question classification for fact-based questions",2005,"Information Retrieval","8","3",,"481","504",,106,"10.1007/s10791-005-6995-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-17444401767&doi=10.1007%2fs10791-005-6995-3&partnerID=40&md5=eec25e52e83b844febc58aff4cbcc3fb","Question classification systems play an important role in question answering systems and can be used in a wide range of other domains. The goal of question classification is to accurately assign labels to questions based on expected answer type. Most approaches in the past have relied on matching questions against hand-crafted rules. However, rules require laborious effort to create and often suffer from being too specific. Statistical question classification methods overcome these issues by employing machine learning techniques. We empirically show that a statistical approach is robust and achieves good performance on three diverse data sets with little or no hand tuning. Furthermore, we examine the role different syntactic and semantic features have on performance. We find that semantic features tend to increase performance more than purely syntactic features. Finally, we analyze common causes of misclassification error and provide insight into ways they may be overcome. © 2005 Springer Science + Business Media, Inc.","Machine learning; Question answering; Question classification; Semantic features; Support Vector Machines; Syntactic features; WordNet",,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-17444401767
"Gangemi A., Presutti V., Reforgiato Recupero D.","55605133800;55885160000;57206674454;","Frame-based detection of opinion holders and topics: A model and a tool",2014,"IEEE Computational Intelligence Magazine","9","1","6710244","20","30",,105,"10.1109/MCI.2013.2291688","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84893375456&doi=10.1109%2fMCI.2013.2291688&partnerID=40&md5=d4133e755c2d02956702b55a9b8dc5ab","Sentilo is a model and a tool to detect holders and topics of opinion sentences. Sentilo implements an approach based on the neo-Davidsonian assumption that events and situations are the primary entities for contextualizing opinions, which makes it able to distinguish holders, main topics, and sub-topics of an opinion. It uses a heuristic graph mining approach that relies on FRED, a machine reader for the Semantic Web that leverages Natural Language Processing (NLP) and Knowledge Representation (KR) components jointly with cognitively-inspired frames. The evaluation results are excellent for holder detection (F1: 95%), very good for subtopic detection (F1: 78%), and good for topic detection (F1: 68%). © 2014 IEEE.",,"Evaluation results; Frame-based; Graph mining; NAtural language processing; Topic detection; Knowledge representation; Machine components; Natural language processing systems; Tools",,,,,,Article,"Final","",Scopus,2-s2.0-84893375456
"Rezaeinia S.M., Rahmani R., Ghodsi A., Veisi H.","55351756400;14822556100;55522191698;55534141083;","Sentiment analysis based on improved pre-trained word embeddings",2019,"Expert Systems with Applications","117",,,"139","147",,103,"10.1016/j.eswa.2018.08.044","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054032852&doi=10.1016%2fj.eswa.2018.08.044&partnerID=40&md5=eccff809f22e10a4da85acd7210143eb","Sentiment analysis is a fast growing area of research in natural language processing (NLP) and text classifications. This technique has become an essential part of a wide range of applications including politics, business, advertising and marketing. There are various techniques for sentiment analysis, but recently word embeddings methods have been widely used in sentiment classification tasks. Word2Vec and GloVe are currently among the most accurate and usable word embedding methods which can convert words into meaningful vectors. However, these methods ignore sentiment information of texts and need a large corpus of texts for training and generating exact vectors. As a result, because of the small size of some corpora, researcher often have to use pre-trained word embeddings which were trained on other large text corpora such as Google News with about 100 billion words. The increasing accuracy of pre-trained word embeddings has a great impact on sentiment analysis research. In this paper, we propose a novel method, Improved Word Vectors (IWV), which increases the accuracy of pre-trained word embeddings in sentiment analysis. Our method is based on Part-of-Speech (POS) tagging techniques, lexicon-based approaches, word position algorithm and Word2Vec/GloVe methods. We tested the accuracy of our method via different deep learning models and benchmark sentiment datasets. Our experiment results show that Improved Word Vectors (IWV) are very effective for sentiment analysis. © 2018","Deep learning; GloVe; Natural language processing; Sentiment analysis; Word embeddings; Word2Vec","Classification (of information); Data mining; Deep learning; Linguistics; Marketing; Sentiment analysis; Vectors; Embeddings; GloVe; Learning models; Part of speech tagging; Sentiment classification; Text classification; Word embedding; Word2Vec; Natural language processing systems",,,,,,Article,"Final","",Scopus,2-s2.0-85054032852
"Poria S., Cambria E., Hazarika D., Vij P.","55316592700;56140547500;57200336259;57204218112;","A deeper look into sarcastic tweets using deep convolutional neural networks",2016,"COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers",,,,"1601","1612",,103,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046580416&partnerID=40&md5=bf924825ff4935dcfed6729e0d35a0d1","Sarcasm detection is a key task for many natural language processing tasks. In sentiment analysis, for example, sarcasm can flip the polarity of an ""apparently positive"" sentence and, hence, negatively affect polarity detection performance. To date, most approaches to sarcasm detection have treated the task primarily as a text categorization problem. Sarcasm, however, can be expressed in very subtle ways and requires a deeper understanding of natural language that standard text categorization techniques cannot grasp. In this work, we develop models based on a pre-trained convolutional neural network for extracting sentiment, emotion and personality features for sarcasm detection. Such features, along with the network's baseline features, allow the proposed models to outperform the state of the art on benchmark datasets. We also address the often ignored generalizability issue of classifying data that have not been seen by the models at learning phase. © 1963-2018 ACL.",,"Computational linguistics; Convolution; Natural language processing systems; Neural networks; Sentiment analysis; Text processing; Benchmark datasets; Convolutional neural network; Deep convolutional neural networks; Detection performance; Learning phase; Natural languages; State of the art; Text categorization; Deep neural networks",,"26th International Conference on Computational Linguistics, COLING 2016","11 December 2016 through 16 December 2016",,136517,Conference Paper,"Final","",Scopus,2-s2.0-85046580416
"Gao J., Galley M., Li L.","55702627000;15055739800;55730767800;","Neural approaches to conversational AI",2019,"Foundations and Trends in Information Retrieval","13","2-3",,"127","298",,102,"10.1561/1500000074","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062068996&doi=10.1561%2f1500000074&partnerID=40&md5=cb62608583853f5f16b8d4bde7bd543c","The present paper surveys neural approaches to conversational AI that have been developed in the last few years. We group conversational systems into three categories: (1) question answering agents, (2) task-oriented dialogue agents, and (3) chatbots. For each category, we present a review of state-of-the-art neural approaches, draw the connection between them and traditional approaches, and discuss the progress that has been made and challenges still being faced, using specific systems and models as case studies. © 2019 Now Publishers Inc. All Rights Reserved.",,"Information systems; Case-studies; Conversational systems; Paper surveys; Question-answering agents; State of the art; Task-oriented; Three categories; Traditional approaches; Computer science",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85062068996
"Long M., Wang J., Cao Y., Sun J., Yu P.S.","36986782000;16306106100;57189341730;7410371821;7402366049;","Deep learning of transferable representation for scalable domain adaptation",2016,"IEEE Transactions on Knowledge and Data Engineering","28","8","7452659","2027","2040",,102,"10.1109/TKDE.2016.2554549","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84978759588&doi=10.1109%2fTKDE.2016.2554549&partnerID=40&md5=550590e91ecad74d8c170e4478eaf29a","Domain adaptation generalizes a learning model across source domain and target domain that are sampled from different distributions. It is widely applied to cross-domain data mining for reusing labeled information and mitigating labeling consumption. Recent studies reveal that deep neural networks can learn abstract feature representation, which can reduce, but not remove, the cross-domain discrepancy. To enhance the invariance of deep representation and make it more transferable across domains, we propose a unified deep adaptation framework for jointly learning transferable representation and classifier to enable scalable domain adaptation, by taking the advantages of both deep learning and optimal two-sample matching. The framework constitutes two inter-dependent paradigms, unsupervised pre-training for effective training of deep models using deep denoising autoencoders, and supervised fine-tuning for effective exploitation of discriminative information using deep neural networks, both learned by embedding the deep representations to reproducing kernel Hilbert spaces (RKHSs) and optimally matching different domain distributions. To enable scalable learning, we develop a linear-time algorithm using unbiased estimate that scales linearly to large samples. Extensive empirical results show that the proposed framework significantly outperforms state of the art methods on diverse adaptation tasks: sentiment polarity prediction, email spam filtering, newsgroup content categorization, and visual object recognition. © 1989-2012 IEEE.","deep learning; denoising autoencoder; Domain adaptation; multiple kernel learning; neural network; two-sample test","Clustering algorithms; Learning systems; Neural networks; Object recognition; Auto encoders; Deep learning; Domain adaptation; Multiple Kernel Learning; Two-sample tests; Data mining",,,,,,Article,"Final","",Scopus,2-s2.0-84978759588
"Zajic D., Dorr B.J., Lin J., Schwartz R.","14016832000;35609548400;56824507200;7404170783;","Multi-candidate reduction: Sentence compression as a tool for document summarization tasks",2007,"Information Processing and Management","43","6",,"1549","1570",,102,"10.1016/j.ipm.2007.01.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547941814&doi=10.1016%2fj.ipm.2007.01.016&partnerID=40&md5=b4d33b7cfe4fece0b0f23dcffcf715d5","This article examines the application of two single-document sentence compression techniques to the problem of multi-document summarization-a ""parse-and-trim"" approach and a statistical noisy-channel approach. We introduce the multi-candidate reduction (MCR) framework for multi-document summarization, in which many compressed candidates are generated for each source sentence. These candidates are then selected for inclusion in the final summary based on a combination of static and dynamic features. Evaluations demonstrate that sentence compression is a valuable component of a larger multi-document summarization framework. © 2007 Elsevier Ltd. All rights reserved.","Headline generation; Hidden Markov model; Parse-and-trim; Summarization","Problem solving; Spurious signal noise; Statistical methods; Headline generation; Hidden Markov model; Parse-and-trim; Summarization; Text processing",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-34547941814
"Hovy E.","","Text summarization",2003,"The Oxford Handbook of Computational Linguistics",,,,"583","598",,102,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-24344475092
"Rocktäschel T., Riedel S.","55899274800;36562438800;","End-to-end differentiable proving",2017,"Advances in Neural Information Processing Systems","2017-December",,,"3789","3801",,101,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047004235&partnerID=40&md5=37880de79001c29848408342d38ed418","We introduce neural networks for end-to-end differentiable proving of queries to knowledge bases by operating on dense vector representations of symbols. These neural networks are constructed recursively by taking inspiration from the backward chaining algorithm as used in Prolog. Specifically, we replace symbolic unification with a differentiable computation on vector representations of symbols using a radial basis function kernel, thereby combining symbolic reasoning with learning subsymbolic vector representations. By using gradient descent, the resulting neural network can be trained to infer facts from a given incomplete knowledge base. It learns to (i) place representations of similar symbols in close proximity in a vector space, (ii) make use of such similarities to prove queries, (iii) induce logical rules, and (iv) use provided and induced logical rules for multi-hop reasoning. We demonstrate that this architecture outperforms ComplEx, a state-of-the-art neural link prediction model, on three out of four benchmark knowledge bases while at the same time inducing interpretable function-free first-order logic rules. © 2017 Neural information processing systems foundation. All rights reserved.",,"Formal logic; Knowledge based systems; Radial basis function networks; Vectors; Backward chaining; First order logic; Gradient descent; Incomplete knowledge; Radial basis function kernels; State of the art; Symbolic reasoning; Vector representations; Vector spaces","","31st Annual Conference on Neural Information Processing Systems, NIPS 2017","4 December 2017 through 9 December 2017",,136033,Conference Paper,"Final","",Scopus,2-s2.0-85047004235
"Aho A.V.","56914007400;","Nested Stack Automata",1969,"Journal of the ACM (JACM)","16","3",,"383","406",,101,"10.1145/321526.321529","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0013359184&doi=10.1145%2f321526.321529&partnerID=40&md5=5d4671b05f49abbc8825bf6af8388856",[No abstract available],,,,,,,,Article,"Final","",Scopus,2-s2.0-0013359184
"Franssen M., Lokhorst G.-J., Van De Poel I.","","Philosophy of technology",2009,"The Stanford Encyclopedia of Philosophy",,,,"","",,100,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-78650604339
"Hovy E., Lin C.-Y.","","Automated text summarization and the summarist system",1998,"Proceedings of A Workshop on Held at Baltimore",,,,"197","214",,100,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0003100146
"Dhingra B., Li L., Li X., Gao J., Chen Y.-N., Ahmed F., Deng L.","57193793350;55730767800;57194875411;55702627000;47061006000;57200331794;36071490500;","Towards end-to-end reinforcement learning of dialogue agents for information access",2017,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)","1",,,"484","495",,99,"10.18653/v1/P17-1045","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030087610&doi=10.18653%2fv1%2fP17-1045&partnerID=40&md5=c571cfd88d6caaedbd803bdfd2feb1df","This paper proposes KB-InfoBot1 - a multi-turn dialogue agent which helps users search Knowledge Bases (KBs) without composing complicated queries. Such goal-oriented dialogue agents typically need to interact with an external database to access real-world knowledge. Previous systems achieved this by issuing a symbolic query to the KB to retrieve entries based on their attributes. However, such symbolic operations break the differentiability of the system and prevent end-to-end training of neural dialogue agents. In this paper, we address this limitation by replacing symbolic queries with an induced ""soft"" posterior distribution over the KB that indicates which entities the user is interested in. Integrating the soft retrieval process with a reinforcement learner leads to higher task success rate and reward in both simulations and against real users. We also present a fully neural end-to-end agent, trained entirely from user feedback, and discuss its application towards personalized dialogue agents. © 2017 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Query processing; Differentiability; External database; Information access; ITS applications; Posterior distributions; Retrieval process; Search knowledge; Symbolic operations; Reinforcement learning","Amazon;Apple;Baidu;et al.;Google;Tencent","55th Annual Meeting of the Association for Computational Linguistics, ACL 2017","30 July 2017 through 4 August 2017",,132950,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85030087610
"Xia Y., Cambria E., Hussain A., Zhao H.","8868919200;56140547500;19734290900;57212617554;","Word Polarity Disambiguation Using Bayesian Model and Opinion-Level Features",2015,"Cognitive Computation","7","3","9298","369","380",,99,"10.1007/s12559-014-9298-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929702394&doi=10.1007%2fs12559-014-9298-4&partnerID=40&md5=72f887e1bd4c97539c82300742ef03f8","Contextual polarity ambiguity is an important problem in sentiment analysis. Many opinion keywords carry varying polarities in different contexts, posing huge challenges for sentiment analysis research. Previous work on contextual polarity disambiguation makes use of term-level context, such as words and patterns, and resolves the polarity with a range of rule-based, statistics-based or machine learning methods. The major shortcoming of these methods lies in that the term-level features sometimes are ineffective in resolving the polarity. In this work, opinion-level context is explored, in which intra-opinion features and inter-opinion features are finely defined. To enable effective use of opinion-level features, the Bayesian model is adopted to resolve the polarity in a probabilistic manner. Experiments with the Opinmine corpus demonstrate that opinion-level features can make a significant contribution in word polarity disambiguation in four domains. © 2014, Springer Science+Business Media New York.","Bayesian model; Opinion-level features; Sentiment analysis; Sentiment disambiguation","Artificial intelligence; Data mining; Learning systems; Bayesian model; Machine learning methods; Opinion-level features; Rule based; Sentiment analysis; Sentiment disambiguation; Bayesian networks",,,,,,Article,"Final","",Scopus,2-s2.0-84929702394
"Byrne R.M.J., Johnson-Laird P.N.","56018937500;7006769146;","'If' and the problems of conditional reasoning",2009,"Trends in Cognitive Sciences","13","7",,"282","287",,99,"10.1016/j.tics.2009.04.003","https://www.scopus.com/inward/record.uri?eid=2-s2.0-67649959757&doi=10.1016%2fj.tics.2009.04.003&partnerID=40&md5=d899a14ec189f5b0f800c0defea6d4bf","'If' is a puzzle. No consensus has existed about its meaning for over two thousand years. Here, we show how the main psychological theories deal with the seven crucial problems that it raises. These competing explanations treat 'if' as though it was a term in a formal logic, or as eliciting the construction of a mental model of the world, or as an instruction to suppose that a proposition holds. The solution to 'if' would be a major step towards understanding how people reason, and towards implementing a computer program that can reason in a human way. We argue that the mental model theory is closer to resolving the puzzle of 'if' than its competitors. © 2009 Elsevier Ltd. All rights reserved.",,"Formal logic; Mental model; Psychological theory; Cognitive systems; article; computer program; logic; mental function; psychological theory; theoretical model",,,,,,Article,"Final","",Scopus,2-s2.0-67649959757
"Cui L., Huang S., Wei F., Tan C., Duan C., Zhou M.","57192309549;57188864311;23995914700;56349942100;57225685102;55587890800;","Superagent: A customer service chatbot for E-commerce websites",2017,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of System Demonstrations",,,,"97","102",,97,"10.18653/v1/P17-4017","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051551317&doi=10.18653%2fv1%2fP17-4017&partnerID=40&md5=8c1195f85a2a659b74f2ac4b7f118cf6","Conventional customer service chatbots are usually based on human dialogue, yet significant issues in terms of data scale and privacy. In this paper, we present SuperAgent, a customer service chatbot that leverages large-scale and publicly available e-commerce data. Distinct from existing counterparts, SuperAgent takes advantage of data from in-page product descriptions as well as user-generated content from e-commerce websites, which is more practical and cost-effective when answering repetitive questions, freeing up human support staff to answer much higher value questions. We demonstrate SuperAgent as an add-on extension to mainstream web browsers and show its usefulness to user's online shopping experience. © 2017 Association for Computational Linguistics",,,"","55th Annual Meeting of the Association for Computational Linguistics, ACL 2017","30 July 2017 through 4 August 2017",,132945,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85051551317
"Mikolov T., Le Q.V., Sutskever I.","","Exploiting similarities among languages for machine translation",2013,"CoRR",,,,"","",,97,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84906927200
"Arditi D., Tokdemir O.B.","35614735000;6507059404;","Comparison of case-based reasoning and artificial neural networks",1999,"Journal of Computing in Civil Engineering","13","3",,"162","169",,97,"10.1061/(ASCE)0887-3801(1999)13:3(162)","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033156950&doi=10.1061%2f%28ASCE%290887-3801%281999%2913%3a3%28162%29&partnerID=40&md5=50387e7b22e956fe0b7abab5098d5df1","The outcome of construction litigation depends on a large number of factors. To predict the outcome of such litigation is difficult because of the complex interrelationships between these many factors. Two attempts are reported in the literature that use, respectively, case-based reasoning (CBR) and artificial neural networks (ANN) to overcome this difficulty. These studies were conducted by using the same 102 Illinois circuit court cases; an additional 12 cases were used for testing. Prediction rates of 83% in the CBR study and 67% in the ANN study were obtained. In this paper, CBR and ANN are compared, and their advantages and disadvantages are discussed in light of these two studies. It appears that CBR is more flexible when the system is updated with new cases, has better explanation facilities, and handles missing data and a large number of features better than ANN in this domain. If the use of CBR and ANN is understood better and if, as a result, the outcome of construction litigation can be predicted with reasonable accuracy and reliability, all parties involved in the construction process could save considerable money and time.",,"Artificial intelligence; Civil engineering; Laws and legislation; Neural networks; Reliability; Case-based reasoning (CBR); Construction litigation; Construction industry",,,,,,Article,"Final","",Scopus,2-s2.0-0033156950
"Han X., Cao S., Lv X., Lin Y., Liu Z., Sun M., Li J.","57205548124;57211208299;57211203656;57155321900;57191691341;7403180987;8304332600;","OpenKE: An open toolkit for knowledge embedding",2018,"EMNLP 2018 - Conference on Empirical Methods in Natural Language Processing: System Demonstrations, Proceedings",,,,"139","144",,96,"10.18653/v1/d18-2024","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067839536&doi=10.18653%2fv1%2fd18-2024&partnerID=40&md5=146296aaf1e48114c770e8c8b01a03f4","We release an open toolkit for knowledge embedding (OpenKE), which provides a unified framework and various fundamental models to embed knowledge graphs into a continuous low-dimensional space. OpenKE prioritizes operational efficiency to support quick model validation and large-scale knowledge representation learning. Meanwhile, OpenKE maintains sufficient modularity and extensibility to easily incorporate new models into the framework. Besides the toolkit, the embeddings of some existing large-scale knowledge graphs pre-trained by OpenKE are also available, which can be directly applied for many applications including information retrieval, personalized recommendation and question answering. The toolkit, documentation, and pre-trained embeddings are all released on http://openke.thunlp.org/. © 2018 Association for Computational Linguistics.",,"Embeddings; Knowledge representation; Open Data; Fundamental models; Knowledge embedding; Low-dimensional spaces; Model validation; Operational efficiencies; Personalized recommendation; Question Answering; Unified framework; Natural language processing systems",,"2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, EMNLP 2018","31 October 2018 through 4 November 2018",,150076,Conference Paper,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-85067839536
"Nguyen D.Q., Sirts K., Qu L., Johnson M.","35932254600;35276281200;57196124952;55574223118;","STransE: A novel embedding model of entities and relationships in knowledge bases",2016,"2016 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016 - Proceedings of the Conference",,,,"460","466",,95,"10.18653/v1/n16-1054","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84994085139&doi=10.18653%2fv1%2fn16-1054&partnerID=40&md5=5c6a41eace3ebae1d02249cb103f609c","Knowledge bases of real-world facts about entities and their relationships are useful resources for a variety of natural language processing tasks. However, because knowledge bases are typically incomplete, it is useful to be able to perform link prediction, i.e., predict whether a relationship not in the knowledge base is likely to be true. This paper combines insights from several previous link prediction models into a new embedding model STransE that represents each entity as a low-dimensional vector, and each relation by two matrices and a translation vector. STransE is a simple combination of the SE and TransE models, but it obtains better link prediction performance on two benchmark datasets than previous embedding models. Thus, STransE can serve as a new baseline for the more complex models in the link prediction task. ©2016 Association for Computational Linguistics.",,"Benchmarking; Computational linguistics; Embeddings; Knowledge based systems; Natural language processing systems; Benchmark datasets; Complex model; Knowledge base; Knowledge basis; Link prediction; Low dimensional; NAtural language processing; Translation vector; Forecasting","Amazon;Baidu;Bloomberg;eBay;et al.;Google","15th Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2016","12 June 2016 through 17 June 2016",,124044,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84994085139
"Xiao H., Huang M., Meng L., Zhu X.","57192390915;7404260571;57195951975;7406185137;","SSP: Semantic space projection for knowledge graph embedding with text descriptions",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"3104","3110",,94,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030468818&partnerID=40&md5=54fa3c0dc89fc2a2e47e93bf4eab3d5f","Knowledge graph embedding represents entities and relations in knowledge graph as low-dimensional, continuous vectors, and thus enables knowledge graph compatible with machine learning models. Though there have been a variety of models for knowledge graph embedding, most methods merely concentrate on the fact triples, while supplementary textual descriptions of entities and relations have not been fully employed. To this end, this paper proposes the semantic space projection (SSP) model which jointly learns from the symbolic triples and textual descriptions. Our model builds interaction between the two information sources, and employs textual descriptions to discover semantic relevance and offer precise semantic embedding. Extensive experiments show that our method achieves substantial improvements against baselines on the tasks of knowledge graph completion and entity classification. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Learning systems; Information sources; Knowledge graphs; Low dimensional; Machine learning models; Semantic embedding; Semantic relevance; Semantic Space; Textual description; Semantics","Amazon;Artificial Intelligence;Baidu;et al.;IBM;Tencent","31st AAAI Conference on Artificial Intelligence, AAAI 2017","4 February 2017 through 10 February 2017",,130407,Conference Paper,"Final","",Scopus,2-s2.0-85030468818
"Poria S., Chaturvedi I., Cambria E., Bisio F.","55316592700;12646151300;56140547500;55547779800;","Sentic LDA: Improving on LDA with semantic similarity for aspect-based sentiment analysis",2016,"Proceedings of the International Joint Conference on Neural Networks","2016-October",,"7727784","4465","4473",,94,"10.1109/IJCNN.2016.7727784","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007199314&doi=10.1109%2fIJCNN.2016.7727784&partnerID=40&md5=cdcf219921fbffc64f4eedb1d6e91f35","The advent of the Social Web has provided netizens with new tools for creating and sharing, in a time- and cost-efficient way, their contents, ideas, and opinions with virtually the millions of people connected to the World Wide Web. This huge amount of information, however, is mainly unstructured as specifically produced for human consumption and, hence, it is not directly machine-processable. In order to enable a more efficient passage from unstructured information to structured data, aspect-based opinion mining models the relations between opinion targets contained in a document and the polarity values associated with these. Because aspects are often implicit, however, spotting them and calculating their respective polarity is an extremely difficult task, which is closer to natural language understanding rather than natural language processing. To this end, Sentic LDA exploits common-sense reasoning to shift LDA clustering from a syntactic to a semantic level. Rather than looking at word co-occurrence frequencies, Sentic LDA leverages on the semantics associated with words and multi-word expressions to improve clustering and, hence, outperform state-of-the-art techniques for aspect extraction. © 2016 IEEE.",,"Data mining; Semantics; Amount of information; Commonsense reasoning; Multi-word expressions; NAtural language processing; Natural language understanding; Semantic similarity; State-of-the-art techniques; Word co-occurrence; Natural language processing systems","IEEE Computational Intelligence Society (IEEE CIS)","2016 International Joint Conference on Neural Networks, IJCNN 2016","24 July 2016 through 29 July 2016",,124695,Conference Paper,"Final","",Scopus,2-s2.0-85007199314
"Mihalkova L., Mooney R.J.","14042018700;7102791999;","Bottom-up learning of Markov logic network structure",2007,"ACM International Conference Proceeding Series","227",,,"625","632",,94,"10.1145/1273496.1273575","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547988135&doi=10.1145%2f1273496.1273575&partnerID=40&md5=74ade22d3a3899db33e40131d9b10199","Markov logic networks (MLNs) are a statistical relational model that consists of weighted firstorder clauses and generalizes first-order logic and Markov networks. The current state-of-the-art algorithm for learning MLN structure follows a top-down paradigm where many potential candidate structures are systematically generated without considering the data and then evaluated using a statistical measure of their fit to the data. Even though this existing algorithm outperforms an impressive array of benchmarks, its greedy search is susceptible to local maxima or plateaus. We present a novel algorithm for learning MLN structure that follows a more bottom-up approach to address this problem. Our algorithm uses a ""propositional"" Markov network learning method to construct ""template"" networks that guide the construction of candidate clauses. Our algorithm significantly improves accuracy and learning time over the existing topdown approach in three real-world domains.",,"Algorithms; Benchmarking; Data reduction; Learning systems; Real time systems; Statistical methods; Candidate clauses; Local maxima; Markov logic networks (MLN); Real-world domains; Markov processes","Oregon State University;Microsoft;The National Science Foundation;Google, Inc;Yahoo Research;et al","24th International Conference on Machine Learning, ICML 2007","20 June 2007 through 24 June 2007","Corvalis, OR",70090,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-34547988135
"Dominey P.F.","7004714357;","Complex sensory-motor sequence learning based on recurrent state representation and reinforcement learning",1995,"Biological Cybernetics","73","3",,"265","274",,93,"10.1007/BF00201428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029352040&doi=10.1007%2fBF00201428&partnerID=40&md5=370d4cf76406010624055cffcbcf85ff","A novel neural network model is presented that learns by trial-and-error to reproduce complex sensory-motor sequences. One subnetwork, corresponding to the prefrontal cortex (PFC), is responsible for generating unique patterns of activity that represent the continuous state of sequence execution. A second subnetwork, corresponding to the striatum, associates these state-encoding patterns with the correct response at each point in the sequence execution. From a neuroscience perspective, the model is based on the known cortical and subcortical anatomy of the primate oculomotor system. From a theoretical perspective, the architecture is similar to that of a finite automaton in which outputs and state transitions are generated as a function of inputs and the current state. Simulation results for complex sequence reproduction and sequence discrimination are presented. © 1995 Springer-Verlag.",,"animal; article; artificial neural network; computer simulation; human; physiology; psychomotor performance; theoretical model; Animal; Computer Simulation; Human; Models, Theoretical; Neural Networks (Computer); Psychomotor Performance; Support, Non-U.S. Gov't",,,,,,Article,"Final","",Scopus,2-s2.0-0029352040
"Belinkov Y., Glass J.","","Analysis methods in neural language processing: A survey",2019,"Transactions of the Association for Computational Linguistics","7",,,"49","72",,92,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85070515558
"Moldovan D., Harabagiu S., Pasca M., Mihalcea R., Girju R., Goodrum R., Rus V.","","The structure and performance of an open-domain question answering system",2000,"Proceedings of the 38th Annual Meeting on Association for Computational Linguistics",,,,"563","570",,92,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0009872219
"Kuiper K.","","Smooth talkers: The linguistic performance of auctioneers and sportscasters",1996,"Smooth Talkers: The Linguistic Performance of Auctioneers and Sportscasters",,,,"","",,92,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0038701254
"Cumby C., Roth D.","7801359589;7401669040;","On Kernel Methods for Relational Learning",2003,"Proceedings, Twentieth International Conference on Machine Learning","1",,,"107","114",,91,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-1942485292&partnerID=40&md5=09b0c932f95fa5da1eb184c89301914a","Kernel methods have gained a great deal of popularity in the machine learning community as a method to learn indirectly in high-dimensional feature spaces. Those interested in relational learning have recently begun to cast learning from structured and relational data in terms of kernel operations. We describe a general family of kernel functions built up from a description language of limited expressivity and use it to study the benefits and drawbacks of kernel learning in relational domains. Learning with kernels in this family directly models learning over an expanded feature space constructed using the same description language. This allows us to examine issues of time complexity in terms of learning with these and other relational kernels, and how these relate to generalization ability. The tradeoffs between using kernels in a very high dimensional implicit space versus a restricted feature space, is highlighted through two experiments, in bioinformatics and in natural language processing.",,"Biotechnology; Data structures; Describing functions; Natural language processing systems; Spatial variables control; Time measurement; Kernel methods; Relational data; Time complexity; Learning systems","American Association for Artificial Intelligence (AAAI)","Proceedings, Twentieth International Conference on Machine Learning","21 August 2003 through 24 August 2003","Washington, DC",62778,Conference Paper,"Final","",Scopus,2-s2.0-1942485292
"Beyer H.-G., Schwefel H.-P., Wegener I.","7102235652;35589058500;7004534842;","How to analyse evolutionary algorithms",2002,"Theoretical Computer Science","287","1",,"101","130",,91,"10.1016/S0304-3975(02)00137-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037174202&doi=10.1016%2fS0304-3975%2802%2900137-8&partnerID=40&md5=781d2e95d4da8a23f7b233c1dd687849","Many variants of evolutionary algorithms have been designed and applied. The experimental knowledge is immense. The rigorous analysis of evolutionary algorithms is difficult, but such a theory can help to understand, design, and teach evolutionary algorithms. In this survey, first the history of attempts to analyse evolutionary algorithms is described and then new methods for continuous as well as discrete search spaces are presented and discussed. © 2002 Elsevier Science B.V. All rights reserved.",,"Computer science; Optimization; Probability; Problem solving; Probabilistic optimization methods; Genetic algorithms",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-0037174202
"Avila Garcez A.S., Zaverucha G.","6507003712;6601938210;","Connectionist inductive learning and logic programming system",1999,"Applied Intelligence","11","1",,"59","77",,91,"10.1023/A:1008328630915","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033164970&doi=10.1023%2fA%3a1008328630915&partnerID=40&md5=beb78bd7d6542cb1b965024908116f37","This paper presents the Connectionist Inductive Learning and Logic Programming System (C-IL2P). C-IL2P is a new massively parallel computational model based on a feedforward Artificial Neural Network that integrates inductive learning from examples and background knowledge, with deductive learning from Logic Programming. Starting with the background knowledge represented by a propositional logic program, a translation algorithm is applied generating a neural network that can be trained with examples. The results obtained with this refined network can be explained by extracting a revised logic program from it. Moreover, the neural network computes the stable model of the logic program inserted in it as background knowledge, or learned with the examples, thus functioning as a parallel system for Logic Programming. We have successfully applied C-IL2P to two real-world problems of computational biology, specifically DNA sequence analyses. Comparisons with the results obtained by some of the main neural, symbolic, and hybrid inductive learning systems, using the same domain knowledge, show the effectiveness of C-IL2P.",,"Algorithms; Feedforward neural networks; Knowledge acquisition; Logic programming; Computational biology; Connectionist inductive learning; Theory refinement; Learning systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0033164970
"De Rainville F.-M., Fortin F.-A., Gardner M.-A., Parizeau M., Gagné C.","24586701800;55334089800;48161157200;12140404700;12140356400;","DEAP: A Python framework for Evolutionary Algorithms",2012,"GECCO'12 - Proceedings of the 14th International Conference on Genetic and Evolutionary Computation Companion",,,,"85","92",,89,"10.1145/2330784.2330799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84865035835&doi=10.1145%2f2330784.2330799&partnerID=40&md5=4075e24ac3e01238ccab8b3f5efe3bb0","DEAP (Distributed Evolutionary Algorithms in Python) is a novel evolutionary computation framework for rapid prototyping and testing of ideas. Its design departs from most other existing frameworks in that it seeks to make algorithms explicit and data structures transparent, as opposed to the more common black box type of frameworks. It also incorporates easy parallelism where users need not concern themselves with gory implementation details like synchronization and load balancing, only functional decomposition. Several examples illustrate the multiple properties of DEAP. Copyright 2012 ACM.","Parallel evolutionary algorithms; Software tools","Common black; Distributed evolutionary algorithms; Functional decomposition; Parallel evolutionary algorithms; Calculations; Computer aided software engineering; Data structures; High level languages; Rapid prototyping; Evolutionary algorithms","ACM SIGEVO","14th International Conference on Genetic and Evolutionary Computation, GECCO'12","7 July 2012 through 11 July 2012","Philadelphia, PA",91623,Conference Paper,"Final","",Scopus,2-s2.0-84865035835
"Lawrence S., Giles C.L., Fong S.","7202646596;55665046700;36622315300;","Natural language grammatical inference with recurrent neural networks",2000,"IEEE Transactions on Knowledge and Data Engineering","12","1",,"126","140",,89,"10.1109/69.842255","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33747598711&doi=10.1109%2f69.842255&partnerID=40&md5=afbca4da9c39aab89104952cebf3a53b","This paper examines the inductive inference of a complex grammar with neural networks - specifically, the task considered is that of training a network to classify natural language sentences as grammatical or ungrammatical, thereby exhibiting the same kind of discriminatory power provided by the Principles and Parameters linguistic framework, or Government-and-Binding theory. Neural networks are trained, without the division into learned vs. innate components assumed by Chomsky, in an attempt to produce the same judgments as native speakers on sharply grammatical/ungrammatical data. How a recurrent neural network could possess linguistic capability and the properties of various common recurrent neural network architectures are discussed. The problem exhibits training behavior which is often not present with smaller grammars and training was initially difficult. However, after implementing several techniques aimed at improving the convergence of the gradient descent backpropagation-through-time training algorithm, significant learning was possible. It was found that certain architectures are better able to learn an appropriate grammar. The operation of the networks and their training is analyzed. Finally, the extraction of rules in the form of deterministic finite state automata is investigated. © 2000 IEEE.","Automata extraction; Government-and-binding theory; Gradient descent; Grammatical inference; Natural language processing; Principles-and-parameters framework; Recurrent neural networks; Simulated annealing",,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-33747598711
"Ai Q., Azizi V., Chen X., Zhang Y.","57119155600;37053356800;57189698899;36816821200;","Learning heterogeneous knowledge base embeddings for explainable recommendation",2018,"Algorithms","11","9","137","","",,88,"10.3390/a11090137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053805293&doi=10.3390%2fa11090137&partnerID=40&md5=6435e39683ffda7a269cdebf9b4bc264","Providing model-generated explanations in recommender systems is important to user experience. State-of-the-art recommendation algorithms-especially the collaborative filtering (CF)-based approaches with shallow or deep models-usually work with various unstructured information sources for recommendation, such as textual reviews, visual images, and various implicit or explicit feedbacks. Though structured knowledge bases were considered in content-based approaches, they have been largely ignored recently due to the availability of vast amounts of data and the learning power of many complex models. However, structured knowledge bases exhibit unique advantages in personalized recommendation systems. When the explicit knowledge about users and items is considered for recommendation, the system could provide highly customized recommendations based on users' historical behaviors and the knowledge is helpful for providing informed explanations regarding the recommended items. A great challenge for using knowledge bases for recommendation is how to integrate large-scale structured and unstructured data, while taking advantage of collaborative filtering for highly accurate performance. Recent achievements in knowledge-base embedding (KBE) sheds light on this problem, which makes it possible to learn user and item representations while preserving the structure of their relationship with external knowledge for explanation. In this work, we propose to explain knowledge-base embeddings for explainable recommendation. Specifically, we propose a knowledge-base representation learning framework to embed heterogeneous entities for recommendation, and based on the embedded knowledge base, a soft matching algorithm is proposed to generate personalized explanations for the recommended items. Experimental results on real-world e-commerce datasets verified the superior recommendation performance and the explainability power of our approach compared with state-of-the-art baselines. © 2018 by the authors.","Collaborative filtering; Explainable recommendation; Knowledge-base embedding; Recommender systems","Electronic commerce; Knowledge based systems; Recommender systems; Content-based approach; Explainable recommendation; Heterogeneous Knowledge; Knowledge base; Personalized recommendation systems; Recommendation algorithms; Recommendation performance; Structured knowledge; Collaborative filtering",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85053805293
"Yang M., Tu W., Wang J., Xu F., Chen X.","56349712700;56801373600;57188647702;56419156700;55739099100;","Attention-based LSTM for target-dependent sentiment classification",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"5013","5014",,88,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030454034&partnerID=40&md5=724f7efc75b17da8b6dd6606e2dad288","We present an attention-based bidirectional LSTM approach to improve the target-dependent sentiment classification. Our method learns the alignment between the target entities and the most distinguishing features. We conduct extensive experiments on a real-life dataset. The experimental results show that our model achieves state-of-the-art results. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Sentiment classification; State of the art; Artificial intelligence","Amazon;Artificial Intelligence;Baidu;et al.;IBM;Tencent","31st AAAI Conference on Artificial Intelligence, AAAI 2017","4 February 2017 through 10 February 2017",,130407,Conference Paper,"Final","",Scopus,2-s2.0-85030454034
"Chaturvedi I., Ragusa E., Gastaldo P., Zunino R., Cambria E.","12646151300;56580248800;35612596100;7006338311;56140547500;","Bayesian network based extreme learning machine for subjectivity detection",2018,"Journal of the Franklin Institute","355","4",,"1780","1797",,87,"10.1016/j.jfranklin.2017.06.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85024868159&doi=10.1016%2fj.jfranklin.2017.06.007&partnerID=40&md5=69c60332b69fac6b72a88f4fea94a8e2","Subjectivity detection is a task of natural language processing that aims to remove ‘factual’ or ‘neutral’ content, i.e., objective text that does not contain any opinion, from online product reviews. Such a pre-processing step is crucial to increase the accuracy of sentiment analysis systems, as these are usually optimized for the binary classification task of distinguishing between positive and negative content. In this paper, we extend the extreme learning machine (ELM) paradigm to a novel framework that exploits the features of both Bayesian networks and fuzzy recurrent neural networks to perform subjectivity detection. In particular, Bayesian networks are used to build a network of connections among the hidden neurons of the conventional ELM configuration in order to capture dependencies in high-dimensional data. Next, a fuzzy recurrent neural network inherits the overall structure generated by the Bayesian networks to model temporal features in the predictor. Experimental results confirmed the ability of the proposed framework to deal with standard subjectivity detection problems and also proved its capacity to address portability across languages in translation tasks. © 2017 The Franklin Institute",,"Education; Fuzzy inference; Fuzzy logic; Fuzzy neural networks; Knowledge acquisition; Learning systems; Natural language processing systems; Recurrent neural networks; Binary classification; Detection problems; Extreme learning machine; Fuzzy recurrent neural networks; High dimensional data; Online product reviews; Pre-processing step; Sentiment analysis; Bayesian networks",,,,,,Article,"Final","",Scopus,2-s2.0-85024868159
"Caponnetto A., Micchelli C.A., Pontil M., Ying Y.","56572559000;7003617412;6603887035;13905725500;","Universal multi-task Kernels",2008,"Journal of Machine Learning Research","9",,,"1615","1646",,87,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-48849098893&partnerID=40&md5=824c6902c837779ecc0141684f786be6","In this paper we are concerned with reproducing kernel Hilbert spaces ℋK of functions from an input space into a Hilbert space script Y; an environment appropriate for multi-task learning. The reproducing kernel K associated to ℋK has its values as operators on script Y. Our primary goal here is to derive conditions which ensure that the kernel K is universal. This means that on every compact subset of the input space, every continuous function with values in script K can be uniformly approximated by sections of the kernel. We provide various characterizations of universal kernels and highlight them with several concrete examples of some practical importance. Our analysis uses basic principles of functional analysis and especially the useful notion of vector measures which we describe in sufficient detail to clarify our results. ©2008 Andrea Caponnetto, Charles A. Micchelli, Massimiliano Pontil and Yiming Ying.","Multi-task kernels; Multi-task learning; Universal approximation; Vector-valued reproducing kernel Hilbert spaces","Banach spaces; Multi-task; Multi-task kernels; Multi-task learning; Reproducing Kernel Hilbert spaces; Universal approximation; Vector-valued reproducing kernel Hilbert spaces; Hilbert spaces",,,,,,Article,"Final","",Scopus,2-s2.0-48849098893
"Craven M., Slattery S.","7006427661;7003299590;","Relational learning with statistical predicate invention: Better models for hypertext",2001,"Machine Learning","43","1-2",,"97","119",,86,"10.1023/A:1007676901476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0035312953&doi=10.1023%2fA%3a1007676901476&partnerID=40&md5=53810a265c31bff9febe64b47c90ce33","We present a new approach to learning hypertext classifiers that combines a statistical text-learning method with a relational rule learner. This approach is well suited to learning in hypertext domains because its statistical component allows it to characterize text in terms of word frequencies, whereas its relational component is able to describe how neighboring documents are related to each other by hyperlinks that connect them. We evaluate our approach by applying it to tasks that involve learning definitions for (i) classes of pages, (ii) particular relations that exist between pairs of pages, and (iii) locating a particular class of information in the internal structure of pages. Our experiments demonstrate that this new approach is able to learn more accurate classifiers than either of its constituent methods alone.","Naive Bayes; Predicate invention; Relational learning; Text categorization","Algorithms; Hypermedia systems; Hypertext systems; Text processing; World Wide Web; Hyperlinks; Naive Bayes; Predicate invention; Relational learning; Statistical predicate; Text categorization; Learning systems",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-0035312953
"Dey L., Chakraborty S., Biswas A., Bose B., Tiwari S.","","Sentiment Analysis of Review Datasets Using Naïve Bayes' and K-NN Classifier",2016,"International Journal of Information Engineering and Electronic Business","8","4",,"54","62",,85,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85028364689
"Le V., Gulwani S.","55602744500;55901318200;","FlashExtract: A framework for data extraction by examples",2014,"Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",,,,"542","553",,85,"10.1145/2594291.2594333","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901616011&doi=10.1145%2f2594291.2594333&partnerID=40&md5=d3013d437c2c6f8f275e7eef584abbe4","Various document types that combine model and view (e.g., text files, webpages, spreadsheets) make it easy to organize (possibly hierarchical) data, but make it difficult to extract raw data for any further manipulation or querying. We present a general framework FlashExtract to extract relevant data from semi-structured documents using examples. It includes: (a) an interaction model that allows end-users to give examples to extract various fields and to relate them in a hierarchical organization using structure and sequence constructs. (b) an inductive synthesis algorithm to synthesize the intended program from few examples in any underlying domainspecific language for data extraction that has been built using our specified algebra of few core operators (map, filter, merge, and pair). We describe instantiation of our framework to three different domains: text files, webpages, and spreadsheets. On our benchmark comprising 75 documents, FlashExtract is able to extract intended data using an average of 2.36 examples in 0.84 seconds per field. Copyright © 2014 ACM.","End-user programming; Program synthesis; Programming by examples","Computer programming languages; Spreadsheets; Websites; Domain specific languages; End user programming; Hierarchical organizations; Interaction model; Program synthesis; Programming by Example; Semi-structured documents; Synthesis algorithms; Data mining","ACM SIGPLAN;NSF","35th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2014","9 June 2014 through 11 June 2014","Edinburgh",105268,Conference Paper,"Final","",Scopus,2-s2.0-84901616011
"Reif M., Shafait F., Dengel A.","24721531700;23390483600;6603764314;","Meta-learning for evolutionary parameter optimization of classifiers",2012,"Machine Learning","87","3",,"357","380",,85,"10.1007/s10994-012-5286-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84862009037&doi=10.1007%2fs10994-012-5286-7&partnerID=40&md5=17959d87dc513ac219afa2a4f0e75752","The performance of most of the classification algorithms on a particular dataset is highly dependent on the learning parameters used for training them. Different approaches like grid search or genetic algorithms are frequently employed to find suitable parameter values for a given dataset. Grid search has the advantage of finding more accurate solutions in general at the cost of higher computation time. Genetic algorithms, on the other hand, are able to find good solutions in less time, but the accuracy of these solutions is usually lower than those of grid search. This paper uses ideas from meta-learning and case-based reasoning to provide good starting points to the genetic algorithm. The presented approach reaches the accuracy of grid search at a significantly lower computational cost. We performed extensive experiments for optimizing learning parameters of the Support Vector Machine (SVM) and the Random Forest classifiers on over 100 datasets from UCI and StatLib repositories. For the SVM classifier, grid search achieved an average accuracy of 81 % and took six hours for training, whereas the standard genetic algorithm obtained 74 % accuracy in close to one hour of training. Our method was able to achieve an average accuracy of 81 % in only about 45 minutes. Similar results were achieved for the Random Forest classifier. Besides a standard genetic algorithm, we also compared the presented method with three state-of-the-art optimization algorithms: Generating Set Search, Dividing Rectangles, and the Covariance Matrix Adaptation Evolution Strategy. Experimental results show that our method achieved the highest average accuracy for both classifiers. Our approach can be particularly useful when training classifiers on large datasets where grid search is not feasible. © 2012 The Author(s).","Feature selection; Genetic algorithm; Meta-learning; Parameter optimization","Classification algorithm; Computation time; Computational costs; Covariance matrix adaptation evolution strategies; Data sets; Dividing rectangles; Evolutionary parameters; Generating set search; Grid search; Large datasets; Learning parameters; Metalearning; Optimization algorithms; Parameter optimization; Parameter values; Random forest classifier; Standard genetic algorithm; SVM classifiers; Classification (of information); Decision trees; Feature extraction; Forestry; Optimization; Support vector machines; Genetic algorithms; Algorithms; Classification; Decision Theory; Optimization; Parameters; Performance",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-84862009037
"Mao J., Gan C., Kohli P., Tenenbaum J.B., Wu J.","57201377790;57141834200;14035707300;7006818404;55553991400;","The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,84,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083954234&partnerID=40&md5=709b7d8abce72bd624da18ff22f35a6a","We propose the Neuro-Symbolic Concept Learner (NS-CL), a model that learns visual concepts, words, and semantic parsing of sentences without explicit supervision on any of them; instead, our model learns by simply looking at images and reading paired questions and answers. Our model builds an object-based scene representation and translates sentences into executable, symbolic programs. To bridge the learning of two modules, we use a neuro-symbolic reasoning module that executes these programs on the latent scene representation. Analogical to human concept learning, the perception module learns visual concepts based on the language description of the object being referred to. Meanwhile, the learned visual concepts facilitate learning new words and parsing new sentences. We use curriculum learning to guide the searching over the large compositional space of images and language. Extensive experiments demonstrate the accuracy and efficiency of our model on learning visual concepts, word representations, and semantic parsing of sentences. Further, our method allows easy generalization to new object attributes, compositions, language concepts, scenes and questions, and even new program domains. It also empowers applications including visual question answering and bidirectional image-text retrieval. © 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved.",,"Context free grammars; Semantics; Syntactics; Visual languages; Concept learning; Language description; Object attributes; Question Answering; Scene representation; Semantic parsing; Symbolic reasoning; Word representations; Learning systems",,"7th International Conference on Learning Representations, ICLR 2019","6 May 2019 through 9 May 2019",,149936,Conference Paper,"Final","",Scopus,2-s2.0-85083954234
"Bröcheler M., Mihalkova L., Getoor L.","34869097100;14042018700;6603498218;","Probabilistic similarity logic",2010,"Proceedings of the 26th Conference on Uncertainty in Artificial Intelligence, UAI 2010",,,,"73","82",,84,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952406643&partnerID=40&md5=e5e3f1309c58c9d97737ddded47014ad","Many machine learning applications require the ability to learn from and reason about noisy multi-relational data. To address this, several effective representations have been developed that provide both a language for expressing the structural regularities of a domain, and principled support for probabilistic inference. In addition to these two aspects, however, many applications also involve a third aspect-the need to reason about similarities-which has not been directly supported in existing frameworks. This paper introduces probabilistic similarity logic (PSL), a general-purpose framework for joint reasoning about similarity in relational domains that incorporates probabilistic reasoning about similarities and relational structure in a principled way. PSL can integrate any existing domain-specific similarity measures and also supports reasoning about similarities between sets of entities. We provide efficient inference and learning techniques for PSL and demonstrate its effectiveness both in common relational tasks and in settings that require reasoning about similarity.",,"Artificial intelligence; Computer circuits; Learning systems; General purpose framework; Learning techniques; Machine learning applications; Probabilistic inference; Probabilistic reasoning; Relational structures; Similarity measure; Structural regularity; Probabilistic logics",,,,,,Conference Paper,"Final","",Scopus,2-s2.0-79952406643
"Dua D., Wang Y., Dasigi P., Stanovsky G., Singh S., Gardner M.","57215317198;57200286333;35229416600;56901231200;55647398500;56577280400;","Drop: A reading comprehension benchmark requiring discrete reasoning over paragraphs",2019,"NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference","1",,,"2368","2378",,83,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084074742&partnerID=40&md5=e13cf9627771e13fd1a8acf25c8ef54c","Reading comprehension has recently seen rapid progress, with systems matching humans on the most popular datasets for the task. However, a large body of work has highlighted the brittleness of these systems, showing that there is much work left to be done. We introduce a new English reading comprehension benchmark, DROP, which requires Discrete Reasoning Over the content of Paragraphs. In this crowdsourced, adversarially-created, 96k-question benchmark, a system must resolve references in a question, perhaps to multiple input positions, and perform discrete operations over them (such as addition, counting, or sorting). These operations require a much more comprehensive understanding of the content of paragraphs than what was necessary for prior datasets. We apply state-of-the-art methods from both the reading comprehension and semantic parsing literatures on this dataset and show that the best systems only achieve 32.7% F1 on our generalized accuracy metric, while expert human performance is 96.4%. We additionally present a new model that combines reading comprehension methods with simple numerical reasoning to achieve 47.0% F. © 2019 Association for Computational Linguistics",,"Computational linguistics; Fracture mechanics; Numerical methods; Semantics; Human performance; Multiple inputs; Numerical reasoning; Reading comprehension; Semantic parsing; State-of-the-art methods; Systems matching; Drops","Amazon;ASAPP;Bloomberg Engineering;et al.;facebook;Google","2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019","2 June 2019 through 7 June 2019",,159851,Conference Paper,"Final","",Scopus,2-s2.0-85084074742
"Gulwani S.","55901318200;","Automating string processing in spreadsheets using input-output examples",2010,"Conference Record of the Annual ACM Symposium on Principles of Programming Languages",,,,"317","329",,83,"10.1145/1926385.1926423","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952012950&doi=10.1145%2f1926385.1926423&partnerID=40&md5=78039e2f66913d03e8caa3995c45a959","We describe the design of a string programming/expression language that supports restricted forms of regular expressions, conditionals and loops. The language is expressive enough to represent a wide variety of string manipulation tasks that end-users struggle with.We describe an algorithm based on several novel concepts for synthesizing a desired program in this language from input-output examples. The synthesis algorithm is very efficient taking a fraction of a second for various benchmark examples. The synthesis algorithm is interactive and has several desirable features: it can rank multiple solutions and has fast convergence, it can detect noise in the user input, and it supports an active interaction model wherein the user is prompted to provide outputs on inputs that may have multiple computational interpretations. The algorithm has been implemented as an interactive add-in for Microsoft Excel spreadsheet system. The prototype tool has met the golden test - it has synthesized part of itself, and has been used to solve problems beyond author's imagination. Copyright © 2011 ACM.","Program synthesis; Programming by example (PBE); Spreadsheet programming; String manipulation; User intent; Version space algebra","Program synthesis; Programming by Example; String manipulation; User intent; Version space algebra; Algebra; Algorithms; Object oriented programming; Spreadsheets; Convergence of numerical methods","ACM SIGPLAN","38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL'11","26 January 2011 through 28 January 2011","Austin, TX",83932,Conference Paper,"Final","",Scopus,2-s2.0-79952012950
"Valdivia A., Luzón M.V., Herrera F.","57194712936;8324389800;7102347190;","Sentiment Analysis in TripAdvisor",2017,"IEEE Intelligent Systems","32","4","8012330","72","77",,81,"10.1109/MIS.2017.3121555","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028771482&doi=10.1109%2fMIS.2017.3121555&partnerID=40&md5=214a5e472ec540723be576c1e1741ce6","Web platforms such as TripAdvisor allow tourists to describe their experiences with hotels, restaurants, and other tourist attractions. This article proposes TripAdvisor as a source of data for sentiment analysis tasks. The authors develop an analysis for studying the matching between users' sentiments and automatic sentiment-detection algorithms. They also discuss some of the challenges regarding sentiment analysis and TripAdvisor. © 2001-2011 IEEE.","artificial intelligence; online recommendation systems; sentiment analysis; TripAdvisor","Artificial intelligence; Online systems; Detection algorithm; Sentiment analysis; Tourist attractions; TripAdvisor; Data mining",,,,,,Article,"Final","",Scopus,2-s2.0-85028771482
"Bollegala D., Mu T., Goulermas J.Y.","13006618600;36793369900;8972539400;","Cross-Domain Sentiment Classification Using Sentiment Sensitive Embeddings",2016,"IEEE Transactions on Knowledge and Data Engineering","28","2","7236887","398","410",,81,"10.1109/TKDE.2015.2475761","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962374573&doi=10.1109%2fTKDE.2015.2475761&partnerID=40&md5=5e5a399139450ce44e1a5405320100bd","Unsupervised Cross-domain Sentiment Classification is the task of adapting a sentiment classifier trained on a particular domain (source domain), to a different domain (target domain), without requiring any labeled data for the target domain. By adapting an existing sentiment classifier to previously unseen target domains, we can avoid the cost for manual data annotation for the target domain. We model this problem as embedding learning, and construct three objective functions that capture: (a) distributional properties of pivots (i.e., common features that appear in both source and target domains), (b) label constraints in the source domain documents, and (c) geometric properties in the unlabeled documents in both source and target domains. Unlike prior proposals that first learn a lower-dimensional embedding independent of the source domain sentiment labels, and next a sentiment classifier in this embedding, our joint optimisation method learns embeddings that are sensitive to sentiment classification. Experimental results on a benchmark dataset show that by jointly optimising the three objectives we can obtain better performances in comparison to optimising each objective function separately, thereby demonstrating the importance of task-specific embedding learning for cross-domain sentiment classification. Among the individual objective functions, the best performance is obtained by (c). Moreover, the proposed method reports cross-domain sentiment classification accuracies that are statistically comparable to the current state-of-the-art embedding learning methods for cross-domain sentiment classification. © 1989-2012 IEEE.","Domain Adaptation; Embedding Learning; Sentiment Classification; Spectral Methods","Computational methods; Information systems; Distributional property; Domain adaptation; Embedding Learning; Geometric properties; Objective functions; Optimisation method; Sentiment classification; Spectral methods; Classification (of information)",,,,,,Conference Paper,"Final","",Scopus,2-s2.0-84962374573
"Cambria E., Hussain A., Havasi C., Eckl C.","56140547500;19734290900;55899855300;36019820400;","Sentic computing: Exploitation of common sense for the development of emotion-sensitive systems",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5967 LNCS",,,"148","156",,81,"10.1007/978-3-642-12397-9_12","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952031812&doi=10.1007%2f978-3-642-12397-9_12&partnerID=40&md5=3715e9e1b630006756a1d462426ca537","Emotions are a fundamental component in human experience, cognition, perception, learning and communication. In this paper we explore how the use of Common Sense Computing can significantly enhance computers' emotional intelligence i.e. their capability of perceiving and expressing emotions, to allow machines to make more human-like decisions and improve the human-computer interaction. © 2010 Springer-Verlag.","AI; Analogies; Common sense computing; Emotion and affective UI; Knowledge base management; NLP; Semantic networks","Artificial intelligence; Emotional intelligence; Interactive computer systems; Knowledge based systems; Knowledge representation; Semantics; Analogies; Common sense; Emotion and affective UI; Knowledge base managements; Semantic network; Human computer interaction","et al.;European COST Action 2102;Int. Inst. Adv. Sci. Stud. (IIASS);Provincia di Salerno;Regione Campania;Second Univ. Naples, Fac. Dep. Psych., et al.","2nd COST 2102 International Training School on Development of Multimodal Interfaces: Active Listening and Synchrony","23 March 2009 through 27 March 2009","Dublin",80167,Conference Paper,"Final","",Scopus,2-s2.0-77952031812
"Shen T., Zhou T., Long G., Jiang J., Zhang C.","57210531549;35323555400;55522990400;55731807500;7405493634;","Bi-directional block self-attention for fast and memory-efficient sequence modeling",2018,"6th International Conference on Learning Representations, ICLR 2018 - Conference Track Proceedings",,,,"","",,80,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083950912&partnerID=40&md5=a29283e88d7beca575811bc1442c00fa","Recurrent neural networks (RNN), convolutional neural networks (CNN) and self-attention networks (SAN) are commonly used to produce context-aware representations. RNN can capture long-range dependency but is hard to parallelize and not time-efficient. CNN focuses on local dependency but does not perform well on some tasks. SAN can model both such dependencies via highly parallelizable computation, but memory requirement grows rapidly in line with sequence length. In this paper, we propose a model, called “bi-directional block self-attention network (Bi-BloSAN)”, for RNN/CNN-free sequence encoding. It requires as little memory as RNN but with all the merits of SAN. Bi-BloSAN splits the entire sequence into blocks, and applies an intra-block SAN to each block for modeling local context, then applies an inter-block SAN to the outputs for all blocks to capture long-range dependency. Thus, each SAN only needs to process a short sequence, and only a small amount of memory is required. Additionally, we use feature-level attention to handle the variation of contexts around the same word, and use forward/backward masks to encode temporal order information. On nine benchmark datasets for different NLP tasks, Bi-BloSAN achieves or improves upon state-of-the-art accuracy, and shows better efficiency-memory trade-off than existing RNN/CNN/SAN. © Learning Representations, ICLR 2018 - Conference Track Proceedings.All right reserved.",,"Automata theory; Economic and social effects; Encoding (symbols); Benchmark datasets; Convolutional neural network; Long-range dependencies; Memory requirements; Recurrent neural network (RNN); Sequence encoding; Sequence modeling; Temporal order information; Recurrent neural networks",,"6th International Conference on Learning Representations, ICLR 2018","30 April 2018 through 3 May 2018",,149806,Conference Paper,"Final","",Scopus,2-s2.0-85083950912
"Hinaut X., Dominey P.F.","53063834600;7004714357;","Real-Time Parallel Processing of Grammatical Structure in the Fronto-Striatal System: A Recurrent Network Simulation Study Using Reservoir Computing",2013,"PLoS ONE","8","2","e52946","","",,80,"10.1371/journal.pone.0052946","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873259375&doi=10.1371%2fjournal.pone.0052946&partnerID=40&md5=5de0a86824b68e94bb05d356a0fdac3a","Sentence processing takes place in real-time. Previous words in the sentence can influence the processing of the current word in the timescale of hundreds of milliseconds. Recent neurophysiological studies in humans suggest that the fronto-striatal system (frontal cortex, and striatum - the major input locus of the basal ganglia) plays a crucial role in this process. The current research provides a possible explanation of how certain aspects of this real-time processing can occur, based on the dynamics of recurrent cortical networks, and plasticity in the cortico-striatal system. We simulate prefrontal area BA47 as a recurrent network that receives on-line input about word categories during sentence processing, with plastic connections between cortex and striatum. We exploit the homology between the cortico-striatal system and reservoir computing, where recurrent frontal cortical networks are the reservoir, and plastic cortico-striatal synapses are the readout. The system is trained on sentence-meaning pairs, where meaning is coded as activation in the striatum corresponding to the roles that different nouns and verbs play in the sentences. The model learns an extended set of grammatical constructions, and demonstrates the ability to generalize to novel constructions. It demonstrates how early in the sentence, a parallel set of predictions are made concerning the meaning, which are then confirmed or updated as the processing of the input sentence proceeds. It demonstrates how on-line responses to words are influenced by previous words in the sentence, and by previous sentences in the discourse, providing new insight into the neurophysiology of the P600 ERP scalp response to grammatical complexity. This demonstrates that a recurrent neural network can decode grammatical structure from sentences in real-time in order to generate a predictive representation of the meaning of the sentences. This can provide insight into the underlying mechanisms of human cortico-striatal function in sentence processing. © 2013 Hinaut, Dominey.",,"article; brain function; brain region; brain size; Brodmann area 47; corpus striatum; discourse analysis; evoked cortical response; frontal cortex; grammar; language processing; mathematical computing; nerve cell network; nerve cell plasticity; neurophysiology; online system; prediction; prefrontal cortex; signal noise ratio; simulation; synapse; Artificial Intelligence; Computer Simulation; Corpus Striatum; Frontal Lobe; Humans; Neural Networks (Computer); Semantics; Speech Perception",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-84873259375
"Genest P.E., Lapalme G.","","Framework for abstractive summarization using text-to-text generation",2011,"Proceedings of the Workshop on Monolingual Text-To-Text Generation",,,,"64","73",,80,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84874648191
"Patwardhan S., Riloff E.","36799752700;6602721887;","Effective information extraction with semantic affinity patterns and relevant regions",2007,"EMNLP-CoNLL 2007 - Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",,,,"717","727",,80,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053377922&partnerID=40&md5=a49b9bab17f0533857d00fc62c4d878a","We present an information extraction system that decouples the tasks of finding relevant regions of text and applying extraction patterns. We create a self-trained relevant sentence classifier to identify relevant regions, and use a semantic affinity measure to automatically learn domain-relevant extraction patterns. We then distinguish primary patterns from secondary patterns and apply the patterns selectively in the relevant regions. The resulting IE system achieves good performance on the MUC-4 terrorism corpus and ProMed disease outbreak stories. This approach requires only a few seed extraction patterns and a collection of relevant and irrelevant documents for training. © 2007 Association for Computational Linguistics.",,"Disease outbreaks; Extraction patterns; Information Extraction; Information extraction systems; Semantic affinity; Sentence classifiers; Computational linguistics; Information analysis; Information retrieval systems; Semantics; Natural language processing systems",,"2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning, EMNLP-CoNLL 2007","28 June 2007 through 28 June 2007","Prague",86711,Conference Paper,"Final","",Scopus,2-s2.0-80053377922
"Roemmele M., Bejan C.A., Gordon A.S.","39862253100;13007466300;35087422700;","Choice of plausible alternatives: An evaluation of commonsense causal reasoning",2011,"AAAI Spring Symposium - Technical Report","SS-11-06",,,"90","95",,79,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051542005&partnerID=40&md5=c30d57777482ec55f028edf7bda48c7e","Research in open-domain commonsense reasoning has been hindered by the lack of evaluation metrics for judging progress and comparing alternative approaches. Taking inspiration from large-scale question sets used in natural language processing research, we authored one thousand English-language questions that directly assess commonsense causal reasoning, called the Choice Of Plausible Alternatives (COPA) evaluation. Using a forced-choice format, each question gives a premise and two plausible causes or effects, where the correct choice is the alternative that is more plausible than the other. This paper describes the authoring methodology that we used to develop a validated question set with sufficient breadth to advance open-domain commonsense reasoning research. We discuss the design decisions made during the authoring process, and explain how these decisions will affect the design of high-scoring systems. We also present the performance of multiple baseline approaches that use statistical natural language processing techniques, establishing initial benchmarks for future systems. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Benchmarking; Authoring process; Causal reasoning; Commonsense reasoning; Design decisions; English languages; Evaluation metrics; NAtural language processing; Statistical natural language processing; Natural language processing systems",,"2011 AAAI Spring Symposium","21 March 2011 through 23 March 2011",,85955,Conference Paper,"Final","",Scopus,2-s2.0-80051542005
"Sodhro A.H., Li Y., Shah M.A.","55389831000;57192879224;36094881000;","Energy-efficient adaptive transmission power control for wireless body area networks",2016,"IET Communications","10","1",,"81","90",,78,"10.1049/iet-com.2015.0368","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956856994&doi=10.1049%2fiet-com.2015.0368&partnerID=40&md5=dfb12f3a0e61ec4fdc6ecb39395cb6af","An important constraint in wireless body area network (WBAN) is to maximise the energy-efficiency of wearable devices due to their limited size and light weight. Two experimental scenarios; 'right wrist to right hip' and 'chest to right hip' with body posture of walking are considered. It is analyzed through extensive real-time data sets that due to large temporal variations in the wireless channel, a constant transmission power and a typical conventional transmission power control (TPC) methods are not suitable choices for WBAN. To overcome these problems a novel energy-efficient adaptive power control (APC) algorithm is proposed that adaptively adjusts transmission power (TP) level based on the feedback from base station. The main advantages of the proposed algorithm are saving more energy with acceptable packet loss ratio (PLR) and lower complexity in implementation of desired tradeoffbetween energy savings and link reliability. We adapt, optimise and theoretically analyse the required parameters to enhance the system performance. The proposed algorithm sequentially achieves significant higher energy savings of 40.9%, which is demonstrated by Monte Carlo simulations in MATLAB. However, the only limitation of proposed algorithm is a slightly higher PLR in comparison to conventional TPC such as Gao's and Xiao's methods. © The Institution of Engineering and Technology.",,"Adaptive control systems; Algorithms; Chip scale packages; Complex networks; Energy conservation; Intelligent systems; MATLAB; Monte Carlo methods; Networks (circuits); Power control; Wearable technology; Wireless local area networks (WLAN); Wireless networks; Adaptive power control; Adaptive transmission; Packet loss ratio; Temporal variation; Transmission power; Transmission power control; Wearable devices; Wireless body area network; Energy efficiency",,,,,,Article,"Final","",Scopus,2-s2.0-84956856994
"Chen B., Cherry C.","","A systematic comparison of smoothing techniques for sentence-level BLEU",2014,"Proceedings of the Ninth Workshop on Statistical Machine Translation 2014",,,,"362","367",,78,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85010213757
"Guo Z., Zhang Y., Lu W.","57215715235;57216622998;57220644305;","Attention guided graph convolutional networks for relation extraction",2020,"ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",,,,"241","251",,77,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084054426&partnerID=40&md5=e507d9ea8fbbb9f92de8aa8c0db88c32","Dependency trees convey rich structural information that is proven useful for extracting relations among entities in text. However, how to effectively make use of relevant information while ignoring irrelevant information from the dependency trees remains a challenging research question. Existing approaches employing rule based hard-pruning strategies for selecting relevant partial dependency structures may not always yield optimal results. In this work, we propose Attention Guided Graph Convolutional Networks (AGGCNs), a novel model which directly takes full dependency trees as inputs. Our model can be understood as a soft-pruning approach that automatically learns how to selectively attend to the relevant sub-structures useful for the relation extraction task. Extensive results on various tasks including cross-sentence n-ary relation extraction and large-scale sentence-level relation extraction show that our model is able to better leverage the structural information of the full dependency trees, giving significantly better results than previous approaches. © 2019 Association for Computational Linguistics",,"Computational linguistics; Convolution; Extraction; Forestry; Convolutional networks; Dependency structures; Dependency trees; Optimal results; Pruning strategy; Relation extraction; Research questions; Structural information; Trees (mathematics)","Apple;ASAPP;Bloomberg Engineering;BOSCH;et al.;Expedia","57th Annual Meeting of the Association for Computational Linguistics, ACL 2019","28 July 2019 through 2 August 2019",,159206,Conference Paper,"Final","",Scopus,2-s2.0-85084054426
"Rajman M., Besancon R.","","Text mining: Natural language techniques and text mining applications",1997,"Proceedings of the Seventh IFIP 2.6 Working Conference on Database Semantics (DS-7)",,,,"7","10",,77,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-31144447775
"Khan A., Salim N., Jaya Kumar Y.","55993914900;12790857000;54405994500;","A framework for multi-document abstractive summarization based on semantic role labelling",2015,"Applied Soft Computing Journal","30",,,"737","747",,75,"10.1016/j.asoc.2015.01.070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84939483207&doi=10.1016%2fj.asoc.2015.01.070&partnerID=40&md5=2df251c3bf3dfbce53f32f6532a4c82d","We propose a framework for abstractive summarization of multi-documents, which aims to select contents of summary not from the source document sentences but from the semantic representation of the source documents. In this framework, contents of the source documents are represented by predicate argument structures by employing semantic role labeling. Content selection for summary is made by ranking the predicate argument structures based on optimized features, and using language generation for generating sentences from predicate argument structures. Our proposed framework differs from other abstractive summarization approaches in a few aspects. First, it employs semantic role labeling for semantic representation of text. Secondly, it analyzes the source text semantically by utilizing semantic similarity measure in order to cluster semantically similar predicate argument structures across the text; and finally it ranks the predicate argument structures based on features weighted by genetic algorithm (GA). Experiment of this study is carried out using DUC-2002, a standard corpus for text summarization. Results indicate that the proposed approach performs better than other summarization systems. © 2015 Elsevier B.V. All rights reserved.","Abstractive summary; Genetic algorithm; Language generation; Semantic role labeling; Semantic similarity measure","Computational linguistics; Genetic algorithms; Natural language processing systems; Text processing; Abstractive summary; Argument structures; Language generation; Semantic representation; Semantic role labeling; Semantic similarity measures; Summarization systems; Text summarization; Semantics",,,,,,Article,"Final","",Scopus,2-s2.0-84939483207
"Rousseau F., Kiagias E., Vazirgiannis M.","36574107700;57224252880;7004248278;","Text categorization as a graph classification problem",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference","1",,,"1702","1712",,74,"10.3115/v1/p15-1164","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943773848&doi=10.3115%2fv1%2fp15-1164&partnerID=40&md5=0132bb4046d37a52912c51c2b31fabd1","In this paper, we consider the task of text categorization as a graph classification problem. By representing textual documents as graph-of-words instead of historical n-gram bag-of-words, we extract more discriminative features that correspond to long-distance n-grams through frequent subgraph mining. Moreover, by capitalizing on the concept of k-core, we reduce the graph representation to its densest part - its main core - speeding up the feature extraction step for little to no cost in prediction performances. Experiments on four standard text classification datasets show statistically significant higher accuracy and macro-Averaged F1-score compared to baseline approaches. © 2015 Association for Computational Linguistics.",,"Classification (of information); Computational linguistics; Text processing; Discriminative features; Frequent subgraph mining; Graph classification; Graph representation; Prediction performance; Text categorization; Text classification; Textual documents; Natural language processing systems","Alibaba Group;Baidu;CreditEase;et al.;Samsung;Tencent","53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015","26 July 2015 through 31 July 2015",,114195,Conference Paper,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-84943773848
"Cambria E., Hussain A., Durrani T., Havasi C., Eckl C., Munro J.","56140547500;19734290900;56897622400;55899855300;36019820400;7102725928;","Sentic computing for patient centered applications",2010,"International Conference on Signal Processing Proceedings, ICSP",,,"5657072","1279","1282",,74,"10.1109/ICOSP.2010.5657072","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78651089151&doi=10.1109%2fICOSP.2010.5657072&partnerID=40&md5=63cc3452a8197b419d3636ae6724d4f4","Next-generation patients are far from being peripheral to health-care. They are central to understanding the effectiveness and efficiency of services and how they can be improved. Today a lot of patients are used to reviewing local health services on-line but this social information is just stored in natural language text and it is not machine-accessible and machine-processable. To distil knowledge from this extremely unstructured information we use Sentic Computing, a new opinion mining and sentiment analysis paradigm which exploits AI and Semantic Web techniques to better recognize, interpret and process opinions and sentiments in natural language text. In particular, we use a language visualization and analysis system, a novel emotion categorization model, a resource for opinion mining based on a web ontology and novel techniques for finding and defining topic dependent concepts, namely spectral association and CF-IOF weighting respectively. © 2010 IEEE.","AI; E-health; Knowledge base management; NLP; Opinion mining and sentiment analysis; Semantic networks","AI; Ehealth; Knowledge base management; NLP; Opinion mining; Semantic networks; Character recognition; Data mining; Health; Knowledge based systems; Natural language processing systems; Ontology; Semantics; Signal processing; Visualization; Semantic Web","IEEE Beijing Section;The Chinese Institute of Electronics (CIE);The Institution of Engineering and Technology (IET);Union Radio Scientifique Internationale (URSI);National Natural Science Foundation of China","2010 IEEE 10th International Conference on Signal Processing, ICSP2010","24 October 2010 through 28 October 2010","Beijing",83255,Conference Paper,"Final","",Scopus,2-s2.0-78651089151
"Hoder K., Voronkov A.","35721922800;7005162881;","Sine qua von for large theory reasoning",2011,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6803 LNAI",,,"299","314",,73,"10.1007/978-3-642-22438-6_23","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051688462&doi=10.1007%2f978-3-642-22438-6_23&partnerID=40&md5=ef159b1fae57af2ad031fe9df16cfcac","One possible way to deal with large theories is to have a good selection method for relevant axioms. This is confirmed by the fact that the largest available first-order knowledge base (the Open CYC) contains over 3 million axioms, while answering queries to it usually requires not more than a few dozen axioms. A method for axiom selection has been proposed by the first author in the Sumo INference Engine (SInE) system. SInE has won the large theory division of CASC in 2008. The method turned out to be so successful that the next two years it was used by the winner as well as by several other competing systems. This paper contains the presentation of the method and describes experiments with it in the theorem prover Vampire. © 2011 Springer-Verlag Berlin Heidelberg.",,"Answering queries; Competing systems; First-order; Knowledge base; Selection methods; Theorem provers; Automation; Knowledge based systems; Automata theory",,"23rd International Conference on Automated Deduction, CADE 23","31 July 2011 through 5 August 2011","Wroclaw",86017,Conference Paper,"Final","",Scopus,2-s2.0-80051688462
"Han X., Zhu H., Yu P., Wang Z., Yao Y., Liu Z., Sun M.","57205548124;57196120154;57215718787;57219499782;57212183905;57191691341;7403180987;","Fewrel: A large-scale supervised few-shot relation classification dataset with state-of-the-art evaluation",2020,"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018",,,,"4803","4809",,72,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081734437&partnerID=40&md5=d1a40593f8ff6f093382bb49b256d708","We present a Few-Shot Relation Classification Dataset (FewRel), consisting of 70, 000 sentences on 100 relations derived from Wikipedia and annotated by crowdworkers. The relation of each sentence is first recognized by distant supervision methods, and then filtered by crowdworkers. We adapt the most recent state-of-the-art few-shot learning methods for relation classification and conduct thorough evaluation of these methods. Empirical results show that even the most competitive few-shot learning models struggle on this task, especially as compared with humans. We also show that a range of different reasoning skills are needed to solve our task. These results indicate that few-shot relation classification remains an open problem and still requires further research. Our detailed analysis points multiple directions for future research. All details and resources about the dataset and baselines are released on http://zhuhao.me/ fewrel. © 2018 Association for Computational Linguistics",,"Classification (of information); Large dataset; Natural language processing systems; Learning methods; Learning models; Recent state; Relation classifications; State of the art; Wikipedia; Learning systems","Apple;Bloomberg;et al.;Facebook;Google;salesforce","2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018","31 October 2018 through 4 November 2018",,158085,Conference Paper,"Final","",Scopus,2-s2.0-85081734437
"Nancy, Garg H.","57191923708;56701049300;","An improved score function for ranking neutrosophic sets and its application to decision-making process",2018,"International Journal for Uncertainty Quantification","6","5",,"377","385",,72,"10.1615/Int.J.UncertaintyQuantification.2016018441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022194366&doi=10.1615%2fInt.J.UncertaintyQuantification.2016018441&partnerID=40&md5=5df02989bb59b3158036842123b2c7ee","The neutrosophic set (NS) is a more general platform which generalizes the concept of crisp, fuzzy, and intuitionistic fuzzy sets to describe the membership functions in terms of truth, indeterminacy, and false degree. Under this environment, the present paper proposes an improved score function for ranking the single as well as interval-valued NSs by incorporating the idea of hesitation degree between the truth and false degrees. Shortcomings of the existing function have been highlighted in it. Further, the decision-making method has been presented based on proposed function and illustrates it with a numerical example to demonstrate its practicality and effectiveness. © 2016 by Begell House, Inc.","Decision making; Expert system; Neutrosophic set; Score function",,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85022194366
"Xu J., Chen D., Qiu X., Huang X.","57196117692;57210637610;56291171900;8983710700;","Cached long short-term memory neural networks for document-level sentiment classification",2016,"EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"1660","1669",,72,"10.18653/v1/d16-1172","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072846529&doi=10.18653%2fv1%2fd16-1172&partnerID=40&md5=19c7ff3f9b6e9cb636425cb2cae694c9","Recently, neural networks have achieved great success on sentiment classification due to their ability to alleviate feature engineering. However, one of the remaining challenges is to model long texts in document-level sentiment classification under a recurrent architecture because of the deficiency of the memory unit. To address this problem, we present a Cached Long Short-Term Memory neural networks (CLSTM) to capture the overall semantic information in long texts. CLSTM introduces a cache mechanism, which divides memory into several groups with different forgetting rates and thus enables the network to keep sentiment information better within a recurrent unit. The proposed CLSTM outperforms the state-of-the-art models on three publicly available document-level sentiment analysis datasets. © 2016 Association for Computational Linguistics",,"Brain; Information retrieval systems; Long short-term memory; Semantics; Sentiment analysis; Cache mechanism; Feature engineerings; Memory units; Semantic information; Sentiment classification; State of the art; Cache memory","Amazon.com;Baidu;et al.;Google;Grammarly;Microsoft","2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016","1 November 2016 through 5 November 2016",,150070,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85072846529
"Fellbaum C.","","WordNet",1998,"The Encyclopedia of Applied Linguistics",,,,"","",,72,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84943740667
"Thorne J., Vlachos A., Christodoulopoulos C., Mittal A.","","The fact extraction and VERification (FEVER) shared task",2018,"Fever: a large-scale dataset for fact extraction and verification",,,,"","",,71,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85070526161
"Kauchak D.","12140512500;","Improving text simplification language modeling using unsimplified text data",2013,"ACL 2013 - 51st Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference","1",,,"1537","1546",,71,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84907300133&partnerID=40&md5=3d77c006e1fbdf2d2f7727ea79f8f020","In this paper we examine language modeling for text simplification. Unlike some text-to-text translation tasks, text simplification is a monolingual translation task allowing for text in both the input and output domain to be used for training the language model. We explore the relationship between normal English and simplified English and compare language models trained on varying amounts of text from each. We evaluate the models intrinsically with perplexity and extrinsically on the lexical simplification task from SemEval 2012. We find that a combined model using both simplified and normal English data achieves a 23% improvement in perplexity and a 24% improvement on the lexical simplification task over a model trained only on simple data. Post-hoc analysis shows that the additional unsimplified data provides better coverage for unseen and rare n-grams. © 2013 Association for Computational Linguistics.",,"Combined model; Input and outputs; Language model; N-grams; Text data; Computational linguistics","Baidu;et al.;Google;Microsoft Research;ontotext;Qatar Computing Research Institute (QCRI)","51st Annual Meeting of the Association for Computational Linguistics, ACL 2013","4 August 2013 through 9 August 2013","Sofia",107371,Conference Paper,"Final","",Scopus,2-s2.0-84907300133
"Quirk C., Poon H.","6701430521;15056546800;","Distant supervision for relation extraction beyond the sentence boundary",2017,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference","2",,,"1171","1182",,70,"10.18653/v1/e17-1110","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021675745&doi=10.18653%2fv1%2fe17-1110&partnerID=40&md5=affe9e0e2f45981a8fac0143ed9d908d","The growing demand for structured knowledge has led to great interest in relation extraction, especially in cases with limited supervision. However, existing distance supervision approaches only extract relations expressed in single sentences. In general, cross-sentence relation extraction is under-explored, even in the supervised-learning setting. In this paper, we propose the first approach for applying distant supervision to crosssentence relation extraction. At the core of our approach is a graph representation that can incorporate both standard dependencies and discourse relations, thus providing a unifying way to model relations within and across sentences. We extract features from multiple paths in this graph, increasing accuracy and robustness when confronted with linguistic variation and analysis error. Experiments on an important extraction task for precision medicine show that our approach can learn an accurate cross-sentence extractor, using only a small existing knowledge base and unlabeled text from biomedical research articles. Compared to the existing distant supervision paradigm, our approach extracted twice as many relations at similar precision, thus demonstrating the prevalence of cross-sentence relations and the promise of our approach. © 2017 Association for Computational Linguistics.",,"Computational linguistics; Extraction; Knowledge based systems; Biomedical research; Graph representation; Growing demand; Knowledge base; Modeling relations; Relation extraction; Sentence boundaries; Structured knowledge; Data mining","CELI: Language Technology;eBay;et al.;Grammarly;Textkernel;Thomson Reuters","15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017","3 April 2017 through 7 April 2017",,127985,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85021675745
"Nute D.","",[No title available],1997,"Defeasible Deontic Logic",,,,"","",,70,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0040974850
"Prabhu Y., Kag A., Harsola S., Agrawal R., Varma M.","56017115600;57202036238;55601914600;57212907262;24823171100;","Parabel: Partitioned label trees for extreme classification with application to dynamic search advertising",2018,"The Web Conference 2018 - Proceedings of the World Wide Web Conference, WWW 2018",,,,"993","1002",,69,"10.1145/3178876.3185998","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084047036&doi=10.1145%2f3178876.3185998&partnerID=40&md5=2768f2155a62df5e1895ad5a064c93e5","This paper develops the Parabel algorithm for extreme multi-label learning where the objective is to learn classifiers that can annotate each data point with the most relevant subset of labels from an extremely large label set. The state-of-the-art 1-vs-All based DiSMEC and PPDSparse algorithms are the most accurate but can take upto months for training and prediction as they learn and apply an independent linear classifier per label. Consequently, they do not scale to large datasets with millions of labels. Parabel addresses both limitations by learning a balanced label hierarchy such that: (a) the 1-vs-All classifiers in the leaf nodes of the label hierarchy can be trained on a small subset of the training set thereby reducing the training time to a few hours on a single core of a standard desktop and (b) novel points can be classified by traversing the learned hierarchy in logarithmic time and applying the 1-vs-All classifiers present in just the leaf thereby reducing the prediction time to a few milliseconds per test point. This allows Parabel to scale to tasks considered infeasible for DiSMEC and PPDSparse such as predicting the subset of 7 million Bing queries that might lead to a click on a given ad-landing page for dynamic search advertising. Experiments on multiple benchmark datasets revealed that Parabel could be almost as accurate as PPDSparse and DiSMEC while being upto 1,000x faster at training and upto 40x-10,000x faster at prediction. Furthermore, Parabel was demonstrated to significantly improve dynamic search advertising on Bing by more than doubling the ad recall and improving the click-through rate by 20%. Source code for Parabel can be downloaded from [1]. © 2018 IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC BY 4.0 License.","Dynamic search advertising; Extreme classification","Forecasting; Large dataset; Learning systems; Marketing; World Wide Web; Benchmark datasets; Click-through rate; Linear classifiers; Logarithmic time; Multi-label learning; Prediction time; Standard desktop; State of the art; Classification (of information)","Amazon;Baidu;et al.;Google;IDEX Lyon;Inria","27th International World Wide Web, WWW 2018","23 April 2018 through 27 April 2018",,159681,Conference Paper,"Final","",Scopus,2-s2.0-85084047036
"Keshavarz H., Abadeh M.S.","55026471300;55663643400;","ALGA: Adaptive lexicon learning using genetic algorithm for sentiment analysis of microblogs",2017,"Knowledge-Based Systems","122",,,"1","16",,69,"10.1016/j.knosys.2017.01.028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85011100006&doi=10.1016%2fj.knosys.2017.01.028&partnerID=40&md5=b51432f9a7383b86b701ef8666e46222","Sentiment analysis is about classifying opinions expressed in text. The aim of this study is to improve polarity classification of sentiments in microblogs by building adaptive sentiment lexicons. In the proposed method, corpora-based and lexicon-based approaches are combined and lexicons are generated from text. The sentiment classification is formulated as an optimization problem, in which the goal is to find optimum sentiment lexicons. A novel genetic algorithm is then proposed to solve this optimization problem and find lexicons to classify text. The algorithm generates adaptive sentiment lexicons, and then a meta-level feature is extracted based on it, which is then used alongside Bing Liu's lexicon and n-gram features. The experiments are conducted on six datasets. In terms of accuracy, the results outperform the state-of-the-art methods proposed in the literature in two of the datasets. Also, in four of the datasets, the proposed approach outperforms in terms of F-measure. Applying the proposed method on six datasets, the accuracy is higher than 80% in all six datasets and the F-measure is higher than 80% in four of these datasets. Using the sentiment lexicons created by the proposed algorithm, one can get a better understanding of the specific language and culture of Twitter users and sentiment orientation of words in different contexts. It is also shown that it is useful not to omit the conventional stop-words, as each word can have its sentimental implications. © 2017 Elsevier B.V.","Evolutionary computation; Genetic algorithm; Sentiment analysis; Sentiment lexicon; Social media; Twitter","Data mining; Evolutionary algorithms; Genetic algorithms; Sentiment analysis; Social networking (online); Novel genetic algorithm; Optimization problems; Polarity classification; Sentiment classification; Sentiment lexicons; Social media; State-of-the-art methods; Twitter; Natural language processing systems",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85011100006
"Bing L., Li P., Liao Y., Lam W., Guo W., Passonneau R.J.","24314897600;36440053900;56898379600;57203073460;51664964500;6505968656;","Abstractive multi-document summarization via phrase selection and merging",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference","1",,,"1587","1597",,69,"10.3115/v1/p15-1153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943785681&doi=10.3115%2fv1%2fp15-1153&partnerID=40&md5=5f34f0b168c4ce5355fb15ddc1380627","We propose an abstraction-based multidocument summarization framework that can construct new sentences by exploring more fine-grained syntactic units than sentences, namely, noun/verb phrases. Different from existing abstraction-based approaches, our method first constructs a pool of concepts and facts represented by phrases from the input documents. Then new sentences are generated by selecting and merging informative phrases to maximize the salience of phrases and meanwhile satisfy the sentence construction constraints. We employ integer linear optimization for conducting phrase selection and merging simultaneously in order to achieve the global optimal solution for a summary. Experimental results on the benchmark data set TAC 2011 show that our framework outperforms the state-ofthe-Art models under automated pyramid evaluation metric, and achieves reasonably well results on manual linguistic quality evaluation. © 2015 Association for Computational Linguistics.",,"Abstracting; Benchmarking; Computational linguistics; Integer programming; Linear programming; Merging; Petroleum reservoir evaluation; Quality control; Syntactics; Benchmark data; Construction constraints; Evaluation metrics; Fine grained; Global optimal solutions; Linear optimization; Multi-document summarization; Quality evaluation; Natural language processing systems","Alibaba Group;Baidu;CreditEase;et al.;Samsung;Tencent","53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015","26 July 2015 through 31 July 2015",,114195,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84943785681
"Lewis M., Steedman M.","","Combined distributional and logical semantics",2013,"Transactions of the Association for Computational Linguistics","1",,,"179","192",,69,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84905695006
"Zhang X., Zhang Y., Wang S., Yao Y., Fang B., Yu P.S.","55949851900;57200151833;56424441500;57191875018;7102405844;7402366049;","Improving stock market prediction via heterogeneous information fusion",2018,"Knowledge-Based Systems","143",,,"236","247",,68,"10.1016/j.knosys.2017.12.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85039943279&doi=10.1016%2fj.knosys.2017.12.025&partnerID=40&md5=84f2b67e8f0b200469d16cbcc4a5124b","Traditional stock market prediction approaches commonly utilize the historical price-related data of the stocks to forecast their future trends. As the Web information grows, recently some works try to explore financial news to improve the prediction. Effective indicators, e.g., the events related to the stocks and the people's sentiments toward the market and stocks, have been proved to play important roles in the stocks’ volatility, and are extracted to feed into the prediction models for improving the prediction accuracy. However, a major limitation of previous methods is that the indicators are obtained from only a single source whose reliability might be low, or from several data sources but their interactions and correlations among the multi-sourced data are largely ignored. In this work, we extract the events from Web news and the users’ sentiments from social media, and investigate their joint impacts on the stock price movements via a coupled matrix and tensor factorization framework. Specifically, a tensor is firstly constructed to fuse heterogeneous data and capture the intrinsic relations among the events and the investors’ sentiments. Due to the sparsity of the tensor, two auxiliary matrices, the stock quantitative feature matrix and the stock correlation matrix, are constructed and incorporated to assist the tensor decomposition. The intuition behind is that stocks that are highly correlated with each other tend to be affected by the same event. Thus, instead of conducting each stock prediction task separately and independently, we predict multiple correlated stocks simultaneously through their commonalities, which are enabled via sharing the collaboratively factorized low rank matrices between matrices and the tensor. Evaluations on the China A-share stock data and the HK stock data in the year 2015 demonstrate the effectiveness of the proposed model. © 2017","Social media; Stock correlation; Stock prediction; Tensor factorization","Commerce; Factorization; Finance; Financial markets; Forecasting; Investments; Social networking (online); Tensors; 00-01; 99-00; Social media; Stock predictions; Tensor factorization; Matrix algebra",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85039943279
"Liu S., Chen H., Ren Z., Feng Y., Liu Q., Yin D.","57207859369;57204468585;53985046100;55747582000;56181387900;35759826200;","Knowledge diffusion for neural dialogue generation",2018,"ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)","1",,,"1489","1498",,68,"10.18653/v1/p18-1138","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063107708&doi=10.18653%2fv1%2fp18-1138&partnerID=40&md5=c8e08d9e8af32bfcc6bd4acf184aba38","End-to-end neural dialogue generation has shown promising results recently, but it does not employ knowledge to guide the generation and hence tends to generate short, general, and meaningless responses. In this paper, we propose a neural knowledge diffusion (NKD) model to introduce knowledge into dialogue generation. This method can not only match the relevant facts for the input utterance but diffuse them to similar entities. With the help of facts matching and entity diffusion, the neural dialogue generation is augmented with the ability of convergent and divergent thinking over the knowledge base. Our empirical study on a real-world dataset proves that our model is capable of generating meaningful, diverse and natural responses for both factoid-questions and knowledge grounded chi-chats. The experiment results also show that our model outperforms competitive baseline models significantly. © 2018 Association for Computational Linguistics",,"Computational linguistics; Knowledge based systems; Baseline models; Dialogue generations; Divergent thinkings; Empirical studies; Factoid questions; Knowledge base; Knowledge diffusion; Natural response; Diffusion","Apple;ByteDance;et al.;Facebook;Google;Samsung Research","56th Annual Meeting of the Association for Computational Linguistics, ACL 2018","15 July 2018 through 20 July 2018",,145927,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85063107708
"Yan Z., Duan N., Chen P., Zhou M., Zhou J., Li Z.","56305783700;52163366000;57193236029;55587890800;57190493235;56024684400;","Building task-oriented dialogue systems for online shopping",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"4618","4625",,68,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85030462261&partnerID=40&md5=8d71c006d5f04f3dc49e5977691641d3","We present a general solution towards building task-oriented dialogue systems for online shopping, aiming to assist online customers in completing various purchase-related tasks, such as searching products and answering questions, in a natural language conversation manner. As a pioneering work, we show what & how existing natural language processing techniques, data resources, and crowdsourcing can be leveraged to build such task-oriented dialogue systems for E-commerce usage. To demonstrate its effectiveness, we integrate our system into a mobile online shopping application. To the best of our knowledge, this is the first time that an dialogue system in Chinese is practically used in online shopping scenario with millions of real consumers. Interesting and insightful observations are shown in the experimental part, based on the analysis of human-bot conversation log. Several current challenges are also pointed out as our future directions. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Data handling; Electronic commerce; Online systems; Speech processing; Data resources; Dialogue systems; General solutions; Natural languages; Online customers; Online shopping; Task-oriented; Natural language processing systems","Amazon;Artificial Intelligence;Baidu;et al.;IBM;Tencent","31st AAAI Conference on Artificial Intelligence, AAAI 2017","4 February 2017 through 10 February 2017",,130407,Conference Paper,"Final","",Scopus,2-s2.0-85030462261
"Das R., Neelakantan A., Belanger D., McCallum A.","57199967575;57159776800;36622170800;7003773569;","Chains of reasoning over entities, relations, & text using recurrent neural networks",2017,"15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017 - Proceedings of Conference","1",,,"132","141",,68,"10.18653/v1/e17-1013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021684800&doi=10.18653%2fv1%2fe17-1013&partnerID=40&md5=238e3979dc658f26c13989acc63d088e","Our goal is to combine the rich multistep inference of symbolic logical reasoning with the generalization capabilities of neural networks. We are particularly interested in complex reasoning about entities and relations in text and large-scale knowledge bases (KBs). Neelakantan et al. (2015) use RNNs to obtain dense representations of multi-hop paths in KBs; however for multiple reasons, the approach lacks accuracy and practicality. This paper proposes three significant modeling advances: (1) we learn to jointly reason about relations, entities, and entity-types; (2) we use neural attention modeling to incorporate multiple paths; (3) we learn to share strength in a single RNN that represents logical composition across all relations. On a largescale Freebase+ClueWeb prediction task, we achieve 25% error reduction, and a 53% error reduction on sparse relations. On chains of reasoning in WordNet we reduce error in mean quantile by 84% versus the previous state of the art. © 2017 Association for Computational Linguistics.",,"Computational linguistics; Errors; Attention model; Error reduction; Generalization capability; Knowledge basis (KBs); Logical reasoning; Multi-hop path; Prediction tasks; State of the art; Recurrent neural networks","CELI: Language Technology;eBay;et al.;Grammarly;Textkernel;Thomson Reuters","15th Conference of the European Chapter of the Association for Computational Linguistics, EACL 2017","3 April 2017 through 7 April 2017",,127985,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85021684800
"Oneto L., Bisio F., Cambria E., Anguita D.","41262130900;55547779800;56140547500;7004764744;","Statistical Learning Theory and ELM for Big Social Data Analysis",2016,"IEEE Computational Intelligence Magazine","11","3","7515290","45","55",,68,"10.1109/MCI.2016.2572540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979680823&doi=10.1109%2fMCI.2016.2572540&partnerID=40&md5=bf47a6e323804fcbe9408a07d2ed9af2","The science of opinion analysis based on data from social networks and other forms of mass media has garnered the interest of the scientific community and the business world. Dealing with the increasing amount of information present on the Web is a critical task and requires efficient models developed by the emerging field of sentiment analysis. To this end, current research proposes an efficient approach to support emotion recognition and polarity detection in natural language text. In this paper, we show how to exploit the most recent technological tools and advances in Statistical Learning Theory (SLT) in order to efficiently build an Extreme Learning Machine (ELM) and assess the resultant model's performance when applied to big social data analysis. ELM represents a powerful learning tool, developed to overcome some issues in back-propagation networks. The main problem with ELM is in training them to work in the event of a large number of available samples, where the generalization performance has to be carefully assessed. For this reason, we propose an ELM implementation that exploits the Spark distributed in memory technology and show how to take advantage of the most recent advances in SLT in order to address the issue of selecting ELM hyperparameters that give the best generalization performance. © 2005-2012 IEEE.",,"Character recognition; Data handling; Information analysis; Learning systems; Natural language processing systems; Public relations; Amount of information; Backpropagation network; Extreme learning machine; Generalization performance; Natural language text; Scientific community; Statistical learning theory; Technological tools; Backpropagation",,,,,,Article,"Final","",Scopus,2-s2.0-84979680823
"Van Seters J.R., Ossevoort M.A., Tramper J., Goedhart M.J.","36775635200;6603849569;7006013388;6602303608;","The influence of student characteristics on the use of adaptive e-learning material",2012,"Computers and Education","58","3",,"942","952",,68,"10.1016/j.compedu.2011.11.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-82055194412&doi=10.1016%2fj.compedu.2011.11.002&partnerID=40&md5=960d837bf90e96f76b5ad8db86359363","Adaptive e-learning materials can help teachers to educate heterogeneous student groups. This study provides empirical data about the way academic students differ in their learning when using adaptive e-learning materials. Ninety-four students participated in the study. We determined characteristics in a heterogeneous student group by collecting demographic data and measuring motivation and prior knowledge. We also measured the learning paths students followed and learning strategies they used when working with adaptive e-learning material in a molecular biology course. We then combined these data to study if and how student characteristics relate to the learning paths and strategies they used. We observed that students did follow different learning paths. Gender did not have an effect, but (mainly Dutch) BSc students differed from (international) MSc students in the intrinsic motivation they had and the learning paths and strategies they followed when using the adaptive e-learning material. © 2011 Published by Elsevier Ltd. All rights reserved.","Intelligent tutoring systems; Interactive learning environments; Multimedia/hypermedia systems; Post-secondary education; Teaching/learning strategies","Intelligent tutoring system; Interactive learning environment; Multimedia/hypermedia systems; Postsecondary education; Teaching/learning strategies; Computer aided instruction; E-learning; Materials; Molecular biology; Motivation; Teaching; Students",,,,,,Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-82055194412
"Domingos P., Richardson M.","","Markov logic: A unifying framework for statistical relational learning",2004,"Proceedings of the ICML-2004 Workshop on Statistical Relational Learning and Its Connections to Other Fields",,,,"49","54",,68,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-22944467390
"Stein R.A., Jaques P.A., Valiati J.F.","57203780037;8546784100;8915545700;","An analysis of hierarchical text classification using word embeddings",2019,"Information Sciences","471",,,"216","232",,67,"10.1016/j.ins.2018.09.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85052975887&doi=10.1016%2fj.ins.2018.09.001&partnerID=40&md5=31204f80a78c1e87e608cf8366d6d5cf","Efficient distributed numerical word representation models (word embeddings) combined with modern machine learning algorithms have recently yielded considerable improvement on automatic document classification tasks. However, the effectiveness of such techniques has not been assessed for the hierarchical text classification (HTC) yet. This study investigates the application of those models and algorithms on this specific problem by means of experimentation and analysis. We trained classification models with prominent machine learning algorithm implementations—fastText, XGBoost, SVM, and Keras’ CNN—and noticeable word embeddings generation methods—GloVe, word2vec, and fastText—with publicly available data and evaluated them with measures specifically appropriate for the hierarchical context. FastText achieved an LCAF1 of 0.893 on a single-labeled version of the RCV1 dataset. An analysis indicates that using word embeddings and its flavors is a very promising approach for HTC. © 2018 Elsevier Inc.","fastText; Gradient tree boosting; Hierarchical text classification; Support vector machines; Word embeddings","Artificial intelligence; Classification (of information); Information retrieval systems; Support vector machines; Text processing; Classification models; Document Classification; Embeddings; fastText; Gradient tree boosting; Models and algorithms; Text classification; Word representations; Learning algorithms",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85052975887
"Jue W., Domingos P.","35367400800;7003565655;","Hybrid markov logic networks",2008,"Proceedings of the National Conference on Artificial Intelligence","2",,,"1106","1111",,67,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-57749175299&partnerID=40&md5=f90edbb7659ba06108dc3ef4fd31e30c","Markov logic networks (MLNs) combine first-order logic and Markov networks, allowing us to handle the complexity and uncertainty of real-world problems in a single consistent framework. However, in MLNs all variables and features are discrete, while most real-world applications also contain continuous ones. In this paper we introduce hybrid MLNs, in which continuous properties (e.g., the distance between two objects) and functions over them can appear as features. Hybrid MLNs have all distributions in the exponential family as special cases (e.g., multivariate Gaussians), and allow much more compact modeling of non-i.i.d. data than prepositional representations like hybrid Bayesian networks. We also introduce inference algorithms for hybrid MLNs, by extending the MaxWalkSAT and MC-SAT algorithms to continuous domains. Experiments in a mobile robot mapping domain-involving joint classification, clustering and regression-illustrate the power of hybrid MLNs as a modeling language, and the accuracy and efficiency of the inference algorithms. Copyright © 2008, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Applications; Artificial intelligence; Bayesian networks; Bionics; Clustering algorithms; Computational methods; Conformal mapping; Formal logic; Inference engines; Wireless networks; Compact modeling; Continuous domains; Do-mains; Exponential families; First orders; Gaussians; Inference algorithms; Markov logic networks; Markov networks; Modeling languages; Robot mappings; Sat algorithms; Knowledge based systems","Cornell Univ. Intelligent Information Systems Inst.;Toyota Motor Engineering and Manufacturing North America Inc.;ACM/SIGART;Boeing","23rd AAAI Conference on Artificial Intelligence and the 20th Innovative Applications of Artificial Intelligence Conference, AAAI-08/IAAI-08","13 July 2008 through 17 July 2008","Chicago, IL",74777,Conference Paper,"Final","",Scopus,2-s2.0-57749175299
"Bengio Y., Schwenk H., Senécal J.-S., Morin F., Gauvain J.-L.","","Neural probabilistic language models",2006,"Innovations in Machine Learning",,"6",,"137","186",,67,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84894576734
"Khan F.H., Qamar U., Bashir S.","36682484300;55746281400;36681876000;","SentiMI: Introducing point-wise mutual information with SentiWordNet to improve sentiment polarity detection",2016,"Applied Soft Computing Journal","39",,,"140","153",,66,"10.1016/j.asoc.2015.11.016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84948424377&doi=10.1016%2fj.asoc.2015.11.016&partnerID=40&md5=b0e7a214ddf9069fc933c07b975b0144","Supervised learning has attracted much attention in recent years. As a consequence, many of the state-of-the-art algorithms are domain dependent as they require a labeled training corpus to learn the domain features. This requires the availability of labeled corpora which is a cumbersome task in itself. However, for text sentiment detection SentiWordNet (SWN) may be used. It is a vocabulary where terms are arranged in synonym groups called synsets. This research makes use of SentiWordNet and treats it as the labeled corpus for training. A sentiment dictionary, SentiMI, builds upon the mutual information calculated from these terms. A complete framework is developed by using feature selection and extracting mutual information, from SentiMI, for the selected features. Training, testing and evaluation of the proposed framework are conducted on a large dataset of 50,000 movie reviews. A notable performance improvement of 7% in accuracy, 14% in specificity, and 8% in F-measure is achieved by the proposed framework as compared to the baseline SentiWordNet classifier. Comparison with the state-of-the-art classifiers is also performed on widely used Cornell Movie Review dataset which also proves the effectiveness of the proposed approach. © 2015 Elsevier B.V.","Data mining; Mutual information; Sentiment analysis; SentiWordNet; Social media; Text mining","Classification (of information); Natural language processing systems; Statistical tests; Mutual informations; Sentiment analysis; SentiWordNet; Social media; Text mining; Data mining",,,,,,Article,"Final","",Scopus,2-s2.0-84948424377
"Alvarez-Alvarez A., Trivino G., Cordón O.","35387455700;6506780896;7003424903;","Human gait modeling using a genetic fuzzy finite state machine",2012,"IEEE Transactions on Fuzzy Systems","20","2","6054027","205","223",,66,"10.1109/TFUZZ.2011.2171973","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84859705290&doi=10.1109%2fTFUZZ.2011.2171973&partnerID=40&md5=8854cac1881098343010850d709a4762","Human gait modeling consists of studying the biomechanics of this human movement. Its importance lies in the fact that its analysis can help in the diagnosis of walking and movement disorders or rehabilitation programs, among other medical situations. Fuzzy finite state machines can be used to model the temporal evolution of this type of phenomenon. Nevertheless, the definition of details of the model in each particular case is a complex task for experts. In this paper, we present an automatic method to learn the model parameters that are based on the hybridization of fuzzy finite state machines and genetic algorithms leading to genetic fuzzy finite state machines. This new genetic fuzzy system automatically learns the fuzzy rules and membership functions of the fuzzy finite state machine, while an expert defines the possible states and allowed transitions. Our final goal is to obtain a specific model for each persons gait in such a way that it can generalize well with different gaits of the same person. The obtained model must become an accurate and human friendly linguistic description of this phenomenon, with the capability to identify the relevant phases of the process. A complete experimentation is developed to test the performance of the new proposal when dealing with datasets of 20 different people, comprising a detailed analysis of results, which shows the advantages of our proposal in comparison with some other classical and computational intelligence techniques. © 2012 IEEE.","Fuzzy finite state machines; fuzzy systems; genetic algorithms (GAs); genetic fuzzy systems; human gait modeling","Automatic method; Complex task; Computational intelligence techniques; Data sets; Genetic fuzzy systems; Genetic-fuzzy; Human gait modeling; Human movements; Human-friendly; Linguistic descriptions; Model parameters; Movement disorders; Rehabilitation programs; Temporal evolution; Artificial intelligence; Biomechanics; Diagnosis; Finite automata; Fuzzy systems; Program diagnostics; Genetic algorithms",,,,,,Article,"Final","",Scopus,2-s2.0-84859705290
"Levesque H.J.","7103141050;","The Winograd schema challenge",2011,"AAAI Spring Symposium - Technical Report","SS-11-06",,,"63","68",,66,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-80051508455&partnerID=40&md5=0df22eb7d04d7e275aaa24aa7f26263f","In this paper, we present an alternative to the Turing Test that has some conceptual and practical advantages. Like the original, it involves responding to typed English sentences, and English-speaking adults will have no difficulty with it. Unlike the original, the subject is not required to engage in a conversation and fool an interrogator into believing she is dealing with a person. Moreover, the test is arranged in such a way that having full access to a large corpus of English text might not help much. Finally, the interrogator or a third party will be able to decide unambiguously after a few minutes whether or not a subject has passed the test. Copyright © 2011, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"English sentences; Large corpora; Third parties; Turing tests; Winograd",,"2011 AAAI Spring Symposium","21 March 2011 through 23 March 2011",,85955,Conference Paper,"Final","",Scopus,2-s2.0-80051508455
"Huynh T.N., Mooney R.J.","23008817700;7102791999;","Discriminative structure and parameter learning for Markov logic networks",2008,"Proceedings of the 25th International Conference on Machine Learning",,,,"416","423",,66,"10.1145/1390156.1390209","https://www.scopus.com/inward/record.uri?eid=2-s2.0-56449093057&doi=10.1145%2f1390156.1390209&partnerID=40&md5=3adf333aa58cf7a9b41ee33ea4dccd4a","Markov logic networks (MLNs) are an expressive representation for statistical relational learning that generalizes both first-order logic and graphical models. Existing methods for learning the logical structure of an MLN are not discriminative; however, many relational learning problems involve specific target predicates that must be inferred from given background information. We found that existing MLN methods perform very poorly on several such ILP benchmark problems, and we present improved discriminative methods for learning MLN clauses and weights that outperform existing MLN and traditional ILP methods. Copyright 2008 by the author(s)/owner(s).",,"Computer circuits; Machine learning; Markov processes; Probabilistic logics; Formal logic; Graphic methods; Learning systems; Robot learning; Background information; Bench-mark problems; Discriminative methods; Logical structure; Markov logic networks; Parameter learning; Relational learning; Statistical relational learning; Inductive logic programming (ILP); Education; Background informations; Bench-mark problems; Discriminative methods; Existing methods; First-order logic; Graphical models; Logical structures; Markov logic networks; Parameter learning; Relational learning; Statistical relational learning","et al.;Federation of Finnish Learned Societies;Helsinki Institute for Information Technology;Intel Corporation;Machine Learning Journal/Springer;University of Helsinki","25th International Conference on Machine Learning","5 July 2008 through 9 July 2008","Helsinki",74109,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-56449093057
"Ma Y., Cambria E., Gao S.","57189601750;56140547500;57201549644;","Label embedding for zero-shot fine-grained named entity typing",2016,"COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers",,,,"171","180",,65,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054979013&partnerID=40&md5=99ae37cbcb6024754b60ef829f66a599","Named entity typing is the task of detecting the types of a named entity in context. For instance, given ""Eric is giving a presentation"", our goal is to infer that 'Eric' is a speaker or a presenter and a person. Existing approaches to named entity typing cannot work with a growing type set and fails to recognize entity mentions of unseen types. In this paper, we present a label embedding method that incorporates prototypical and hierarchical information to learn pre-trained label embeddings. In addition, we adapt a zero-shot framework that can predict both seen and previously unseen entity types. We perform evaluation on three benchmark datasets with two settings: 1) few-shots recognition where all types are covered by the training set; and 2) zero-shot recognition where fine-grained types are assumed absent from training set. Results show that prior knowledge encoded using our label embedding methods can significantly boost the performance of classification for both cases. © 1963-2018 ACL.",,"Computational linguistics; Benchmark datasets; Embedding method; Entity-types; Fine grained; Hierarchical information; Named entities; Prior knowledge; Training sets; Classification (of information)",,"26th International Conference on Computational Linguistics, COLING 2016","11 December 2016 through 16 December 2016",,136517,Conference Paper,"Final","",Scopus,2-s2.0-85054979013
"Roy S., Roth D.","57155543800;7401669040;","Solving general arithmetic word problems",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",,,,"1743","1752",,65,"10.18653/v1/d15-1202","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84959934544&doi=10.18653%2fv1%2fd15-1202&partnerID=40&md5=f61e93cbdf947cc3db5c29593bd5120a","This paper presents a novel approach to automatically solving arithmetic word problems. This is the first algorithmic approach that can handle arithmetic problems with multiple steps and operations, without depending on additional annotations or predefined templates. We develop a theory for expression trees that can be used to represent and evaluate the target arithmetic expressions; we use it to uniquely decompose the target arithmetic problem to multiple classification problems; we then compose an expression tree, combining these with world knowledge through a constrained inference framework. Our classifiers gain from the use of quantity schemas that supports better extraction of features. Experimental results show that our method outperforms existing systems, achieving state of the art performance on benchmark datasets of arithmetic word problems. © 2015 Association for Computational Linguistics.",,"Benchmarking; Forestry; Algorithmic approach; Arithmetic expression; Benchmark datasets; Existing systems; Multiple Classification; State-of-the-art performance; Word problem; World knowledge; Natural language processing systems","Baidu;Bloomberg;et al.;facebook;Google;Linkedin","Conference on Empirical Methods in Natural Language Processing, EMNLP 2015","17 September 2015 through 21 September 2015",,116677,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84959934544
"Howard N., Cambria E.","55326572900;56140547500;","Intention awareness: improving upon situation awareness in human-centric environments",2013,"Human-centric Computing and Information Sciences","3","1","9","1","17",,65,"10.1186/2192-1962-3-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84941104015&doi=10.1186%2f2192-1962-3-9&partnerID=40&md5=035bac60412f9366383b6e698097cf9c","As the gap between human and machine shrinks, it becomes increasingly important to develop computer systems that incorporate or enhance existing Situation Awareness. However, these tend to focus on raw quantitative parameters, such as position and speed of objects. When these situations are governed by human actors, such parameters leave significant margins of uncertainty. In this paper, we discuss the potential of applying the characteristics intrinsic to the human actors that comprise a given situation to Situation Awareness, and the capacity that these concepts have to improve situation-aware systems. We argue that intention-aware based systems offer an advantage over situation-aware based systems in that they reduce the informational burden on humans without limiting effectiveness. We argue that computational analysis and tracking of semantic and affective information associated with human actors' intentions are an effective way to minimize miscommunication and uncertainty, particularly in time-sensitive and information-saturated situations. © 2013, Howard and Cambria; licensee BioMed Central Ltd.","Intention awareness; Sentic computing; Situation awareness","Semantics; Computational analysis; Human actor; Human-centric; Intention awareness; Quantitative parameters; Sentic Computing; Situation awareness; Situation-aware; Uncertainty analysis",,,,,,Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84941104015
"Cambria E., Hussain A., Havasi C., Eckl C.","56140547500;19734290900;55899855300;36019820400;","Common sense computing: From the society of mind to digital intuition and beyond",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5707 LNCS",,,"252","259",,65,"10.1007/978-3-642-04391-8_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77952049760&doi=10.1007%2f978-3-642-04391-8_33&partnerID=40&md5=fa0eef99c93f9fdb45fd98930a00ac32","What is Common Sense Computing? And why is it so important for the technological evolution of humankind? This paper presents an overview of past, present and future efforts of the AI community to give computers the capacity for Common Sense reasoning, from Minsky's Society of Mind to Media Laboratory's Digital Intuition theory, and beyond. Is it actually possible to build a machine with Common Sense or is it just an utopia? This is the question this paper is trying to answer. © 2009 Springer-Verlag.","AI; Knowledge base management; NLP; Semantic networks","AI; Common sense; Commonsense reasoning; Knowledge base; Semantic network; Technological evolution; Biology; Biometrics; Knowledge based systems; Natural language processing systems; Semantics",,"Joint COST 2101 and 2102 International Conference on Biometric ID Management and Multimodal Communication, BioID_MultiComm 2009","16 September 2009 through 18 September 2009","Madrid",80162,Conference Paper,"Final","",Scopus,2-s2.0-77952049760
"Hitzler P., Hölldobler S., Seda A.K.","8876646400;55879266700;6603487478;","Logic programs and connectionist networks",2004,"Journal of Applied Logic","2","3",,"245","272",,65,"10.1016/j.jal.2004.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-10944269335&doi=10.1016%2fj.jal.2004.03.002&partnerID=40&md5=e6ec97207054dbdcade0516c86c40008","One facet of the question of integration of Logic and Connectionist Systems, and how these can complement each other, concerns the points of contact, in terms of semantics, between neural networks and logic programs. In this paper, we show that certain semantic operators for propositional logic programs can be computed by feedforward connectionist networks, and that the same semantic operators for first-order normal logic programs can be approximated by feedforward connectionist networks. Turning the networks into recurrent ones allows one also to approximate the models associated with the semantic operators. Our methods depend on a well-known theorem of Funahashi, and necessitate the study of when Funahashi's theorem can be applied, and also the study of what means of approximation are appropriate and significant. © 2004 Elsevier B.V. All rights reserved.","Connectionist networks; Logic programming; Metric spaces","Approximation theory; Artificial intelligence; Feedforward neural networks; Mathematical models; Mathematical transformations; Polynomials; Semantics; Theorem proving; Connectionist networks; Contraction mappings; Feedforward connectionist network; Funahashi theorem; Metric spaces; Logic programming",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-10944269335
"Shi P., Lin J.","","Simple BERT models for relation extraction and semantic role labeling",2019,"Simple BERT Models for Relation Extraction and Semantic Role Labeling",,,,"","",,64,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078731735
"Saif H., Fernandez M., He Y., Alani H.","55330697500;8928406600;22985368400;8892548000;","Evaluation datasets for Twitter sentiment analysis a survey and a new dataset, the STS-Gold",2013,"CEUR Workshop Proceedings","1096",,,"9","21",,64,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908157393&partnerID=40&md5=cc68d8aa78e8b62f4f1724747bbdd1dc","Sentiment analysis over Twitter offers organisations and individuals a fast and effective way to monitor the publics' feelings towards them and their competitors. To assess the performance of sentiment analysis methods over Twitter a small set of evaluation datasets have been released in the last few years. In this paper we present an overview of eight publicly available and manually annotated evaluation datasets for Twitter sentiment analysis. Based on this review, we show that a common limitation of most of these datasets, when assessing sentiment analysis at target (entity) level, is the lack of distinctive sentiment annotations among the tweets and the entities contained in them. For example, the tweet ""I love iPhone, but I hate iPad"" can be annotated with a mixed sentiment label, but the entity iPhone within this tweet should be annotated with a positive sentiment label. Aiming to overcome this limitation, and to complement current evaluation datasets, we present STS-Gold, a new evaluation dataset where tweets and targets (entities) are annotated individually and therefore may present different sentiment labels. This paper also provides a comparative study of the various datasets along several dimensions including: total number of tweets, vocabulary size and sparsity. We also investigate the pair-wise correlation among these dimensions as well as their correlations to the sentiment classification performance on different datasets.","Datasets; Sentiment analysis; Twitter","Artificial intelligence; Data mining; Gold; Smartphones; Social networking (online); Comparative studies; Datasets; Sentiment analysis; Sentiment classification; Twitter; Vocabulary size; Classification (of information)","","1st International Workshop on Emotion and Sentiment in Social and Expressive Media, ESSEM 2013","3 December 2013",,110725,Conference Paper,"Final","",Scopus,2-s2.0-84908157393
"Reitter D., Moore J.D., Keller F.","","Priming of syntactic rules in task-oriented dialogue and spontaneous conversation",2006,"Proceedings of the 28th Annual Conference of the Cognitive Science Society",,,,"685","690",,64,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-56149098648
"Xiao H.","",[No title available],2018,"bert-as-service",,,,"","",,63,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85074734684
"Lu Y., Dhillon P.S., Foster D., Ungar L.","56121775800;26646326500;7403219373;57201020481;","Faster ridge regression via the subsampled randomized hadamard transform",2013,"Advances in Neural Information Processing Systems",,,,"","",,63,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84898956628&partnerID=40&md5=8853c1506f74c82b2c0123027e668e25","We propose a fast algorithm for ridge regression when the number of features is much larger than the number of observations (p ≫ n). The standard way to solve ridge regression in this setting works in the dual space and gives a running time of O(n2p). Our algorithm Subsampled Randomized Hadamard Transform- Dual Ridge Regression (SRHT-DRR) runs in time O(np log(n)) and works by preconditioning the design matrix by a Randomized Walsh-Hadamard Transform with a subsequent subsampling of features. We provide risk bounds for our SRHT-DRR algorithm in the fixed design setting and show experimental results on synthetic and real datasets.",,"Algorithms; Hadamard transforms; Design matrix; Dual spaces; Fast algorithms; Real data sets; Ridge regression; Risk bounds; Running time; Walsh Hadamard Transforms; Regression analysis","Air Force Office of Scientific Research (AFOSR);Amazon.com;et al.;Facebook;Google;Microsoft Research","27th Annual Conference on Neural Information Processing Systems, NIPS 2013","5 December 2013 through 10 December 2013","Lake Tahoe, NV",104690,Conference Paper,"Final","",Scopus,2-s2.0-84898956628
"Gulhane P., Madaan A., Mehta R., Ramamirtham J., Rastogi R., Satpal S., Sengamedu S.H., Tengli A., Tiwari C.","26031396700;25825250100;56678866000;6506879958;7202417523;37108359200;23135605300;36171137600;38863076500;","Web-scale information extraction with vertex",2011,"Proceedings - International Conference on Data Engineering",,,"5767842","1209","1220",,63,"10.1109/ICDE.2011.5767842","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79957798352&doi=10.1109%2fICDE.2011.5767842&partnerID=40&md5=e209861d14cc1b45d3f2eea3d81682d8","Vertex is a Wrapper Induction system developed at Yahoo! for extracting structured records from template-based Web pages. To operate at Web scale, Vertex employs a host of novel algorithms for (1) Grouping similar structured pages in a Web site, (2) Picking the appropriate sample pages for wrapper inference, (3) Learning XPath-based extraction rules that are robust to variations in site structure, (4) Detecting site changes by monitoring sample pages, and (5) Optimizing editorial costs by reusing rules, etc. The system is deployed in production and currently extracts more than 250 million records from more than 200 Web sites. To the best of our knowledge, Vertex is the first system to do high-precision information extraction at Web scale. © 2011 IEEE.",,"Extraction rule; First systems; High-precision; Information Extraction; Novel algorithm; Template-based; Web page; Wrapper induction; Algorithms; Inference engines; Information analysis; Websites; User interfaces","Microsoft;EMC2;Greenplum;IBM;HP","2011 IEEE 27th International Conference on Data Engineering, ICDE 2011","11 April 2011 through 16 April 2011","Hannover",85026,Conference Paper,"Final","",Scopus,2-s2.0-79957798352
"Smarandache F.","","Neutrosophic set-a generalization of the intuitionistic fuzzy set",2010,"Journal of Defense Resourses Management","1","1",,"107","116",,63,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84962487851
"Li D., Qian J.","57203225696;55730662300;","Text sentiment analysis based on long short-term memory",2016,"2016 1st IEEE International Conference on Computer Communication and the Internet, ICCCI 2016",,,"7778967","471","475",,62,"10.1109/CCI.2016.7778967","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85010281232&doi=10.1109%2fCCI.2016.7778967&partnerID=40&md5=a4e87d24abbcea31ddc63a83962c408b","With the rapid development of Internet and big explosion of text data, it has been a very significant research subject to extract valuable information from text ocean. To realize multi-classification for text sentiment, this paper promotes a RNN language model based on Long Short Term Memory (LSTM), which can get complete sequence information effectively. Compared with the traditional RNN language model, LSTM is better in analyzing emotion of long sentences. And as a language model, LSTM is applied to achieve multi-classification for text emotional attributes. So though training different emotion models, we can know which emotion the sentence belongs to by using these emotion models. And numerical experiments show that it can produce better accuracy rate and recall rate than the conventional RNN. © 2016 IEEE.","LSTM; RNN; sentiment analysis","Brain; Classification (of information); Computational linguistics; Data mining; Language model; Long short term memory; LSTM; Multi-classification; Numerical experiments; Research subjects; Sentiment analysis; Sequence informations; Text processing","","1st IEEE International Conference on Computer Communication and the Internet, ICCCI 2016","13 October 2016 through 15 October 2016",,125300,Conference Paper,"Final","",Scopus,2-s2.0-85010281232
"Spröwitz A., Moeckel R., Vespignani M., Bonardi S., Ijspeert A.J.","6504737432;23767606800;55117972000;36236829300;7003779012;","Roombots: A hardware perspective on 3D self-reconfiguration and locomotion with a homogeneous modular robot",2014,"Robotics and Autonomous Systems","62","7",,"1016","1033",,62,"10.1016/j.robot.2013.08.011","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84901237418&doi=10.1016%2fj.robot.2013.08.011&partnerID=40&md5=510b24b846f36e9352e9a52b2bc9bef5","In this work we provide hands-on experience on designing and testing a self-reconfiguring modular robotic system, Roombots (RB), to be used among others for adaptive furniture. In the long term, we envision that RB can be used to create sets of furniture, such as stools, chairs and tables that can move in their environment and that change shape and functionality during the day. In this article, we present the first, incremental results towards that long term vision. We demonstrate locomotion and reconfiguration of single and metamodule RB over 3D surfaces, in a structured environment equipped with embedded connection ports. RB assemblies can move around in non-structured environments, by using rotational or wheel-like locomotion. We show a proof of concept for transferring a Roombots metamodule (two in-series coupled RB modules) from the non-structured environment back into the structured grid, by aligning the RB metamodule in an entrapment mechanism. Finally, we analyze the remaining challenges to master the full Roombots scenario, and discuss the impact on future Roombots hardware. © 2013 Elsevier B.V. All rights reserved.","Active connection mechanism; Grid environment; Homogeneous modular robotic system; Modular robots; Module climbing; Module joining; Self-reconfiguring","Hardware; Grid environments; Long term vision; Modular robotics; Module climbing; Proof of concept; Self reconfiguration; Self-reconfiguring; Structured environment; Modular robots",,,,,,Article,"Final","",Scopus,2-s2.0-84901237418
"Cambria E., Rajagopal D., Olsher D., Das D.","56140547500;55970564600;55341324600;56965150200;","Big social data analysis",2013,"Big Data Computing",,,,"401","414",,62,"10.1201/b16014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018097555&doi=10.1201%2fb16014&partnerID=40&md5=1c36255b4f48d68ece96a437ac8b3db3","As the Web rapidly evolves, Web users too are evolving with it. In an era of social connectedness, people are becoming increasingly enthusiastic about interacting, sharing, and collaborating through social networks, online communities, blogs, Wikis, and other online collaborative media. In recent years, this collective intelligence has spread to many different areas, with particular focus on fields related to everyday life such as commerce, tourism, education, and health, causing the size of the social Web to expand exponentially. The distillation of knowledge from such a large amount of unstructured information, however, is an extremely difficult task, as the contents of today’s Web are perfectly suitable for human consumption, but remain hardly accessible to machines. Big social data analysis grows out of this need and it includes disciplines such as social network analysis, multimedia management, social media analytics, trend discovery, and opinion mining. The opportunity to capture the opinions of the general public about social events, political movements, company strategies, marketing campaigns, and product preferences, in particular, has raised growing interest both within the scientific community, leading to many exciting open challenges, as well as in the business world, due to the remarkable benefits to be had from marketing and financial market prediction. This has led to the emerging fields of opinion mining and sentiment analysis, which deal with information retrieval and knowledge discovery from text using data mining and natural language processing (NLP) techniques to distill knowledge and opinions from the huge amount of information on the World Wide Web. Opinion mining and sentiment analysis are branches of the broad field of text data mining [21] and refer generally to the process of extracting interesting and nontrivial patterns or knowledge from unstructured text documents. They can be viewed as an extension of data mining or knowledge discovery from (structured) databases [12,36]. As the most natural form of storing information is text, opinion mining is believed to have a commercial potential higher than that of data mining. Opinion mining, however, is also a much more complex task, as it involves dealing with text data that are inherently big, unstructured, and fuzzy. © 2014 by Taylor & Francis Group, LLC.",,,,,,,,Book Chapter,"Final","",Scopus,2-s2.0-85018097555
"Tanaka M., Okutomi M.","57080530000;7003654289;","A Novel Inference of a Restricted Boltzmann Machine",2014,"Proceedings - International Conference on Pattern Recognition",,,"6976981","1526","1531",,61,"10.1109/ICPR.2014.271","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84919904236&doi=10.1109%2fICPR.2014.271&partnerID=40&md5=6881c6e68e9d7f4b99d1cfd5dba0d6ea","A deep neural network (DNN) pre-trained via stacking restricted Boltzmann machines (RBMs) demonstrates high performance. The binary RBM is usually used to construct the DNN. However, a continuous probability of each node is used as real value state, although the state of the binary RBM's node should be represented by a random binary variable. One of main reasons of this abuse is that it works. One of others is to reduce a computational cost. In this paper, we propose a novel inference of the RBM, considering that the input of the RBM is the random binary variable. Straight forward derivation of the proposed inference is intractable. Then, we also propose the closed-form approximation of it. We convince that the proposed inference is more reasonable than a conventional algorithm of the RBM. Experimental comparisons demonstrate that the proposed inference improves the performance of the DNN. © 2014 IEEE.",,"Deep neural networks; Neural networks; Pattern recognition; Binary variables; Closed form approximations; Computational costs; Conventional algorithms; Experimental comparison; Real values; Restricted boltzmann machine; Inference engines",,"22nd International Conference on Pattern Recognition, ICPR 2014","24 August 2014 through 28 August 2014",,109641,Conference Paper,"Final","",Scopus,2-s2.0-84919904236
"Siddharthan A.","57203429695;","An architecture for a text simplification system",2002,"Proceedings - Language Engineering Conference, LEC 2002",,,"1182292","64","71",,61,"10.1109/LEC.2002.1182292","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84961806393&doi=10.1109%2fLEC.2002.1182292&partnerID=40&md5=db236eb714f234d1d94574357ddb062c","We present a pipelined architecture for a text simplification system and describe our implementation of the three stages-analysis, transformation and regeneration. Our architecture allows each component to be developed and evaluated independently. We lay particular emphasis on the discourse level aspects of syntactic simplification as these are crucial to the process and have not been dealt with by previous research in the field. These aspects include generating referring expressions, deciding determiners, deciding sentence order and preserving rhetorical and anaphoric structure. © 2002 IEEE.",,"Architecture; Computational linguistics; Generating referring expressions; Pipelined architecture; Sentence ordering; Computer architecture","","Language Engineering Conference, LEC 2002","13 December 2002 through 15 December 2002",,116245,Conference Paper,"Final","",Scopus,2-s2.0-84961806393
"Vafaie H., De Jong K.","56118759900;7102352958;","Feature space transformation using genetic algorithms",1998,"IEEE Intelligent Systems and Their Applications","13","2",,"57","65",,61,"10.1109/5254.671093","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0032028849&doi=10.1109%2f5254.671093&partnerID=40&md5=19ac3b5328be48e36729d09e8c411074","The authors have developed an autonomous system that transforms feature spaces to improve classification techniques. They apply their method to an eye-detection face recognition system, demonstrating substantially better classification rates than competing systems.",,"Artificial intelligence; Pattern recognition; Pattern recognition systems; Face recognition; Genetic algorithms",,,,,,Article,"Final","",Scopus,2-s2.0-0032028849
"Eiben A.E., Aarts E.H.L., Van Hee K.M.","","Global convergence of genetic algorithms: A Markov chain analysis",1991,"Parallel Problem Solving from Nature",,,,"4","12",,61,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0002650735
"Chaturvedi I., Ong Y.-S., Tsang I.W., Welsch R.E., Cambria E.","12646151300;7006735298;7004570429;8214812500;56140547500;","Learning word dependencies in text by means of a deep recurrent belief network",2016,"Knowledge-Based Systems","108",,,"144","154",,59,"10.1016/j.knosys.2016.07.019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84981722706&doi=10.1016%2fj.knosys.2016.07.019&partnerID=40&md5=b6ec6ed502ca8aa6097e42977745edf5","We propose a deep recurrent belief network with distributed time delays for learning multivariate Gaussians. Learning long time delays in deep belief networks is difficult due to the problem of vanishing or exploding gradients with increase in delay. To mitigate this problem and improve the transparency of learning time-delays, we introduce the use of Gaussian networks with time-delays to initialize the weights of each hidden neuron. From our knowledge of time delays, it is possible to learn the long delays from short delays in a hierarchical manner. In contrast to previous works, here dynamic Gaussian Bayesian networks over training samples are evolved using Markov Chain Monte Carlo to determine the initial weights of each hidden layer of neurons. In this way, the time-delayed network motifs of increasing Markov order across layers can be modeled hierarchically using a deep model. To validate the proposed Variable-order Belief Network (VBN) framework, it is applied for modeling word dependencies in text. To explore the generality of VBN, it is further considered for a real-world scenario where the dynamic movements of basketball players are modeled. Experimental results obtained showed that the proposed VBN could achieve over 30% improvement in accuracy on real-world scenarios compared to the state-of-the-art baselines. © 2016","Deep belief networks; Gaussian networks; Markov Chain Monte Carlo; Time-delays; Variable-order","Chains; Gaussian distribution; Markov processes; Monte Carlo methods; Time delay; Deep belief networks; Distributed time delays; Dynamic movements; Gaussian bayesian networks; Gaussian networks; Markov Chain Monte-Carlo; Real-world scenario; Variable order; Bayesian networks",,,,,,Article,"Final","",Scopus,2-s2.0-84981722706
"Dinan E., Roller S., Shuster K., Fan A., Auli M., Weston J.","57204527802;55141662200;57219007543;57202451995;51664595600;8865128200;","Of Wikipedia: Knowledge-powered conversational agents",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,58,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083953978&partnerID=40&md5=8b7a45a3604ccbed2964498f288c657a","In open-domain dialogue intelligent agents should exhibit the use of knowledge, however there are few convincing demonstrations of this to date. The most popular sequence to sequence models typically “generate and hope” generic utterances that can be memorized in the weights of the model when mapping from input utterance(s) to output, rather than employing recalled knowledge as context. Use of knowledge has so far proved difficult, in part because of the lack of a supervised learning benchmark task which exhibits knowledgeable open dialogue with clear grounding. To that end we collect and release a large dataset with conversations directly grounded with knowledge retrieved from Wikipedia. We then design architectures capable of retrieving knowledge, reading and conditioning on it, and finally generating natural responses. Our best performing dialogue models are able to conduct knowledgeable discussions on open-domain topics as evaluated by automatic metrics and human evaluations, while our new benchmark allows for measuring further improvements in this important research direction. © 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved.",,"Intelligent agents; Automatic metrics; Conversational agents; Design architecture; Dialogue models; Human evaluation; Natural response; Sequence models; Wikipedia; Large dataset",,"7th International Conference on Learning Representations, ICLR 2019","6 May 2019 through 9 May 2019",,149936,Conference Paper,"Final","",Scopus,2-s2.0-85083953978
"Yenter A., Verma A.","57201885494;57213158945;","Deep CNN-LSTM with combined kernels from multiple branches for IMDb review sentiment analysis",2017,"2017 IEEE 8th Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2017","2018-January",,,"540","546",,58,"10.1109/UEMCON.2017.8249013","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046357381&doi=10.1109%2fUEMCON.2017.8249013&partnerID=40&md5=086c8708604a79526e5bf9f7be53263d","Deep learning neural networks have made significant progress in the area of image and video analysis. This success of neural networks can be directed towards improvements in textual sentiment classification. In this paper, we describe a novel approach to sentiment analysis through the use of combined kernel from multiple branches of convolutional neural network (CNN) with Long Short-term Memory (LSTM) layers. Our combination of CNN and LSTM schemes produces a model with the highest reported accuracy on the Internet Movie Database (IMDb) review sentiment dataset. Additionally, we present multiple architecture variations of our proposed model to illustrate our attempts to increase accuracy while minimizing overfitting. We experiment with numerous regularization techniques, network structures, and kernel sizes to create five high-performing models for comparison. These models are capable of predicting the sentiment polarity of reviews from the IMDb dataset with accuracy above 89%. Firstly, the accuracy of our best performing proposed model surpasses the previously published models and secondly it vastly improves upon the baseline CNN+LSTM model. The capability of the combined kernel from multiple branches of CNN based LSTM architecture could also be lucrative towards other datasets for sentiment analysis or simply text classification. Furthermore, the proposed model has the potential in machine learning in video and audio. © 2017 IEEE.","CNN; IMDb; LSTM; Neural network; Sentiment analysis; Text classification","Classification (of information); Deep learning; Mobile telecommunication systems; Multilayer neural networks; Network architecture; Neural networks; Sentiment analysis; Ubiquitous computing; Convolutional neural network; IMDb; Internet movie database; Learning neural networks; LSTM; Regularization technique; Sentiment classification; Text classification; Long short-term memory","IEEE New York Section;IEEE Region 1;IEEE USA;University of Engineering and Management (UEM)","8th IEEE Annual Ubiquitous Computing, Electronics and Mobile Communication Conference, UEMCON 2017","19 October 2017 through 21 October 2017",,134335,Conference Paper,"Final","",Scopus,2-s2.0-85046357381
"Bouckaert R.R., Frank E., Hall M., Kirkby R., Reutemann P., Seewald A., Scuse D.","",[No title available],2013,"WEKA Manual for Version 3-7-8",,,,"","",,58,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84891087783
"Fung G., Mangasarian O.L., Shavlik J.","","Knowledge-based support vector machine classifiers",2003,"Advances in Neural Information Processing Systems","15","C528",,"521","528",,58,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-29144457113
"Lakkaraju H., Socher R., Manning C.","","Aspect specific sentiment analysis using hierarchical deep learning",2014,"NIPS Workshop on Deep Learning and Representation Learning",,,,"1","9",,57,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84951860502
"Novák V.","7103396254;","First-order fuzzy logic",1987,"Studia Logica","46","1",,"87","109",,57,"10.1007/BF00396907","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0002131962&doi=10.1007%2fBF00396907&partnerID=40&md5=b1f7f596a91aaf84d3f6d5a583d510ae","This paper is an attempt to develop the many-valued first-order fuzzy logic. The set of its truth, values is supposed to be either a finite chain or the interval 〈0, 1〉 of reals. These are special cases of a residuated lattice 〈L, ∨, ∧, ⊗, →, 1, 0〉. It has been previously proved that the fuzzy propositional logic based on the same sets of truth values is semantically complete. In this paper the syntax and semantics of the first-order fuzzy logic is developed. Except for the basic connectives and quantifiers, its language may contain also additional n-ary connectives and quantifiers. Many propositions analogous to those in the classical logic are proved. The notion of the fuzzy theory in the first-order fuzzy logic is introduced and its canonical model is constructed. Finally, the extensions of Gödel's completeness theorems are proved which confirm that the first-order fuzzy logic is also semantically complete. © 1987 Polish Academy of Sciences.",,,,,,,,Article,"Final","",Scopus,2-s2.0-0002131962
"Ebrahimi M., Yazdavar A.H., Sheth A.","56304131500;56195193600;57200763252;","Challenges of Sentiment Analysis for Dynamic Events",2017,"IEEE Intelligent Systems","32","5","8070926","70","75",,56,"10.1109/MIS.2017.3711649","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032644590&doi=10.1109%2fMIS.2017.3711649&partnerID=40&md5=05d786c51d0bc89232f650a4c943a07d","Efforts to assess people's sentiments on Twitter have suggested that Twitter could be a valuable resource for studying political sentiment and that it reflects the offline political landscape. Many opinion mining systems and tools provide users with people's attitudes toward products, people, or topics and their attributes/aspects. However, although it may appear simple, using sentiment analysis to predict election results is difficult, since it is empirically challenging to train a successful model to conduct sentiment analysis on tweet streams for a dynamic event such as an election. This article highlights some of the challenges related to sentiment analysis encountered during monitoring of the presidential election using Kno.e.sis's Twitris system. © 2001-2011 IEEE.","artificial intelligence; deep learning; intelligent systems; machine learning; natural language processing; sentiment analysis; sentiment for dynamic events; sentiment for volatile content; social media analysis","Artificial intelligence; Data mining; Deep learning; Intelligent systems; Learning algorithms; Learning systems; Social networking (online); Dynamic events; Offline; Opinion mining; Presidential election; Sentiment analysis; Social media analysis; Volatile contents; Natural language processing systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85032644590
"Gupta Y., Saini A., Saxena A.K.","56040653600;12809691900;7401482294;","A new fuzzy logic based ranking function for efficient Information Retrieval system",2015,"Expert Systems with Applications","42","3",,"1223","1234",,56,"10.1016/j.eswa.2014.09.009","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84908043092&doi=10.1016%2fj.eswa.2014.09.009&partnerID=40&md5=c351946f839339ffc7a0e03bd417542d","The relevant documents from large data sets are retrieved with the help of ranking function in Information Retrieval system. In this paper, a new fuzzy logic based ranking function is proposed and implemented to enhance the performance of Information Retrieval system. The proposed ranking function is based on the computation of different terms of term-weighting schema such as term frequency, inverse document frequency and normalization. Fuzzy logic is used at two levels to compute relevance score of a document with respect to the query in present work. All the experiments are performed on CACM and CISI benchmark data sets. The experimental results reveal that the performance of our proposed ranking function is much better than the fuzzy based ranking function developed by Rubens along with other widely used ranking function Okapi-BM25 in terms of precision, recall and F-measure. © 2014 Elsevier Ltd. All rights reserved.","Fuzzy Logic Controller; Information Retrieval; Precision; Ranking function; Recall","Fuzzy logic controllers; Precision; Ranking functions; Recall; Information retrieval",,,,,,Article,"Final","",Scopus,2-s2.0-84908043092
"Bao Y., Quan C., Wang L., Ren F.","56288637200;8536551500;57196332647;7202066506;","The role of pre-processing in twitter sentiment analysis",2014,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8589 LNAI",,,"615","624",,56,"10.1007/978-3-319-09339-0_62","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84958541163&doi=10.1007%2f978-3-319-09339-0_62&partnerID=40&md5=0f0d7266ee5764703b027390968fe519","Recently, increasing attention has been attracted to Social Networking Sentiment Analysis. Twitter as one of the most fashional social networking platforms has been researched as a hot topic in this domain. Normally, sentiment analysis is regarded as a classification problem. Training a classifier with tweets data, there is a large amount of noise due to tweets' shortness, marks, irregular words etc. In this work we explore the impact pre-processing methods make on twitter sentiment classification. We evaluate the effects of URLs, negation, repeated letters, stemming and lemmatization. Experimental results on the Stanford Twitter Sentiment Dataset show that sentiment classification accuracy rises when URLs features reservation, negation transformation and repeated letters normalization are employed while descends when stemming and lemmatization are applied. Moreover, we get a better result by augmenting the original feature space with bigram and emotions features. Comprehensive application of these measures makes us achieve classification accuracy of 85.5%. © 2014 Springer International Publishing Switzerland.","Pre-processing; Sentiment Analysis; Twitter","Classification (of information); Data mining; Intelligent computing; Social networking (online); Text processing; Classification accuracy; Feature space; Large amounts; Pre-processing; Pre-processing method; Sentiment analysis; Sentiment classification; Twitter; Natural language processing systems","IEEE Computational Intelligence Society;International Neural Network Society;National Science Foundation of China","10th International Conference on Intelligent Computing, ICIC 2014","3 August 2014 through 6 August 2014","Taiyuan",106532,Conference Paper,"Final","",Scopus,2-s2.0-84958541163
"Landwehr N., Passerini A., De Raedt L., Frasconi P.","10244371900;7006983227;55760010700;7003350874;","KFOIL: Learning simple relational kernels",2006,"Proceedings of the National Conference on Artificial Intelligence","1",,,"389","394",,56,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750734364&partnerID=40&md5=3787c4454372d2288591dc65d9942e4a","A novel and simple combination of inductive logic programming with kernel methods is presented. The kFOIL algorithm integrates the well-known inductive logic programming system FOIL with kernel methods. The feature space is constructed by leveraging FOIL search for a set of relevant clauses. The search is driven by the performance obtained by a support vector machine based on the resulting kernel. In this way, kFOIL implements a dynamic propositionalization approach. Both classification and regression tasks can be naturally handled. Experiments in applying kFOIL to well-known benchmarks in chemoinformatics show the promise of the approach. Copyright © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",,"Algorithms; Artificial intelligence; Information analysis; Logic programming; Chemoinformatics; Kernel methods; Vector machine; Learning systems","American Association for Artificial Intelligence","21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference, AAAI-06/IAAI-06","16 July 2006 through 20 July 2006","Boston, MA",68475,Conference Paper,"Final","",Scopus,2-s2.0-33750734364
"Jennings P.C., Lysgaard S., Hummelshøj J.S., Vegge T., Bligaard T.","54980793300;36968953400;11540547000;7003740838;6602881019;","Genetic algorithms for computational materials discovery accelerated by machine learning",2019,"npj Computational Materials","5","1","46","","",,55,"10.1038/s41524-019-0181-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064263097&doi=10.1038%2fs41524-019-0181-4&partnerID=40&md5=361812428f9fe1c2f291d78e5495e724","Materials discovery is increasingly being impelled by machine learning methods that rely on pre-existing datasets. Where datasets are lacking, unbiased data generation can be achieved with genetic algorithms. Here a machine learning model is trained on-the-fly as a computationally inexpensive energy predictor before analyzing how to augment convergence in genetic algorithm-based approaches by using the model as a surrogate. This leads to a machine learning accelerated genetic algorithm combining robust qualities of the genetic algorithm with rapid machine learning. The approach is used to search for stable, compositionally variant, geometrically similar nanoparticle alloys to illustrate its capability for accelerated materials discovery, e.g., nanoalloy catalysts. The machine learning accelerated approach, in this case, yields a 50-fold reduction in the number of required energy calculations compared to a traditional “brute force” genetic algorithm. This makes searching through the space of all homotops and compositions of a binary alloy particle in a given structure feasible, using density functional theory calculations. © 2019, The Author(s).",,"Binary alloys; Computation theory; Density functional theory; Genetic algorithms; Learning algorithms; Nanocatalysts; Alloy particles; Computational materials; Energy calculation; Inexpensive energy; Machine learning methods; Machine learning models; Nanoalloy catalysts; Nanoparticle-alloys; Machine learning",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85064263097
"Rosa H., Pereira N., Ribeiro R., Ferreira P.C., Carvalho J.P., Oliveira S., Coheur L., Paulino P., Veiga Simão A.M., Trancoso I.","56313322600;57158847600;13609216700;56293098000;7202738810;57205332058;8075360000;54795681100;54974120700;6603959337;","Automatic cyberbullying detection: A systematic review",2019,"Computers in Human Behavior","93",,,"333","345",,55,"10.1016/j.chb.2018.12.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85059576433&doi=10.1016%2fj.chb.2018.12.021&partnerID=40&md5=cb6edc3a3e3154c338b2a5ce47090b27","Automatic cyberbullying detection is a task of growing interest, particularly in the Natural Language Processing and Machine Learning communities. Not only is it challenging, but it is also a relevant need given how social networks have become a vital part of individuals' lives and how dire the consequences of cyberbullying can be, especially among adolescents. In this work, we conduct an in-depth analysis of 22 studies on automatic cyberbullying detection, complemented by an experiment to validate current practices through the analysis of two datasets. Results indicated that cyberbullying is often misrepresented in the literature, leading to inaccurate systems that would have little real-world application. Criteria concerning cyberbullying definitions and other methodological concerns seem to be often dismissed. Additionally, there is no uniformity regarding the methodology to evaluate said systems and the natural imbalance of datasets remains an issue. This paper aims to direct future research on the subject towards a viewpoint that is more coherent with the definition and representation of the phenomenon, so that future systems can have a practical and impactful application. Recommendations on future works are also made. © 2018 Elsevier Ltd","Abusive language; Automatic cyberbullying detection; Cyberbullying; Machine learning; Natural language processing; Social networks","Artificial intelligence; Learning algorithms; Learning systems; Natural language processing systems; Social networking (online); Abusive language; Current practices; Cyber bullying; In-depth analysis; Machine learning communities; Real-world; Systematic Review; Computer crime; adolescent; article; human; machine learning; natural language processing; social network; systematic review",,,,,,Article,"Final","",Scopus,2-s2.0-85059576433
"Ahn S., Choi H., Pärnamaa T., Bengio Y.","",[No title available],2016,"A Neural Knowledge Language Model",,,,"","",,55,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85029221292
"Minsky M.","","Size and structure of universal Turing machines using tag systems",1962,"Recursive Function Theory: Proceedings, Symposium in Pure Mathematics","5",,,"229","238",,55,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0040534095
"Mostafazadeh N., Roth M., Louis A., Chambers N., Allen J.F.","","Lsdsem 2017 shared task: The story cloze test",2017,"LSDSem",,,,"46","51",,54,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85040584650
"Ailon N., Chazelle B.","9271489900;7005144537;","Faster dimension reduction",2010,"Communications of the ACM","53","2",,"97","104",,54,"10.1145/1646353.1646379","https://www.scopus.com/inward/record.uri?eid=2-s2.0-75749141980&doi=10.1145%2f1646353.1646379&partnerID=40&md5=cdaf3641d6a5ce73320d5cb898380481","Data represented geometrically in high-dimensional vector spaces can be found in many applications. Images and videos, are often represented by assigning a dimension for every pixel (and time). Text documents may be represented in a vector space where each word in the dictionary incurs a dimension. The need to manipulate such data in huge corpora such as the web and to support various query types gives rise to the question of how to represent the data in a lower-dimensional space to allow more space and time efficient computation. Linear mappings are an attractive approach to this problem because the mapped input can be readily fed into popular algorithms that operate on linear spaces (such as principal-component analysis, PCA) while avoiding the curse of dimensionality. The fact that such mappings even exist became known in computer science following seminal work by Johnson and Lindenstrauss in the early 1980s. The underlying technique is often called ""random projection."" The complexity of the mapping itself, essentially the product of a vector with a dense matrix, did not attract much attention until recently. In 2006, we discovered a way to ""sparsify"" the matrix via a computational version of Heisenberg's Uncertainty Principle. This led to a significant speedup, which also retained the practical simplicity of the standard Johnson-Lindenstrauss projection. We describe the improvement in this article, together with some of its applications. © 2010 ACM.",,"Curse of dimensionality; Dense matrices; Dimension reduction; Dimensional spaces; Efficient computation; Heisenberg's Uncertainty principle; High-dimensional; Linear mapping; Linear spaces; matrix; Query types; Random projections; Space and time; Text document; Computational efficiency; Mapping; Principal component analysis; Vector spaces",,,,,,Article,"Final","",Scopus,2-s2.0-75749141980
"Langley P.","","Crafting papers on machine learning",2000,"Proceedings of the Seventeenth International Conference on Machine Learning",,,,"1207","1212",,54,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-18744411494
"Besold T. R., Garcez A. d., Bader S., Bowman H., Domingos P., Hitzler P., Kühnberger K.-U., Lamb L. C., Lowd D., Lima P. M. V.","","Neural-symbolic learning and reasoning: A survey and interpretation",2017,"Neural-symbolic learning and reasoning: A survey and interpretation",,,,"","",,53,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85050181940
"França M.V.M., Zaverucha G., D'Avila Garcez A.S.","55979263100;6601938210;6603397393;","Fast relational learning using bottom clause propositionalization with artificial neural networks",2014,"Machine Learning","94","1",,"81","104",,53,"10.1007/s10994-013-5392-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891373440&doi=10.1007%2fs10994-013-5392-1&partnerID=40&md5=8731ad71554ee4a0916f866be773a0d9","Relational learning can be described as the task of learning first-order logic rules from examples. It has enabled a number of new machine learning applications, e.g. graph mining and link analysis. Inductive Logic Programming (ILP) performs relational learning either directly by manipulating first-order rules or through propositionalization, which translates the relational task into an attribute-value learning task by representing subsets of relations as features. In this paper, we introduce a fast method and system for relational learning based on a novel propositionalization called Bottom Clause Propositionalization (BCP). Bottom clauses are boundaries in the hypothesis search space used by ILP systems Progol and Aleph. Bottom clauses carry semantic meaning and can be mapped directly onto numerical vectors, simplifying the feature extraction process. We have integrated BCP with a well-known neural-symbolic system, C-IL2P, to perform learning from numerical vectors. C-IL2P uses background knowledge in the form of propositional logic programs to build a neural network. The integrated system, which we call CILP++, handles first-order logic knowledge and is available for download from Sourceforge. We have evaluated CILP++ on seven ILP datasets, comparing results with Aleph and a well-known propositionalization method, RSD. The results show that CILP++ can achieve accuracy comparable to Aleph, while being generally faster, BCP achieved statistically significant improvement in accuracy in comparison with RSD when running with a neural network, but BCP and RSD perform similarly when running with C4.5. We have also extended CILP++ to include a statistical feature selection method, mRMR, with preliminary results indicating that a reduction of more than 90 % of features can be achieved with a small loss of accuracy. © 2013 The Author(s).","Artificial neural networks; Inductive logic programming; Neural-symbolic integration; Propositionalization; Relational learning","Back-ground knowledge; Machine learning applications; Neural-symbolic integration; Neural-symbolic systems; Propositional logic; Propositionalization; Relational learning; Statistical features; C (programming language); Feature extraction; Formal logic; Neural networks; Semantics; Inductive logic programming (ILP)",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-84891373440
"Rajagopal D., Cambria E., Olsher D., Kwok K.","55970564600;56140547500;55341324600;57226032645;","A graph-based approach to commonsense concept extraction and semantic similarity detection",2013,"WWW 2013 Companion - Proceedings of the 22nd International Conference on World Wide Web",,,,"565","570",,53,"10.1145/2487788.2487995","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84890636931&doi=10.1145%2f2487788.2487995&partnerID=40&md5=170ad8a4efb79450966ef4f8b1888ab6","Commonsense knowledge representation and reasoning support a wide variety of potential applications in fields such as document auto-categorization, Web search enhancement, topic gisting, social process modeling, and concept-level opinion and sentiment analysis. Solutions to these problems, however, demand robust knowledge bases capable of supporting exible, nuanced reasoning. Populating such knowledge bases is highly time-consuming, making it necessary to develop techniques for deconstructing natural language texts into commonsense concepts. In this work, we propose an approach for effective multi-word commonsense expression extraction from unrestricted English text, in addition to a semantic similarity detection technique allowing additional matches to be found for specific concepts not already present in knowledge bases.","Ai; Commonsense knowledge representation and reasoning; Natural language processing; Semantic similarity","Artificial intelligence; Extraction; Knowledge representation; Semantics; Sentiment analysis; World Wide Web; Commonsense knowledge; Concept extraction; Concept levels; Knowledge basis; NAtural language processing; Natural language text; Semantic similarity; Social process; Graphic methods","Banco do Brasil;BR PETROBRAS;Comite Gestor da Internet no Brazil (CGI.BR);et al.;Microsoft;Nucleo de Informatcao e Coordenacao do Ponto BR (NIC.BR)","22nd International Conference on World Wide Web, WWW 2013","13 May 2013 through 17 May 2013","Rio de Janeiro",102216,Conference Paper,"Final","",Scopus,2-s2.0-84890636931
"Jing H.","8063973800;","Using hidden Markov modeling to decompose human-written summaries",2002,"Computational Linguistics","28","4",,"527","543",,53,"10.1162/089120102762671972","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0039486312&doi=10.1162%2f089120102762671972&partnerID=40&md5=0b9e558bad9325d7e674f04e6d279ac1","Professional summarizers often reuse original documents to generate summaries. The task of summary sentence decomposition is to deduce whether a summary sentence is constructed by reusing the original text and to identify reused phrases. Specifically, the decomposition program needs to answer three questions for a given summary sentence: (1) Is this summary sentence constructed by reusing the text in the original document? (2) If so, what phrases in the sentence come from the original document? and (3) From where in the document do the phrases come? Solving the decomposition problem can lead to better text generation techniques for summarization. Decomposition can also provide large training and testing corpora for extraction-based summarizers. We propose a hidden Markov model solution to the decomposition problem. Evaluations show that the proposed algorithm performs well.",,"Decomposition problems; Text generations; Training and testing; Hidden Markov models",,,,,,Article,"Final","All Open Access, Hybrid Gold",Scopus,2-s2.0-0039486312
"Nie Y., Chen H., Bansal M.","57216692353;57218925413;16466939600;","Combining fact extraction and verification with neural semantic matching networks",2019,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",,,,"6859","6866",,52,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85070497878&partnerID=40&md5=6cf9721be93d536070502f7bebf88c76","The increasing concern with misinformation has stimulated research efforts on automatic fact checking. The recently-released FEVER dataset introduced a benchmark fact-verification task in which a system is asked to verify a claim using evidential sentences from Wikipedia documents. In this paper, we present a connected system consisting of three homogeneous neural semantic matching models that conduct document retrieval, sentence selection, and claim verification jointly for fact extraction and verification. For evidence retrieval (document retrieval and sentence selection), unlike traditional vector space IR models in which queries and sources are matched in some pre-designed term vector space, we develop neural models to perform deep semantic matching from raw textual input, assuming no intermediate term representation and no access to structured external knowledge bases. We also show that Pageview frequency can also help improve the performance of evidence retrieval results, that later can be matched by using our neural semantic matching network. For claim verification, unlike previous approaches that simply feed upstream retrieved evidence and the claim to a natural language inference (NLI) model, we further enhance the NLI model by providing it with internal semantic relatedness scores (hence integrating it with the evidence retrieval modules) and ontological WordNet features. Experiments on the FEVER dataset indicate that (1) our neural semantic matching method outperforms popular TF-IDF and encoder models, by significant margins on all evidence retrieval metrics, (2) the additional relatedness score and WordNet features improve the NLI model via better semantic awareness, and (3) by formalizing all three subtasks as a similar semantic matching problem and improving on all three stages, the complete model is able to achieve the state-of-the-art results on the FEVER test set (two times greater than baseline results). © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Extraction; Information retrieval; Natural language processing systems; Ontology; Semantic Web; Semantics; Statistical tests; Vector spaces; Connected systems; Document Retrieval; External knowledge; Semantic matching; Semantic relatedness; Sentence selection; Term representation; Verification task; Artificial intelligence","Association for the Advancement of Artificial Intelligence","33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019","27 January 2019 through 1 February 2019",,160302,Conference Paper,"Final","",Scopus,2-s2.0-85070497878
"Cambria E., Hussain A., Havasi C., Eckl C.","56140547500;19734290900;55899855300;36019820400;","SenticSpace: Visualizing opinions and sentiments in a multi-dimensional vector space",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6279 LNAI","PART 4",,"385","393",,52,"10.1007/978-3-642-15384-6_41","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649272705&doi=10.1007%2f978-3-642-15384-6_41&partnerID=40&md5=866c1473470446700c5e97784b7e3b0e","In a world in which millions of people express their feelings and opinions about any issue in blogs, wikis, fora, chats and social networks, the distillation of knowledge from this huge amount of unstructured information is a challenging task. In this work we build a knowledge base which merges common sense and affective knowledge and visualize it in a multi-dimensional vector space, which we call SenticSpace. In particular we blend ConceptNet and WordNet-Affect and use dimensionality reduction on the resulting knowledge base to build a 24-dimensional vector space in which different vectors represent different ways of making binary distinctions among concepts and sentiments. © Springer-Verlag 2010.","AI; Knowledge base management; NLP; Opinion mining and sentiment analysis; Semantic networks; Sentic computing","AI; Knowledge base; NLP; Opinion mining; Semantic networks; Sentic computing; Data mining; Distillation; Knowledge based systems; Natural language processing systems; Vectors; Vector spaces",,"14th International Conference on Knowledge-Based and Intelligent Information and Engineering Systems, KES 2010","8 September 2010 through 10 September 2010","Cardiff",82394,Conference Paper,"Final","",Scopus,2-s2.0-78649272705
"Wang G., Qiao J., Bi J., Li W., Zhou M.","57195606371;9634105900;55847050400;57200498007;7403506743;","TL-GDBN: Growing Deep Belief Network with Transfer Learning",2019,"IEEE Transactions on Automation Science and Engineering","16","2","8478799","874","885",,51,"10.1109/TASE.2018.2865663","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054471522&doi=10.1109%2fTASE.2018.2865663&partnerID=40&md5=085b71cdc907c9f2e3e6fa926cfab15b","A deep belief network (DBN) is effective to create a powerful generative model by using training data. However, it is difficult to fast determine its optimal structure given specific applications. In this paper, a growing DBN with transfer learning (TL-GDBN) is proposed to automatically decide its structure size, which can accelerate its learning process and improve model accuracy. First, a basic DBN structure with single hidden layer is initialized and then pretrained, and the learned weight parameters are frozen. Second, TL-GDBN uses TL to transfer the knowledge from the learned weight parameters to newly added neurons and hidden layers, which can achieve a growing structure until the stopping criterion for pretraining is satisfied. Third, the weight parameters derived from pretraining of TL-GDBN are further fine-tuned by using layer-by-layer partial least square regression from top to bottom, which can avoid many problems of traditional backpropagation algorithm-based fine-tuning. Moreover, the convergence analysis of the TL-GDBN is presented. Finally, TL-GDBN is tested on two benchmark data sets and a practical wastewater treatment system. The simulation results show that it has better modeling performance, faster learning speed, and more robust structure than existing models. Note to Practitioners - Transfer learning (TL) aims to improve training effectiveness by transferring knowledge from a source domain to target domain. This paper presents a growing deep belief network (DBN) with TL to improve the training effectiveness and determine the optimal model size. Facing a complex process and real-world workflow, DBN tends to require long time for its successful training. The proposed growing DBN with TL (TL-GDBN) accelerates the learning process by instantaneously transferring the knowledge from a source domain to each new deeper or wider substructure. The experimental results show that the proposed TL-GDBN model has a great potential to deal with complex system, especially the systems with high nonlinearity. As a result, it can be readily applicable to some industrial nonlinear systems. © 2004-2012 IEEE.","Convergence analysis; deep belief network (DBN); growing DBN with transfer learning (TL-GDBN); partial least square regression (PLSR)-based fine-tuning; TL","Backpropagation algorithms; Data structures; Neurons; Personnel training; Structural optimization; Supervised learning; Thallium; Unsupervised learning; Wastewater treatment; Computational model; Convergence; Convergence analysis; Deep belief network (DBN); Fine tuning; Transfer learning; Deep learning",,,,,,Article,"Final","",Scopus,2-s2.0-85054471522
"Ellis K., Solar-Lezama A., Ritchie D., Tenenbaum J.B.","56522550400;12141220900;36060024800;7006818404;","Learning to infer graphics programs from hand-drawn images",2018,"Advances in Neural Information Processing Systems","2018-December",,,"6059","6068",,51,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064845801&partnerID=40&md5=94b242e105eb0140602f2951b5802f57","We introduce a model that learns to convert simple hand drawings into graphics programs written in a subset of LATEX. The model combines techniques from deep learning and program synthesis. We learn a convolutional neural network that proposes plausible drawing primitives that explain an image. These drawing primitives are a specification (spec) of what the graphics program needs to draw. We learn a model that uses program synthesis techniques to recover a graphics program from that spec. These programs have constructs like variable bindings, iterative loops, or simple kinds of conditionals. With a graphics program in hand, we can correct errors made by the deep network and extrapolate drawings. © 2018 Curran Associates Inc.All rights reserved.",,"Deep learning; Iterative methods; Neural networks; Convolutional neural network; Correct error; Graphics programs; Hand-drawn; Program synthesis; Variable binding; Drawing (graphics)",,"32nd Conference on Neural Information Processing Systems, NeurIPS 2018","2 December 2018 through 8 December 2018",,147412,Conference Paper,"Final","",Scopus,2-s2.0-85064845801
"Judge J., Cahill A., Van Genabith J.","57040782700;9939609500;9940414000;","QuestionBank: Creating a corpus of parse-annotated questions",2006,"COLING/ACL 2006 - 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference","1",,,"497","504",,51,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860542597&partnerID=40&md5=855dcb5cb6cdffd66d371ba9ebba5559","This paper describes the development of QuestionBank, a corpus of 4000 parseannotated questions for (i) use in training parsers employed in QA, and (ii) evaluation of question parsing. We present a series of experiments to investigate the effectiveness of QuestionBank as both an exclusive and supplementary training resource for a state-of-the-art parser in parsing both question and non-question test sets. We introduce a new method for recovering empty nodes and their antecedents (capturing long distance dependencies) from parser output in CFG trees using LFG f-structure reentrancies. Our main findings are (i) using QuestionBank training data improves parser performance to 89.75% labelled bracketing f-score, an increase of almost 11% over the baseline; (ii) back-testing experiments on nonquestion data (Penn-II WSJ Section 23) shows that the retrained parser does not suffer a performance drop on non-question material; (iii) ablation experiments show that the size of training material provided by QuestionBank is sufficient to achieve optimal results; (iv) our method for recovering empty nodes captures long distance dependencies in questions from the ATIS corpus with high precision (96.82%) and low recall (39.38%). In summary, QuestionBank provides a useful new resource in parser-based QA research. © 2006 Association for Computational Linguistics.",,"Ablation experiments; F-score; High precision; Long-distance dependencies; Optimal results; Test sets; Training data; Training material; Experiments; Computational linguistics",,"21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics, COLING/ACL 2006","17 July 2006 through 21 July 2006","Sydney, NSW",89520,Conference Paper,"Final","",Scopus,2-s2.0-84860542597
"Reitter D., Keller F., Moore J.D.","","Computational modelling of structural priming in dialogue",2006,"Proceedings of the Human Language Technology Conference of the North American Chapter of the Association for Computational Linguistics",,,,"121","124",,51,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-70450147063
"Goel A.K., Craw S.","7201830074;6603885506;","Design, innovation and case-based reasoning",2005,"Knowledge Engineering Review","20","3",,"271","276",,51,"10.1017/S0269888906000609","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646560304&doi=10.1017%2fS0269888906000609&partnerID=40&md5=f826b1882500d1fff7e7b4b2f6e1606c","The design task is especially appropriate for applying, integrating, exploring and pushing the boundaries of case-based reasoning. In this paper, we briefly review the challenges that design poses for case-based reasoning and survey research on case-based design ranging from early explorations to more recent work on innovative design. We also summarize the theoretical contributions this research has made to case-based reasoning itself. © 2006, Cambridge University Press.",,"Systems analysis; Case-based design; Case-based reasoning; Knowledge based systems",,,,,,Review,"Final","All Open Access, Green",Scopus,2-s2.0-33646560304
"English T.M.","","Evaluation of evolutionary and genetic optimizers: No free lunch",1996,"Evolutionary Programming V: Proceedings of the Fifth Annual Conference on Evolutionary Programming",,,,"163","169",,51,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-4344680742
"Tay Y., Tuan L.A., Hui S.C.","57193627780;55490655100;7202831758;","Hyperbolic representation learning for fast and efficient neural question answering",2018,"WSDM 2018 - Proceedings of the 11th ACM International Conference on Web Search and Data Mining","2018-Febuary",,,"583","591",,50,"10.1145/3159652.3159664","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046889344&doi=10.1145%2f3159652.3159664&partnerID=40&md5=a54b4791e1834a3ae2958a690de14cf1","The dominant neural architectures in question answer retrieval are based on recurrent or convolutional encoders configured with complex word matching layers. Given that recent architectural innovations are mostly new word interaction layers or attention-based matching mechanisms, it seems to be a well-established fact that these components are mandatory for good performance. Unfortunately, the memory and computation cost incurred by these complex mechanisms are undesirable for practical applications. As such, this paper tackles the question of whether it is possible to achieve competitive performance with simple neural architectures. We propose a simple but novel deep learning architecture for fast and efficient question-answer ranking and retrieval. More specifically, our proposed model, HyperQA, is a parameter efficient neural network that outperforms other parameter intensive models such as Attentive Pooling BiLSTMs and Multi-Perspective CNNs on multiple QA benchmarks. The novelty behind HyperQA is a pairwise ranking objective that models the relationship between question and answer embeddings in Hyperbolic space instead of Euclidean space. This empowers our model with a self-organizing ability and enables automatic discovery of latent hierarchies while learning embeddings of questions and answers. Our model requires no feature engineering, no similarity matrix matching, no complicated attention mechanisms nor over-parameterized layers and yet outperforms and remains competitive to many models that have these functionalities on multiple benchmarks. © 2018 Association for Computing Machinery.","Deep learning; Learning to rank; Question answering","Complex networks; Data mining; Information retrieval; Network architecture; Neural networks; Websites; Architectural innovation; Attention mechanisms; Competitive performance; Convolutional encoders; Feature engineerings; Learning architectures; Learning to rank; Question Answering; Deep learning","ACM SIGIR;ACM SIGKDD;ACM SIGMOD;ACM SIGWEB","11th ACM International Conference on Web Search and Data Mining, WSDM 2018","5 February 2018 through 9 February 2018",,134206,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85046889344
"Grave E., Joulin A., Usunier N.","55208388000;36466607100;8558715300;","Improving neural language models with a continuous cache",2017,"5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings",,,,"","",,50,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088228120&partnerID=40&md5=bc860517b4c6667d70c2357bfb11cb3e","We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks. © ICLR 2019 - Conference Track Proceedings. All rights reserved.",,"Chemical activation; Computational linguistics; External memory; Language model; Memory size; Network language; Cache memory",,"5th International Conference on Learning Representations, ICLR 2017","24 April 2017 through 26 April 2017",,149804,Conference Paper,"Final","",Scopus,2-s2.0-85088228120
"Vanbelle S.","16235616700;","A New Interpretation of the Weighted Kappa Coefficients",2016,"Psychometrika","81","2",,"399","410",,50,"10.1007/s11336-014-9439-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84971247409&doi=10.1007%2fs11336-014-9439-4&partnerID=40&md5=fb87145fffb068df747e230e1e606c4d","Reliability and agreement studies are of paramount importance. They do contribute to the quality of studies by providing information about the amount of error inherent to any diagnosis, score or measurement. Guidelines for reporting reliability and agreement studies were recently provided. While the use of the kappa-like family is advised for categorical and ordinal scales, no further guideline in the choice of a weighting scheme is given. In the present paper, a new simple and practical interpretation of the linear- and quadratic-weighted kappa coefficients is given. This will help researchers in motivating their choice of a weighting scheme. © 2014, The Psychometric Society.","agreement; linear; ordinal scale; quadratic; reliability","human; observer variation; psychometry; reproducibility; statistical model; statistics; Humans; Linear Models; Observer Variation; Psychometrics; Reproducibility of Results; Statistics as Topic",,,,,,Article,"Final","",Scopus,2-s2.0-84971247409
"Ansari A.Q., Biswas R., Aggarwal S.","24723556800;55737985200;55417140800;","Neutrosophic classifier: An extension of fuzzy classifer",2013,"Applied Soft Computing Journal","13","1",,"563","573",,50,"10.1016/j.asoc.2012.08.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84869486427&doi=10.1016%2fj.asoc.2012.08.002&partnerID=40&md5=27eb2988039972d0a104afcba5fae86b","Fuzzy classification has become of great interest because of its ability to utilize simple linguistically interpretable rules and has overcome the limitations of symbolic or crisp rule based classifiers. This paper introduces an extension to fuzzy classifier: a neutrosophic classifier, which would utilize neutrosophic logic for its working. Neutrosophic logic is a generalized logic that is capable of effectively handling indeterminacy, stochasticity acquisition errors that fuzzy logic cannot handle. The proposed neutrosophic classifier employs neutrosophic logic for its working and is an extension of commonly used fuzzy classifier. It is compared with the commonly used fuzzy classifiers on the following parameters: nature of membership functions, number of rules and indeterminacy in the results generated. It is proved in the paper that extended fuzzy classifier: neutrosophic classifier; optimizes the said parameters in comparison to the fuzzy counterpart. Finally the paper is concluded with justifying that neutrosophic logic though in its nascent stage still holds the potential to be experimented for further exploration in different domains. © 2012 Elsevier B.V.","Neutrosophic classifier; Neutrosophic logic","Different domains; Fuzzy classification; Fuzzy classifiers; Interpretable rules; Neutrosophic logic; Rule-based classifier; Stochasticity; Fuzzy logic; Membership functions; Fuzzy systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84869486427
"Yager R.R., Kelman A.","35618760400;11039577700;","Fusion of fuzzy information with considerations for compatibility, partial aggregation, and reinforcement",1996,"International Journal of Approximate Reasoning","15","2",,"93","122",,50,"10.1016/0888-613X(96)00026-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0030216366&doi=10.1016%2f0888-613X%2896%2900026-6&partnerID=40&md5=a584b0e4b76a2df974f940b056882967","We present a general method for fusing fuzzy data. This approach allows for the inclusion of a number of considerations that help make the fusion more intelligent. Using the concept of a compatibility function, we are able to include information about the acceptability of fusing values from different parts of the observation space. OWA operators are used to implement different fusion functions, allowing for different emphasis on larger and smaller observed values. The fuzzy measure is used to allow for the fusion of subsets of our observed sources by providing a measure of the reliability of any subset of observed values. Finally, the MICA operators are used to allow for a reinforcement of possibilities that are commonly agreed upon as being high (upward reinforcement) and low (downward reinforcement).","Compatibility measures; Fuzzy aggregation; Fuzzy measures; Information fusion; MICA; OWA operators","Computational methods; Data processing; Functions; Fuzzy sets; Information science; Compatibility measures; Fuzzy aggregation; Fuzzy information; Fuzzy measures; Information fusion; MICA operators; OWA operators; Artificial intelligence",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-0030216366
"Mohr F., Wever M., Hüllermeier E.","56040199100;57195224973;6701552637;","ML-Plan: Automated machine learning via hierarchical planning",2018,"Machine Learning","107","8-10",,"1495","1515",,49,"10.1007/s10994-018-5735-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049584587&doi=10.1007%2fs10994-018-5735-z&partnerID=40&md5=40cc94bac179901c82145ba476b39f63","Automated machine learning (AutoML) seeks to automatically select, compose, and parametrize machine learning algorithms, so as to achieve optimal performance on a given task (dataset). Although current approaches to AutoML have already produced impressive results, the field is still far from mature, and new techniques are still being developed. In this paper, we present ML-Plan, a new approach to AutoML based on hierarchical planning. To highlight the potential of this approach, we compare ML-Plan to the state-of-the-art frameworks Auto-WEKA, auto-sklearn, and TPOT. In an extensive series of experiments, we show that ML-Plan is highly competitive and often outperforms existing approaches. © 2018, The Author(s).","Algorithm configuration; Algorithm selection; Automated machine learning; Automated planning; Heuristic search","Artificial intelligence; Automation; Heuristic algorithms; Learning systems; Algorithm configurations; Algorithm selection; Automated machines; Automated planning; Heuristic search; Learning algorithms",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85049584587
"Zhang C.","","Deep Dive: A data management system for automatic knowledge base construction",2015,"DeepDive: A Data Management System for Automatic Knowledge Base Construction",,,,"","",,49,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84976303340
"Snidaro L., Visentini I., Bryan K.","6507499895;17436461200;24722712500;","Fusing uncertain knowledge and evidence for maritime situational awareness via Markov Logic Networks",2015,"Information Fusion","21","1",,"159","172",,49,"10.1016/j.inffus.2013.03.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904793445&doi=10.1016%2fj.inffus.2013.03.004&partnerID=40&md5=f9be3ff2dfe06fa8ec0de7496d5d5a5f","The concepts of event and anomaly are important building blocks for developing a situational picture of the observed environment. We here relate these concepts to the JDL fusion model and demonstrate the power of Markov Logic Networks (MLNs) for encoding uncertain knowledge and compute inferences according to observed evidence. MLNs combine the expressive power of first-order logic and the probabilistic uncertainty management of Markov networks. Within this framework, different types of knowledge (e.g. a priori, contextual) with associated uncertainty can be fused together for situation assessment by expressing unobservable complex events as a logical combination of simpler evidences. We also develop a mechanism to evaluate the level of completion of complex events and show how, along with event probability, it could provide additional useful information to the operator. Examples are demonstrated on two maritime scenarios of rules for event and anomaly detection. © 2013 NATO Science and Technology Organization, Centre for Maritime Research and Experimentation. Published by Elsevier B.V. All rights reserved.","Context-based fusion; Markov Logic Networks; Situational awareness; Uncertainty management","Markov processes; Context-based; Logical combination; Markov logic networks; Probabilistic uncertainty; Situation assessment; Situational awareness; Uncertain knowledge; Uncertainty management; Complex networks",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84904793445
"Shavlik J., Natarajan S.","7004146387;57203254125;","Speeding up inference in Markov Logic Networks by preprocessing to reduce the size of the resulting grounded network",2009,"IJCAI International Joint Conference on Artificial Intelligence",,,,"1951","1956",,49,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-77958567180&partnerID=40&md5=2758a72c2140f0e8ae676ef0e934b294","Statistical-relational reasoning has received much attention due to its ability to robustly model complex relationships. A key challenge is tractable inference, especially in domains involving many objects, due to the combinatorics involved. One can accelerate inference by using approximation techniques, ""lazy"" algorithms, etc. We consider Markov Logic Networks (MLNs), which involve counting how often logical formulae are satisfied. We propose a preprocessing algorithm that can substantially reduce the effective size of MLNs by rapidly counting how often the evidence satisfies each formula, regardless of the truth values of the query literals. This is a general preprocessing method that loses no information and can be used for any MLN inference algorithm. We evaluate our algorithm empirically in three real-world domains, greatly reducing the work needed during subsequent inference. Such reduction might even allow exact inference to be performed when sampling methods would be otherwise necessary.",,"Approximation algorithms; Computer circuits; Markov processes; Probabilistic logics; Approximation techniques; Grounded networks; Inference algorithm; Markov logic networks; Pre-processing algorithms; Pre-processing method; Real world domain; Relational reasoning; Inference engines",,"21st International Joint Conference on Artificial Intelligence, IJCAI 2009","11 July 2009 through 16 July 2009",,155747,Conference Paper,"Final","",Scopus,2-s2.0-77958567180
"Higashinaka R., Isozaki H.","","Corpus-based question answering for why-questions",2008,"Proc. of IJCNLP",,,,"418","425",,49,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-44649163645
"Touretzky D.S.","6701591677;","BoltzCONS: Dynamic symbol structures in a connectionist network",1990,"Artificial Intelligence","46","1-2",,"5","46",,49,"10.1016/0004-3702(90)90003-I","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0025521210&doi=10.1016%2f0004-3702%2890%2990003-I&partnerID=40&md5=7f1b71105b920dba4934d422aa79cc5e","BoltzCONS is a connectionist model that dynamically creates and manipulates composite symbol structures. These structures are implemented using a functional analog of linked lists, but BoltzCONS employs distributed representations and associative retrieval in place of a conventional memory organization. Associative retrieval leads to some interesting properties, e.g., the model can instantaneously access any uniquely-named internal node of a tree. But the point of the work is not to reimplement linked lists in some peculiar new way; it is to show how neural networks can exhibit compositionality and distal access (the ability to reference a complex structure via an abbreviated tag), two properties that distinguish symbol processing from lower-level cognitive functions such as pattern recognition. Unlike certain other neural net models, BoltzCONS represents objects as a collection of superimposed activity patterns rather than as a set of weights. It can therefore create new structured objects dynamically, without reliance on iterative training procedures, without rehearsal of previously-learned patterns, and without resorting to grandmother cells. © 1990.",,"Data Processing--Data Structures; Connectionist Networks; Symbol Processing; Neural Networks",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0025521210
"Kohn M.S., Sun J., Knoop S., Shabo A., Carmeli B., Sow D., Syed-Mahmood T., Rapp W.","55438425500;57209850519;23090705700;14049224500;8674022800;6603822769;56879180400;57193875096;","IBM's Health Analytics and Clinical Decision Support",2014,"Yearbook of medical informatics","9",,,"154","162",,48,"10.15265/IY-2014-0002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017328375&doi=10.15265%2fIY-2014-0002&partnerID=40&md5=577c80bf00975fba84961dfc7aef3a5f","OBJECTIVES: This survey explores the role of big data and health analytics developed by IBM in supporting the transformation of healthcare by augmenting evidence-based decision-making.METHODS: Some problems in healthcare and strategies for change are described. It is argued that change requires better decisions, which, in turn, require better use of the many kinds of healthcare information. Analytic resources that address each of the information challenges are described. Examples of the role of each of the resources are given.RESULTS: There are powerful analytic tools that utilize the various kinds of big data in healthcare to help clinicians make more personalized, evidenced-based decisions. Such resources can extract relevant information and provide insights that clinicians can use to make evidence-supported decisions. There are early suggestions that these resources have clinical value. As with all analytic tools, they are limited by the amount and quality of data.CONCLUSION: Big data is an inevitable part of the future of healthcare. There is a compelling need to manage and use big data to make better decisions to support the transformation of healthcare to the personalized, evidence-supported model of the future. Cognitive computing resources are necessary to manage the challenges in employing big data in healthcare. Such tools have been and are being developed. The analytic resources, themselves, do not drive, but support healthcare transformation.","Big Data; evidence-supported decision making; healthcare analytics; healthcare transformation","artificial intelligence; clinical decision support system; data mining; evidence based practice; genomics; health care delivery; human; information processing; natural language processing; Artificial Intelligence; Data Mining; Datasets as Topic; Decision Support Systems, Clinical; Delivery of Health Care; Evidence-Based Practice; Genomics; Humans; Natural Language Processing",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85017328375
"Asghar M.Z., Kundi F.M., Ahmad S., Khan A., Khan F.","56177682300;56178369300;7401993492;55723461700;56224297600;","T-SAF: Twitter sentiment analysis framework using a hybrid classification scheme",2018,"Expert Systems","35","1","e12233","","",,47,"10.1111/exsy.12233","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85042213085&doi=10.1111%2fexsy.12233&partnerID=40&md5=f4b05cd821e625b41c366a79812653f8","Of the many social media sites available, users prefer microblogging services such as Twitter to learn about product services, social events, and political trends. Twitter is considered an important source of information in sentiment analysis applications. Supervised and unsupervised machine learning-based techniques for Twitter data analysis have been investigated in the last few years, often resulting in an incorrect classification of sentiments. In this paper, we focus on these issues and present a unified framework for classifying tweets using a hybrid classification scheme. The proposed method aims at improving the performance of Twitter-based sentiment analysis systems by incorporating 4 classifiers: (a) a slang classifier, (b) an emoticon classifier, (c) the SentiWordNet classifier, and (d) an improved domain-specific classifier. After applying the preprocessing steps, the input text is passed through the emoticon and slang classifiers. In the next stage, SentiWordNet-based and domain-specific classifiers are applied to classify the text more accurately. Finally, sentiment classification is performed at sentence and document levels. The findings revealed that the proposed method overcomes the limitations of previous methods by considering slang, emoticons, and domain-specific terms. Copyright © 2017 John Wiley & Sons, Ltd","emoticons; hybrid; microblog; sentiment analysis; slang; Twitter","Data mining; Information retrieval systems; Learning systems; Natural language processing systems; Social networking (online); emoticons; hybrid; Micro-blog; slang; Twitter; Sentiment analysis",,,,,,Article,"Final","",Scopus,2-s2.0-85042213085
"Yu T., Li J., Yu Q., Tian Y., Shun X., Xu L., Zhu L., Gao H.","35270424000;55583712000;57203598850;57189040974;57194107952;56500879700;56046703000;56046792800;","Knowledge graph for TCM health preservation: Design, construction, and applications",2017,"Artificial Intelligence in Medicine","77",,,"48","52",,47,"10.1016/j.artmed.2017.04.001","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85018768730&doi=10.1016%2fj.artmed.2017.04.001&partnerID=40&md5=6f98526ef2b2966531742ed5f47dc5c5","Traditional Chinese Medicine (TCM) is one of the important non-material cultural heritages of the Chinese nation. It is an important development strategy of Chinese medicine to collect, analyzes, and manages the knowledge assets of TCM health care. As a novel and massive knowledge management technology, knowledge graph provides an ideal technical means to solve the problem of “Knowledge Island” in the field of traditional Chinese medicine. In this study, we construct a large-scale knowledge graph, which integrates terms, documents, databases and other knowledge resources. This knowledge graph can facilitate various knowledge services such as knowledge visualization, knowledge retrieval, and knowledge recommendation, and helps the sharing, interpretation, and utilization of TCM health care knowledge. © 2017 Elsevier B.V.","Health preservation; Knowledge graph; Knowledge services; Ontology; Traditional Chinese medicine","Health care; Knowledge management; Ontology; Cultural heritages; Development strategies; Knowledge graphs; Knowledge management technology; Knowledge retrieval; Knowledge service; Knowledge Visualization; Traditional Chinese Medicine; Medicine; Article; Chinese Biomedical database; Chinese medicine; health care; human; information retrieval; knowledge management; medical informatics; preservation; priority journal; automated pattern recognition; factual database; health care delivery; Databases, Factual; Delivery of Health Care; Humans; Medicine, Chinese Traditional; Pattern Recognition, Automated",,,,,,Article,"Final","",Scopus,2-s2.0-85018768730
"Henaff M., Weston J., Szlam A., Bordes A., LeCun Y.","55760206000;8865128200;11539079600;18933923000;55666793600;","Tracking the world state with recurrent entity networks",2017,"5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings",,,,"","",,47,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85088230593&partnerID=40&md5=125646b8edab0f6f33b0d71d352270e2","We introduce a new model, the Recurrent Entity Network (EntNet). It is equipped with a dynamic long-term memory which allows it to maintain and update a representation of the state of the world as it receives new data. For language understanding tasks, it can reason on-the-fly as it reads text, not just when it is required to answer a question or respond as is the case for a Memory Network (Sukhbaatar et al., 2015). Like a Neural Turing Machine or Differentiable Neural Computer (Graves et al., 2014; 2016) it maintains a fixed size memory and can learn to perform location and content-based read and write operations. However, unlike those models it has a simple parallel architecture in which several memory locations can be updated simultaneously. The EntNet sets a new state-of-the-art on the bAbI tasks, and is the first method to solve all the tasks in the 10k training examples setting. We also demonstrate that it can solve a reasoning task which requires a large number of supporting facts, which other methods are not able to solve, and can generalize past its training horizon. It can also be practically used on large scale datasets such as Children's Book Test, where it obtains competitive performance, reading the story in a single pass. © ICLR 2019 - Conference Track Proceedings. All rights reserved.",,"Parallel architectures; Turing machines; Competitive performance; Language understanding; Large-scale datasets; Long term memory; Neural computers; State of the art; Training example; Write operations; Large dataset",,"5th International Conference on Learning Representations, ICLR 2017","24 April 2017 through 26 April 2017",,149804,Conference Paper,"Final","",Scopus,2-s2.0-85088230593
"Somasundaran S., Wiebe J., Ruppenhofer J.","51665813900;7005437314;52164338600;","Discourse level opinion interpretation",2008,"Coling 2008 - 22nd International Conference on Computational Linguistics, Proceedings of the Conference","1",,,"801","808",,47,"10.3115/1599081.1599182","https://www.scopus.com/inward/record.uri?eid=2-s2.0-60649117277&doi=10.3115%2f1599081.1599182&partnerID=40&md5=fb1dfe766bf6cc96eea1e2de669b2d36","This work proposes opinion frames as a representation of discourse-level associations which arise from related opinion topics. We illustrate how opinion frames help gather more information and also assist disambiguation. Finally we present the results of our experiments to detect these associations. © 2008. Licensed under the Creative Commons.",,"Computational linguistics",,"22nd International Conference on Computational Linguistics, Coling 2008","18 August 2008 through 22 August 2008","Manchester",86714,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-60649117277
"Miller G.","7404978556;","Toward ethnographies of institutional discourse: Proposal and Suggestions",1994,"Journal of Contemporary Ethnography","23","3",,"280","306",,47,"10.1177/089124194023003002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84965520928&doi=10.1177%2f089124194023003002&partnerID=40&md5=cae8c4b6ab6af00a2727fa9564862cac","The article describes the ethnography of institutional discourse perspective. Institutional discourses are made up of the assumptions, concerns, and vocabularies of members of socially organized settings, and the ways in which they interact. The perspective is an analytic framework and research program that combines ethnographers' concern for in-depth observations of everyday life with aspects of ethnomethodology, conversation analysis, and Foucauldian discourse studies. The perspective focuses on the ways in which setting members use discursive resources in organizing their practical actions, and how members' actions are constrained by the resources available in settings. The perspective is developed by considering how institutional discourses and settings are reflexively linked, institutions are organized as situated conventions, and institutional discourses involve talk and interpretation and are dispersed within and across settings. © 1994, SAGE Periodicals Press. All rights reserved.",,,,,,,,Article,"Final","",Scopus,2-s2.0-84965520928
"Hu L., Yang T., Shi C., Ji H., Li X.","56181376700;57204945425;55447999200;57203507149;35487931000;","Heterogeneous graph attention networks for semi-supervised short text classification",2020,"EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference",,,,"4821","4830",,46,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084289136&partnerID=40&md5=338f5cc9a10c8585c6979e60cdb04524","Short text classification has found rich and critical applications in news and tweet tagging to help users find relevant information. Due to lack of labeled training data in many practical use cases, there is a pressing need for studying semi-supervised short text classification. Most existing studies focus on long texts and achieve unsatisfactory performance on short texts due to the sparsity and limited labeled data. In this paper, we propose a novel heterogeneous graph neural network based method for semi-supervised short text classification, leveraging full advantage of few labeled data and large unlabeled data through information propagation along the graph. In particular, we first present a flexible HIN (heterogeneous information network) framework for modeling the short texts, which can integrate any type of additional information as well as capture their relations to address the semantic sparsity. Then, we propose Heterogeneous Graph ATtention networks (HGAT) to embed the HIN for short text classification based on a dual-level attention mechanism, including node-level and type-level attentions. The attention mechanism can learn the importance of different neighboring nodes as well as the importance of different node (information) types to a current node. Extensive experimental results have demonstrated that our proposed model outperforms state-of-the-art methods across six benchmark datasets significantly. © 2019 Association for Computational Linguistics",,"Backpropagation; Information dissemination; Information services; Labeled data; Natural language processing systems; Neural networks; Semantics; Attention mechanisms; Critical applications; Heterogeneous graph; Heterogeneous information; Information propagation; Labeled training data; Short text classifications; State-of-the-art methods; Text processing","Apple;ASAPP;et al.;Facebook;Google;salesforce","2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019","3 November 2019 through 7 November 2019",,159367,Conference Paper,"Final","",Scopus,2-s2.0-85084289136
"Yang Z., Dai Z., Salakhutdinov R., Cohen W.W.","",[No title available],2017,"Breaking The Softmax Bottleneck: A High-Rank Rnn Language Model",,,,"","",,46,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049009940
"Thomas N.T.","56287366500;","An e-business chatbot using AIML and LSA",2016,"2016 International Conference on Advances in Computing, Communications and Informatics, ICACCI 2016",,,"7732476","2740","2742",,46,"10.1109/ICACCI.2016.7732476","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85007325164&doi=10.1109%2fICACCI.2016.7732476&partnerID=40&md5=90521a1aee2a734655cfaa9abbc4b226","The e-business has completely changed the way of selling products. E-commerce is one of the e-business models which mostly do business over the internet. The major drawback of this field is quality of customer service they provide. In every e-business model, customers have to wait for a long time to get response from the customer service representative. Especially in case of live chat, they talk to multiple customers at a time. The responses may not be relevant as they copy paste pre-written answers. Also, the slow response and the long time wait for the service agent is the biggest headache in this field of online services. As a solution to this problem, we propose a chatbot which automatically gives immediate responses to the users based on the data set of Frequently Answered Questions(FAQs), using Artificial Intelligence Markup Language (AIML) and Latent Semantic Analysis (LSA). Template based questions like greetings and general questions will be answered using AIML and other service related questions use LSA to give responses. © 2016 IEEE.","AIML; E-business; LSA","Information science; Markup languages; Sales; Semantics; AIML; Artificial intelligence mark up languages; Customer service representatives; Customer services; E-business models; eBusiness; Latent Semantic Analysis; On-line service; Electronic commerce","","5th International Conference on Advances in Computing, Communications and Informatics, ICACCI 2016","21 September 2016 through 24 September 2016",,124693,Conference Paper,"Final","",Scopus,2-s2.0-85007325164
"Cambria E., Olsher D., Kwok K.","56140547500;55341324600;57226032645;","Sentic activation: A two-level affective common sense reasoning framework",2012,"Proceedings of the National Conference on Artificial Intelligence","1",,,"186","192",,46,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868296994&partnerID=40&md5=319976c75734e8ccd48659d28f56c243","An important difference between traditional AI systems and human intelligence is our ability to harness common sense knowledge gleaned from a lifetime of learning and experiences to inform our decision making and behavior. This allows humans to adapt easily to novel situations where AI fails catastrophically for lack of situation-specific rules and generalization capabilities. Common sense knowledge also provides the background knowledge for humans to successfully operate in social situations where such knowledge is typically assumed. In order for machines to exploit common sense knowledge in reasoning as humans do, moreover, we need to endow them with human-like reasoning strategies. In this work, we propose a two-level affective reasoning framework that concurrently employs multi-dimensionality reduction and graph mining techniques to mimic the integration of conscious and unconscious reasoning, and exploit it for sentiment analysis. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"AI systems; Background knowledge; Commonsense knowledge; Commonsense reasoning; Generalization capability; Graph mining; Human intelligence; Multidimensionality; Reasoning framework; Sentiment analysis; Artificial intelligence","Association for the Advancement of Artificial Intelligence (AAAI);AI Journal;Steven Kuhn, Pine River Capital;National Science Foundation;Microsoft Research","26th AAAI Conference on Artificial Intelligence and the 24th Innovative Applications of Artificial Intelligence Conference, AAAI-12 / IAAI-12","22 July 2012 through 26 July 2012","Toronto, ON",93437,Conference Paper,"Final","",Scopus,2-s2.0-84868296994
"Ostermann S., Roth M., Modi A., Thater S., Pinkal M.","","SemEval-2018 Task 11: Machine comprehension using commonsense knowledge",2018,"Proceedings of the 12Th International Workshop on Semantic Evaluation",,,,"747","757",,45,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85055623871
"Xing F.Z., Cambria E., Welsch R.E.","57196019442;56140547500;8214812500;","Intelligent asset allocation via market sentiment views",2018,"IEEE Computational Intelligence Magazine","13","4","8492384","25","34",,45,"10.1109/MCI.2018.2866727","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055283489&doi=10.1109%2fMCI.2018.2866727&partnerID=40&md5=0e8f34fc50dc754e8ab95076fe26d2bc","The sentiment index of market participants has been extensively used for stock market prediction in recent years. Many financial information vendors also provide it as a service. However, utilizing market sentiment under the asset allocation framework has been rarely discussed. In this article, we investigate the role of market sentiment in an asset allocation problem. We propose to compute sentiment time series from social media with the help of sentiment analysis and text mining techniques. A novel neural network design, built upon an ensemble of evolving clustering and long short-term memory, is used to formalize sentiment information into market views. These views are later integrated into modern portfolio theory through a Bayesian approach. We analyze the performance of this asset allocation model from many aspects, such as stability of portfolios, computing of sentiment time series, and profitability in our simulations. Experimental results show that our model outperforms some of the most successful forecasting techniques. Thanks to the introduction of the evolving clustering method, the estimation accuracy of market views is significantly improved. © 2018 IEEE.",,"Bayesian networks; Computation theory; Data mining; Investments; Natural language processing systems; Neural networks; Sentiment analysis; Time series; Time series analysis; Evolving clustering methods; Financial information; Forecasting techniques; Market participants; Modern portfolio theories; Novel neural network; Stock market prediction; Text mining techniques; Commerce",,,,,,Article,"Final","",Scopus,2-s2.0-85055283489
"Fuster-Parra P., Tauler P., Bennasar-Veny M., Ligeza A., López-González A.A., Aguiló A.","6506174304;6603455183;26221020200;15127004800;26538754600;55754040600;","Bayesian network modeling: A case study of an epidemiologic system analysis of cardiovascular risk",2016,"Computer Methods and Programs in Biomedicine","126",,,"128","142",,45,"10.1016/j.cmpb.2015.12.010","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84960082158&doi=10.1016%2fj.cmpb.2015.12.010&partnerID=40&md5=9571e893482ac3d01afd70d3f81f6b91","An extensive, in-depth study of cardiovascular risk factors (CVRF) seems to be of crucial importance in the research of cardiovascular disease (CVD) in order to prevent (or reduce) the chance of developing or dying from CVD. The main focus of data analysis is on the use of models able to discover and understand the relationships between different CVRF. In this paper a report on applying Bayesian network (BN) modeling to discover the relationships among thirteen relevant epidemiological features of heart age domain in order to analyze cardiovascular lost years (CVLY), cardiovascular risk score (CVRS), and metabolic syndrome (MetS) is presented. Furthermore, the induced BN was used to make inference taking into account three reasoning patterns: causal reasoning, evidential reasoning, and intercausal reasoning. Application of BN tools has led to discovery of several direct and indirect relationships between different CVRF. The BN analysis showed several interesting results, among them: CVLY was highly influenced by smoking being the group of men the one with highest risk in CVLY; MetS was highly influence by physical activity (PA) being again the group of men the one with highest risk in MetS, and smoking did not show any influence. BNs produce an intuitive, transparent, graphical representation of the relationships between different CVRF. The ability of BNs to predict new scenarios when hypothetical information is introduced makes BN modeling an Artificial Intelligence (AI) tool of special interest in epidemiological studies. As CVD is multifactorial the use of BNs seems to be an adequate modeling tool. © 2015 Elsevier Ireland Ltd.","Bayesian networks; Cardiovascular lost years; Cardiovascular risk score; Causal dependency discovery; Metabolic syndrome; Model averaging","Artificial intelligence; Knowledge based systems; Metabolism; Risk assessment; Cardiovascular lost years; Cardiovascular risk; Causal dependencies; Metabolic syndromes; Model averaging; Bayesian networks; glucose; triacylglycerol; adult; algorithm; Article; artificial intelligence; Bayes theorem; Bayesian learning; Bayesian network model; blood pressure; body weight; cardiovascular disease; cardiovascular risk; comparative study; controlled study; epidemiological data; female; Framingham risk score; human; male; mathematical model; measurement accuracy; metabolic syndrome X; middle aged; normal human; physical activity; probability; risk assessment; scoring system; smoking; system analysis; Bayes theorem; biological model; body mass; Cardiovascular Diseases; cardiovascular system; risk factor; statistical model; Adult; Algorithms; Artificial Intelligence; Bayes Theorem; Body Mass Index; Body Weight; Cardiovascular Diseases; Cardiovascular System; Female; Humans; Male; Middle Aged; Models, Cardiovascular; Models, Statistical; Risk Factors; Smoking",,,,,,Article,"Final","",Scopus,2-s2.0-84960082158
"Hinaut X., Petit M., Pointeau G., Dominey P.F.","53063834600;55803519000;55804281800;7004714357;","Exploring the acquisition and production of grammatical constructions through human-robot interaction with echo state networks",2014,"Frontiers in Neurorobotics","8","MAY","016","","",,45,"10.3389/fnbot.2014.00016","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84904874296&doi=10.3389%2ffnbot.2014.00016&partnerID=40&md5=09e184c076a7b8c5e3bd80d64d72a020","One of the principal functions of human language is to allow people to coordinate joint action. This includes the description of events, requests for action, and their organization in time. A crucial component of language acquisition is learning the grammatical structures that allow the expression of such complex meaning related to physical events. The current research investigates the learning of grammatical constructions and their temporal organization in the context of human-robot physical interaction with the embodied sensorimotor humanoid platform, the iCub. We demonstrate three noteworthy phenomena. First, a recurrent network model is used in conjunction with this robotic platform to learn the mappings between grammatical forms and predicate-argument representations of meanings related to events, and the robot's execution of these events in time. Second, this learning mechanism functions in the inverse sense, i.e., in a language production mode, where rather than executing commanded actions, the robot will describe the results of human generated actions. Finally, we collect data from naïve subjects who interact with the robot via spoken language, and demonstrate significant learning and generalization results. This allows us to conclude that such a neural language learning system not only helps to characterize and understand some aspects of human language acquisition, but also that it can be useful in adaptive human-robot interaction. © 2014 Hinaut, Petit, Pointeau and Dominey.","Anytime processing; Grammatical constructions; Human-robot interaction; ICub humanoid; Language acquisition; Language production; Recurrent neural networks; Reservoir computing","article; artificial neural network; comprehension; grammar; human robot interaction; information processing; language processing; learning; speech intelligibility",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-84904874296
"Yarandi M., Jahankhani H., Tawil A.-R.H.","55576501900;35097301800;7004486328;","A personalized adaptive e-learning approach based on semantic web technology",2013,"Webology","10","2","110","","",,45,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84938071442&partnerID=40&md5=8f22cc43b1305c24215473c76c9bc505","Recent developments in semantic web technologies heightened the need for online adaptive learning environment. Adaptive learning is an important research topic in the field of web-based systems as there are no fixed learning paths which are appropriate for all learners. However, most studies in this field have only focused on learning styles and habits of learners. Far too little attention has been paid on understanding the ability of learners. Therefore, it is becoming increasingly difficult to ignore adaptation in the field of e-learning systems. Many researchers are adopting semantic web technologies to find new ways for designing adaptive learning systems based on describing knowledge using ontological models. Ontologies have the potential to design content and learner models required to create adaptive e-learning systems based on various characteristics of learners. The aim of this paper is to present an ontology-based approach to develop adaptive e-learning system based on the design of semantic content, learner and domain models to tailor the teaching process for individual learner's needs. The proposed new adaptive e-learning has the ability to support personalization based on learner's ability, learning style, preferences and levels of knowledge. In our approach the ontological user profile is updated based on achieved learner's abilities. © 2013, Maryam Yarandi, Hossein Jahankhani, & Abdel-Rahman H. Tawil.","Adaptive learning; E-learning; Ontology; Personalized learning; Semantic web",,,,,,,Article,"Final","",Scopus,2-s2.0-84938071442
"Smarandache F.","","Definition of neutrosophic logic - A generalization of the intuitionistic fuzzy logic",2003,"Proceedings of the third conference of the european society for fuzzy logic and technology",,,,"141","146",,45,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-33751111060
"Vashishtha S., Susan S.","57208887769;26423246100;","Fuzzy rule based unsupervised sentiment analysis from social media posts",2019,"Expert Systems with Applications","138",,"112834","","",,44,"10.1016/j.eswa.2019.112834","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069805425&doi=10.1016%2fj.eswa.2019.112834&partnerID=40&md5=93adfcab9263b61455f694f19e36ec3e","In this paper, we compute the sentiment of social media posts using a novel set of fuzzy rules involving multiple lexicons and datasets. The proposed fuzzy system integrates Natural Language Processing techniques and Word Sense Disambiguation using a novel unsupervised nine fuzzy rule based system to classify the post into: positive, negative or neutral sentiment class. We perform a comparative analysis of our method on nine public twitter datasets, three sentiment lexicons, four state-of-the-art approaches for unsupervised Sentiment Analysis and one state-of-the-art method for supervised machine learning. Traditionally, Sentiment Analysis of twitter data is performed using a single lexicon. Our results can give an insight to researchers to choose which lexicon is best for social media. The fusion of fuzzy logic with lexicons for sentiment classification provides a new paradigm in Sentiment Analysis. Our method can be adapted to any lexicon and any dataset (two-class or three-class sentiment). The experiments on benchmark datasets yield higher performance for our approach as compared to the state-of-the-art. © 2019 Elsevier Ltd","Fuzzy rule; Lexicon; Sentiment analysis; Social media; Twitter","Benchmarking; Fuzzy logic; Fuzzy rules; Learning algorithms; Sentiment analysis; Social networking (online); Supervised learning; Lexicon; NAtural language processing; Sentiment classification; Social media; State-of-the-art approach; Supervised machine learning; Twitter; Word Sense Disambiguation; Fuzzy inference",,,,,,Article,"Final","",Scopus,2-s2.0-85069805425
"Lafore R.","",[No title available],2001,"Object Oriented Programming in C++",,,,"","",,44,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0038358827
"Singla P., Donaingos P.","10244352500;7003565655;","Memory-efficient inference in relational domains",2006,"Proceedings of the National Conference on Artificial Intelligence","1",,,"488","493",,43,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750696315&partnerID=40&md5=21b0b4a8fe02580cbe6aa372852eee2f","Propositionalization of a first-order theory followed by satisfiability testing has proved to be a remarkably efficient approach to inference in relational domains such as planning (Kautz & Selman 1996) and verification (Jackson 2000). More recently, weighted satisfiability solvers have been used successfully for MPE inference in statistical relational learners (Singla & Domingos 2005). However, fully instantiating a finite first-order theory requires memory on the order of the number of constants raised to the arity of the clauses, which significantly limits the size of domains it can be applied to. In this paper we propose LazySAT, a variation of the WalkSAT solver that avoids this blowup by taking advantage of the extreme sparseness that is typical of relational domains (i.e., only a small fraction of ground atoms are true, and most clauses are trivially satisfied). Experiments on entity resolution and planning problems show that LazySAT reduces memory usage by orders of magnitude compared to Walk-SAT, while taking comparable time to run and producing the same solutions. Copyright © 2006, American Association for Artificial Intelligence (www.aaai.org). All rights reserved.",,"Data structures; Finite element method; Learning systems; Problem solving; Statistical methods; Storage allocation (computer); Finite first order theory; Propositionalization; Relational domains; Satisfiability testing; Statistical relational learners; Relational database systems","American Association for Artificial Intelligence","21st National Conference on Artificial Intelligence and the 18th Innovative Applications of Artificial Intelligence Conference, AAAI-06/IAAI-06","16 July 2006 through 20 July 2006","Boston, MA",68475,Conference Paper,"Final","",Scopus,2-s2.0-33750696315
"Peng B., Li X., Gao J., Liu J., Wong K.-F.","56182809200;57194875411;55702627000;57189311527;7404759311;","Deep DYnA-Q: Integrating planning for task-completion dialogue policy learning",2018,"ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)","1",,,"2182","2192",,42,"10.18653/v1/p18-1203","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062523587&doi=10.18653%2fv1%2fp18-1203&partnerID=40&md5=52762121c002e88d56eb420919fd0acc","Training a task-completion dialogue agent via reinforcement learning (RL) is costly because it requires many interactions with real users. One common alternative is to use a user simulator. However, a user simulator usually lacks the language complexity of human interlocutors and the biases in its design may tend to degrade the agent. To address these issues, we present Deep Dyna-Q, which to our knowledge is the first deep RL framework that integrates planning for task-completion dialogue policy learning. We incorporate into the dialogue agent a model of the environment, referred to as the world model, to mimic real user response and generate simulated experience. During dialogue policy learning, the world model is constantly updated with real user experience to approach real user behavior, and in turn, the dialogue agent is optimized using both real experience and simulated experience. The effectiveness of our approach is demonstrated on a movie-ticket booking task in both simulated and human-in-the-loop settings. © 2018 Association for Computational Linguistics",,"Computational linguistics; Deep learning; Reinforcement learning; Human-in-the-loop; Language complexity; Policy learning; RL framework; Ticket booking; User behaviors; User experience; World model; Behavioral research","Apple;ByteDance;et al.;Facebook;Google;Samsung Research","56th Annual Meeting of the Association for Computational Linguistics, ACL 2018","15 July 2018 through 20 July 2018",,145927,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85062523587
"Agarwal B., Mittal N.","55634231900;36642976800;","Text classification using machine learning methods-a survey",2014,"Advances in Intelligent Systems and Computing","236",,,"701","709",,42,"10.1007/978-81-322-1602-5_75","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84928017675&doi=10.1007%2f978-81-322-1602-5_75&partnerID=40&md5=3bae97c51486741b8238aacbaf847e9b","Text classification is used to organize documents in a predefined set of classes. It is very useful inWeb content management, search engines; email filtering, etc. Text classification is a difficult task due to high- dimensional feature vector comprising noisy and irrelevant features. Various feature reduction methods have been proposed for eliminating irrelevant features as well as for reducing the dimension of feature vector. Relevant and reduced feature vector is used by machine learning model for better classification results. This paper presents various text classification approaches using machine learning techniques, and feature selection techniques for reducing the high-dimensional feature vector. © Springer India 2014.","Feature selection; Machine learning Algorithms; Text classification","Artificial intelligence; Classification (of information); Feature extraction; Information retrieval systems; Learning algorithms; Learning systems; Problem solving; Search engines; Soft computing; Vectors; Classification results; Content management; High dimensional feature; Machine learning methods; Machine learning models; Machine learning techniques; Selection techniques; Text classification; Text processing","","2nd International Conference on Soft Computing for Problem Solving, SocProS 2012","28 December 2012 through 30 December 2012",,116609,Conference Paper,"Final","",Scopus,2-s2.0-84928017675
"Mehler A., Lücking A., Weiß P.","13907942400;55918401600;36465862600;","A network model of interpersonal alignment in dialog",2010,"Entropy","12","6",,"1440","1483",,42,"10.3390/e12061440","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77956414002&doi=10.3390%2fe12061440&partnerID=40&md5=cc51063ecc779707b225c0463a09da00","In dyadic communication, both interlocutors adapt to each other linguistically, that is, they align interpersonally. In this article, we develop a framework for modeling interpersonal alignment in terms of the structural similarity of the interlocutors' dialog lexica. This is done by means of so-called two-layer time-aligned network series, that is, a time-adjusted graph model. The graph model is partitioned into two layers, so that the interlocutors' lexica are captured as subgraphs of an encompassing dialog graph. Each constituent network of the series is updated utterance-wise. Thus, both the inherent bipartition of dyadic conversations and their gradual development are modeled. The notion of alignment is then operationalized within a quantitative model of structure formation based on the mutual information of the subgraphs that represent the interlocutor's dialog lexica. By adapting and further developing several models of complex network theory, we show that dialog lexica evolve as a novel class of graphs that have not been considered before in the area of complex (linguistic) networks. Additionally, we show that our framework allows for classifying dialogs according to their alignment status. To the best of our knowledge, this is the first approach to measuring alignment in communication that explores the similarities of graph-like cognitive representations. © 2010.","Alignment in communication; Graph distance measures; Linguistic networks; Mutual information of graphs; Quantitative network analysis; Structural coupling",,,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-77956414002
"Wu S., Hsiao L., Cheng X., Hancock B., Rekatsinas T., Levis P., Ré C.","34868994500;57202588153;57202584027;57192671855;36242517700;22835832700;10739281400;","Fonduer: Knowledge base construction from richly formatted data",2018,"Proceedings of the ACM SIGMOD International Conference on Management of Data",,,,"1301","1316",,41,"10.1145/3183713.3183729","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048825604&doi=10.1145%2f3183713.3183729&partnerID=40&md5=b72981cc6bbdd508a686b2a972be7798","We focus on knowledge base construction (KBC) from richly formatted data. In contrast to KBC from text or tabular data, KBC from richly formatted data aims to extract relations conveyed jointly via textual, structural, tabular, and visual expressions. We introduce Fonduer, a machine-learning-based KBC system for richly formatted data. Fonduer presents a newdata model that accounts for three challenging characteristics of richly formatted data: (1) prevalent document-level relations, (2) multimodality, and (3) data variety. Fonduer uses a new deep-learning model to automatically capture the representation (i.e., features) needed to learn how to extract relations from richly formatted data. Finally, Fonduer provides a new programming model that enables users to convert domain expertise, based on multiple modalities of information, to meaningful signals of supervision for training a KBC system. Fonduer-based KBC systems are in production for a range of use cases, including at a major online retailer. We compare Fonduer against state-ofthe-art KBC approaches in four different domains. We show that Fonduer achieves an average improvement of 41 F1 points on the quality of the output knowledge base-and in some cases produces up to 1.87× the number of correct entries-compared to expertcurated public knowledge bases. We also conduct a user study to assess the usability of Fonduer's new programming model. We show that after using Fonduer for only 30 minutes, non-domain experts are able to design KBC systems that achieve on average 23 F1 points higher quality than traditional machine-learning-based KBC approaches. © 2018 Association for Computing Machinery.",,"Artificial intelligence; Expert systems; Online systems; Different domains; Domain expertise; Knowledge-base construction; Learning models; Multiple modalities; Online retailers; Programming models; Public knowledge; Deep learning","ACM SIGMOD;et al.;Facebook;Microsoft;Oracle;Tableau","44th ACM SIGMOD International Conference on Management of Data, SIGMOD 2018","10 June 2018 through 15 June 2018",,136864,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85048825604
"Fan C.-T., Wang Y.-K., Huang C.-R.","36696452300;35281195900;35326748400;","Heterogeneous information fusion and visualization for a large-scale intelligent video surveillance system",2017,"IEEE Transactions on Systems, Man, and Cybernetics: Systems","47","4","7428948","593","604",,41,"10.1109/TSMC.2016.2531671","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85017619710&doi=10.1109%2fTSMC.2016.2531671&partnerID=40&md5=159b6549bffbd9a83336321583120d7f","Wide-area monitoring for a smart community can be challenging in systems engineering because of its large scale and heterogeneity at the sensor, algorithm, and visualization levels. A smart interface to visualize high-level information fused from a diversity of low-level surveillance data, and to facilitate rapid response of events, is critical for the design of the system. This paper presents an event-driven visualization mechanism fusing multimodal information for a large-scale intelligent video surveillance system. The mechanism proactively helps security personnel intuitively be aware of events through close cooperation among visualization, data fusion, and sensor tasking. The visualization not only displays 2-D, 3-D, and geographical information within a condensed form of interface but also automatically shows the only important video streams corresponding to spontaneous alerts and events by a decision process called display switching arbitration. The display switching arbitration decides the importance of cameras by score ranking that considers event urgency and semantic object features. This system has been successfully deployed in a campus to demonstrate its usability and efficiency for an installation with two camera clusters that include dozens of cameras, and with a lot of video analytics to detect alerts and events. A further simulation comparing the display switching arbitration with similar camera selection methods shows that our method improves the visualization by selecting better representative camera views and reducing redundant switchover among multiview videos. © 2016 IEEE.","Display switching arbitration; information fusion; third-generation surveillance system (3GSS); visual surveillance; visualizability; visualization","Cameras; Data fusion; Data visualization; Flow visualization; Information fusion; Monitoring; Semantics; Sensor data fusion; Switching; Three dimensional computer graphics; Video streaming; Visualization; Camera selection methods; Geographical information; Heterogeneous information; Intelligent video surveillance systems; Multi-modal information; Surveillance systems; Visual surveillance; visualizability; Security systems",,,,,,Article,"Final","",Scopus,2-s2.0-85017619710
"Muggleton S., Lodhi H., Amini A., Sternberg M.J.E.","7003491952;8849911200;14021059700;7101911184;","Support vector inductive logic programming",2005,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","3735 LNAI",,,"163","175",,41,"10.1007/11563983_15","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33646363498&doi=10.1007%2f11563983_15&partnerID=40&md5=24fbaf2a44b73a10203d3d812da4b881","In this paper we explore a topic which is at the intersection of two areas of Machine Learning: namely Support Vector Machines (SVMs) and Inductive Logic Programming (ILP). We propose a general method for constructing kernels for Support Vector Inductive Logic Programming (SVILP). The kernel not only captures the semantic and syntactic relational information contained in the data but also provides the flexibility of using arbitrary forms of structured and non-structured data coded in a relational way. While specialised kernels have been developed for strings, trees and graphs our approach uses declarative background knowledge to provide the learning bias. The use of explicitly encoded background knowledge distinguishes SVILP from existing relational kernels which in ILP-terms work purely at the atomic generalisation level. The SVILP approach is a form of generalisation relative to background knowledge, though the final combining function for the ILP-leamed clauses is an SVM rather than a logical conjunction. We evaluate SVILP empirically against related approaches, including an industry-standard toxin predictor called TOPKAT. Evaluation is conducted on a new broad-ranging toxicity dataset (DSSTox). The experimental results demonstrate that our approach significantly outperforms all other approaches in the study. © Springer-Verlag Berlin Heidelberg 2005.",,"Database systems; Knowledge based systems; Learning systems; Semantics; Atomic generalisation level; Logical conjunction; Non-structured data; Support Vector Machines (SVM); Logic programming",,"8th International Conference on Discovery Science, DS 2005","8 October 2005 through 11 October 2005",,67553,Conference Paper,"Final","",Scopus,2-s2.0-33646363498
"Krishnan V., Das S., Chakrabarti S.","36824768800;57213459647;7403254661;","Enhanced answer type inference from questions using sequential models",2005,"HLT/EMNLP 2005 - Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, Proceedings of the Conference",,,,"315","322",,41,"10.3115/1220575.1220615","https://www.scopus.com/inward/record.uri?eid=2-s2.0-34547441174&doi=10.3115%2f1220575.1220615&partnerID=40&md5=e042b4f32dbf298fb2da7363b5d82f9c","Question classification is an important step in factual question answering (QA) and other dialog systems. Several attempts have been made to apply statistical machine learning approaches, including Support Vector Machines (SVMs) with sophisticated features and kernels. Curiously, the payoff beyond a simple bag-ofwords representation has been small. We show that most questions reveal their class through a short contiguous token subsequence, which we call its informer span. Perfect knowledge of informer spans can enhance accuracy from 79.4% to 88% using linear SVMs on standard benchmarks. In contrast, standard heuristics based on shallow pattern-matching give only a 3% improvement, showing that the notion of an informer is non-trivial. Using a novel multi-resolution encoding of the question's parse tree, we induce a Conditional Random Field (CRF) to identify informer spans with about 85% accuracy. Then we build a meta-classifier using a linear SVM on the CRF output, enhancing accuracy to 86.2%, which is better than all published numbers. © 2005 Association for Computational Linguistics.",,"Learning algorithms; Pattern matching; Random processes; Support vector machines; Conditional random field; Meta-classifiers; Question Answering; Question classification; Sequential model; Statistical machine learning; Support vector machine (SVMs); Type inferences; Natural language processing systems","eLDa;et al.;Google;Microsoft;SHARP;XEROX - Research Centre Europe","Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing, HLT/EMNLP 2005, Co-located with the 2005 Document Understanding Conference, DUC and the 9th International Workshop on Parsing Technologies, IWPT","6 October 2005 through 8 October 2005","Vancouver, BC",86710,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-34547441174
"Girardi-Schappo M., Tragtenberg M.H.R., Kinouchi O.","55795908100;6506060564;6701584586;","A brief history of excitable map-based neurons and neural networks",2013,"Journal of Neuroscience Methods","220","2",,"116","130",,40,"10.1016/j.jneumeth.2013.07.014","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84887618382&doi=10.1016%2fj.jneumeth.2013.07.014&partnerID=40&md5=ee4150ee571581e0246ea6a591a4adc0","This review gives a short historical account of the excitable maps approach for modeling neurons and neuronal networks. Some early models, due to Pasemann (1993), Chialvo (1995) and Kinouchi and Tragtenberg (1996), are compared with more recent proposals by Rulkov (2002) and Izhikevich (2003). We also review map-based schemes for electrical and chemical synapses and some recent findings as critical avalanches in map-based neural networks. We conclude with suggestions for further work in this area like more efficient maps, compartmental modeling and close dynamical comparison with conductance-based models. © 2013 Elsevier B.V.","Bursting; Coupled map lattices; Difference equations; Excitable dynamics; Excitable media; Map-based neuron; Map-based synapses; Neural networks; Neuron models","brain mapping; computer model; Hodgkin Huxley equation; ion current; mathematical model; membrane potential; molecular dynamics; nerve cell; nerve cell excitability; nerve cell network; nerve conduction; oscillator; pacemaker; presynaptic nerve; priority journal; review; Bursting; Coupled map lattices; Difference equations; Excitable dynamics; Excitable media; Map-based neuron; Map-based synapses; Neural networks; Neuron models; Action Potentials; Animals; Humans; Models, Neurological; Nerve Net; Neural Networks (Computer); Neurons",,,,,,Review,"Final","All Open Access, Green",Scopus,2-s2.0-84887618382
"Lippi M., Frasconi P.","7003730739;7003350874;","Prediction of protein β-residue contacts by Markov logic networks with grounding-specific weights",2009,"Bioinformatics","25","18",,"2326","2333",,40,"10.1093/bioinformatics/btp421","https://www.scopus.com/inward/record.uri?eid=2-s2.0-69849084410&doi=10.1093%2fbioinformatics%2fbtp421&partnerID=40&md5=4ebb88a6c5957e9483062e29e4b7f469","Motivation: Accurate prediction of contacts between β-strand residues can significantly contribute towards ab initio prediction of the 3D structure of many proteins. Contacts in the same protein are highly interdependent. Therefore, significant improvements can be expected by applying statistical relational learners that overcome the usual machine learning assumption that examples are independent and identically distributed. Furthermore, the dependencies among β-residue contacts are subject to strong regularities, many of which are known a priori. In this article, we take advantage of Markov logic, a statistical relational learning framework that is able to capture dependencies between contacts, and constrain the solution according to domain knowledge expressed by means of weighted rules in a logical language. Results: We introduce a novel hybrid architecture based on neural and Markov logic networks with grounding-specific weights. On a non-redundant dataset, our method achieves 44.9% F1 measure, with 47.3% precision and 42.7% recall, which is significantly better (P &lt; 0.01) than previously reported performance obtained by 2D recursive neural networks. Our approach also significantly improves the number of chains for which β-strands are nearly perfectly paired (36% of the chains are predicted with F1 ≥ 70% on coarse map). It also outperforms more general contact predictors on recent CASP 2008 targets. © The Author 2009. Published by Oxford University Press. All rights reserved.",,"ab initio calculation; accuracy; article; artificial neural network; beta sheet; classifier; computer prediction; controlled study; hidden Markov model; machine learning; priority journal; protein secondary structure; structural proteomics; Computational Biology; Databases, Protein; Markov Chains; Neural Networks (Computer); Protein Conformation; Proteins",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-69849084410
"Blunsom P., Kocik K., Curran J.R.","15043692000;15044608000;14628332800;","Question classification with log-linear models",2006,"Proceedings of the Twenty-Ninth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","2006",,,"615","616",,40,"10.1145/1148170.1148282","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33750376143&doi=10.1145%2f1148170.1148282&partnerID=40&md5=4973c779dab8ee59bf61cd4de2a244fc","Question classification has become a crucial step in modern question answering systems. Previous work has demonstrated the effectiveness of statistical machine learning approaches to this problem. This paper presents a new approach to building a question classifier using log-linear models. Evidence from a rich and diverse set of syntactic and semantic features is evaluated, as well as approaches which exploit the hierarchical structure of the question classes.","Machine Learning; Maximum entropy; Question Answering; Question Classification","Entropy; Learning systems; Mathematical models; Optimization; Statistical methods; Hierarchical systems; Learning systems; Linear systems; Mathematical models; Problem solving; Semantics; Question Answering; Question Classification; Query languages; Query processing; Maximum entropy; Question answering; Question classification","ACM SIGIR","29th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval","6 August 2006 through 11 August 2006","Seatttle, WA",68395,Conference Paper,"Final","",Scopus,2-s2.0-33750376143
"Sha L., Chang B., Sui Z., Li S.","56441043200;55837183200;23091994600;53984734300;","Reading and thinking: Re-read LSTM unit for textual entailment recognition",2016,"COLING 2016 - 26th International Conference on Computational Linguistics, Proceedings of COLING 2016: Technical Papers",,,,"2870","2879",,39,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85031900378&partnerID=40&md5=b3ec0a5fa228533c1c379779b081181f","Recognizing Textual Entailment (RTE) is a fundamentally important task in natural language processing that has many applications. The recently released Stanford Natural Language Inference (SNLI1) corpus has made it possible to develop and evaluate deep neural network methods for the RTE task. Previous neural network based methods usually try to encode the two sentences and send them together into multi-layer perceptron, or use LSTM-RNN to link two sentence together while using attention mechanic to enhance the model's ability. In this paper, we propose to use the intensive reading mechanic, which means to re-read the sentence (read the sentence again) according to the memory of the other sentence for a better understanding of the sentence pair. The re-read process can be applied alternatively between the two sentences. Experiments show that we achieve results better than current state-of-art equivalents. © 1963-2018 ACL.",,"Computational linguistics; Deep neural networks; Natural language processing systems; Text processing; Multi layer perceptron; Natural languages; Neural network method; Recognizing textual entailments; Stanford; Textual entailment; Long short-term memory",,"26th International Conference on Computational Linguistics, COLING 2016","11 December 2016 through 16 December 2016",,136517,Conference Paper,"Final","",Scopus,2-s2.0-85031900378
"Qian M., Zhai C.","35146579100;35232046000;","Unsupervised feature selection for multi-view clustering on text-image web news data",2014,"CIKM 2014 - Proceedings of the 2014 ACM International Conference on Information and Knowledge Management",,,,"1963","1966",,39,"10.1145/2661829.2661993","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937622297&doi=10.1145%2f2661829.2661993&partnerID=40&md5=faaceb3ac7952ce9c2dfb3861d1c6266","Unlabeled high-dimensional text-image web news data are produced every day, presenting new challenges to unsuper-vised feature selection on multi-view data. State-of-the-art multi-view unsupervised feature selection methods learn pseudo class labels by spectral analysis, which is sensitive to the choice of similarity metric for each view. For textimage data, the raw text itself contains more discriminative information than similarity graph which loses information during construction, and thus the text feature can be directly used for label learning, avoiding information loss as in spectral analysis. We propose a new multi-view unsupervised feature selection method in which image local learning regularized orthogonal nonnegative matrix factorization is used to learn pseudo labels and simultaneously robust joint l2,1-norm minimization is performed to select discriminative features. Cross-view consensus on pseudo labels can be obtained as much as possible. We systematically evaluate the proposed method in multi-view textimage web news datasets. Our extensive experiments on web news datasets crawled from two major US media channels: CNN and FOXNews demonstrate the efficacy of the new method over state-of-the-art multi-view and single-view unsupervised feature selection methods. Copyright 2014 ACM.","Multi-view unsupervised feature selection","Factorization; Knowledge management; Matrix algebra; Spectrum analysis; 1-norm minimizations; Discriminative features; High-dimensional; Multi-view clustering; Nonnegative matrix factorization; Similarity metrics; State of the art; Unsupervised feature selection; Feature extraction","ACM SIGWEB;Baidu;et al.;Google;SAP;Special Interest Group on Information Retrieval (ACM SIGIR)","23rd ACM International Conference on Information and Knowledge Management, CIKM 2014","3 November 2014 through 7 November 2014",,109104,Conference Paper,"Final","",Scopus,2-s2.0-84937622297
"Kraskov A., Grassberger P.","6602907373;7005432814;","MIC: Mutual information based hierarchical clustering",2009,"Information Theory and Statistical Learning",,,,"101","123",,39,"10.1007/978-0-387-84816-7_5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84891479270&doi=10.1007%2f978-0-387-84816-7_5&partnerID=40&md5=0a5da0c065963af3dc61ccbe289103e4","Clustering is a concept used in a huge variety of applications. We review a conceptually very simple algorithm for hierarchical clustering called in the following the mutual information clustering (MIC) algorithm. It uses mutual information (MI) as a similarity measure and exploits its grouping property: The MI between three objects X,Y, and Z is equal to the sum of the MI between X and Y, plus the MI between Z and the combined object (XY). We use MIC both in the Shannon (probabilistic) version of information theory, where the objects are probability distributions represented by random samples, and in the Kolmogorov (algorithmic) version, where the objects are symbol sequences. We apply our method to the construction of phylogenetic trees from mitochondrial DNA sequences and we reconstruct the fetal ECG from the output of independent components analysis (ICA) applied to the ECG of a pregnant woman. © 2009 Springer US.",,,,,,,,Book Chapter,"Final","All Open Access, Green",Scopus,2-s2.0-84891479270
"Smolensky P., Legendre G.","",[No title available],2006,"The Harmonic Mind",,,,"","",,39,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-34250883940
"Song L., Zhang Y., Wang Z., Gildea D.","57198355269;56066648800;53664832900;6603603942;","N-ary relation extraction using graph state LSTM",2020,"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018",,,,"2226","2235",,38,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85078670482&partnerID=40&md5=302e0dd35677a24d6dc7d294ad2acb26","Cross-sentence n-ary relation extraction detects relations among n entities across multiple sentences. Typical methods formulate an input as a document graph, integrating various intra-sentential and inter-sentential dependencies. The current state-of-the-art method splits the input graph into two DAGs, adopting a DAG-structured LSTM for each. Though being able to model rich linguistic knowledge by leveraging graph edges, important information can be lost in the splitting procedure. We propose a graph-state LSTM model, which uses a parallel state to model each word, recurrently enriching state values via message passing. Compared with DAG LSTMs, our graph LSTM keeps the original graph structure, and speeds up computation by allowing more parallelization. On a standard benchmark, our model shows the best result in the literature. © 2018 Association for Computational Linguistics",,"Extraction; Graph structures; Knowledge management; Message passing; Natural language processing systems; Graph edges; Graph state; Input graphs; Linguistic knowledge; Parallelizations; Relation extraction; State values; State-of-the-art methods; Long short-term memory","Apple;Bloomberg;et al.;Facebook;Google;salesforce","2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018","31 October 2018 through 4 November 2018",,158085,Conference Paper,"Final","",Scopus,2-s2.0-85078670482
"Dodge J., Gane A., Zhang X., Bordes A., Chopra S., Miller A.H., Szlam A., Weston J.","55365971500;57076664300;57221514997;18933923000;56248489500;57210582956;11539079600;8865128200;","Evaluating prerequisite qualities for learning end-to-end dialog systems",2016,"4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings",,,,"","",,38,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083950683&partnerID=40&md5=6bdff3b9d4da92a407d90ed72cbc2804","A long-term goal of machine learning is to build intelligent conversational agents. One recent popular approach is to train end-to-end models on a large amount of real dialog transcripts between humans (Sordoni et al., 2015; Vinyals & Le, 2015; Shang et al., 2015). However, this approach leaves many questions unanswered as an understanding of the precise successes and shortcomings of each model is hard to assess. A contrasting recent proposal are the bAbI tasks (Weston et al., 2015b) which are synthetic data that measure the ability of learning machines at various reasoning tasks over toy language. Unfortunately, those tests are very small and hence may encourage methods that do not scale. In this work, we propose a suite of new tasks of a much larger scale that attempt to bridge the gap between the two regimes. Choosing the domain of movies, we provide tasks that test the ability of models to answer factual questions (utilizing OMDB), provide personalization (utilizing MovieLens), carry short conversations about the two, and finally to perform on natural dialogs from Reddit. We provide a dataset covering ∼75k movie entities and with ∼3.5M training examples. We present results of various models on these tasks, and evaluate their performance. © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.",,"Conversational agents; End-to-end models; Learning machines; Long-term goals; Personalizations; Reasoning tasks; Synthetic data; Training example; Machine learning",,"4th International Conference on Learning Representations, ICLR 2016","2 May 2016 through 4 May 2016",,149803,Conference Paper,"Final","",Scopus,2-s2.0-85083950683
"Raza M., Gulwani S., Milic-Frayling N.","56394011400;55901318200;55663600700;","Compositional program synthesis from natural language and examples",2015,"IJCAI International Joint Conference on Artificial Intelligence","2015-January",,,"792","800",,38,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949750713&partnerID=40&md5=28f9ed4b409aef30fe5fe0f015437f73","Compositionality is a fundamental notion in computation whereby complex abstractions can be constructed from simpler ones, yet this property has so far escaped the paradigm of end-user programming from examples or natural language. Existing approaches restrict end users to only give holistic specifications of tasks, which limits the expressivity and scalability of these approaches to relatively simple programs in very restricted domains. In this paper we propose Compositional Program Synthesis (CPS): an approach in which tasks can be specified in a compositional manner through a combination of natural language and examples. We present a domain-agnostic program synthesis algorithm and demonstrate its application to an expressive string manipulation language. We evaluate our approach on complex tasks from online help forums that are beyond the scope of current state-of-the-art methods.",,"Artificial intelligence; Computational linguistics; Computer programming; Synthesis (chemical); Compositionality; Domain agnostics; End user programming; ITS applications; Natural languages; Program synthesis; Restricted-domain; State-of-the-art methods; Application programs","Alibaba.com;Department of Computer Science and Engineering at Universidad Nacional del Sur;Department of Computer Science at the School of Exact and Natural Sciences of Buenos Aires University;et al.;International Joint Conferences on Artificial Intelligence (IJCAI);Ministry of Science, Technology and Productive Innovation","24th International Joint Conference on Artificial Intelligence, IJCAI 2015","25 July 2015 through 31 July 2015",,116754,Conference Paper,"Final","",Scopus,2-s2.0-84949750713
"Shavlik J.W., Towell G.G.","7004146387;6506442915;","An Approach to Combining Explanation-based and Neural Learning Algorithms",1989,"Connection Science","1","3",,"231","253",,38,"10.1080/09540098908915640","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945789128&doi=10.1080%2f09540098908915640&partnerID=40&md5=376d047331616780e6cb8e351d9731ab","Machine learning is an area where both symbolic and neural approaches to artificial intelligence have been heavily investigated. However, there has been little research into the synergies achievable by combining these two learning paradigms. A hybrid system that combines the symbolically-oriented explanation-based learning paradigm with the neural backpropagation algorithm is described. In the presented ebl-ann algorithm, the initial neural network configuration is determined by the generalized explanation of the solution to a specific classification task. This approach overcomes problems that arise when using imperfect theories to build explanations and addresses the problem of choosing a good initial neural network configuration. Empirical results show that the hybrid system more accurately learns a concept than the explanation-based system by itself and learns faster and generalizes better than the neural learning system by itself. © 1989, Taylor & Francis Group, LLC. All rights reserved.","connectionism; Explanation-based learning; hybrid machine learning systems; imperfect domain theories; neural network topologies; neural networks; symbolic systems",,,,,,,Article,"Final","",Scopus,2-s2.0-84945789128
"Rocktäschel T., Bosnjak M., Singh S., Riedel S.","","Low-dimensional embeddings of logic",0000,"Proceedings of the ACL 2014 Workshop on Semantic Parsing, 2014",,,,"45","49",,38,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84937214491
"Moghe N., Arora S., Banerjee S., Khapra M.M.","57215716985;57211289575;57215721689;36028388700;","Towards exploiting background knowledge for building conversation systems",2020,"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018",,,,"2322","2332",,37,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071146068&partnerID=40&md5=5ec58fb873f504380543dbdc3b2e5987","Existing dialog datasets contain a sequence of utterances and responses without any explicit background knowledge associated with them. This has resulted in the development of models which treat conversation as a sequence-to-sequence generation task (i.e., given a sequence of utterances generate the response sequence). This is not only an overly simplistic view of conversation but it is also emphatically different from the way humans converse by heavily relying on their background knowledge about the topic (as opposed to simply relying on the previous sequence of utterances). For example, it is common for humans to (involuntarily) produce utterances which are copied or suitably modified from background articles they have read about the topic. To facilitate the development of such natural conversation models which mimic the human process of conversing, we create a new dataset containing movie chats wherein each response is explicitly generated by copying and/or modifying sentences from unstructured background knowledge such as plots, comments and reviews about the movie. We establish baseline results on this dataset (90K utterances from 9K conversations) using three different models: (i) pure generation based models which ignore the background knowledge (ii) generation based models which learn to copy information from the background knowledge when required and (iii) span prediction based models which predict the appropriate response span in the background knowledge. © 2018 Association for Computational Linguistics",,"Back-ground knowledge; Baseline results; Conversation systems; Prediction-based; Sequence generation; Natural language processing systems","Apple;Bloomberg;et al.;Facebook;Google;salesforce","2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018","31 October 2018 through 4 November 2018",,158085,Conference Paper,"Final","",Scopus,2-s2.0-85071146068
"Diligenti M., Gori M., Maggini M., Rigutini L.","55972544500;7005254436;7004693296;14625509300;","Bridging logic and kernel machines",2012,"Machine Learning","86","1",,"57","88",,37,"10.1007/s10994-011-5243-x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84855713701&doi=10.1007%2fs10994-011-5243-x&partnerID=40&md5=4538437a6d83a80412bb8bc5aa2b989a","We propose a general framework to incorporate first-order logic (FOL) clauses, that are thought of as an abstract and partial representation of the environment, into kernel machines that learn within a semi-supervised scheme. We rely on a multi-task learning scheme where each task is associated with a unary predicate defined on the feature space, while higher level abstract representations consist of FOL clauses made of those predicates. We re-use the kernel machine mathematical apparatus to solve the problem as primal optimization of a function composed of the loss on the supervised examples, the regularization term, and a penalty term deriving from forcing real-valued constraints deriving from the predicates. Unlike for classic kernel machines, however, depending on the logic clauses, the overall function to be optimized is not convex anymore. An important contribution is to show that while tackling the optimization by classic numerical schemes is likely to be hopeless, a stage-based learning scheme, in which we start learning the supervised examples until convergence is reached, and then continue by forcing the logic clauses is a viable direction to attack the problem. Some promising experimental results are given on artificial learning tasks and on the automatic tagging of bibtex entries to emphasize the comparison with plain kernel machines. © 2011 The Author(s).","First-order logic; Kernel machines; Learning from constraints; Learning with prior knowledge; Multi-task learning; Semantic-based regularization","First order logic; Kernel machine; Learning from constraints; Multitask learning; Prior knowledge; Semantic-based regularization; Abstracting; Convergence of numerical methods; Formal logic; Functions; Optimization; Semantics; Inductive logic programming (ILP)",,,,,,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-84855713701
"Yao Y., Ye D., Li P., Han X., Lin Y., Liu Z., Liu Z., Huang L., Zhou J., Sun M.","57212183905;57204292315;57211755481;57205548124;57155321900;57194902552;57191691341;57216618570;57211746430;7403180987;","Docred: A large-scale document-level relation extraction dataset",2020,"ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",,,,"764","777",,36,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081561313&partnerID=40&md5=26e8f0cb3ae33a25e691f011cad30378","Multiple entities in a document generally exhibit complex inter-sentence relations, and cannot be well handled by existing relation extraction (RE) methods that typically focus on extracting intra-sentence relations for single entity pairs. In order to accelerate the research on document-level RE, we introduce DocRED, a new dataset constructed from Wikipedia and Wikidata with three features: (1) DocRED annotates both named entities and relations, and is the largest human-annotated dataset for document-level RE from plain text; (2) DocRED requires reading multiple sentences in a document to extract entities and infer their relations by synthesizing all information of the document; (3) along with the human-annotated data, we also offer large-scale distantly supervised data, which enables DocRED to be adopted for both supervised and weakly supervised scenarios. In order to verify the challenges of document-level RE, we implement recent state-of-the-art methods for RE and conduct a thorough evaluation of these methods on DocRED. Empirical results show that DocRED is challenging for existing RE methods, which indicates that document-level RE remains an open problem and requires further efforts. Based on the detailed analysis on the experiments, we discuss multiple promising directions for future research. We make DocRED and the code for our baselines publicly available at https://github.com/thunlp/DocRED. © 2019 Association for Computational Linguistics",,"Computational linguistics; Extraction; Named entities; Plain text; Recent state; Relation extraction; Wikipedia; Large dataset","Apple;ASAPP;Bloomberg Engineering;BOSCH;et al.;Expedia","57th Annual Meeting of the Association for Computational Linguistics, ACL 2019","28 July 2019 through 2 August 2019",,159206,Conference Paper,"Final","",Scopus,2-s2.0-85081561313
"Neelakantan A., Roth B., McCallum A.","57159776800;36165172500;7003773569;","Compositional vector space models for knowledge base inference",2015,"AAAI Spring Symposium - Technical Report","SS-15-03",,,"31","34",,36,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84987623391&partnerID=40&md5=a619852acb605e693c9b7d16108cd7a3","Traditional approaches to knowledge base completion have been based on symbolic representations. Low-dimensional vector embedding models proposed recently for this task are attractive since they generalize to possibly unlimited sets of relations. A significant drawback of previous embedding models for KB completion is that they merely support reasoning on individual relations (e.g. bomlri(X. Y) ⇒ nationality(X. Y)). In this work, we develop models for KB completion that support chains of reasoning on paths of any length using compositional vector space models. We construct compositional vector representations for the paths in the KB graph from the semantic vector representations of the binary relations in that path and perform inference directly in the vector space. Unlike previous methods, our approach can generalize to paths that are unseen in training and. in a zero-shot setting, predict target relations without supervised training data for that relation. Copyright © 2015. Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Embeddings; Knowledge based systems; Knowledge representation; Semantics; Vectors; Binary relation; Low dimensional; Semantic vectors; Supervised trainings; Symbolic representation; Traditional approaches; Vector representations; Vector space models; Vector spaces",,"2015 AAAI Spring Symposium","23 March 2015 through 25 March 2015",,113922,Conference Paper,"Final","",Scopus,2-s2.0-84987623391
"Beim Graben P., Potthast R.","23110284000;56597775000;","Inverse problems in dynamic cognitive modeling",2009,"Chaos","19","1","015103","","",,36,"10.1063/1.3097067","https://www.scopus.com/inward/record.uri?eid=2-s2.0-63849262991&doi=10.1063%2f1.3097067&partnerID=40&md5=4fa0f3ef4a5a9217177abc635ad2977d","Inverse problems for dynamical system models of cognitive processes comprise the determination of synaptic weight matrices or kernel functions for neural networks or neural/dynamic field models, respectively. We introduce dynamic cognitive modeling as a three tier top-down approach where cognitive processes are first described as algorithms that operate on complex symbolic data structures. Second, symbolic expressions and operations are represented by states and transformations in abstract vector spaces. Third, prescribed trajectories through representation space are implemented in neurodynamical systems. We discuss the Amari equation for a neural/dynamic field theory as a special case and show that the kernel construction problem is particularly ill-posed. We suggest a Tikhonov-Hebbian learning method as regularization technique and demonstrate its validity and robustness for basic examples of cognitive computations. © 2009 American Institute of Physics.",,"algorithm; animal; artificial neural network; biological model; cognition; fractal analysis; human; memory; nerve cell; nerve cell network; neuroscience; nonlinear system; physiology; procedures; theoretical model; Algorithms; Animals; Cognition; Fractals; Humans; Memory; Models, Biological; Models, Theoretical; Nerve Net; Neural Networks (Computer); Neurons; Neurosciences; Nonlinear Dynamics",,,,,,Article,"Final","",Scopus,2-s2.0-63849262991
"Bhargava H., Power D.J.","","Decision support systems and web technologies: A status report",2001,"Proceedings of the 2001 Americas Conference on Information Systems",,,,"","",,36,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-1442287203
"Lewis R.L.","","Reanalysis and limited repair parsing: Leaping off the garden path",1998,"Reanalysis in Sentence Processing",,,,"247","286",,36,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0008662253
"Tharwat A.","56065337100;","Parameter investigation of support vector machine classifier with kernel functions",2019,"Knowledge and Information Systems","61","3",,"1269","1302",,35,"10.1007/s10115-019-01335-4","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060920705&doi=10.1007%2fs10115-019-01335-4&partnerID=40&md5=ad2f4ade049e21d1cb043e31b5b4e042","Support vector machine (SVM) is one of the well-known learning algorithms for classification and regression problems. SVM parameters such as kernel parameters and penalty parameter have a great influence on the complexity and performance of predicting models. Hence, the model selection in SVM involves the penalty parameter and kernel parameters. However, these parameters are usually selected and used as a black box, without understanding the internal details. In this paper, the behavior of the SVM classifier is analyzed when these parameters take different values. This analysis consists of illustrative examples, visualization, and mathematical and geometrical interpretations with the aim of providing the basics of kernel functions with SVM and to show how it works to serve as a comprehensive source for researchers who are interested in this field. This paper starts by highlighting the definition and underlying principles of SVM in details. Moreover, different kernel functions are introduced and the impact of each parameter in these kernel functions is explained from different perspectives. © 2019, Springer-Verlag London Ltd., part of Springer Nature.","Gaussian kernel; Kernel functions; Linear kernel; Parameter optimization; Polynomial kernel; Radial basis function; Support vector machine (SVM)","Functions; Learning algorithms; Support vector regression; Gaussian kernels; Kernel function; Linear kernel; Parameter optimization; Polynomial kernels; Radial basis functions; Support vector machines",,,,,,Article,"Final","",Scopus,2-s2.0-85060920705
"Pourjavad E., Mayorga R.V.","55308024000;7004283458;","A comparative study and measuring performance of manufacturing systems with Mamdani fuzzy inference system",2019,"Journal of Intelligent Manufacturing","30","3",,"1085","1097",,35,"10.1007/s10845-017-1307-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013223151&doi=10.1007%2fs10845-017-1307-5&partnerID=40&md5=ac4aac796b31ab3747838856abfc0eab","In today’s competitive environment, measuring companies’ performance properly has become a vital subject not only for investors but also for the companies that are working in the same sector. The achieved results of performance measurement can help managers to identify means of improvement, measure progress and find unknown problems in the company. There are many efficiency frontier analysis methods to evaluate performance; but, each of these methods has its strength as well as major limitations. In this article, a fuzzy approach based on Mamdani fuzzy inference system is presented for performance measurement of manufacturing systems. The generation of fuzzy rules is the biggest consideration in designing the proposed model. In fact, fuzzy inference rules model human reasoning and are embedded in the system, which is an advantage when compared to approaches that combine fuzzy set theory with multi-criteria decision-making methods. A fuzzy inference system is constructed and applied to measure the performance or efficiency of manufacturing systems. Implementation of the proposed model is analyzed and discussed using a real case. The results reveal the usefulness of the proposed model in evaluating the performance of manufacturing companies. © 2017, Springer Science+Business Media New York.","Criteria; Efficiency; Fuzzy inference system; Manufacturing systems; Performance measurement","Decision making; Decision theory; Efficiency; Fuzzy set theory; Fuzzy systems; Manufacture; Competitive environment; Criteria; Fuzzy inference rules; Fuzzy inference systems; Mamdani fuzzy inferences; Manufacturing companies; Multi-criteria decision making methods; Performance measurements; Fuzzy inference",,,,,,Article,"Final","",Scopus,2-s2.0-85013223151
"Campos R., Mangaravite V., Pasquali A., Jorge A.M., Nunes C., Jatowt A.","15130810800;55560704600;56500181000;55938897400;57194580125;14826985000;","A text feature based automatic keyword extraction method for single documents",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10772 LNCS",,,"684","691",,35,"10.1007/978-3-319-76941-7_63","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044470670&doi=10.1007%2f978-3-319-76941-7_63&partnerID=40&md5=8f546b9e8ee095d6753849d20e2fa067","In this work, we propose a lightweight approach for keyword extraction and ranking based on an unsupervised methodology to select the most important keywords of a single document. To understand the merits of our proposal, we compare it against RAKE, TextRank and SingleRank methods (three well-known unsupervised approaches) and the baseline TF.IDF, over four different collections to illustrate the generality of our approach. The experimental results suggest that extracting keywords from documents using our method results in a superior effectiveness when compared to similar approaches. © Springer International Publishing AG, part of Springer Nature 2018.","Feature extraction; Information extraction; Keyword extraction","Artificial intelligence; Computer science; Computers; Feature extraction; Keyword extraction; Text feature; Unsupervised approaches; Information retrieval","etal.;Google;Grenoble Alpes Data Institute;Grenoble INP;Naver Labs;Universite Grenoble Alpes","40th European Conference on Information Retrieval, ECIR 2018","26 March 2018 through 29 March 2018",,212079,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85044470670
"Liu H., Cocea M.","44361194700;23090077000;","Fuzzy rule based systems for interpretable sentiment analysis",2017,"9th International Conference on Advanced Computational Intelligence, ICACI 2017",,,"7974497","129","136",,35,"10.1109/ICACI.2017.7974497","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027447753&doi=10.1109%2fICACI.2017.7974497&partnerID=40&md5=18b0983d52e21af8a3b05857d5d2766c","Sentiment analysis, which is also known as opinion mining, aims to recognise the attitude or emotion of people through natural language processing, text analysis and computational linguistics. In recent years, many studies have focused on sentiment classification in the context of machine learning, e.g. to identify that a sentiment is positive or negative. In particular, the bag-of-words method has been popularly used to transform textual data into structured data, in order to enable the direct use of machine learning algorithms for sentiment classification. Through the bag-of-words method, each single term in a text document is turned into a single attribute to make up a structured data set, which results in high dimensionality of the data set and thus negative impact on the interpretability of computational models for sentiment analysis. This paper proposes the use of fuzzy rule based systems as computational models towards accurate and interpretable analysis of sentiments. The use of fuzzy logic is better aligned with the inherent uncertainty of language, while the 'white box' characteristic of the rule based learning approaches leads to better interpretability of the results. The proposed approach is tested on four datasets containing movie reviews; the aim is to compare its performance in terms of accuracy with two other approaches for sentiment analysis that are known to perform very well. The results indicate that the fuzzy rule based approach performs marginally better than the well-known machine learning techniques, while reducing the computational complexity and increasing the interpretability. © 2017 IEEE.","data mining; fuzzy rule based systems; machine learning; sentiment analysis; text classification","Artificial intelligence; Character recognition; Classification (of information); Computation theory; Computational methods; Fuzzy inference; Fuzzy logic; Fuzzy rules; Learning algorithms; Learning systems; Natural language processing systems; Text processing; Computational model; High dimensionality; Machine learning techniques; Rule-based learning; Sentiment analysis; Sentiment classification; Text classification; Use of fuzzy logic; Data mining","Texas A and M University at Qatar (ATM)","9th International Conference on Advanced Computational Intelligence, ICACI 2017","4 February 2017 through 6 February 2017",,129102,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85027447753
"Zhong X., Sun A., Cambria E.","57200334013;7202552214;56140547500;","Time expression analysis and recognition using syntactic token types and general heuristic rules",2017,"ACL 2017 - 55th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)","1",,,"420","429",,35,"10.18653/v1/P17-1039","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85038556598&doi=10.18653%2fv1%2fP17-1039&partnerID=40&md5=5aa093022b8f9a768f71ba6763dc4e9b","Extracting time expressions from free text is a fundamental task for many applications. We analyze time expressions from four different datasets and find that only a small group of words are used to express time information and that the words in time expressions demonstrate similar syntactic behaviour. Based on the findings, we propose a type-based approach named SynTime1 for time expression recognition. Specifically, we define three main syntactic token types, namely time token, modifier, and numeral, to group time-related token regular expressions. On the types we design general heuristic rules to recognize time expressions. In recognition, SynTime first identifies time tokens from raw text, then searches their surroundings for modifiers and numerals to form time segments, and finally merges the time segments to time expressions. As a lightweight rule-based tagger, SynTime runs in real time, and can be easily expanded by simply adding keywords for the text from different domains and different text types. Experiments on benchmark datasets and tweets data show that SynTime outperforms state-of-the-art methods. © 2017 Association for Computational Linguistics.",,"Computational linguistics; Linguistics; Syntactics; Benchmark datasets; Different domains; Expression analysis; Expression recognition; Heuristic rules; Regular expressions; State-of-the-art methods; Time information; Character recognition","Amazon;Apple;Baidu;et al.;Google;Tencent","55th Annual Meeting of the Association for Computational Linguistics, ACL 2017","30 July 2017 through 4 August 2017",,132950,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85038556598
"Peng H., Li J., Song Y., Liu Y.","57190014707;55720560100;14039604300;57194433649;","Incrementally learning the hierarchical softmax function for neural language models",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"3267","3273",,35,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028599250&partnerID=40&md5=6cafc5cd82372fd26f5a06a91411056b","Neural network language models (NNLMs) have attracted a lot of attention recently. In this paper, we present a training method that can incrementally train the hierarchical softmax function for NNMLs. We split the cost function to model old and update corpora separately, and factorize the objective function for the hierarchical softmax. Then we provide a new stochastic gradient based method to update all the word vectors and parameters, by comparing the old tree generated based on the old corpus and the new tree generated based on the combined (old and update) corpus. Theoretical analysis shows that the mean square error of the parameter vectors can be bounded by a function of the number of changed words related to the parameter node. Experimental results show that incremental training can save a lot of time. The smaller the update corpus is, the faster the update training process is, where an up to 30 times speedup has been achieved. We also use both word similarity/relatedness tasks and dependency parsing task as our benchmarks to evaluate the correctness of the updated word vectors. © Copyright 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Cost functions; Forestry; Mean square error; Stochastic systems; Dependency parsing; Incremental training; Network language; Objective functions; Parameter vectors; Stochastic gradient; Training methods; Training process; Computational linguistics","Amazon;Artificial Intelligence;Baidu;et al.;IBM;Tencent","31st AAAI Conference on Artificial Intelligence, AAAI 2017","4 February 2017 through 10 February 2017",,130407,Conference Paper,"Final","",Scopus,2-s2.0-85028599250
"Fraihat S., Shambour Q.","","A framework of semantic recommender system for e-learning",2015,"Journal of Software","10","3",,"317","330",,35,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85006973391
"Peng B., Lu Z., Li H., Wong K.-F.","","Towards neural network-based reasoning",2015,"Towards Neural Network-Based Reasoning",,,,"","",,35,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84965134124
"Tang L.R., Mooney R.J.","","Automated construction of database interfaces: Integrating statistical and relational learning for semantic parsing",2000,"Joint Conference on Empirical Methods in Natural Language Processing and Very Large Corpora",,,,"133","141",,35,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-22344445654
"Tabor W.","6603814753;","Fractal encoding of context-free grammars in connectionist networks",2000,"Expert Systems","17","1",,"41","56",,35,"10.1111/1468-0394.00126","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0033743045&doi=10.1111%2f1468-0394.00126&partnerID=40&md5=135b831a82e348a0bb2ee7c801fd8e46","Connectionist network learning of context-free languages has so far been applied only to very simple cases and has often made use of an external stack. Learning complex context-free languages with a homogeneous neural mechanism looks like a much harder problem. The current paper takes a step toward solving this problem by analyzing context-free grammar computation (without addressing learning) in a class of analog computers called dynamical automata, which are naturally implemented in connectionist networks. The result is a widely applicable method of using fractal sets to organize infinite-state computations in a bounded state space. An appealing consequence is the development of parameter-space maps, which locate various complex computers in spatial relationships to one another. An example suggests that such a global perspective on the organization of the parameter space may be helpful for solving the hard problem of getting connectionist networks to learn complex grammars from examples.",,"Automata theory; Computational complexity; Context free grammars; Context free languages; Encoding (symbols); Fractals; Learning systems; Connectionist networks; Neural networks",,,,,,Article,"Final","",Scopus,2-s2.0-0033743045
"Johnson D.D.","57197154237;","Learning graphical state transitions",2017,"5th International Conference on Learning Representations, ICLR 2017 - Conference Track Proceedings",,,,"","",,34,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084086160&partnerID=40&md5=8ff2c805751e89a1c21698e3e2c5b5ab","Graph-structured data is important in modeling relationships between multiple entities, and can be used to represent states of the world as well as many data structures. Li et al. (2016) describe a model known as a Gated Graph Sequence Neural Network (GGS-NN) that produces sequences from graph-structured input. In this work I introduce the Gated Graph Transformer Neural Network (GGT-NN), an extension of GGS-NNs that uses graph-structured data as an intermediate representation. The model can learn to construct and modify graphs in sophisticated ways based on textual input, and also to use the graphs to produce a variety of outputs. For example, the model successfully learns to solve almost all of the bAbI tasks (Weston et al., 2016), and also discovers the rules governing graphical formulations of a simple cellular automaton and a family of Turing machines. © ICLR 2019 - Conference Track Proceedings. All rights reserved.",,"Graphic methods; Graph sequences; Graph structured data; Intermediate representations; Model relationships; State transitions; Turing machines",,"5th International Conference on Learning Representations, ICLR 2017","24 April 2017 through 26 April 2017",,149804,Conference Paper,"Final","",Scopus,2-s2.0-85084086160
"Le B., Nguyen H.","27267916200;57210949828;","Twitter sentiment analysis using machine learning techniques",2015,"Advances in Intelligent Systems and Computing","358",,,"279","289",,34,"10.1007/978-3-319-17996-4_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942586533&doi=10.1007%2f978-3-319-17996-4_25&partnerID=40&md5=cafc360be5becc18d0bc6244eb5dc648","Twitter is a microblogging site in which users can post updates (tweets) to friends (followers). It has become an immense dataset of the so-called sentiments. In this paper, we introduce an approach to selection of a new feature set based on Information Gain, Bigram, Objectoriented extraction methods in sentiment analysis on social networking side. In addition, we also proposes a sentiment analysis model based on Naive Bayes and Support Vector Machine. Its purpose is to analyze sentiment more effectively. This model proved to be highly effective and accurate on the analysis of feelings. © Springer International Publishing Switzerland 2015.","Sentiment analysis; Sentiment classification; Twitter","Artificial intelligence; Data mining; Social networking (online); Extraction method; Information gain; Machine learning techniques; Microblogging; Object oriented; Sentiment analysis; Sentiment classification; Twitter; Learning systems","Budapest University of Technology and economics;et al;Laboratory of Theoretical and Applied Computer Science;Springer;University of Lorraine;Wroclaw University of Technology","3rd International Conference on Computer Science, Applied Mathematics and Applications, ICCSAMA 2015","11 May 2015 through 13 May 2015",,142589,Conference Paper,"Final","",Scopus,2-s2.0-84942586533
"Menon A.K., Elkan C.","36179673700;6701846660;","Fast algorithms for approximating the singular value decomposition",2011,"ACM Transactions on Knowledge Discovery from Data","5","2","13","","",,34,"10.1145/1921632.1921639","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79952555116&doi=10.1145%2f1921632.1921639&partnerID=40&md5=18d318d1721c12b937727b9c64b820e0","A low-rank approximation to a matrix A is a matrix with significantly smaller rank than A, and which is close to A according to some norm. Many practical applications involving the use of large matrices focus on low-rank approximations. By reducing the rank or dimensionality of the data, we reduce the complexity of analyzing the data. The singular value decomposition is the most popular low-rank matrix approximation. However, due to its expensive computational requirements, it has often been considered intractable for practical applications involving massive data. Recent developments have tried to address this problem, with several methods proposed to approximate the decomposition with better asymptotic runtime. We present an empirical study of these techniques on a variety of dense and sparse datasets. We find that a sampling approach of Drineas, Kannan and Mahoney is often, but not always, the best performing method. This method gives solutions with high accuracy much faster than classical SVD algorithms, on large sparse datasets in particular. Other modern methods, such as a recent algorithm by Rokhlin and Tygert, also offer savings compared to classical SVD algorithms. The older sampling methods of Achlioptas and McSherry are shown to sometimes take longer than classical SVD. © 2011 ACM.","Experimental evaluation; Low rank approximation; Singular value decomposition","Computational requirements; Data sets; Empirical studies; Experimental evaluation; Fast algorithms; Low rank approximation; Low rank approximations; Low-rank matrices; Massive data; matrix; Runtimes; Sampling method; Singular values; Algorithms; Singular value decomposition",,,,,,Article,"Final","",Scopus,2-s2.0-79952555116
"Beim Graben P., Gerth S., Vasishth S.","23110284000;24080037600;8615749400;","Towards dynamical system models of language-related brain potentials",2008,"Cognitive Neurodynamics","2","3",,"229","255",,34,"10.1007/s11571-008-9041-5","https://www.scopus.com/inward/record.uri?eid=2-s2.0-49249139448&doi=10.1007%2fs11571-008-9041-5&partnerID=40&md5=970c02ebb3e8a2e82b2066f945b4f6f6","Event-related brain potentials (ERP) are important neural correlates of cognitive processes. In the domain of language processing, the N400 and P600 reflect lexical-semantic integration and syntactic processing problems, respectively. We suggest an interpretation of these markers in terms of dynamical system theory and present two nonlinear dynamical models for syntactic computations where different processing strategies correspond to functionally different regions in the system's phase space. © 2008 Springer Science+Business Media B.V.","Computational psycholinguistics; Dynamical systems; Event-related brain potentials; Language processing","article; cognition; controlled study; correlation function; dynamics; event related potential; human; human experiment; language disability; language processing; linguistics; mathematical model; nerve function; nonlinear system; normal human; process optimization; space; systems theory",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-49249139448
"Li F., Zhang X., Yuan J., Zhu X.","36554500800;56245927600;56301107300;7406185137;","Classifying what-type questions by head noun tagging",2008,"Coling 2008 - 22nd International Conference on Computational Linguistics, Proceedings of the Conference","1",,,"481","488",,34,"10.3115/1599081.1599142","https://www.scopus.com/inward/record.uri?eid=2-s2.0-79956091185&doi=10.3115%2f1599081.1599142&partnerID=40&md5=a2b56d834f2409fb56c81fbd3c99ab59","Classifying what-type questions into proper semantic categories is found more challenging than classifying other types in question answering systems. In this paper, we propose to classify what-type questions by head noun tagging. The approach highlights the role of head nouns as the category discriminator of what-type questions. To reduce the semantic ambiguities of head noun, we integrate local syntactic feature, semantic feature and category dependency among adjacent nouns with Conditional Random Fields (CRFs). Experiments on standard question classification data set show that the approach achieves state-of-the-art performances. © 2008. Licensed under the Creative Commons.",,"Computational linguistics; Semantics; Conditional Random Fields(CRFs); Question answering systems; Question classification; Semantic ambiguities; Semantic category; Semantic features; State-of-the-art performance; Syntactic features; Classification (of information)",,"22nd International Conference on Computational Linguistics, Coling 2008","18 August 2008 through 22 August 2008","Manchester",86714,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-79956091185
"Mizraji E.","6701816755;","Context-dependent associations in linear distributed memories",1989,"Bulletin of Mathematical Biology","51","2",,"195","205",,34,"10.1007/BF02458441","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0024526764&doi=10.1007%2fBF02458441&partnerID=40&md5=f32230d597e5547852efb6670d10ae2b","In this article we present a method that allows conditioning of the response of a linear distributed memory to a variable context. This method requires a system of two neural networks. The first net constructs the Kronecker product between the vector input and the vector context, and the second net supports a linear associative memory. This system is easily adaptable for different goals. We analyse here its capacity for the conditional extraction of features from a complex perceptual input, its capacity to perform quasi-logical operations (for instance, of the kind of ""exclusive-or""), and its capacity to structurate a memory for temporal sequences which access is conditioned by the context. Finally, we evaluate the potential importance of the capacity to establish arbitrary contexts, for the evolution of biological cognitive systems. © 1989 Society for Mathematical Biology.",,"article; artificial intelligence; biological model; human; memory; model; nerve cell network; physiology; theoretical model; Artificial Intelligence; Human; Memory; Models, Neurological; Models, Psychological; Models, Theoretical; Nerve Net",,,,,,Article,"Final","",Scopus,2-s2.0-0024526764
"Saleena B., Srivatsa S.K.","36705331100;7003518102;","Using concept similarity in cross ontology for adaptive e-Learning systems",2015,"Journal of King Saud University - Computer and Information Sciences","27","1",,"1","12",,33,"10.1016/j.jksuci.2014.03.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929142664&doi=10.1016%2fj.jksuci.2014.03.007&partnerID=40&md5=71b7ce8696684b111282ebc30e905a28","e-Learning is one of the most preferred media of learning by the learners. The learners search the web to gather knowledge about a particular topic from the information in the repositories. Retrieval of relevant materials from a domain can be easily implemented if the information is organized and related in some way. Ontologies are a key concept that helps us to relate information for providing the more relevant lessons to the learner. This paper proposes an adaptive e-Learning system, which generates a user specific e-Learning content by comparing the concepts with more than one system using similarity measures. A cross ontology measure is defined, which consists of fuzzy domain ontology as the primary ontology and the domain expert's ontology as the secondary ontology, for the comparison process. A personalized document is provided to the user with a user profile, which includes the data obtained from the processing of the proposed method under a User score, which is obtained through the user evaluation. The results of the proposed e-Learning system under the designed cross ontology similarity measure show a significant increase in performance and accuracy under different conditions. The assessment of the comparative analysis, showed the difference in performance of our proposed method over other methods. Based on the assessment results it is proved that the proposed approach is effective over other methods. © 2014 King Saud University.","Cross ontology; e-Learning; Fuzzy domain ontology; Semantic similarity measure",,,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-84929142664
"Desroches M., Krupa M., Rodrigues S.","23990353700;7003885195;57202415938;","Inflection, canards and excitability threshold in neuronal models",2013,"Journal of Mathematical Biology","67","4",,"989","1017",,33,"10.1007/s00285-012-0576-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84883781759&doi=10.1007%2fs00285-012-0576-z&partnerID=40&md5=3b8d7eb390f2566f3608d6d07b831e58","A technique is presented, based on the differential geometry of planar curves, to evaluate the excitability threshold of neuronal models. The aim is to determine regions of the phase plane where solutions to the model equations have zero local curvature, thereby defining a zero-curvature (inflection) set that discerns between sub-threshold and spiking electrical activity. This transition can arise through a Hopf bifurcation, via the so-called canard explosion that happens in an exponentially small parameter variation, and this is typical for a large class of planar neuronal models (FitzHugh-Nagumo, reduced Hodgkin-Huxley), namely, type II neurons (resonators). This transition can also correspond to the crossing of the stable manifold of a saddle equilibrium, in the case of type I neurons (integrators). We compute inflection sets and study how well they approximate the excitability threshold of these neuron models, that is, both in the canard and in the non-canard regime, using tools from invariant manifold theory and singularity theory. With the latter, we investigate the topological changes that inflection sets undergo upon parameter variation. Finally, we show that the concept of inflection set gives a good approximation of the threshold in both the so-called resonator and integrator neuronal cases. © 2012 Springer-Verlag.","Canard solutions; Excitability; Inflection set; Integrators; Resonators; Singularity theory; Slow-fast systems; Threshold","action potential; article; biological model; human; nerve cell; physiology; Action Potentials; Humans; Models, Neurological; Neurons",,,,,,Article,"Final","",Scopus,2-s2.0-84883781759
"O'Hare N., Murdock V.","55884793700;23012639300;","Modeling locations with social media",2013,"Information Retrieval","16","1",,"30","62",,33,"10.1007/s10791-012-9195-y","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84873526618&doi=10.1007%2fs10791-012-9195-y&partnerID=40&md5=2705b72e5782f585ab65edd77cecfeed","In this paper we focus on the locations explicit and implicit in users descriptions of their surroundings. We propose a statistical language modeling approach to identifying locations in arbitrary text, and investigate several ways to estimate the models, based on the term frequency and the user frequency. The geotagged public photos in Flickr serve as a convenient ground truth. Our results show that we can predict location within a one kilometer by one kilometer cell with 17 % accuracy, and within a three kilometer radius around such a one kilometer cell with 40 % accuracy, using only a photo's tags. This is significantly better than the state of the art. Further we examine several estimation strategies that leverage the physical proximity of places, and show that for sparsely represented locations, smoothing from the immediate neighborhood improves results. We also show that estimation strategies based on user frequency are much more reliable than approaches based on the raw term frequency. © 2012 Springer Science+Business Media, LLC.","Flickr; Geographic context; Geotagging; Language models; User-generated content",,,,,,,Article,"Final","",Scopus,2-s2.0-84873526618
"Saquete E., Vicedo J.L., Martínez-Barco P., Muñoz R., Llorens H.","6602658984;6701478723;8927588100;7202035977;25723480600;","Enhancing Qa systems with complex temporal question processing capabilities",2009,"Journal of Artificial Intelligence Research","35",,,"775","811",,33,"10.1613/jair.2805","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041757317&doi=10.1613%2fjair.2805&partnerID=40&md5=1918cbe72a570a36481b07855405ccc8","This paper presents a multilayered architecture that enhances the capabilities of current QA systems and allows different types of complex questions or queries to be processed. The answers to these questions need to be gathered from factual information scattered throughout different documents. Specifically, we designed a specialized layer to process the different types of temporal questions. Complex temporal questions are first decomposed into simple questions, according to the temporal relations expressed in the original question. In the same way, the answers to the resulting simple questions are recomposed, fulfilling the temporal restrictions of the original complex question. A novel aspect of this approach resides in the decomposition which uses a minimal quantity of resources, with the final aim of obtaining a portable platform that is easily extensible to other languages. In this paper we also present a methodology for evaluation of the decomposition of the questions as well as the ability of the implemented temporal layer to perform at a multilingual level. The temporal layer was first performed for English, then evaluated and compared with: a) a general purpose QA system (F-measure 65.47% for QA plus English temporal layer vs. 38.01% for the general QA system), and b) a well-known QA system. Much better results were obtained for temporal questions with the multilayered system. This system was therefore extended to Spanish and very good results were again obtained in the evaluation (F-measure 40.36% for QA plus Spanish temporal layer vs. 22.94% for the general QA system). © 2009 AI Access Foundation. All rights reserved.",,"Petroleum reservoir evaluation; Complex questions; Factual information; Multi-layered systems; Multilayered architecture; Temporal layers; Temporal questions; Temporal relation; Temporal restrictions; Natural language processing systems",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85041757317
"Asghar M.Z., Khan A., Khan F., Kundi F.M.","56177682300;55723461700;56224297600;56178369300;","RIFT: A Rule Induction Framework for Twitter Sentiment Analysis",2018,"Arabian Journal for Science and Engineering","43","2",,"857","877",,32,"10.1007/s13369-017-2770-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040327740&doi=10.1007%2fs13369-017-2770-1&partnerID=40&md5=59a0a6a19d744ce430a57986a9867f04","The rapid evolution of microblogging and the emergence of sites such as Twitter have propelled online communities to flourish by enabling people to create, share and disseminate free-flowing messages and information globally. The exponential growth of product-based user reviews has become an ever-increasing resource playing a key role in emerging Twitter-based sentiment analysis (SA) techniques and applications to collect and analyse customer trends and reviews. Existing studies on supervised black-box sentiment analysis systems do not provide adequate information, regarding rules as to why a certain review was classified to a class or classification. The accuracy in some ways is less than our personal judgement. To address these shortcomings, alternative approaches, such as supervised white-box classification algorithms, need to be developed to improve the classification of Twitter-based microblogs. The purpose of this study was to develop a supervised white-box microblogging SA system to analyse user reviews on certain products using rough set theory (RST)-based rule induction algorithms. RST classifies microblogging reviews of products into positive, negative, or neutral class using different rules extracted from training decision tables using RST-centric rule induction algorithms. The primary focus of this study is also to perform sentiment classification of microblogs (i.e. also known as tweets) of product reviews using conventional, and RST-based rule induction algorithms. The proposed RST-centric rule induction algorithm, namely Learning from Examples Module version: 2, and LEM2 + Corpus-based rules (LEM2 + CBR),which is an extension of the traditional LEM2 algorithm, are used. Corpus-based rules are generated from tweets, which are unclassified using other conventional LEM2 algorithm rules. Experimental results show the proposed method, when compared with baseline methods, is excellent, with regard to accuracy, coverage and the number of rules employed. The approach using this method achieves an average accuracy of 92.57% and an average coverage of 100%, with an average number of rules of 19.14. © 2017, King Fahd University of Petroleum & Minerals.","Emoticons; LEM2; Rough set theory; Rule induction; Sentiment analysis; Slang; Twitter",,,,,,,Article,"Final","",Scopus,2-s2.0-85040327740
"Cohen W.W.","","Tensorlog: A differentiable deductive database",2016,"Tensorlog: A differentiable deductive database",,,,"","",,32,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85044416806
"Khemlani S.S., Barbey A.K., Johnson-Laird P.N.","24332201800;6602255046;7006769146;","Causal reasoning with mental models",2014,"Frontiers in Human Neuroscience","8","October","849","","",15,32,"10.3389/fnhum.2014.00849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84933671372&doi=10.3389%2ffnhum.2014.00849&partnerID=40&md5=79c2c04e9f64c1d0dceb5afd1b816e57","This paper outlines the model-based theory of causal reasoning. It postulates that the core meanings of causal assertions are deterministic and refer to temporally-ordered sets of possibilities: A causes B to occur means that given A, B occurs, whereas A enables B to occur means that given A, it is possible for B to occur. The paper shows how mental models represent such assertions, and how these models underlie deductive, inductive, and abductive reasoning yielding explanations. It reviews evidence both to corroborate the theory and to account for phenomena sometimes taken to be incompatible with it. Finally, it reviews neuroscience evidence indicating that mental models for causal inference are implemented within lateral prefrontal cortex. © 2014 Khemlani, Barbey and Johnson-Laird.","Causal reasoning; Enabling conditions; Explanations; Lateral prefrontal cortex; Mental models","brain damage; causal reasoning; cognition; comprehension; human; lateral prefrontal cortex; learning; mental model; model; nonhuman; prefrontal cortex; Review; thinking",,,,,,Review,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-84933671372
"Fung G.M., Mangasarian O.L., Shavlik J.W.","55630041100;35610392800;7004146387;","Knowledge-based nonlinear kernel classifiers",2003,"Lecture Notes in Artificial Intelligence (Subseries of Lecture Notes in Computer Science)","2777",,,"102","113",,32,"10.1007/978-3-540-45167-9_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-9444280785&doi=10.1007%2f978-3-540-45167-9_9&partnerID=40&md5=df4103451fd57d2f1374670389e5fd91","Prior knowledge in the form of multiple polyhedral sets, each belonging to one of two categories, is introduced into a reformulation of a nonlinear kernel support vector machine (SVM) classifier. The resulting formulation leads to a linear program that can be solved efficiently. This extends, in a rather unobvious fashion, previous work [3] that incorporated similar prior knowledge into a linear SVM classifier. Numerical tests on standard-type test problems, such as exclusive-or prior knowledge sets and a checkerboard with 16 points and prior knowledge instead of the usual 1000 points, show the effectiveness of the proposed approach in generating sharp nonlinear classifiers based mostly or totally on prior knowledge.","Linear programming; Prior knowledge; Support vector machines","Classifiers; Computer vision; Data acquisition; Linear programming; Neural networks; Nonlinear control systems; Theorem proving; Vectors; Knowledge based systems; Linear programming; Input space; Kernel classifiers; Prior knowledge; Support vector machines (SVM); Knowledge based; Linear programs; Nonlinear classifiers; Nonlinear kernels; Numerical tests; Polyhedral set; Standard type; Knowledge based systems; Support vector machines","Max Planck Institute for Biological Cybernetics;National Information and Communications Technology, Australia;University of Maryland, Institute for Advanced Computer Studies","16th Annual Conference on Learning Theory and 7th Kernel Workshop, COLT/Kernel 2003","24 August 2003 through 27 August 2003","Washington, DC",63969,Conference Paper,"Final","",Scopus,2-s2.0-9444280785
"Hinton G.E.","7006699573;","Preface to the special issue on connectionist symbol processing",1990,"Artificial Intelligence","46","1-2",,"1","4",,32,"10.1016/0004-3702(90)90002-H","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0009438133&doi=10.1016%2f0004-3702%2890%2990002-H&partnerID=40&md5=e0456a4becc2e21ac0a0d17efbcd6c6b",[No abstract available],,,,,,,,Article,"Final","",Scopus,2-s2.0-0009438133
"Dong H., Mao J., Lin T., Wang C., Li L., Zhou D.","57210645677;57201377790;57210644829;55694209700;55730767800;57210638147;","Neural logic machines",2019,"7th International Conference on Learning Representations, ICLR 2019",,,,"","",,31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083952231&partnerID=40&md5=b1aa83e9946b1e6d44bb4c81bf5cd27d","We propose the Neural Logic Machine (NLM), a neural-symbolic architecture for both inductive learning and logic reasoning. NLMs exploit the power of both neural networks-as function approximators, and logic programming-as a symbolic processor for objects with properties, relations, logic connectives, and quantifiers. After being trained on small-scale tasks (such as sorting short arrays), NLMs can recover lifted rules, and generalize to large-scale tasks (such as sorting longer arrays). In our experiments, NLMs achieve perfect generalization in a number of tasks, from relational reasoning tasks on the family tree and general graphs, to decision making tasks including sorting arrays, finding shortest paths, and playing the blocks world. Most of these tasks are hard to accomplish for neural networks or inductive logic programming alone. © 7th International Conference on Learning Representations, ICLR 2019. All Rights Reserved.",,"Computer circuits; Decision making; Learning systems; Trees (mathematics); Blocks worlds; Function approximators; General graph; Inductive learning; Logic machines; Logic reasoning; Relational reasoning; Shortest path; Inductive logic programming (ILP)",,"7th International Conference on Learning Representations, ICLR 2019","6 May 2019 through 9 May 2019",,149936,Conference Paper,"Final","",Scopus,2-s2.0-85083952231
"Lian R., Xie M., Wang F., Peng J., Wu H.","57204210310;56272894700;57211759827;57209224671;7405584325;","Learning to select knowledge for response generation in dialog systems",2019,"IJCAI International Joint Conference on Artificial Intelligence","2019-August",,,"5081","5087",,31,"10.24963/ijcai.2019/706","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074950160&doi=10.24963%2fijcai.2019%2f706&partnerID=40&md5=017dcccaf23da52f83d259ed992ec89d","End-to-end neural models for intelligent dialogue systems suffer from the problem of generating uninformative responses. Various methods were proposed to generate more informative responses by leveraging external knowledge. However, few previous work has focused on selecting appropriate knowledge in the learning process. The inappropriate selection of knowledge could prohibit the model from learning to make full use of the knowledge. Motivated by this, we propose an end-to-end neural model which employs a novel knowledge selection mechanism where both prior and posterior distributions over knowledge are used to facilitate knowledge selection. Specifically, a posterior distribution over knowledge is inferred from both utterances and responses, and it ensures the appropriate selection of knowledge during the training process. Meanwhile, a prior distribution, which is inferred from utterances only, is used to approximate the posterior distribution so that appropriate knowledge can be selected even without responses during the inference process. Compared with the previous work, our model can better incorporate appropriate knowledge in response generation. Experiments on both automatic and human evaluation verify the superiority of our model over previous baselines. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.",,,"Baidu;et al.;Huawei;International Joint Conferences on Artifical Intelligence (IJCAI);Sony;Xiao-i","28th International Joint Conference on Artificial Intelligence, IJCAI 2019","10 August 2019 through 16 August 2019",,153611,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85074950160
"Yoneda T., Mitchell J., Welbl J., Stenetorp P., Riedel S.","","Four factor framework for fact finding (HeXAF)",2018,"Proceedings of the First Workshop on Fact Extraction and VERification",,,,"97","102",,31,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85066898914
"Liu Q., Wei Z., Peng B., Dai X., Tou H., Chen T., Huang X., Wong K.-F.","57207472895;51666060600;56182809200;57207879366;57201547336;57207880261;8983710700;7404759311;","Task-oriented dialogue system for automatic diagnosis",2018,"ACL 2018 - 56th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference (Long Papers)","2",,,"201","207",,31,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85062544074&partnerID=40&md5=6b5d135b65f2887fb1e05bb961140e49","In this paper, we make a move to build a dialogue system for automatic diagnosis. We first build a dataset collected from an online medical forum by extracting symptoms from both patients’ self-reports and conversational data between patients and doctors. Then we propose a task-oriented dialogue system framework to make the diagnosis for patients automatically, which can converse with patients to collect additional symptoms beyond their self-reports. Experimental results on our dataset show that additional symptoms extracted from conversation can greatly improve the accuracy for disease identification and our dialogue system is able to collect these symptoms automatically and make a better diagnosis. © 2018 Association for Computational Linguistics.",,"Computational linguistics; Speech processing; Automatic diagnosis; Dialogue systems; Task-oriented; Diagnosis","Apple;ByteDance;et al.;Facebook;Google;Samsung Research","56th Annual Meeting of the Association for Computational Linguistics, ACL 2018","15 July 2018 through 20 July 2018",,145927,Conference Paper,"Final","",Scopus,2-s2.0-85062544074
"Boudin F., Mougard H., Favre B.","23088264000;57156577400;24280563700;","Concept-based summarization using integer linear programming: From concept pruning to multiple optimal solutions",2015,"Conference Proceedings - EMNLP 2015: Conference on Empirical Methods in Natural Language Processing",,,,"1914","1918",,31,"10.18653/v1/d15-1220","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84957547564&doi=10.18653%2fv1%2fd15-1220&partnerID=40&md5=871cd1812819d1c91c4c02e87f419990","In concept-based summarization, sentence selection is modelled as a budgeted maximum coverage problem. As this problem is NP-hard, pruning low-weight concepts is required for the solver to find optimal solutions efficiently. This work shows that reducing the number of concepts in the model leads to lower Rouge scores, and more importantly to the presence of multiple optimal solutions. We address these issues by extending the model to provide a single optimal solution, and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference. © 2015 Association for Computational Linguistics.",,"Approximation algorithms; Budget control; Inference engines; Integer programming; Optimal systems; Budgeted maximum coverage problems; Concept-based; Exact inference; Integer Linear Programming; Multiple optimal solutions; NP-hard; Optimal solutions; Sentence selection; Natural language processing systems","Baidu;Bloomberg;et al.;facebook;Google;Linkedin","Conference on Empirical Methods in Natural Language Processing, EMNLP 2015","17 September 2015 through 21 September 2015",,116677,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-84957547564
"Viola S.R., Graf S., Kinshuk, Leo T.","16231997000;15050609100;16230394700;7004497150;","Analysis of felder-silverman index of learning styles by a data-driven statistical approach",2006,"ISM 2006 - 8th IEEE International Symposium on Multimedia",,,"4061286","959","964",,31,"10.1109/ISM.2006.30","https://www.scopus.com/inward/record.uri?eid=2-s2.0-46249107412&doi=10.1109%2fISM.2006.30&partnerID=40&md5=699bdc3156a567f1da9c725ea4a21cd8","In this paper a data driven analysis of Felder-Silverman Index of Learning Styles (ILS) is given. Results, obtained by Multiple Correspondence Analysis and cross-validated by correlation analysis, show the consistent dependencies between some styles; some latent dimensions present in data, that are unexpected, are discussed. Results are then compared with the ones given by literature concerning validity and reliability of ILS questionnaire. Both the results and the comparisons show the effectiveness of data driven methods for patterns extraction even when unexpected dependencies are found and the importance of coherence and consistency of mathematical representation of data with respect to the methods selected for an effective, precise and accurate modeling. © 2006 IEEE.",,"Extraction; Ketones; Landing; Learning systems; (2+1) dimensions; Accurate modeling; Correlation analysis; Data driven (DD); Data driven analysis; Data-driven methods; International symposium; Learning styles; Mathematical representations; Multiple correspondence analysis (MCA); Statistical approaches; Technical presentations","IEEE Computer Society","ISM 2006 - 8th IEEE International Symposium on Multimedia","11 December 2006 through 13 December 2006","San Diego, CA",72422,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-46249107412
"Connolly P., Neill J.","57225367653;57208996759;","Constructions of locality and gender and their impact on the educational aspirations of working-class children",2001,"International Studies in Sociology of Education","11","2",,"107","130",,31,"10.1080/09620210100200070","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937344673&doi=10.1080%2f09620210100200070&partnerID=40&md5=7e7cf0ff0bd9e5f2b417fc9e6d4d7940","Over recent years the moral panic that has surrounded 'boys' underachievement' has tended to encourage crude and essentialist comparisons between allboys and allgirls and to eclipse the continuing and more profound effects on educational achievement exerted by social class and 'race'/ethnicity. While there are differences in educational achievement between working-class boys and girls, these differences are relatively minor when comparing the overall achievement levels of working-class children with those from higher, professional social class backgrounds. This article argues that a need exists therefore for researchers to fully contextualize the gender differences that exist in educational achievement within the overriding contexts provided by social class and 'race'/ethnicity. The article provides an example of how this can be done through a case study of 11-year-old children from a Catholic, working-class area in Belfast. The article shows how the children's general educational aspirations are significantly mediated by their experiences of the local area in which they live. However, the way in which the children come to experience and construct a sense of locality differs between the boys and girls and this, it is argued, helps to explain the more positive educational aspirations held by some of the girls compared with the boys. The article concludes by considering the relevance of locality for understanding its effects on educational aspirations among other working-class and/or minority ethnic communities. © 2001, Taylor & Francis Group, LLC.",,,,,,,,Article,"Final","",Scopus,2-s2.0-84937344673
"Sahu S.K., Christopoulou F., Miwa M., Ananiadou S.","56804410500;57197872683;35208894900;6602788919;","Inter-sentence relation extraction with document-level graph convolutional neural network",2020,"ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",,,,"4309","4316",,30,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084031927&partnerID=40&md5=d7d87b91eae59eff750403fb7cca7485","Inter-sentence relation extraction deals with a number of complex semantic relationships in documents, which require local, non-local, syntactic and semantic dependencies. Existing methods do not fully exploit such dependencies. We present a novel inter-sentence relation extraction model that builds a labelled edge graph convolutional neural network model on a document-level graph. The graph is constructed using various inter- and intra-sentence dependencies to capture local and non-local dependency information. In order to predict the relation of an entity pair, we utilise multi-instance learning with bi-affine pairwise scoring. Experimental results show that our model achieves comparable performance to the state-of-the-art neural models on two biochemistry datasets. Our analysis shows that all the types in the graph are effective for inter-sentence relation extraction. © 2019 Association for Computational Linguistics",,"Computational linguistics; Convolution; Extraction; Semantics; Dependency informations; Level graphs; Multi-instance learning; Neural models; Relation extraction; Semantic dependency; Semantic relationships; State of the art; Convolutional neural networks","Apple;ASAPP;Bloomberg Engineering;BOSCH;et al.;Expedia","57th Annual Meeting of the Association for Computational Linguistics, ACL 2019","28 July 2019 through 2 August 2019",,159206,Conference Paper,"Final","",Scopus,2-s2.0-85084031927
"Gupta P., Rajaram S., Schütze H., Runkler T.","57189588439;57216432328;7003432991;6701574352;","Neural relation extraction within and across sentence boundaries",2019,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",,,,"6513","6520",,30,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075427540&partnerID=40&md5=3f36ceee71c41500c1c33b9cb6ebb72e","Past work in relation extraction mostly focuses on binary relation between entity pairs within single sentence. Recently, the NLP community has gained interest in relation extraction in entity pairs spanning multiple sentences. In this paper, we propose a novel architecture for this task: inter-sentential dependency-based neural networks (iDepNN). iDepNN models the shortest and augmented dependency paths via recurrent and recursive neural networks to extract relationships within (intra-) and across (inter-) sentence boundaries. Compared to SVM and neural network baselines, iDepNN is more robust to false positives in relationships spanning sentences. We evaluate our models on four datasets from newswire (MUC6) and medical (BioNLP shared task) domains that achieve state-of-the-art performance and show a better balance in precision and recall for inter-sentential relationships. We perform better than 11 teams participating in the BioNLP shared task 2016 and achieve a gain of 5.2% (0.587 vs 0.558) in F1 over the winning team. We also release the cross-sentence annotations for MUC6. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Extraction; Support vector machines; Binary relation; False positive; Novel architecture; Precision and recall; Recursive neural networks; Relation extraction; Sentence boundaries; State-of-the-art performance; Recurrent neural networks","Association for the Advancement of Artificial Intelligence","33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019","27 January 2019 through 1 February 2019",,160302,Conference Paper,"Final","",Scopus,2-s2.0-85075427540
"Chasins S.E., Mueller M., Bodik R.","54395086400;57204724611;6701821028;","Rousillon: Scraping distributed hierarchical web data",2018,"UIST 2018 - Proceedings of the 31st Annual ACM Symposium on User Interface Software and Technology",,,,"963","975",,30,"10.1145/3242587.3242661","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056827807&doi=10.1145%2f3242587.3242661&partnerID=40&md5=d9a7ff1fd68b995098222dc350dd5913","Programming by Demonstration (PBD) promises to enable data scientists to collect web data. However, in formative interviews with social scientists, we learned that current PBD tools are insufficient for many real-world web scraping tasks. The missing piece is the capability to collect hierarchically-structured data from across many different webpages. We present Rousillon, a programming system for writing complex web automation scripts by demonstration. Users demonstrate how to collect the first row of a 'universal table' view of a hierarchical dataset to teach Rousillon how to collect all rows. To offer this new demonstration model, we developed novel relation selection and generalization algorithms. In a within-subject user study on 15 computer scientists, users can write hierarchical web scrapers 8 times more quickly with Rousillon than with traditional programming. © 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.",,"Budget control; Computer programming; Data acquisition; Demonstrations; Human computer interaction; Computer scientists; Demonstration models; Generalization algorithms; Programming by demonstration; Programming system; Real world web; Social scientists; Structured data; User interfaces","ACM SIGCHI;ACM SIGGRAPH","31st Annual ACM Symposium on User Interface Software and Technology, UIST 2018","14 October 2018 through 17 October 2018",,141121,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85056827807
"Almeida F., Xexéo G.","",[No title available],2019,"Word Embeddings: A Survey",,,,"","",,29,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075689764
"Bornholt J., Torlak E., Grossman D., Ceze L.","56103636000;23089529200;7102998878;12344963100;","Optimizing synthesis with metasketches",2016,"Conference Record of the Annual ACM Symposium on Principles of Programming Languages","20-22-January-2016",,,"775","788",,29,"10.1145/2837614.2837666","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84962589882&doi=10.1145%2f2837614.2837666&partnerID=40&md5=ca91b814f63780d98fd02326332df8e8","Many advanced programming tools-for both end-users and expert developers-rely on program synthesis to automatically generate implementations from high-level specifications. These tools often need to employ tricky, custom-built synthesis algorithms because they require synthesized programs to be not only correct, but also optimal with respect to a desired cost metric, such as program size. Finding these optimal solutions efficiently requires domain-specific search strategies, but existing synthesizers hard-code the strategy, making them difficult to reuse. This paper presents metasketches, a general framework for specifying and solving optimal synthesis problems. Metasketches make the search strategy a part of the problem definition by specifying a fragmentation of the search space into an ordered set of classic sketches. We provide two cooperating search algorithms to effectively solve metasketches. A global optimizing search coordinates the activities of local searches, informing them of the costs of potentially-optimal solutions as they explore different regions of the candidate space in parallel. The local searches execute an incremental form of counterexample-guided inductive synthesis to incorporate information sent from the global search. We present SYNAPSE, an implementation of these algorithms, and show that it effectively solves optimal synthesis problems with a variety of different cost functions. In addition, metasketches can be used to accelerate classic (non-optimal) synthesis by explicitly controlling the search strategy, and we show that SYNAPSE solves classic synthesis problems that state-of-the-art tools cannot.","Program synthesis","Computational linguistics; Cost functions; Costs; Global optimization; Optimal systems; Domain specific searches; High level specification; Optimal synthesis; Problem definition; Program synthesis; Search Algorithms; Synthesis algorithms; Synthesis problems; Problem solving","Association for Computing Machinery (ACM) SIGPLAN","43rd Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL 2016","20 January 2016 through 22 January 2016",,119492,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-84962589882
"Garrod S., Pickering M.J.","","Alignment in dialogue",2007,"The Oxford Handbook of Psycholinguistics",,,,"443","451",,29,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-42649109299
"Wang X., Dillig I., Singh R.","","Program synthesis using abstraction refinement",2018,"PACMPL","2","1–63",,"631","6330",,28,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85051104896
"Ramadhan W.P., Novianty A., Setianingsih C.","57201738987;57170900000;57201737786;","Sentiment analysis using multinomial logistic regression",2017,"ICCREC 2017 - 2017 International Conference on Control, Electronics, Renewable Energy, and Communications, Proceedings","2017-January",,,"46","49",,28,"10.1109/ICCEREC.2017.8226700","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045328391&doi=10.1109%2fICCEREC.2017.8226700&partnerID=40&md5=4d506258a7a048fec6d98d5173dc2f66","Data amount becomes rapidly increased in today's era. Data can be in form of text, picture, voice, and video. Social media is one factor of the data increase as everybody expresses, gives opinion, and even complains in social media. The first step is data collection used API twitter with each candidate names on Jakarta Governor Election. The collected data then became input for preprocessing step. The next step is extracted-each tweet's feature to be listed. The list of features were transformed into feature vector in binary form and transformed again used Tf-idf method. Dataset consists of two kinds of data, training and testing. Training was labeled manually. K-Fold Cross Validation is used to test algorithm performance. Based on the result of the test, accuracy obtained reached 74% in average with composition of training data and testing data by 90:10. Changed folding amount gave no impact to the accuracy level. © 2017 IEEE.","multinomial logistic regression; softmax regression; text mining; twitter","Data mining; Natural language processing systems; Sentiment analysis; Social networking (online); Statistical tests; Data collection; K fold cross validations; Multinomial logistic regression; Pre-processing step; Softmax regressions; Text mining; Training and testing; twitter; Regression analysis","","3rd International Conference on Control, Electronics, Renewable Energy, and Communications, ICCREC 2017","26 September 2017 through 28 September 2017",,134007,Conference Paper,"Final","",Scopus,2-s2.0-85045328391
"Rani M., Srivastava K.V., Vyas O.P.","55517104800;57217714799;8876697200;","An ontological learning management system",2016,"Computer Applications in Engineering Education","24","5",,"706","722",,28,"10.1002/cae.21742","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84969799393&doi=10.1002%2fcae.21742&partnerID=40&md5=c53f9522705c97170048a67c6e2ed4ab","The current learning systems typically lack the level of meta-cognitive awareness, self-directed learning, and time management skills. Most of the ontologically based learning management systems are in the proposed phase and those which are developed do not provide the necessary path guidance for proper learning. The systems available are not as adaptive from the viewpoint of the learner as required. Ontology engineering has become an important pillar for knowledge management and representation in recent years. The design, approach, and implementation of ontology in e- and m-learning systems have made them more effective. In this paper, we have proposed a system for the betterment of knowledge management and representation of associated data as compared to the previously available learning management systems. Here, we have presented the application and implementation of ontological engineering methodology in the Computer Science domain. For knowledge management, we have created a domain associated ontology which represents knowledge of a single domain. Subsequently, ontology has been created to manage a learner profile so that a learner may be aligned to a proper path of learning. The learner ontology will use the VARK learning model which classifies what kind of learning does the learner requires so that necessary resources could be provided. © 2016 Wiley Periodicals, Inc. Comput Appl Eng Educ 24:706–722, 2016; View this article online at wileyonlinelibrary.com/journal/cae; DOI 10.1002/cae.21742. © 2016 Wiley Periodicals, Inc.","e-learning; learning management system (LMS); m-learning; ontology; semantic web","Cognitive systems; E-learning; Information management; Knowledge management; Management science; Ontology; Semantic Web; Learner ontologies; Learner profiles; Learning management system; M-Learning; Ontological engineering; Ontology engineering; Proper learning; Self-directed learning; Learning systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84969799393
"Sourek G., Aschenbrenner V., Zelezny F., Kuzelka O.","","Lifted relational neural networks",2015,"Lifted Relational Neural Networks",,,,"52","60",,28,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84977515818
"Lloret E., Boldrini E., Vodolazova T., Martínez-Barco P., Muñoz R., Palomar M.","25929448300;27667461800;55345584900;8927588100;7202035977;55664073900;","A novel concept-level approach for ultra-concise opinion summarization",2015,"Expert Systems with Applications","42","20",,"7148","7156",,28,"10.1016/j.eswa.2015.05.026","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84930636185&doi=10.1016%2fj.eswa.2015.05.026&partnerID=40&md5=2fa2340e269413530fefd87fe322c900","The Web 2.0 has resulted in a shift as to how users consume and interact with the information, and has introduced a wide range of new textual genres, such as reviews or microblogs, through which users communicate, exchange, and share opinions. The exploitation of all this user-generated content is of great value both for users and companies, in order to assist them in their decision-making processes. Given this context, the analysis and development of automatic methods that can help manage online information in a quicker manner are needed. Therefore, this article proposes and evaluates a novel concept-level approach for ultra-concise opinion abstractive summarization. Our approach is characterized by the integration of syntactic sentence simplification, sentence regeneration and internal concept representation into the summarization process, thus being able to generate abstractive summaries, which is one the most challenging issues for this task. In order to be able to analyze different settings for our approach, the use of the sentence regeneration module was made optional, leading to two different versions of the system (one with sentence regeneration and one without). For testing them, a corpus of 400 English texts, gathered from reviews and tweets belonging to two different domains, was used. Although both versions were shown to be reliable methods for generating this type of summaries, the results obtained indicate that the version without sentence regeneration yielded to better results, improving the results of a number of state-of-the-art systems by 9%, whereas the version with sentence regeneration proved to be more robust to noisy data. © 2015 Elsevier Ltd. All rights reserved.","Electronic Word of Mouth; Natural language generation; Text summarization; Ultra-concise opinion summarization","Decision making; Social networking (online); Decision making process; Different domains; Electronic word of mouths; Natural language generation; On-line information; Text summarization; Ultra-concise opinion summarization; User-generated content; Natural language processing systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84930636185
"Sterckx L., Demeester T., Deleu J., Develder C.","55319048700;35415697600;24331318000;6602998454;","Topical word importance for fast keyphrase extraction",2015,"WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web",,,,"121","122",,28,"10.1145/2740908.2742730","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968531517&doi=10.1145%2f2740908.2742730&partnerID=40&md5=c79cee1713b6298a83e2a90650d3da7c","We propose an improvement on a state-of-the-art keyphrase extraction algorithm, Topical PageRank (TPR), incorporating topical information from topic models. While the original algorithm requires a random walk for each topic in the topic model being used, ours is independent of the topic model, computing but a single PageRank for each text regardless of the amount of topics in the model. This increases the speed drastically and enables it for use on large collections of text using vast topic models, while not altering performance of the original algorithm.",,"Algorithms; Extraction; World Wide Web; Keyphrase extraction; Original algorithms; PageRank; Random Walk; State of the art; Topic model; Topic Modeling; Data mining","International World Wide Web Conference Steering Committee (IW3C2)","24th International Conference on World Wide Web, WWW 2015","18 May 2015 through 22 May 2015",,119451,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84968531517
"Kundi F.M., Khan A., Ahmad S., Asghar M.Z.","","Lexicon-based sentiment analysis in the social web",2014,"J Basic Appl Sci Res","4","2",,"238","248",,28,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84979239226
"Pasupat P., Liang P.","55756364000;56646712700;","Zero-shot entity extraction from web pages",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference","1",,,"391","401",,28,"10.3115/v1/p14-1037","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906921910&doi=10.3115%2fv1%2fp14-1037&partnerID=40&md5=04079a2a3227164f921059f3e2401185","In order to extract entities of a fine-grained category from semi-structured data in web pages, existing information extraction systems rely on seed examples or redundancy across multiple web pages. In this paper, we consider a new zero-shot learning task of extracting entities specified by a natural language query (in place of seeds) given only a single web page. Our approach defines a log-linear model over latent extraction predicates, which select lists of entities from the web page. The main challenge is to define features on widely varying candidate entity lists. We tackle this by abstracting list elements and using aggregate statistics to define features. Finally, we created a new dataset of diverse queries and web pages, and show that our system achieves significantly better accuracy than a natural baseline. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Regression analysis; Entity extractions; Fine grained; Information extraction systems; Learning tasks; Loglinear model; Natural language queries; Semi structured data; Websites","amazon.com;Baidu;Bloomberg;et al.;Google;IBM Watson","52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014","22 June 2014 through 27 June 2014","Baltimore, MD",107373,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84906921910
"SMARANDACHE F.","","Neutrosophy, a new Branch of Philosophy",2002,"Neutrosophy, a new Branch of Philosophy",,,,"","",,28,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85089006225
"Goertzel B., Silverman K., Hartley C., Bugaj S., Ross M.","","The baby webmind project",2000,"The Baby Webmind Project",,,,"","",,28,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-77952049934
"Peng H., Li J., Gong Q., Song Y., Ning Y., Lai K., Yu P.S.","57190014707;55720560100;57211742382;14039604300;57211751494;57192063326;7402366049;","Fine-grained event categorization with heterogeneous graph convolutional networks",2019,"IJCAI International Joint Conference on Artificial Intelligence","2019-August",,,"3238","3245",,27,"10.24963/ijcai.2019/449","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074946070&doi=10.24963%2fijcai.2019%2f449&partnerID=40&md5=d9289332703f5477225f85d8c0db5b32","Events are happening in real-world and real-time, which can be planned and organized occasions involving multiple people and objects. Social media platforms publish a lot of text messages containing public events with comprehensive topics. However, mining social events is challenging due to the heterogeneous event elements in texts and explicit and implicit social network structures. In this paper, we design an event meta-schema to characterize the semantic relatedness of social events and build an event-based heterogeneous information network (HIN) integrating information from external knowledge base, and propose a novel Pairwise Popularity Graph Convolutional Network (PP-GCN) based fine-grained social event categorization model. We propose a Knowledgeable meta-paths Instances based social Event Similarity (KIES) between events and build a weighted adjacent matrix as input to the PP-GCN model. Comprehensive experiments on real data collections are conducted to compare various social event detection and clustering tasks. Experimental results demonstrate that our proposed framework outperforms other alternative social event categorization techniques. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.",,,"Baidu;et al.;Huawei;International Joint Conferences on Artifical Intelligence (IJCAI);Sony;Xiao-i","28th International Joint Conference on Artificial Intelligence, IJCAI 2019","10 August 2019 through 16 August 2019",,153611,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85074946070
"Shen Y., Chen J., Huang P.-S., Guo Y., Gao J.","56729408300;39160902900;47061231500;57208445968;55702627000;","M-Walk: Learning to Walk over Graphs using Monte Carlo Tree Search",2018,"Advances in Neural Information Processing Systems","2018-December",,,"6786","6797",,27,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064843335&partnerID=40&md5=964d9befd18016e57fb9f24150c2ce41","Learning to walk over a graph towards a target node for a given query and a source node is an important problem in applications such as knowledge base completion (KBC). It can be formulated as a reinforcement learning (RL) problem with a known state transition model. To overcome the challenge of sparse rewards, we develop a graph-walking agent called M-Walk, which consists of a deep recurrent neural network (RNN) and Monte Carlo Tree Search (MCTS). The RNN encodes the state (i.e., history of the walked path) and maps it separately to a policy and Q-values. In order to effectively train the agent from sparse rewards, we combine MCTS with the neural policy to generate trajectories yielding more positive rewards. From these trajectories, the network is improved in an off-policy manner using Q-learning, which modifies the RNN policy via parameter sharing. Our proposed RL algorithm repeatedly applies this policy-improvement step to learn the model. At test time, MCTS is combined with the neural policy to predict the target node. Experimental results on several graph-walking benchmarks show that M-Walk is able to learn better policies than other RL-based methods, which are mainly based on policy gradients. M-Walk also outperforms traditional KBC baselines. © 2018 Curran Associates Inc.All rights reserved.",,"Knowledge based systems; Monte Carlo methods; Recurrent neural networks; Reinforcement learning; Trees (mathematics); Knowledge base; Monte Carlo tree search (MCTS); Monte-Carlo tree searches; Parameter sharing; Policy gradient; Recurrent neural network (RNN); Source nodes; State transition models; Graph theory",,"32nd Conference on Neural Information Processing Systems, NeurIPS 2018","2 December 2018 through 8 December 2018",,147412,Conference Paper,"Final","",Scopus,2-s2.0-85064843335
"Marini S., Trifoglio E., Barbarini N., Sambo F., Di Camillo B., Malovini A., Manfrini M., Cobelli C., Bellazzi R.","53981812300;36926325100;24472577500;36176302600;12771391500;23005132600;55198587100;7101795724;18343389900;","A Dynamic Bayesian Network model for long-term simulation of clinical complications in type 1 diabetes",2015,"Journal of Biomedical Informatics","57",,,"369","376",,27,"10.1016/j.jbi.2015.08.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949501612&doi=10.1016%2fj.jbi.2015.08.021&partnerID=40&md5=597fd07cd7afb8904355b46c2a98b3bd","The increasing prevalence of diabetes and its related complications is raising the need for effective methods to predict patient evolution and for stratifying cohorts in terms of risk of developing diabetes-related complications. In this paper, we present a novel approach to the simulation of a type 1 diabetes population, based on Dynamic Bayesian Networks, which combines literature knowledge with data mining of a rich longitudinal cohort of type 1 diabetes patients, the DCCT/EDIC study. In particular, in our approach we simulate the patient health state and complications through discretized variables. Two types of models are presented, one entirely learned from the data and the other partially driven by literature derived knowledge. The whole cohort is simulated for fifteen years, and the simulation error (i.e. for each variable, the percentage of patients predicted in the wrong state) is calculated every year on independent test data. For each variable, the population predicted in the wrong state is below 10% on both models over time. Furthermore, the distributions of real vs. simulated patients greatly overlap. Thus, the proposed models are viable tools to support decision making in type 1 diabetes. © 2015 Elsevier Inc..","CVD; Dynamic Bayesian Network; Nephropaty; Simulation; Tabu search; Type 1 diabetes","Chemical vapor deposition; Data mining; Decision making; Population statistics; Tabu search; Clinical complications; Diabetes-related complications; Dynamic Bayesian networks; Long term simulation; Nephropaty; Simulated patients; Simulation; Type 1 diabetes; Bayesian networks; Article; Bayes theorem; cohort analysis; controlled study; data analysis; data mining; disease association; dynamic Bayesian network model; health status; insulin dependent diabetes mellitus; intermethod comparison; longitudinal study; measurement error; prediction; priority journal; process development; process model; computer simulation; data mining; diabetic complication; human; Bayes Theorem; Computer Simulation; Data Mining; Diabetes Complications; Diabetes Mellitus, Type 1; Humans",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-84949501612
"Lücking A., Bergmann K., Hahn F., Kopp S., Rieser H.","","The Bielefeld speech and gesture alignment corpus (SaGA)",2010,"Proceedings of the LREC 2010 Workshop: Multimodal Corpora-advances in Capturing, Coding and Analyzing Multimodality",,,,"92","98",,27,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-79952274250
"Landwehr N., Passerini A., De Raedt L., Frasconi P.","10244371900;7006983227;55760010700;7003350874;","Fast learning of relational kernels",2010,"Machine Learning","78","3",,"305","342",,27,"10.1007/s10994-009-5163-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053138684&doi=10.1007%2fs10994-009-5163-1&partnerID=40&md5=389cb4111a50738dc7398a35503c6f2c","We develop a general theoretical framework for statistical logical learning with kernels based on dynamic propositionalization, where structure learning corresponds to inferring a suitable kernel on logical objects, and parameter learning corresponds to function learning in the resulting reproducing kernel Hilbert space. In particular, we study the case where structure learning is performed by a simple FOIL-like algorithm, and propose alternative scoring functions for guiding the search process. We present an empirical evaluation on several data sets in the single-task as well as in the multi-task setting. © The Author(s) 2009.","Dynamic propositionalization; Inductive logic programming; Kernel learning; Kernel methods; Multi-task learning; Statistical relational learning","Artificial intelligence; Inductive logic programming (ILP); Kernel learning; Kernel methods; Multitask learning; Propositionalization; Statistical relational learning; Software engineering",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-80053138684
"Giles H., Powesland P.F.","",[No title available],1975,"Speech styles and social evaluation",,,,"","",,27,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84861783439
"Christopoulou F., Miwa M., Ananiadou S.","57197872683;35208894900;6602788919;","Connecting the dots: Document-level neural relation extraction with edge-oriented graphs",2020,"EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference",,,,"4925","4936",,26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084317655&partnerID=40&md5=e51786f5ab17c4a0d04d2defaacf2cd1","Document-level relation extraction is a complex human process that requires logical inference to extract relationships between named entities in text. Existing approaches use graph-based neural models with words as nodes and edges as relations between them, to encode relations across sentences. These models are node-based, i.e., they form pair representations based solely on the two target node representations. However, entity relations can be better expressed through unique edge representations formed as paths between nodes. We thus propose an edge-oriented graph neural model for document-level relation extraction. The model utilises different types of nodes and edges to create a document-level graph. An inference mechanism on the graph edges enables to learn intra- and inter-sentence relations using multi-instance learning internally. Experiments on two document-level biomedical datasets for chemical-disease and gene-disease associations show the usefulness of the proposed edge-oriented approach.1. © 2019 Association for Computational Linguistics",,"Extraction; Graph theory; Graphic methods; Natural language processing systems; Gene-disease associations; Inference mechanism; Logical inference; Multi-instance learning; Named entities; Neural modeling; Oriented graph; Relation extraction; Graph structures","Apple;ASAPP;et al.;Facebook;Google;salesforce","2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019","3 November 2019 through 7 November 2019",,159367,Conference Paper,"Final","",Scopus,2-s2.0-85084317655
"Shirdastian H., Laroche M., Richard M.-O.","55647848500;35866711700;9237918700;","Using big data analytics to study brand authenticity sentiments: The case of Starbucks on Twitter",2019,"International Journal of Information Management","48",,,"291","307",,26,"10.1016/j.ijinfomgt.2017.09.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85032746012&doi=10.1016%2fj.ijinfomgt.2017.09.007&partnerID=40&md5=b65f8a395942e2336ee089a7c540878c","There is a strong interest among academics and practitioners in studying branding issues in the big data era. In this article, we examine the sentiments toward a brand, via brand authenticity, to identify the reasons for positive or negative sentiments on social media. Moreover, in order to increase precision, we investigate sentiment polarity on a five-point scale. From a database containing 2,282,912 English tweets with the keyword ‘Starbucks’, we use a set of 2204 coded tweets both for analyzing brand authenticity and sentiment polarity. First, we examine the tweets qualitatively to gain insights about brand authenticity sentiments. Then we analyze the data quantitatively to establish a framework in which we predict both the brand authenticity dimensions and their sentiment polarity. Through three qualitative studies, we discuss several tweets from the dataset that can be classified under the quality commitment, heritage, uniqueness, and symbolism categories. Using latent semantic analysis (LSA), we extract the common words in each category. We verify the robustness of previous findings with an in-lab experiment. Results from the support vector machine (SVM), as the quantitative research method, illustrate the effectiveness of the proposed procedure of brand authenticity sentiment analysis. It shows high accuracy for both the brand authenticity dimensions’ predictions and their sentiment polarity. We then discuss the theoretical and managerial implications of the studies. © 2017 Elsevier Ltd","Big data analytics; Brand authenticity; Brand sentiment analysis; Latent semantic analysis (LSA); Social media; Support Vector Machine (SVM)","Big data; Data Analytics; Semantics; Sentiment analysis; Social networking (online); Support vector machines; Brand authenticity; High-accuracy; Latent Semantic Analysis; Managerial implications; Negative sentiments; Qualitative study; Quantitative research methods; Social media; Authentication",,,,,,Article,"Final","",Scopus,2-s2.0-85032746012
"Cheng K., Li J., Tang J., Liu H.","56829905100;56186042700;56245477300;7409751811;","Unsupervised sentiment analysis with signed social networks",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"3429","3435",,26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029067784&partnerID=40&md5=b72a2a52b0be36872ca1009188830ced","Huge volumes of opinion-rich data is user-generated in social media at an unprecedented rate, easing the analysis of individual and public sentiments. Sentiment analysis has shown to be useful in probing and understanding emotions, expressions and attitudes in the text. However, the distinct characteristics of social media data present challenges to traditional sentiment analysis. First, social media data is often noisy, incomplete and fast-evolved which necessitates the design of a sophisticated learning model. Second, sentiment labels are hard to collect which further exacerbates the problem by not being able to discriminate sentiment polarities. Meanwhile, opportunities are also unequivocally presented. Social media contains rich sources of sentiment signals in textual terms and user interactions, which could be helpful in sentiment analysis. While there are some attempts to leverage implicit sentiment signals in positive user interactions, little attention is paid on signed social networks with both positive and negative links. The availability of signed social networks motivates us to investigate if negative links also contain useful sentiment signals. In this paper, we study a novel problem of unsupervised sentiment analysis with signed social networks. In particular, we incorporate explicit sentiment signals in textual terms and implicit sentiment signals from signed social networks into a coherent model SignedSenti for unsupervised sentiment analysis. Empirical experiments on two real-world datasets corroborate its effectiveness. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Social networking (online); Empirical experiments; Learning models; Public sentiments; Real-world datasets; Sentiment analysis; Social media datum; User interaction; User-generated; Data mining","Amazon;Artificial Intelligence;Baidu;et al.;IBM;Tencent","31st AAAI Conference on Artificial Intelligence, AAAI 2017","4 February 2017 through 10 February 2017",,130407,Conference Paper,"Final","",Scopus,2-s2.0-85029067784
"Malt B.C., Gennari S.P., Imai M., Ameel E., Saji N., Majid A.","","Where are the concepts? What words can and can’t reveal",2015,"The conceptual mind: New directions in the study of concepts",,,,"291","326",,26,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84939237309
"Haque Md., Rahman T.","","Sentiment Analysis by using Fuzzy Logic",2014,"International Journal of Computer Science, Engineering and Information Technology (IJCSEIT)","4","1",,"33","48",,26,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84945278024
"Cabessa J., Siegelmann H.T.","24400916700;57189345629;","The computational power of interactive recurrent neural networks",2012,"Neural Computation","24","4",,"996","1019",,26,"10.1162/NECO_a_00263","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84861112430&doi=10.1162%2fNECO_a_00263&partnerID=40&md5=5d45b68710fbc327e933e9df736fb3a1","In classical computation, rational- and real-weighted recurrent neural networks were shown to be respectively equivalent to and strictly more powerful than the standard Turing machine model. Here, we study the computational power of recurrent neural networks in a more biologically oriented computational framework, capturing the aspects of sequential interactivity and persistence of memory. In this context, we prove that socalled interactive rational- and real-weighted neural networks show the same computational powers as interactive Turing machines and interactive Turing machineswith advice, respectively. A mathematical characterization of each of these computational powers is also provided. It follows from these results that interactive real-weighted neural networks can perform uncountablymanymore translations of information than interactive Turing machines, making them capable of super-Turing capabilities. © 2012 Massachusetts Institute of Technology.",,"artificial neural network; biological model; computer simulation; letter; memory; nerve cell; physiology; Computer Simulation; Memory; Models, Neurological; Neural Networks (Computer); Neurons",,,,,,Letter,"Final","All Open Access, Green",Scopus,2-s2.0-84861112430
"Neary T., Woods D.","23135156100;8967518900;","Four Small Universal Turing Machines",2009,"Fundamenta Informaticae","91","1",,"123","144",,26,"10.3233/FI-2009-0036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-68249106606&doi=10.3233%2fFI-2009-0036&partnerID=40&md5=b3bb3fc45723b21da09fe20a85ffb96a","We present universal Turing machines with state-symbol pairs of (5, 5), (6, 4), (9, 3) and (15, 2). These machines simulate our new variant of tag system, the bi-tag system and are the smallest known single-tape universal Turing machines with 5, 4, 3 and 2-symbols, respectively. Our 5-symbolmachine uses the same number of instructions (22) as the smallest known universal Turing machine by Rogozhin. Also, all of the universalmachines we present here simulate Turing machines in polynomial time.","2-tag system; Bi-tag systems; Computational complexity; Polynomial time; Post system; Small universal Turing machine","2-tag system; Bi-tag systems; Polynomial time; Post system; Small universal Turing machine; Computational complexity; Polynomial approximation; Turing machines; Machinery",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-68249106606
"Anton T.","55599798100;","X path-wrapper induction by generalizing tree traversal patterns",2005,"Lernen, Wissensentdeckung und Adaptivitat, LWA 2005",,,,"","",8,26,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-70350652136&partnerID=40&md5=3e397c44277ae475ac89dac421ca9153","We introduce a wrapper induction algorithm for extracting information from tree-structured documents like HTML or XML. It derives XPathcompatible extraction rules from a set of annotated example documents. The approach builds a minimally generalized tree traversal pattern, and augments it with conditions. Another variant selects a subset of conditions so that (a) the pattern is consistent with the training data, (b) the pattern's document coverage is minimized, and (c) conditions that match structures preceding the target nodes are preferred. We discuss the robustness of rules induced by this selection strategy and we illustrate how these rules exhibit knowledge of the target concept.",,"(c) condition; Extracting information; Extraction rule; Target concept; Target nodes; Training data; Tree traversal; Tree-structured documents; Wrapper induction; Forestry; Trees (mathematics)",,"Lernen, Wissensentdeckung und Adaptivitat, LWA 2005","10 October 2005 through 12 October 2005","Saarbrucken",95548,Conference Paper,"Final","",Scopus,2-s2.0-70350652136
"Lin C.Y.","","Knowledge-based automatic topic identification",1995,"Proceedings Meeting of the Association for Computational Linguistics (ACL 95)",,,,"308","310",,26,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-4544268174
"Wang G., Jia Q.-S., Qiao J., Bi J., Liu C.","57226171686;8353811800;9634105900;55847050400;57211215736;","A sparse deep belief network with efficient fuzzy learning framework",2020,"Neural Networks","121",,,"430","440",,25,"10.1016/j.neunet.2019.09.035","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073015074&doi=10.1016%2fj.neunet.2019.09.035&partnerID=40&md5=6bfde4b2aed5b3dec2ea75bf3f96855e","Deep belief network (DBN) is one of the most feasible ways to realize deep learning (DL) technique, and it has been attracting more and more attentions in nonlinear system modeling. However, DBN cannot provide satisfactory results in learning speed, modeling accuracy and robustness, which is mainly caused by dense representation and gradient diffusion. To address these problems and promote DBN's development in cross-models, we propose a Sparse Deep Belief Network with Fuzzy Neural Network (SDBFNN) for nonlinear system modeling. In this novel framework, the sparse DBN is considered as a pre-training technique to realize fast weight-initialization and to obtain feature vectors. It can balance the dense representation to improve its robustness. A fuzzy neural network is developed for supervised modeling so as to eliminate the gradient diffusion. Its input happens to be the obtained feature vector. As a novel cross-model, SDBFNN combines the advantages of both pre-training technique and fuzzy neural network to improve modeling capability. Its convergence is also analyzed as well. A benchmark problem and a practical problem in wastewater treatment are conducted to demonstrate the superiority of SDBFNN. The extensive experimental results show that SDBFNN achieves better performance than the existing methods in learning speed, modeling accuracy and robustness. © 2019 Elsevier Ltd","Deep belief network; Deep learning; Fuzzy neural network; Nonlinear system modeling; Sparse representation","Deep learning; Deep neural networks; Fuzzy inference; Fuzzy logic; Nonlinear systems; Wastewater treatment; Bench-mark problems; Deep belief network (DBN); Deep belief networks; Feature vectors; Nonlinear system modeling; Practical problems; Sparse representation; Weight initialization; Fuzzy neural networks; article; deep belief network; deep learning; diffusion; nonlinear system; velocity; waste water management; nonlinear system; Deep Learning; Nonlinear Dynamics",,,,,,Article,"Final","",Scopus,2-s2.0-85073015074
"Garcez A.D., Gori M., Lamb L.C., Serafini L., Spranger M., Tran S.N.","6603397393;7005254436;10045847500;7005585409;23092382400;56272651200;","Neural-symbolic computing: An effective methodology for principled integration of machine learning and reasoning",2019,"Journal of Applied Logics","6","4",,"611","631",,25,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071341558&partnerID=40&md5=e1d012e084b1620e3ad817e17155d3dc","Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems. © 2019, College Publications. All rights reserved.",,,,,,,,Editorial,"Final","",Scopus,2-s2.0-85071341558
"Wang F., Hu L., Zhou J., Hu J., Zhao K.","57199195626;34770075600;55793835900;56720561300;18435554700;","A semantics-based approach to multi-source heterogeneous information fusion in the internet of things",2017,"Soft Computing","21","8",,"2005","2013",,25,"10.1007/s00500-015-1899-7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84945218970&doi=10.1007%2fs00500-015-1899-7&partnerID=40&md5=f280efcd925519e2a06ff97f6530990c","Multi-source heterogeneous information fusion in the Internet of Things (IoT) is a type of information processing that attempts to provide a comprehensive, timely and accurate perception and feedback relative to physical things. Traditional multi-sensor data fusion can deal with the same type of data effectively. However, as new characteristics emerge in the IoT, interoperable service-oriented technologies are required to share real-world data among heterogeneous devices to integrate and fuse the multi-source heterogeneous IoT data. To address these issues, an architecture that can provide guidance for the development of IoT information fusion is required. We compare features of IoT data and information with an existing wireless sensor network and the Internet, which, to the best of our knowledge, is the first comparison of this kind. Then, we design a framework for multi-source heterogeneous information fusion in the IoT and use an experimental simulation platform to build an environmental monitoring system to assess the framework. © 2015, Springer-Verlag Berlin Heidelberg.","Heterogeneous information fusion; Internet of things; Semantic sensor network","Data fusion; Information fusion; Internet; Interoperability; Semantic Web; Semantics; Sensor data fusion; Wireless sensor networks; Environmental monitoring system; Experimental simulations; Heterogeneous devices; Heterogeneous information; Internet of thing (IOT); Interoperable services; Multisensor data fusion; Semantic sensors; Internet of things",,,,,,Article,"Final","",Scopus,2-s2.0-84945218970
"Norton D., Heath D., Ventura D.","57196523811;55354928800;24402340500;","Finding creativity in an artificial artist",2013,"Journal of Creative Behavior","47","2",,"106","124",,25,"10.1002/jocb.27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880355377&doi=10.1002%2fjocb.27&partnerID=40&md5=42a1295eadf1dff928c6093965e9f5be","Creativity is an important component of human intelligence, and imbuing artificially intelligent systems with creativity is an interesting challenge. In particular, it is difficult to quantify (or even qualify) creativity. Recently, it has been suggested that conditions for attributing creativity to a system include: appreciation, imagination, and skill. We demonstrate and describe an original computer system (called DARCI) that is designed to produce images through creative means. We present methods for evaluating DARCI and other artificially creative systems with respect to appreciation, imagination, and skill, and use these methods to show that DARCI is arguably a creative system. © 2013 by the Creative Education Foundation, Inc.","Computational creativity; Evaluation of creativity; Evolutionary algorithm; Visual art",,,,,,,Article,"Final","",Scopus,2-s2.0-84880355377
"Wennekers T., Palm G.","7003286476;55159447300;","Syntactic sequencing in Hebbian cell assemblies",2009,"Cognitive Neurodynamics","3","4",,"429","441",,25,"10.1007/s11571-009-9095-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249093570&doi=10.1007%2fs11571-009-9095-z&partnerID=40&md5=49ee8a720339515c9b7b639ab3b02ac6","Hebbian cell assemblies provide a theoretical framework for the modeling of cognitive processes that grounds them in the underlying physiological neural circuits. Recently we have presented an extension of cell assemblies by operational components which allows to model aspects of language, rules, and complex behaviour. In the present work we study the generation of syntactic sequences using operational cell assemblies timed by unspecific trigger signals. Syntactic patterns are implemented in terms of hetero-associative transition graphs in attractor networks which cause a directed flow of activity through the neural state space. We provide regimes for parameters that enable an unspecific excitatory control signal to switch reliably between attractors in accordance with the implemented syntactic rules. If several target attractors are possible in a given state, noise in the system in conjunction with a winner-takes-all mechanism can randomly choose a target. Disambiguation can also be guided by context signals or specific additional external signals. Given a permanently elevated level of external excitation the model can enter an autonomous mode, where it generates temporal grammatical patterns continuously. © 2009 Springer Science+Business Media B.V.","Attractor networks; Behaviour; Cell assemblies; Grammar; Language","article; cognition; conceptual framework; grammar; hebbian cell assembly; language; model; noise; syntactic sequencing",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-72249093570
"Hameed Z., Garcia-Zapirain B.","57215612782;35732954700;","Sentiment Classification Using a Single-Layered BiLSTM Model",2020,"IEEE Access","8",,"9069952","73992","74001",,24,"10.1109/ACCESS.2020.2988550","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084349433&doi=10.1109%2fACCESS.2020.2988550&partnerID=40&md5=cf436a52659dc3f086da807a30e7dd3d","This study presents a computationally efficient deep learning model for binary sentiment classification, which aims to decide the sentiment polarity of people's opinions, attitudes, and emotions expressed in written text. To achieve this, we exploited three widely practiced datasets based on public opinions about movies. We utilized merely one bidirectional long short-term memory (BiLSTM) layer along with a global pooling mechanism and achieved an accuracy of 80.500%, 85.780%, and 90.585% on MR, SST2 and IMDb datasets, respectively. We concluded that the performance metrics of our proposed approach are competitive with the recently published models, having comparatively complex architectures. Also, it is inferred that the proposed single-layered BiLSTM based architecture is computationally efficient and can be recommended for real-time applications in the field of sentiment analysis. © 2013 IEEE.","Bidirectional long short-term memory; deep learning; long-term dependencies; natural language processing; sentiment analysis","Behavioral research; Computational efficiency; Sentiment analysis; Complex architectures; Computationally efficient; Learning models; Performance metrics; Public opinions; Real-time application; Sentiment classification; Written texts; Deep learning",,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85084349433
"Wu W., Guo Z., Zhou X., Wu H., Zhang X., Lian R., Wang H.","57212065561;57212062976;57192070492;7405584325;57216621355;57204210310;57192674011;","Proactive human-machine conversation with explicit conversation goals",2020,"ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",,,,"3794","3804",,24,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084072500&partnerID=40&md5=c8c271ac709cf166e1a5f7a3c2be9908","Though great progress has been made for human-machine conversation, current dialogue system is still in its infancy: it usually converses passively and utters words more as a matter of response, rather than on its own initiatives. In this paper, we take a radical step towards building a human-like conversational agent: endowing it with the ability of proactively leading the conversation (introducing a new topic or maintaining the current topic). To facilitate the development of such conversation systems, we create a new dataset named DuConv where one acts as a conversation leader and the other acts as the follower. The leader is provided with a knowledge graph and asked to sequentially change the discussion topics, following the given conversation goal, and meanwhile keep the dialogue as natural and engaging as possible. DuConv enables a very challenging task as the model needs to both understand dialogue and plan over the given knowledge graph. We establish baseline results on this dataset (about 270K utterances and 30k dialogues) using several state-of-the-art models. Experimental results show that dialogue models that plan over the knowledge graph can make full use of related knowledge to generate more diverse multi-turn conversations. The baseline systems along with the dataset are publicly available. © 2019 Association for Computational Linguistics",,"Speech processing; Baseline results; Baseline systems; Conversation systems; Conversational agents; Dialogue models; Dialogue systems; Knowledge graphs; State of the art; Computational linguistics","Apple;ASAPP;Bloomberg Engineering;BOSCH;et al.;Expedia","57th Annual Meeting of the Association for Computational Linguistics, ACL 2019","28 July 2019 through 2 August 2019",,159206,Conference Paper,"Final","",Scopus,2-s2.0-85084072500
"Mihalcea R., Garimella A.","8619220500;57191076846;","What Men Say, What Women Hear: Finding Gender-Specific Meaning Shades",2016,"IEEE Intelligent Systems","31","4","7515120","62","67",,24,"10.1109/MIS.2016.71","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84986275095&doi=10.1109%2fMIS.2016.71&partnerID=40&md5=74ec2830ea5b451d11bf91e4900b68f3","The authors examine the problem of gender discrimination and attempt to move beyond the typical surface-level text classification approach by identifying differences between genders in the ways they use the same words. They present several experiments using data from a large collection of blogs authored by men and women, and they report results for a new task of 'gender-based word disambiguation' for a set of over 350 words. © 2016 IEEE.","affective computing; gender discrimination; gender-based word disambiguation; intelligent systems; sentiment analysis","Classification (of information); Intelligent systems; Text processing; Affective Computing; Gender discrimination; gender-based word disambiguation; Sentiment analysis; Text classification; Social sciences",,,,,,Article,"Final","",Scopus,2-s2.0-84986275095
"Carlson A.J., Cumby C.M., Rosen J.L., Roth D.","","Snow user guide",1999,"SNoW User Guide",,,,"","",,24,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-25544477442
"Chen Wenhu, Wang Hongmin, Chen Jianshu, Zhang Yunkai, Wang Hong, Li Shiyang, Zhou Xiyou, Wang William Yang","","Tabfact: A large-scale dataset for table-based fact verification",2020,"International Conference on Learning Representations",,,,"","",,23,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85097812080
"Yang C., Akimoto Y., Kim D.W., Udell M.","57210634828;57210643418;57210635842;55985937900;","OBoe: Collaborative filtering for automl model selection",2019,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"1173","1183",,23,"10.1145/3292500.3330909","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071193000&doi=10.1145%2f3292500.3330909&partnerID=40&md5=17a480fcb9105767bc3185ff76f4f4da","Algorithm selection and hyperparameter tuning remain two of the most challenging tasks in machine learning. Automated machine learning (AutoML) seeks to automate these tasks to enable widespread use of machine learning by non-experts. This paper introduces Oboe, a collaborative filtering method for time-constrained model selection and hyperparameter tuning. Oboe forms a matrix of the cross-validated errors of a large number of supervised learning models (algorithms together with hyperparameters) on a large number of datasets, and fits a low rank model to learn the low-dimensional feature vectors for the models and datasets that best predict the cross-validated errors. To find promising models for a new dataset, Oboe runs a set of fast but informative algorithms on the new dataset and uses their cross-validated errors to infer the feature vector for the new dataset. Oboe can find good models under constraints on the number of models fit or the total time budget. To this end, this paper develops a new heuristic for active learning in time-constrained matrix completion based on optimal experiment design. Our experiments demonstrate that Oboe delivers state-of-the-art performance faster than competing approaches on a test bed of supervised learning problems. Moreover, the success of the bilinear model used by Oboe suggests that AutoML may be simpler than was previously understood. © 2019 Association for Computing Machinery.","AutoML; Collaborative filtering; Meta-learning; Model selection; Time-constrained","Budget control; Collaborative filtering; Data mining; Errors; Large dataset; Learning algorithms; Matrix algebra; Supervised learning; AutoML; Collaborative filtering methods; Metalearning; Model Selection; Optimal experiment design; State-of-the-art performance; Supervised learning problems; Time-constrained; Machine learning","ACM SIGKDD;ACM SIGMOD","25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2019","4 August 2019 through 8 August 2019",,149966,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85071193000
"Liu C.-Z., Sheng Y.-X., Wei Z.-Q., Yang Y.-Q.","57204705116;36642846900;7402259116;36095104900;","Research of Text Classification Based on Improved TF-IDF Algorithm",2018,"2018 IEEE International Conference of Intelligent Robotic and Control Engineering, IRCE 2018",,,"8492945","69","73",,23,"10.1109/IRCE.2018.8492945","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85056800899&doi=10.1109%2fIRCE.2018.8492945&partnerID=40&md5=daabe5f6259c0f81278f5d99145db43a","In recent years, with the rapid development of Internet Technology, text data is growing rapidly every day. Users need to filter out the information they need from a large amount of text. Therefore, automatic text classification technology can help users find information. In order to address problems, such as ignoring contextual semantic links and different vocabulary importance in traditional text classification techniques, this paper presents a vector representation of feature words based on the deep learning tool Word2vec, and the weight of the feature words is calculated by the improved TF-IDF algorithm. By multiplying the weight of the word and the word vector, the vector representation of the word is realized. Finally, each text is represented by accumulating all the word vectors. Thus, text classification is carried out. © 2018 IEEE.","text classification; text representation; TF-IDF; Word2vec model","Classification (of information); Deep learning; Intelligent robots; Robotics; Semantics; Vectors; Automatic text classification; Contextual semantics; Improved TF-IDF; Internet technology; Text classification; Text representation; TF-IDF; Vector representations; Text processing","IEEE","2018 IEEE International Conference of Intelligent Robotic and Control Engineering, IRCE 2018","24 August 2018 through 27 August 2018",,141211,Conference Paper,"Final","",Scopus,2-s2.0-85056800899
"Kurach K., Andrychowicz M., Sutskever I.","55566223600;56333296900;24831264500;","Neural random-access machines",2016,"4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings",,,,"","",,23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083953578&partnerID=40&md5=743475195425b03ac7c066a70e4c01d0","In this paper, we propose and investigate a new neural network architecture called Neural Random Access Machine. It can manipulate and dereference pointers to an external variable-size random-access memory. The model is trained from pure input-output examples using backpropagation. We evaluate the new model on a number of simple algorithmic tasks whose solutions require pointer manipulation and dereferencing. Our results show that the proposed model can learn to solve algorithmic tasks of such type and is capable of operating on simple data structures like linked-lists and binary trees. For easier tasks, the learned solutions generalize to sequences of arbitrary length. Moreover, memory access during inference can be done in a constant time under some assumptions. © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.",,"Backpropagation algorithms; Binary trees; Memory architecture; Network architecture; Trees (mathematics); Constant time; Input-output; Memory access; Random access machines; Random access memory; Variable sizes; Random access storage",,"4th International Conference on Learning Representations, ICLR 2016","2 May 2016 through 4 May 2016",,149803,Conference Paper,"Final","",Scopus,2-s2.0-85083953578
"Dang S., Ahmad P.H.","","Text mining: Techniques and its application",2014,"International Journal of Engineering & Technology Innovations","1","4",,"22","25",,23,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85044921073
"Álvez J., Lucio P., Rigau G.","57199053482;8642037500;22735472300;","Adimen-SUMO: Reengineering an ontology for first-order reasoning",2012,"International Journal on Semantic Web and Information Systems","8","4",,"80","116",,23,"10.4018/jswis.2012100105","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84877914117&doi=10.4018%2fjswis.2012100105&partnerID=40&md5=68dd6557c7f2ca09dc73574641fce7c0","In this paper, the authors present Adimen-SUMO, an operational ontology to be used by first-order theorem provers in intelligent systems that require sophisticated reasoning capabilities (e.g. Natural Language Processing, Knowledge Engineering, Semantic Web infrastructure, etc.). Adimen-SUMO has been obtained by automatically translating around 88% of the original axioms of SUMO (Suggested Upper Merged Ontology). Their main interest is to present in a practical way the advantages of using first-order theorem provers during the design and development of first-order ontologies. First-order theorem provers are applied as inference engines for reengineering a large and complex ontology in order to allow for formal reasoning. In particular, the authors' study focuses on providing first-order reasoning support to SUMO. During the process, they detect, explain and repair several important design flaws and problems of the SUMO axiomatization. As a by-product, they also provide general design decisions and good practices for creating operational first-order ontologies of any kind. Copyright © 2012, IGI Global.","Automated reasoning; First-order logic; Knowledge engineering; Linked open data; Ontologies","Automated reasoning; Design and Development; First order logic; Formal reasoning; General designs; Linked open datum; NAtural language processing; Reasoning capabilities; Intelligent systems; Knowledge engineering; Natural language processing systems; Ontology; Repair; Theorem proving; Reengineering",,,,,,Article,"Final","",Scopus,2-s2.0-84877914117
"Tanaka H., Kinoshita A., Kobayakawa T., Kumano T., Kato N.","","Syntax-driven sentence revision for broadcast news summarization",2009,"Proceedings of the 2009 Workshop on Language Generation and Summarisation",,,,"39","47",,23,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84878186415
"Rao D.H., Saraf S.S.","56209933200;57210717317;","Study of defuzzification methods of fuzzy logic controller for speed control of a DC motor",1996,"Proceedings of the IEEE International Conference on Power Electronics, Drives & Energy Systems for Industrial Growth, PEDES","2",,,"782","787",,23,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029702850&partnerID=40&md5=e31599ca31f20b21b5ba03d26b00e800","A typical Fuzzy Logic Controller (FLC) has the following components: fuzzification, knowledge base, decision making and defuzzification. Various defuzzification techniques have been proposed in the literature. The efficacy of a FLC depends very much on the defuzzification process. This is so because the overall performance of the system under control is determined by the controlling signal (the defuzzified output of the FLC) the system receives. The aim of this paper is to evaluate qualitatively the performance of the different defuzzification techniques as applied to speed control of a DC motor.",,"Control theory; DC motors; Decision making; Error analysis; Fuzzy sets; Knowledge based systems; Performance; Speed control; Defuzzification methods; Fuzzification; Fuzzy control","IEEE","Proceedings of the 1996 International Conference on Power Electronics, Drives & Energy Systems for Industrial Growth, PEDES'96. Part 1 (of 2)","8 January 1996 through 11 January 1996","New Delhi, India",45232,Conference Paper,"Final","",Scopus,2-s2.0-0029702850
"Cohn T., Lapata M.","15043872200;55910108500;","An abstractive approach to sentence compression",2013,"ACM Transactions on Intelligent Systems and Technology","4","3","41","","",,22,"10.1145/2483669.2483674","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880227825&doi=10.1145%2f2483669.2483674&partnerID=40&md5=cb0645b273c934d554dac6f734b3cbd7","In this article we generalize the sentence compression task. Rather than simply shorten a sentence by deleting words or constituents, as in previous work, we rewrite it using additional operations such as substitution, reordering, and insertion. We present an experimental study showing that humans can naturally create abstractive sentences using a variety of rewrite operations, not just deletion. We next create a new corpus that is suited to the abstractive compression task and formulate a discriminative tree-to-tree transduction model that can account for structural and lexical mismatches. The model incorporates a grammar extraction method, uses a language model for coherent output, and can be easily tuned to a wide range of compression-specific loss functions. ©2013 ACM.","Language generation; Language models; Machine translation; Paraphrases; Sentence compression; Synchronous grammars; Transduction","Language generation; Language model; Machine translations; Paraphrases; Sentence compression; Synchronous grammars; Transduction; Bacteriophages; Forestry; Computational linguistics; Forestry; Languages; Models",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84880227825
"Cabessa J., Villa A.E.P.","24400916700;7201597625;","The expressive power of analog recurrent neural networks on infinite input streams",2012,"Theoretical Computer Science","436",,,"23","34",,22,"10.1016/j.tcs.2012.01.042","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84860776133&doi=10.1016%2fj.tcs.2012.01.042&partnerID=40&md5=e73044ec68ec98c8ddcbe784cf89406a","We consider analog recurrent neural networks working on infinite input streams, provide a complete topological characterization of their expressive power, and compare it to the expressive power of classical infinite word reading abstract machines. More precisely, we consider analog recurrent neural networks as language recognizers over the Cantor space, and prove that the classes of ω-languages recognized by deterministic and non-deterministic analog networks correspond precisely to the respective classes of Π20-sets and Σ11-sets of the Cantor space. Furthermore, we show that the result can be generalized to more expressive analog networks equipped with any kind of Borel accepting condition. Therefore, in the deterministic case, the expressive power of analog neural nets turns out to be comparable to the expressive power of any kind of Büchi abstract machine, whereas in the non-deterministic case, analog recurrent networks turn out to be strictly more expressive than any other kind of Büchi or Muller abstract machine, including the main cases of classical automata, 1-counter automata, k-counter automata, pushdown automata, and Turing machines. © 2011 Elsevier B.V. All rights reserved.","ω-Automata; Analog computation; Analog neural networks; Analytic sets; Borel sets; Topology; Turing machines","Abstract machines; Analog computation; Analog neural network; Analog recurrent neural network; Analytic sets; Borel set; Cantor spaces; Expressive power; Infinite word; Input streams; Push-down automata; Recurrent networks; Analog computers; Machinery; Topology; Turing machines; Recurrent neural networks",,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-84860776133
"Baranyi Peter, Lei Kin-fong, Yam Yeung","7006304237;7102208056;7005965300;","Complexity reduction of singleton based neuro-fuzzy algorithm",2000,"Proceedings of the IEEE International Conference on Systems, Man and Cybernetics","4",,,"2503","2508",,22,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0034505020&partnerID=40&md5=10142c2f146cef3afd1d7dba63ae0e71","During the past few years efficient singular value-based complexity reduction tools have been developed to fuzzy logic techniques. This paper introduces a singular value-based reduction method to the generalised type neural network. The method conducts singular value decomposition of the weighting functions defined on the connections among the neurons and generates certain linear combinations of the original weighting functions to form a new connection-net for the complexity reduced neural network.",,"Algorithms; Computational complexity; Fuzzy sets; Mathematical models; Complexity reduction tools; Neural networks","IEEE","2000 IEEE International Conference on Systems, Man and Cybernetics","8 October 2000 through 11 October 2000","Nashville, TN, USA",57755,Conference Paper,"Final","",Scopus,2-s2.0-0034505020
"Bazzanella C.","",[No title available],1996,"Repetition in Dialogue",,,,"","",,22,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0009297005
"Cummins N., Amiriparian S., Ottl S., Gerczuk M., Schmitt M., Schuller B.","55350497300;56964296000;57200084423;57200079240;57191862153;6603767415;","Multimodal bag-of-words for cross domains sentiment analysis",2018,"ICASSP, IEEE International Conference on Acoustics, Speech and Signal Processing - Proceedings","2018-April",,"8462660","4954","4958",,21,"10.1109/ICASSP.2018.8462660","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85054241452&doi=10.1109%2fICASSP.2018.8462660&partnerID=40&md5=7fa49bb28abafacc166b91a30c6804bc","The advantages of using cross domain data when performing text-based sentiment analysis have been established; however, similar findings have yet to be observed when performing multimodal sentiment analysis. A potential reason for this is that systems based on feature extracted from speech and facial features are susceptible to confounding effecting caused by different recording conditions associated with data collected in different locations. In this regard, we herein explore different Bag-of-Words paradigms to aid sentiment detection by providing training material from an additional dataset. Key results presented indicate that using a Bag-of-Words extraction paradigm that takes into account information from both the test domain and the out of domain datasets yields gains in system performance. © 2018 IEEE.","Bag-of-Words; Cross Domain; Deep Spectrum Features; Multimodal; Sentiment Analysis",,"The Institute of Electrical and Electronics Engineers Signal Processing Society","2018 IEEE International Conference on Acoustics, Speech, and Signal Processing, ICASSP 2018","15 April 2018 through 20 April 2018",,139797,Conference Paper,"Final","",Scopus,2-s2.0-85054241452
"González J.-A., Pla F., Hurtado L.-F.","","ELiRF-UPV at SemEval-2017 Task 4: Sentiment Analysis using Deep Learning",2017,"Proceedings of the 11th International Workshop on Semantic Evaluation",,,,"722","726",,21,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85027870841
"Ge L., Moh T.-S.","57215130219;6603612117;","Improving text classification with word embedding",2017,"Proceedings - 2017 IEEE International Conference on Big Data, Big Data 2017","2018-January",,,"1796","1805",,21,"10.1109/BigData.2017.8258123","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85047810548&doi=10.1109%2fBigData.2017.8258123&partnerID=40&md5=9f45b9a98ec17b302a9f70d925c7a59c","One challenge in text classification is that it is difficult to make feature reductions based on the definition of the features. An ineffective feature reduction may even worsen the classification accuracy. Word2Vec, a word embedding method, has recently been gaining popularity due to its high precision rate of analyzing the semantic similarity between words at relatively low computational cost. However, there is limited research about feature reduction using Word2Vec. In this project, we developed a method using Word2Vec to reduce the feature size while increasing the classification accuracy. We achieved feature reduction by loosely clustering similar features using graph search techniques. Similarity thresholds above 0.5 were used in our method to pair and cluster the features. Finally, we utilized Multinomial Naïve Bayes classifier, Support Vector Machine, K Nearest Neighbor and Random Forest classifier to evaluate the effect of our method. Four datasets with dimensions up to 100,000 feature size and 400,000 document size were used to evaluate the result of our method. The result showed that around 4-10% feature reduction was achieved with up to 1-4% improvement of classification accuracy in terms of different datasets and classifiers. Meanwhile, we also succeeded in improving feature reduction and classification accuracy by combining our method with other classic feature reduction techniques such as chi-square and mutual information. © 2017 IEEE.","Feature reduction; KNN; Machine learning; Naïve Bayes; SVM; Word embedding; Word2Vec","Big data; Data reduction; Decision trees; Embeddings; Learning systems; Nearest neighbor search; Reduction; Semantics; Support vector machines; Text processing; Classification accuracy; Computational costs; Feature reduction; Mutual informations; Random forest classifier; Similarity threshold; Word embedding; Word2Vec; Classification (of information)","Cisco;Elsevier;IEEE;IEEE Computer Society;The Mit Press","5th IEEE International Conference on Big Data, Big Data 2017","11 December 2017 through 14 December 2017",,134260,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85047810548
"Guo S., Wang Q., Wang B., Wang L., Guo L.","55515151500;57190702381;55584805140;57007714500;56564475400;","SSE: Semantically Smooth Embedding for Knowledge Graphs",2017,"IEEE Transactions on Knowledge and Data Engineering","29","4","7779046","884","897",,21,"10.1109/TKDE.2016.2638425","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015772213&doi=10.1109%2fTKDE.2016.2638425&partnerID=40&md5=acaec73c01113a655e9d60136b074259","This paper considers the problem of embedding Knowledge Graphs (KGs) consisting of entities and relations into low-dimensional vector spaces. Most of the existing methods perform this task based solely on observed facts. The only requirement is that the learned embeddings should be compatible within each individual fact. In this paper, aiming at further discovering the intrinsic geometric structure of the embedding space, we propose Semantically Smooth Embedding (SSE). The key idea of SSE is to take full advantage of additional semantic information and enforce the embedding space to be semantically smooth, i.e., entities belonging to the same semantic category will lie close to each other in the embedding space. Two manifold learning algorithms Laplacian Eigenmaps and Locally Linear Embedding are used to model the smoothness assumption. Both are formulated as geometrically based regularization terms to constrain the embedding task. Two lines of embedding strategies are tested, i.e., strategies based on latent distance models and strategies based on tensor factorization techniques. We empirically evaluate SSE on two benchmark tasks of link prediction and triple classification, and achieve significant and consistent improvements over state-of-the-art methods. The results demonstrate the superiority and generality of SSE. © 2016 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.","Knowledge graph embedding; Laplacian eigenmaps; locally linear embedding; semantic smoothness","Laplace transforms; Semantics; Embedding strategies; Knowledge graphs; Laplacian eigenmaps; Locally linear embedding; Manifold learning algorithm; Regularization terms; State-of-the-art methods; Tensor factorization; Vector spaces",,,,,,Article,"Final","",Scopus,2-s2.0-85015772213
"Huang X., Rao Y., Xie H., Wong T.-L., Wang F.L.","57195955074;55753260800;57219619828;9734661600;7501312845;","Cross-domain sentiment classification via topic-related tradaboost",2017,"31st AAAI Conference on Artificial Intelligence, AAAI 2017",,,,"4939","4940",,21,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028593472&partnerID=40&md5=d3aec6a2d0b3ef1a6a715783825373b6","Cross-domain sentiment classification aims to tag sentiments for a target domain by labeled data from a source domain. Due to the difference between domains, the accuracy of a trained classifier may be very low. In this paper, we propose a boosting-based learning framework named TR-TrAdaBoost for cross-domain sentiment classification. We firstly explore the topic distribution of documents, and then combine it with the unigram TrAdaBoost. The topic distribution captures the domain information of documents, which is valuable for cross-domain sentiment classification. Experimental results indicate that TR-TrAdaBoost represents documents well and boost the performance and robustness of TrAdaBoost. Copyright © 2017, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Classification (of information); Cross-domain; Domain informations; Labeled data; Learning frameworks; Sentiment classification; Target domain; Topic distributions; Tradaboost; Information retrieval systems","Amazon;Artificial Intelligence;Baidu;et al.;IBM;Tencent","31st AAAI Conference on Artificial Intelligence, AAAI 2017","4 February 2017 through 10 February 2017",,130407,Conference Paper,"Final","",Scopus,2-s2.0-85028593472
"Mehdad Y., Carenini G., Ng R.T.","36705115700;6604008365;7102153783;","Abstractive summarization of spoken and written conversations based on phrasal queries",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference","1",,,"1220","1230",,21,"10.3115/v1/p14-1115","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906930162&doi=10.3115%2fv1%2fp14-1115&partnerID=40&md5=459564107009ab9e80a385f2a143c42a","We propose a novel abstractive querybased summarization system for conversations, where queries are defined as phrases reflecting a user information needs. We rank and extract the utterances in a conversation based on the overall content and the phrasal query information. We cluster the selected sentences based on their lexical similarity and aggregate the sentences in each cluster by means of a word graph model. We propose a ranking strategy to select the best path in the constructed graph as a query-based abstract sentence for each cluster. A resulting summary consists of abstractive sentences representing the phrasal query information and the overall content of the conversation. Automatic and manual evaluation results over meeting, chat and email conversations show that our approach significantly outperforms baselines and previous extractive models. © 2014 Association for Computational Linguistics.",,"Abstracting; Computational linguistics; Graph theory; Best paths; Evaluation results; Lexical similarity; Query information; Ranking strategy; Summarization systems; User information need; Word graphs; Query processing","amazon.com;Baidu;Bloomberg;et al.;Google;IBM Watson","52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014","22 June 2014 through 27 June 2014","Baltimore, MD",107373,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84906930162
"Bredeweg B., Liem J., Beek W., Linnebank F., Gracia J., Lozano E., Wißner M., Bühling R., Salles P., Noble R., Zitek A., Borisova P., Mioduser D.","6602372016;22938338200;36463094500;24070828900;55392626700;57215482982;12768108900;36463169300;6701350463;12788053400;6505965907;55583448500;6601961381;","DynaLearn - An intelligent learning environment for learning conceptual knowledge",2013,"AI Magazine","34","4",,"46","65",,21,"10.1609/aimag.v34i4.2489","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84896343330&doi=10.1609%2faimag.v34i4.2489&partnerID=40&md5=9e4295558701fc23cb09ae7a75d3bc12","Articulating thought in computerbased media is a powerful means for humans to develop their understanding of phenomena. We have created DynaLearn, an intelligent learning environment that allows learners to acquire conceptual knowledge by constructing and simulating qualitative models of how systems behave. DynaLearn uses diagrammatic representations for learners to express their ideas. The environment is equipped with semantic technology components that are capable of generating knowledge- based feedback and virtual characters that enhance the interaction with learners. Teachers have created course material, and successful evaluation studies have been performed. This article presents an overview of the DynaLearn system. Copyright © 2013, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Knowledge based systems; Semantics; Teaching; Conceptual knowledge; Course material; Diagrammatic representations; Evaluation study; Intelligent learning environments; Qualitative model; Semantic technologies; Virtual character; Computer aided instruction",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84896343330
"Polosukhin I., Skidanov A.","57193222468;57190488374;","Neural program search: Solving programming tasks from description and examples",2018,"6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings",,,,"","",,20,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083953158&partnerID=40&md5=3bab14528a7be7ce527783e7d8f4eb85","We present a Neural Program Search, an algorithm to generate programs from natural language description and a small number of input / output examples. The algorithm combines methods from Deep Learning and Program Synthesis fields by designing rich domain-specific language (DSL) and defining efficient search algorithm guided by a Seq2Tree model on it. To evaluate the quality of the approach we also present a semi-synthetic dataset of descriptions with test examples and corresponding programs. We show that our algorithm significantly outperforms a sequence-to-sequence model with attention baseline. © 6th International Conference on Learning Representations, ICLR 2018 - Workshop Track Proceedings. All rights reserved.",,"Natural language processing systems; Problem oriented languages; Statistical tests; Domain specific languages; Input/output; Natural languages; Program synthesis; Programming tasks; Search Algorithms; Sequence modeling; Test examples; Deep learning",,"6th International Conference on Learning Representations, ICLR 2018","30 April 2018 through 3 May 2018",,149807,Conference Paper,"Final","",Scopus,2-s2.0-85083953158
"Gopal S., Yang Y.","36447640800;35231480000;","Hierarchical bayesian inference and recursive regularization for large-scale classification",2015,"ACM Transactions on Knowledge Discovery from Data","9","3","18","","",,20,"10.1145/2629585","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84929156906&doi=10.1145%2f2629585&partnerID=40&md5=4219efde9b2b8c8bea3647fc5a1971cd","In this article, we address open challenges in large-scale classification, focusing on how to effectively leverage the dependency structures (hierarchical or graphical) among class labels, and how to make the inference scalable in jointly optimizing all model parameters. We propose two main approaches, namely the hierarchical Bayesian inference framework and the recursive regularization scheme. The key idea in both approaches is to reinforce the similarity among parameter across the nodes in a hierarchy or network based on the proximity and connectivity of the nodes. For scalability, we develop hierarchical variational inference algorithms and fast dual coordinate descent training procedures with parallelization. In our experiments for classification problems with hundreds of thousands of classes and millions of training instances with terabytes of parameters, the proposed methods show consistent and statistically significant improvements over other competing approaches, and the best results on multiple benchmark datasets for large-scale classification. © 2015 ACM.","Bayesian methods; Hierarchical classification; Large-scale optimization","Bayesian networks; Classification (of information); Bayesian methods; Dependency structures; Hierarchical bayesian; Hierarchical classification; Large scale classifications; Large-scale optimization; Regularization schemes; Variational inference; Inference engines",,,,,,Article,"Final","",Scopus,2-s2.0-84929156906
"Choi E., Kwiatkowski T., Zettlemoyer L.","56581051000;51665167200;6506824663;","Scalable semantic parsing with partial ontologies",2015,"ACL-IJCNLP 2015 - 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, Proceedings of the Conference","1",,,"1311","1320",,20,"10.3115/v1/p15-1127","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84943740568&doi=10.3115%2fv1%2fp15-1127&partnerID=40&md5=868c5daf1bfa883e53d8869a214b8189","We consider the problem of building scalable semantic parsers for Freebase, and present a new approach for learning to do partial analyses that ground as much of the input text as possible without requiring that all content words be mapped to Freebase concepts. We study this problem on two newly introduced large-scale noun phrase datasets, and present a new semantic parsing model and semi-supervised learning approach for reasoning with partial ontological support. Experiments demonstrate strong performance on two tasks: referring expression resolution and entity attribute extraction. In both cases, the partial analyses allow us to improve precision over strong baselines, while parsing many phrases that would be ignored by existing techniques. © 2015 Association for Computational Linguistics.",,"Computational linguistics; Large dataset; Learning algorithms; Machine learning; Ontology; Semantics; Supervised learning; Entity attribute extractions; New approaches; Noun phrase; Referring expressions; Semantic parsing; Semi- supervised learning; Natural language processing systems","Alibaba Group;Baidu;CreditEase;et al.;Samsung;Tencent","53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing of the Asian Federation of Natural Language Processing, ACL-IJCNLP 2015","26 July 2015 through 31 July 2015",,114195,Conference Paper,"Final","",Scopus,2-s2.0-84943740568
"Feblowitz D., Kauchak D.","","Sentence simplification as tree transduction",2013,"Proceedings of the Second Workshop on Predicting and Improving Text Readability for Target Reader Populations",,,,"1","10",,20,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84907531672
"Kandasamy I., Vasantha W.B., Obbineni J.M., Smarandache F.","57209205242;57210213215;55263583000;6506230265;","Sentiment analysis of tweets using refined neutrosophic sets",2020,"Computers in Industry","115",,"103180","","",,19,"10.1016/j.compind.2019.103180","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076731765&doi=10.1016%2fj.compind.2019.103180&partnerID=40&md5=a3708414891acfbd05f004373b309f17","In the last decade, opinion mining and sentiment analysis have been the subject of fascinating interdisciplinary research. Alongside the evolution of social media networks, the sheer volume of social media text available for sentiment analysis has increased multi-fold, leading to a formidable corpus. Sentiment analysis of tweets have been carried out to gauge public opinion on breaking news, various policies, legislations, personalities and social movements. Fuzzy logic has been used in the sentiment analysis of twitter data, whereas neutrosophy which factors in the concept of indeterminacy has not been used to analyse tweets. In this paper, the concept of multi refined neutrosophic set (MRNS) with two positive, three indeterminate and two negative memberships is proposed. Single valued neutrosophic set (SVNS), triple refined indeterminate neutrosophic set (TRINS) and MRNS have been used in the sentiment analysis of tweets on ten different topics. Eight of these topics chosen for sentiment analysis are related to Indian scenario and two topics to international scenario. A comparative analysis of the methods show that the approach with MRNS provides better refinement to the indeterminacy present in the data. © 2019 Elsevier B.V.","Indeterminacy; Multi refined neutrosophic set (MRNS); Neutrosophic logic; Neutrosophy; Opinion mining; Refined neutrosophic sets; Sentiment analysis; Single valued neutrosophic set (SVNS); Social media; Text mining; Triple refined indeterminate neutrosophic set (TRINS); Tweets","Computer circuits; Data mining; Fuzzy logic; Social aspects; Social networking (online); Indeterminacy; Neutrosophic logic; Neutrosophic sets; Neutrosophy; Opinion mining; Social media; Text mining; Tweets; Sentiment analysis",,,,,,Article,"Final","",Scopus,2-s2.0-85076731765
"Martin A.E.","55506353600;","A compositional neural architecture for language",2020,"Journal of Cognitive Neuroscience","32","8",,"1407","1427",,19,"10.1162/jocn_a_01552","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084999818&doi=10.1162%2fjocn_a_01552&partnerID=40&md5=91c6cb89ac31b4fe489afff5c1ca06e1","Hierarchical structure and compositionality imbue human language with unparalleled expressive power and set it apart from other perception–action systems. However, neither formal nor neurobiological models account for how these defining computational properties might arise in a physiological system. I attempt to reconcile hierarchy and compositionality with principles from cell assembly computation in neuroscience; the result is an emerging theory of how the brain could convert distributed perceptual representations into hierarchical structures across multiple timescales while representing interpretable incremental stages of (de) compositional meaning. The model’s architecture—a multidimensional coordinate system based on neurophysiological models of sensory processing—proposes that a manifold of neural trajectories encodes sensory, motor, and abstract linguistic states. Gain modulation, including inhibition, tunes the path in the manifold in accordance with behavior and is how latent structure is inferred. As a consequence, predictive information about upcoming sensory input during production and comprehension is available without a separate operation. The proposed processing mechanism is synthesized from current models of neural entrainment to speech, concepts from systems neuroscience and category theory, and a symbolic-connectionist computational model that uses time and rhythm to structure information. I build on evidence from cognitive neuroscience and computational modeling that suggests a formal and mechanistic alignment between structure building and neural oscillations, and moves toward unifying basic insights from linguistics and psycholinguistics with the currency of neural computation. © 2020 Massachusetts Institute of Technology",,"Computational linguistics; Hierarchical systems; Neurology; Physiological models; Cognitive neurosciences; Computational properties; Hierarchical structures; Neurobiological model; Neurophysiological model; Perceptual representations; Predictive information; Structure information; Computational neuroscience; article; brain; cognitive neuroscience; comprehension; computer model; human; human experiment; oscillation; perception; psycholinguistics; rhythm; sensory stimulation; speech; theoretical study",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85084999818
"Peng H., Li J., Wang S., Wang L., Gong Q., Yang R., Li B., Yu P., He L.","","Hierarchical taxonomy-aware and attentional graph capsule RCNNs for large-scale multi-label text classification",2019,"IEEE Trans. Knowl. Data Eng.",,,,"","",,19,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85080033838
"Qian Y., Santus E., Jin Z., Guo J., Barzilay R.","57204470508;57189268006;57216691426;55863382800;23007765100;","Graphie: A graph-based framework for information extraction",2019,"NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference","1",,,"751","761",,19,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084071725&partnerID=40&md5=8585317537c8d47e9f0ffdcad86041cf","Most modern Information Extraction (IE) systems are implemented as sequential taggers and only model local dependencies. Non-local and non-sequential context is, however, a valuable source of information to improve predictions. In this paper, we introduce GraphIE, a framework that operates over a graph representing a broad set of dependencies between textual units (i.e. words or sentences). The algorithm propagates information between connected nodes through graph convolutions, generating a richer representation that can be exploited to improve word-level predictions. Evaluation on three different tasks - namely textual, social media and visual information extraction - shows that GraphIE consistently outperforms the state-of-the-art sequence tagging model by a significant margin. © 2019 Association for Computational Linguistics",,"Computational linguistics; Graphic methods; Information retrieval; Graph-based; Information extraction systems; Nonlocal; Social media; State of the art; Tagging models; Word level; Graph algorithms","Amazon;ASAPP;Bloomberg Engineering;et al.;facebook;Google","2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019","2 June 2019 through 7 June 2019",,159851,Conference Paper,"Final","",Scopus,2-s2.0-85084071725
"Li J., Peng H., Liu L., Xiong G., Du B., Ma H., Wang L., Zakirul Alam Bhuiyan M.","56622508700;57190014707;56895334500;8208800300;25930830100;57198976028;57007714500;57205499884;","Graph CNNs for urban traffic passenger flows prediction",2018,"Proceedings - 2018 IEEE SmartWorld, Ubiquitous Intelligence and Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People and Smart City Innovations, SmartWorld/UIC/ATC/ScalCom/CBDCom/IoP/SCI 2018",,,"8560019","29","36",,19,"10.1109/SmartWorld.2018.00041","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060285351&doi=10.1109%2fSmartWorld.2018.00041&partnerID=40&md5=553abc2cb0944f7b77b1b7bb80b70af2","Urban traffic passenger flows prediction has always been a great challenge in transportation field. Efficiently and correctly predicting the future flows of various regions can improve traffic resources scheduling and reduce the possibility of accidents. However, factors which affect the change of traffic passenger flows are complex, including interlaced lines and stations in large areas, diversified traveling demands for people, accidents and bad weathers. So the predicting algorithms or models should be more sensitive to multiply elements and their effecting patterns. Recently, deep learning performs the excellent ability to extract high dimensional spatial-temporal characters in regression and classification tasks. In this paper, we propose a new modeling method for urban traffic passenger flows. Instead of the grid matrices, we quantify the relationship between stations and represent it by a undirected graph. Then we sort the stations by their passenger flows and construct the two-channels graph flows matrices as the input of deep convolutional neural networks. To increase the temporal information of inputs, we also combine the input matrices with recent historical samples. In addition, we add date markers to correct the final prediction flows to further improve the accuracy. Finally we evaluate our model with the real Beijing subway data and compare with other traditional models on short-term passenger flows prediction tasks. Experiments show that our model including multidimensional flows graph matrices and the deep learning model can significantly improve the prediction accuracy. © 2018 IEEE.","Deep learning; Graph cnn; Graph flows matrices; Urban traffic passenger flows prediction","Accidents; Big data; Deep learning; Deep neural networks; Forecasting; Matrix algebra; Neural networks; Smart city; Trusted computing; Ubiquitous computing; Urban transportation; Classification tasks; Deep convolutional neural networks; Graph cnn; Multidimensional flow; Passenger flows; Prediction accuracy; Temporal information; Traditional models; Flow graphs",,"4th IEEE SmartWorld, 15th IEEE International Conference on Ubiquitous Intelligence and Computing, Advanced and Trusted Computing, Scalable Computing and Communications, Cloud and Big Data Computing, Internet of People and Smart City Innovations, SmartWorld/UIC/ATC/ScalCom/CBDCom/IoP/SCI 2018","7 October 2018 through 11 October 2018",,143372,Conference Paper,"Final","",Scopus,2-s2.0-85060285351
"He Y., Li J., Song Y., He M., Peng H.","57194429723;55720560100;14039604300;57204466271;57190014707;","Time-evolving text classification with deep neural networks",2018,"IJCAI International Joint Conference on Artificial Intelligence","2018-July",,,"2241","2247",,19,"10.24963/ijcai.2018/310","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055700079&doi=10.24963%2fijcai.2018%2f310&partnerID=40&md5=3e6342d5cbcf50803062f66b3a107925","Traditional text classification algorithms are based on the assumption that data are independent and identically distributed. However, in most non-stationary scenarios, data may change smoothly due to long-term evolution and short-term fluctuation, which raises new challenges to traditional methods. In this paper, we present the first attempt to explore evolutionary neural network models for time-evolving text classification. We first introduce a simple way to extend arbitrary neural networks to evolutionary learning by using a temporal smoothness framework, and then propose a diachronic propagation framework to incorporate the historical impact into currently learned features through diachronic connections. Experiments on real-world news data demonstrate that our approaches greatly and consistently outperform traditional neural network models in both accuracy and stability. © 2018 International Joint Conferences on Artificial Intelligence. All right reserved.",,"Backpropagation; Classification (of information); Long Term Evolution (LTE); Neural networks; Text processing; Evolutionary Learning; Evolutionary neural network; Neural network model; Nonstationary; Real-world; Short term; Text classification; Deep neural networks","International Joint Conferences on Artifical Intelligence (IJCAI)","27th International Joint Conference on Artificial Intelligence, IJCAI 2018","13 July 2018 through 19 July 2018",,140653,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85055700079
"Pighin D., Cornolti M., Alfonseca E., Filippova K.","24399735700;35955724700;8966688000;22937480400;","Modelling events through memory-based, open-IE patterns for abstractive summarization",2014,"52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014 - Proceedings of the Conference","1",,,"892","901",,19,"10.3115/v1/p14-1084","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906922800&doi=10.3115%2fv1%2fp14-1084&partnerID=40&md5=84f52316136b83c7ccc0c16fabf67be5","Abstrective text summarization of news requires a way of representing events, such as a collection of pattern clusters in which every cluster represents an event (e.g., marriage) and every pattern in the cluster is a way of expressing the event (e.g., X married Y, X and Y tied the knot). We compare three ways of extracting event patterns: heuristics-based, compressionbased and memory-based. While the former has been used previously in multidocument abstraction, the latter two have never been used for this task. Compared with the first two techniques, the memorybased method allows for generating significantly more grammatical and informative sentences, at the cost of searching a vast space of hundreds of millions of parse trees of known grammatical utterances. To this end, we introduce a data structure and a search method that make it possible to efficiently extrapolate from every sentence the parse sub-trees that match against any of the stored utterances. © 2014 Association for Computational Linguistics.",,"Computational linguistics; Forestry; Event pattern; Multi-document; Parse trees; Search method; Sub trees; Text summarization; Trees (mathematics); Forestry; Trees","amazon.com;Baidu;Bloomberg;et al.;Google;IBM Watson","52nd Annual Meeting of the Association for Computational Linguistics, ACL 2014","22 June 2014 through 27 June 2014","Baltimore, MD",107373,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-84906922800
"Huyck C.R.","9246912400;","A psycholinguistic model of natural language parsing implemented in simulated neurons",2009,"Cognitive Neurodynamics","3","4",,"317","330",,19,"10.1007/s11571-009-9080-6","https://www.scopus.com/inward/record.uri?eid=2-s2.0-72249096236&doi=10.1007%2fs11571-009-9080-6&partnerID=40&md5=fa2a72b7dd3c635c06329b8ff304742c","A natural language parser implemented entirely in simulated neurons is described. It produces a semantic representation based on frames. It parses solely using simulated fatiguing Leaky Integrate and Fire neurons, that are a relatively accurate biological model that is simulated efficiently. The model works on discrete cycles that simulate 10 ms of biological time, so the parser has a simple mapping to psychological parsing time. Comparisons to human parsing studies show that the parser closely approximates this data. The parser makes use of Cell Assemblies and the semantics of lexical items is represented by overlapping hierarchical Cell Assemblies so that semantically related items share neurons. This semantic encoding is used to resolve prepositional phrase attachment ambiguities encountered during parsing. Consequently, the parser provides a neurally-based cognitive model of parsing. © 2009 Springer Science+Business Media B.V.","Fatiguing Leaky Integrate and Fire (fLIF) neurons; Natural language parsing; Prepositional phrase attachment; Timing","ambiguity; article; biological model; controlled study; language; linguistics; nerve cell; neuropsychology; semantics; simulation; speech",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-72249096236
"Qiu R.G.","7102716181;","Towards ontology-driven knowledge synthesis for heterogeneous information systems",2006,"Journal of Intelligent Manufacturing","17","1",,"99","109",,19,"10.1007/s10845-005-5515-z","https://www.scopus.com/inward/record.uri?eid=2-s2.0-30844471985&doi=10.1007%2fs10845-005-5515-z&partnerID=40&md5=49ce03a3b725f28f3920c1f062f0bfea","Information integration enables delivery of the right information to the right user in a timely manner giving manufacturers a competitive edge in today's global manufacturing market. However, as enterprise information is usually aggregated from a variety of heterogeneous information sources, without using an adequate integration framework it is difficult to extract pertinent information and apply current knowledge to assessing production situations and making informed decisions. This paper investigates a method of facilitating knowledge synthesis in a distributed computing environment. A formal model of domain ontology and knowledge base is presented, which aims at providing a vehicle for representing information and knowledge using a common shared semantics in a given application domain. As a result, a common knowledge representation based architecture is proposed, creating a foundation for establishing a systematic approach for ease of knowledge synthesis in a manufacturing environment. © 2006 Springer Science+Business Media, Inc.","Decision-making; E-manufacturing; Heterogeneity; Information fusion; Knowledge synthesis","Decision making; Electronic commerce; Information science; Manufacturing data processing; Mathematical models; Semantics; E-manufacturing; Heterogeneity; Information fusion; Knowledge synthesis; Shared semantics; Knowledge based systems",,,,,,Article,"Final","",Scopus,2-s2.0-30844471985
"Strumillo P., Kaminski W.","","Radial basis function neural networks: Theory and applicatiuons",2003,"Neural Networks and Soft Computing",,,,"107","119",,19,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-9444256993
"Hölldobler S.","","A structured connectionist unification algorithm",1990,"Proceedings of the AAAI National Conference on Artificial Intelligence",,,,"587","593",,19,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-4243893462
"Duque A., Stevenson M., Martinez-Romo J., Araujo L.","56533533900;7201804246;24476245900;57197210585;","Co-occurrence graphs for word sense disambiguation in the biomedical domain",2018,"Artificial Intelligence in Medicine","87",,,"9","19",,18,"10.1016/j.artmed.2018.03.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044304673&doi=10.1016%2fj.artmed.2018.03.002&partnerID=40&md5=e692687380879e9ff244b191089df3b5","Word sense disambiguation is a key step for many natural language processing tasks (e.g. summarization, text classification, relation extraction) and presents a challenge to any system that aims to process documents from the biomedical domain. In this paper, we present a new graph-based unsupervised technique to address this problem. The knowledge base used in this work is a graph built with co-occurrence information from medical concepts found in scientific abstracts, and hence adapted to the specific domain. Unlike other unsupervised approaches based on static graphs such as UMLS, in this work the knowledge base takes the context of the ambiguous terms into account. Abstracts downloaded from PubMed are used for building the graph and disambiguation is performed using the personalized PageRank algorithm. Evaluation is carried out over two test datasets widely explored in the literature. Different parameters of the system are also evaluated to test robustness and scalability. Results show that the system is able to outperform state-of-the-art knowledge-based systems, obtaining more than 10% of accuracy improvement in some cases, while only requiring minimal external resources. © 2018 Elsevier B.V.","Graph-based systems; Information extraction; Natural language processing; Unified medical language system; Unsupervised machine learning; Word sense disambiguation","Abstracting; Classification (of information); Graphic methods; Information retrieval; Information retrieval systems; Knowledge based systems; Learning algorithms; Learning systems; Medical information systems; Text processing; Co-occurrence informations; Graph-based; Personalized PageRank; Unified medical language systems; Unsupervised approaches; Unsupervised machine learning; Unsupervised techniques; Word Sense Disambiguation; Natural language processing systems; accuracy; algorithm; ambiguity; Article; classification; human; information processing; knowledge base; Medline; natural language processing; priority journal; Unified Medical Language System; unsupervised machine learning; word sense disambiguation; information processing; knowledge base; natural language processing; semantics; Algorithms; Datasets as Topic; Knowledge Bases; Natural Language Processing; PubMed; Semantics; Unified Medical Language System",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85044304673
"Wu K., Zhou M., Sean Lu X., Huang L.","57201284881;7403506743;57190165354;57190885511;","Fuzzy logic-based text classification method for social media data",2017,"2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017","2017-January",,,"1942","1947",,18,"10.1109/SMC.2017.8122902","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85044188867&doi=10.1109%2fSMC.2017.8122902&partnerID=40&md5=d4e1207c8eba067cf86431761ead0d34","Social media offer abundant information for studying people's behaviors, emotions and opinions during the evolution of various rare events such as natural disasters. It is useful to analyze the correlation between social media and human-affected events. This study uses Hurricane Sandy 2012 related Twitter text data to conduct information extraction and text classification. Considering that the original data contains different topics, we need to find the data related to Hurricane Sandy. A fuzzy logic-based approach is introduced to solve the problem of text classification. Inputs used in the proposed fuzzy logic-based model are multiple useful features extracted from each Twitter's message. The output is its degree of relevance for each message to Sandy. A number of fuzzy rules are designed and different defuzzification methods are combined in order to obtain desired classification results. We compare the proposed method with the well-known keyword search method in terms of correctness rate and quantity. The result shows that the proposed fuzzy logic-based approach is more suitable to classify Twitter messages than keyword word method. © 2017 IEEE.","Fuzzy logic; Social media; Text classification","Computer circuits; Cybernetics; Data mining; Disasters; Electroplating shops; Fuzzy inference; Fuzzy logic; Hurricanes; Search engines; Social networking (online); Text processing; Classification results; Defuzzification method; Degree of relevance; Fuzzy logic based approach; Social media; Social media datum; Text classification; Text classification methods; Classification (of information)","","2017 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2017","5 October 2017 through 8 October 2017",,133297,Conference Paper,"Final","",Scopus,2-s2.0-85044188867
"Effrosynidis D., Symeonidis S., Arampatzis A.","57195719948;56292325400;8246621200;","A comparison of pre-processing techniques for twitter sentiment analysis",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10450 LNCS",,,"394","406",,18,"10.1007/978-3-319-67008-9_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85029572033&doi=10.1007%2f978-3-319-67008-9_31&partnerID=40&md5=6365acddf7a69d0da5d81d515f6cfa44","Pre-processing is considered to be the first step in text classification, and choosing the right pre-processing techniques can improve classification effectiveness. We experimentally compare 15 commonly used pre-processing techniques on two Twitter datasets. We employ three different machine learning algorithms, namely, Linear SVC, Bernoulli Naïve Bayes, and Logistic Regression, and report the classification accuracy and the resulting number of features for each pre-processing technique. Finally, based on our results, we categorize these techniques based on their performance. We find that techniques like stemming, removing numbers, and replacing elongated words improve accuracy, while others like removing punctuation do not. © Springer International Publishing AG 2017.","Machine learning; Sentiment analysis; Text classification; Text pre-processing","Artificial intelligence; Classification (of information); Data mining; Digital libraries; Learning algorithms; Learning systems; Natural language processing systems; Social networking (online); Bernoulli; Classification accuracy; Logistic regressions; Pre-processing; Sentiment analysis; Text classification; Text processing","The Coalition for Networked Information (CNI)","21st International Conference on Theory and Practice of Digital Libraries, TPDL 2017","18 September 2017 through 21 September 2017",,197829,Conference Paper,"Final","",Scopus,2-s2.0-85029572033
"Barrès V., Simons A., Arbib M.","55484891100;55510177800;7006188087;","Synthetic event-related potentials: A computational bridge between neurolinguistic models and experiments",2013,"Neural Networks","37",,,"66","92",,18,"10.1016/j.neunet.2012.09.021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870451109&doi=10.1016%2fj.neunet.2012.09.021&partnerID=40&md5=b9dc4a486a8cea4858e1d12e64d87786","Our previous work developed Synthetic Brain Imaging to link neural and schema network models of cognition and behavior to PET and fMRI studies of brain function. We here extend this approach to Synthetic Event-Related Potentials (Synthetic ERP). Although the method is of general applicability, we focus on ERP correlates of language processing in the human brain. The method has two components: Phase 1: To generate cortical electro-magnetic source activity from neural or schema network models; and Phase 2: To generate known neurolinguistic ERP data (ERP scalp voltage topographies and waveforms) from putative cortical source distributions and activities within a realistic anatomical model of the human brain and head. To illustrate the challenges of Phase 2 of the methodology, spatiotemporal information from Friederici's 2002 model of auditory language comprehension was used to define cortical regions and time courses of activation for implementation within a forward model of ERP data. The cortical regions from the 2002 model were modeled using atlas-based masks overlaid on the MNI high definition single subject cortical mesh. The electromagnetic contribution of each region was modeled using current dipoles whose position and orientation were constrained by the cortical geometry. In linking neural network computation via EEG forward modeling to empirical results in neurolinguistics, we emphasize the need for neural network models to link their architecture to geometrically sound models of the cortical surface, and the need for conceptual models to refine and adopt brain-atlas based approaches to allow precise brain anchoring of their modules. The detailed analysis of Phase 2 sets the stage for a brief introduction to Phase 1 of the program, including the case for a schema-theoretic approach to language production and perception presented in detail elsewhere. Unlike Dynamic Causal Modeling (DCM) and Bojak's mean field model, Synthetic ERP builds on models of networks that mediate the relation between the brain's inputs, outputs, and internal states in executing a specific task. The neural networks used for Synthetic ERP must include neuroanatomically realistic placement and orientation of the cortical pyramidal neurons. These constraints pose exciting challenges for future work in neural network modeling that is applicable to systems and cognitive neuroscience. © 2012 Elsevier Ltd.","Computational model; Conceptual model; EEG; Event-related potential; Forward model; Language; Neural network; Neurolinguistics; Schema theory; Synthetic Brain Imaging","Brain imaging; Computational model; Conceptual model; Event related potentials; Forward models; Language; Neurolinguistics; Schema theory; Brain; Brain models; Cognitive systems; Electroencephalography; Linguistics; Mean field theory; Neural networks; Neuroimaging; article; auditory discrimination; auditory nervous system; biological model; brain cortex; brain depth stimulation; brain electrophysiology; brain function; cognition; comprehension; electric potential; electromagnetic field; event related potential; executive function; geometry; language development; language processing; left hemisphere; linguistics; mental task; nerve cell network; neuroanatomy; neuroscience; priority journal; pyramidal nerve cell; right hemisphere; statistical model; artificial neural network; biological model; brain cortex; cytology; electroencephalography; evoked response; human; methodology; nuclear magnetic resonance imaging; physiology; brain function; brain mapping; brain region; functional anatomy; mathematical computing; mathematical model; Cerebral Cortex; Cognition; Electroencephalography; Evoked Potentials; Humans; Magnetic Resonance Imaging; Models, Neurological; Nerve Net; Neural Networks (Computer); Psycholinguistics; Pyramidal Cells",,,,,,Article,"Final","",Scopus,2-s2.0-84870451109
"Pollak S., Coesemans R., Daelemans W., Lavrač N.","55543643800;55062129000;57191413898;7004388979;","Detecting contrast patterns in newspaper articles by combining discourse analysis and text mining",2011,"Pragmatics","21","4",,"647","684",,18,"10.1075/prag.21.4.07pol","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84857871448&doi=10.1075%2fprag.21.4.07pol&partnerID=40&md5=9238a16e6561cb9974cbf31c3c37393c","Text mining aims at constructing classification models and finding interesting patterns in large text collections. This paper investigates the utility of applying these techniques to media analysis, more specifically to support discourse analysis of news reports about the 2007 Kenyan elections and postelection crisis in local (Kenyan) and Western (British and US) newspapers. It illustrates how text mining methods can assist discourse analysis by finding contrast patterns which provide evidence for ideological differences between local and international press coverage. Our experiments indicate that most significant differences pertain to the interpretive frame of the news events: whereas the newspapers from the UK and the US focus on ethnicity in their coverage, the Kenyan press concentrateson sociopolitical aspects.","Discourse analysis; Ideology; Kenyan elections; Pragmatics; Text mining",,,,,,,Article,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-84857871448
"McKeown K., Rosenthal S., Thadani K., Moore C.","56269175400;56266449200;55105832400;55099770000;","Time-efficient creation of an accurate sentence fusion corpus",2010,"NAACL HLT 2010 - Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics, Proceedings of the Main Conference",,,,"317","320",,18,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84858422326&partnerID=40&md5=3267cbcadbc58fed8f9eb1dcf6fa70ff","Sentence fusion enables summarization and question-answering systems to produce output by combining fully formed phrases from different sentences. Yet there is little data that can be used to develop and evaluate fusion techniques. In this paper, we present a methodology for collecting fusions of similar sentence pairs using Amazon's Mechanical Turk, selecting the input pairs in a semi-automated fashion. We evaluate the results using a novel technique for automatically selecting a representative sentence from multiple responses. Our approach allows for rapid construction of a high accuracy fusion corpus. © 2010 Association for Computational Linguistics.",,"Amazon's mechanical turks; Fusion techniques; Multiple response; Novel techniques; Question answering systems; Rapid construction; Semi-automated; Artificial intelligence; Computational linguistics","AT and T Interactive;Microsoft Research;USC Information Sciences Institute;Google;J.D. Power and Associates","2010 Human Language Technologies Conference ofthe North American Chapter of the Association for Computational Linguistics, NAACL HLT 2010","2 June 2010 through 4 June 2010","Los Angeles, CA",88929,Conference Paper,"Final","",Scopus,2-s2.0-84858422326
"Pulvermüller F.","",[No title available],2002,"The Neuroscience of Language",,,,"","",,18,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84855721958
"Ghaeini R., Hasan S.A., Datla V., Liu J., Lee K., Qadir A., Ling Y., Prakash A., Fern X.Z., Farri O.","","Dr-bilstm: Dependent reading bidirectional lstm for natural language inference",2018,"Dr-bilstm: Dependent reading bidirectional lstm for natural language inference",,,,"","",,17,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85052973703
"Thorne J., Vlachos A., Cocarascu O., Christodoulopoulos C., Mittal A.","","The fact extraction and verification (fever) shared task",2018,"Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)",,"29",,"1","9",,17,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85066906816
"Zhao C., Jiang J., Guan Y., Guo X., He B.","57189217850;56376858400;7202924009;24777794600;56393635700;","EMR-based medical knowledge representation and inference via Markov random fields and distributed representation learning",2018,"Artificial Intelligence in Medicine","87",,,"49","59",,17,"10.1016/j.artmed.2018.03.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85046118718&doi=10.1016%2fj.artmed.2018.03.005&partnerID=40&md5=a63f664cd177a74d4e85360084bd5b84","Objective: Electronic medical records (EMRs) contain medical knowledge that can be used for clinical decision support (CDS). Our objective is to develop a general system that can extract and represent knowledge contained in EMRs to support three CDS tasks—test recommendation, initial diagnosis, and treatment plan recommendation—given the condition of a patient. Methods: We extracted four kinds of medical entities from records and constructed an EMR-based medical knowledge network (EMKN), in which nodes are entities and edges reflect their co-occurrence in a record. Three bipartite subgraphs (bigraphs) were extracted from the EMKN, one to support each task. One part of the bigraph was the given condition (e.g., symptoms), and the other was the condition to be inferred (e.g., diseases). Each bigraph was regarded as a Markov random field (MRF) to support the inference. We proposed three graph-based energy functions and three likelihood-based energy functions. Two of these functions are based on knowledge representation learning and can provide distributed representations of medical entities. Two EMR datasets and three metrics were utilized to evaluate the performance. Results: As a whole, the evaluation results indicate that the proposed system outperformed the baseline methods. The distributed representation of medical entities does reflect similarity relationships with respect to knowledge level. Conclusion: Combining EMKN and MRF is an effective approach for general medical knowledge representation and inference. Different tasks, however, require individually designed energy functions. © 2018 Elsevier B.V.","Clinical decision support; Distributed representation; Electronic medical record; Markov random field; Medical knowledge network","Decision support systems; Function evaluation; Graphic methods; Image segmentation; Knowledge representation; Markov processes; Medical computing; Patient treatment; Clinical decision support; Distributed representation; Electronic medical record; Markov Random Fields; Medical knowledge; Diagnosis; Article; clinical decision support system; clinical effectiveness; clinical evaluation; diagnostic test; early diagnosis; electronic medical record; electronic medical record based medical knowledge network; hidden Markov model; human; knowledge discovery; learning algorithm; Markov random field; mathematical analysis; outcome assessment; priority journal; probability; symptom; treatment indication; treatment planning; algorithm; clinical decision support system; electronic health record; machine learning; Markov chain; statistical model; Algorithms; Decision Support Systems, Clinical; Electronic Health Records; Likelihood Functions; Machine Learning; Markov Chains",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85046118718
"Yoshikawa K., Iida R., Hirao T., Okumura M.","36731906900;14015980400;7201386393;8860407300;","Sentence compression with semantic role constraints",2012,"50th Annual Meeting of the Association for Computational Linguistics, ACL 2012 - Proceedings of the Conference","2",,,"349","353",,17,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876812246&partnerID=40&md5=07b1057d3dbdeb755a7cc50c488c4cee","For sentence compression, we propose new semantic constraints to directly capture the relations between a predicate and its arguments, whereas the existing approaches have focused on relatively shallow linguistic properties, such as lexical and syntactic information. These constraints are based on semantic roles and superior to the constraints of syntactic dependencies. Our empirical evaluation on the Written News Compression Corpus (Clarke and Lapata, 2008) demonstrates that our system achieves results comparable to other state-of-the-art techniques. © 2012 Association for Computational Linguistics.",,"Empirical evaluations; Linguistic properties; Semantic constraints; Semantic roles; Sentence compression; State-of-the-art techniques; Syntactic dependencies; Syntactic information; Computational linguistics; Semantics","Baidu;Google;Elsevier;Microsoft Research;Korea Advanced Institute of Science and Technology (KAIST)","50th Annual Meeting of the Association for Computational Linguistics, ACL 2012","8 July 2012 through 14 July 2012","Jeju Island",97156,Conference Paper,"Final","",Scopus,2-s2.0-84876812246
"Yu T., Simoff S., Jan T.","49664359100;6601912069;7004322283;","VQSVM: A case study for incorporating prior domain knowledge into inductive machine learning",2010,"Neurocomputing","73","13-15",,"2614","2623",,17,"10.1016/j.neucom.2010.05.007","https://www.scopus.com/inward/record.uri?eid=2-s2.0-77955331482&doi=10.1016%2fj.neucom.2010.05.007&partnerID=40&md5=081bc5a16463d105af7b6c5e06052827","When dealing with real-world problems, there is considerable amount of prior domain knowledge that can provide insights on various aspect of the problem. On the other hand, many machine learning methods rely solely on the data sets for their learning phase and do not take into account any explicitly expressed domain knowledge. This paper proposes a framework that investigates and enables the incorporation of prior domain knowledge with respect to three key characteristics of inductive machine learning algorithms: consistency, generalization and convergence. The framework is used to review, classify and analyse key existing approaches to incorporating domain knowledge into inductive machine learning, as well as to consider the risks of doing so. The paper also demonstrates the design of a novel hierarchical semi-parametric machine learning method, capable of incorporating prior domain knowledge. The method-VQSVM-extends the support vector machine (SVM) family of methods with vector quantization (VQ) techniques to address the problem of learning from imbalanced data sets. The paper presents the results of testing the method on a collection of imbalanced data sets with various imbalance ratios and various numbers of subclasses. The learning process of the VQSVM method utilizes some domain knowledge to solve problem of fitting imbalance data. The experiments in the paper demonstrate that enabling the incorporation of prior domain knowledge into the SVM framework is an effective way to overcome the sensitivity of SVM towards the imbalance ratio in a data set. © 2010 Elsevier B.V.","Imbalance data; Inductive machine learning; Prior domain knowledge; Support vector machine","Data sets; Domain knowledge; Imbalanced data sets; Inductive machine learning; Key characteristics; Learning phase; Learning process; Machine learning methods; Real-world problem; Semiparametric; Convergence of numerical methods; Machine design; Problem solving; Support vector machines; Vector quantization; Vectors; Learning algorithms; article; binocular convergence; hypothesis; internal consistency; Internet; knowledge; learning; learning algorithm; machine learning; parametric test; priority journal; support vector machine",,,,,,Article,"Final","",Scopus,2-s2.0-77955331482
"Frisch S., Graben P.B., Schlesewsky M.","7006533679;23110284000;6601962210;","Parallelizing grammatical functions: P600 and P345 reflect different cost of reanalysis",2004,"International Journal of Bifurcation and Chaos in Applied Sciences and Engineering","14","2",,"531","549",,17,"10.1142/S0218127404009533","https://www.scopus.com/inward/record.uri?eid=2-s2.0-3242723750&doi=10.1142%2fS0218127404009533&partnerID=40&md5=e4ae866b089d4cea438577b0de557ec4","It is well-known from psycholinguistic literature that the human language processing system exhibits preferences when sentence constituents are ambiguous with respect to their grammatical function. Generally, many theories assume that an interpretation towards the subject is preferred in such cases. Later disambiguations which contradict such a preference induce enhanced processing difficulty (i.e. reanalysis) which reflects itself in late positive deflections (P345/P600) in event-related brain potentials (ERPs). In the case of phoric elements such as pronouns, a second strategy is known according to which an ambiguous pronoun preferentially receives the grammatical function that its antecedent has (parallel function strategy). In an ERP study, we show that this strategy can in principle override the general subject preference strategy (known for both pronominal and nonpronominal constituents) and induce an object preference, in case that the pronoun's antecedent is itself an object. Interestingly, the revision of a subject preference leads to a P600 component, whereas the revision of an object preference induces an earlier positivity (P345). In order to show that the latter component is indeed a positivity and not an N400-like negativity in the same time range, we apply an additional analysis based on symbolic dynamics which allows to determine the polarity of an ERP effect on purely methodological grounds. With respect to the two positivities, we argue that the latency differences reflect qualitative differences in the reanalysis processes.","Event-related brain potentials (ERP); P345; P600; Parallel function; Subject preference; Symbolic dynamics","Codes (symbols); Computational linguistics; Computer programming languages; Context free grammars; Formal languages; Information analysis; Parallel processing systems; Syntactics; Event-related brain potentials (ERP); P345; P600; Parallel functions; Subject preference; Symbolic dynamics; Functions",,,,,,Conference Paper,"Final","",Scopus,2-s2.0-3242723750
"Moradi M., Dorffner G., Samwald M.","57212307727;56122707500;55876919000;","Deep contextualized embeddings for quantifying the informative content in biomedical text summarization",2020,"Computer Methods and Programs in Biomedicine","184",,"105117","","",,16,"10.1016/j.cmpb.2019.105117","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073186155&doi=10.1016%2fj.cmpb.2019.105117&partnerID=40&md5=e83ca21c07065c195c8e9f537fc69da1","Background and Objective: Capturing the context of text is a challenging task in biomedical text summarization. The objective of this research is to show how contextualized embeddings produced by a deep bidirectional language model can be utilized to quantify the informative content of sentences in biomedical text summarization. Methods: We propose a novel summarization method that utilizes contextualized embeddings generated by the Bidirectional Encoder Representations from Transformers (BERT) model, a deep learning model that recently demonstrated state-of-the-art results in several natural language processing tasks. We combine different versions of BERT with a clustering method to identify the most relevant and informative sentences of input documents. Using the ROUGE toolkit, we evaluate the summarizer against several methods previously described in literature. Results: The summarizer obtains state-of-the-art results and significantly improves the performance of biomedical text summarization in comparison to a set of domain-specific and domain-independent methods. The largest language model not specifically pretrained on biomedical text outperformed other models. However, among language models of the same size, the one further pretrained on biomedical text obtained best results. Conclusions: We demonstrate that a hybrid system combining a deep bidirectional language model and a clustering method yields state-of-the-art results without requiring labor-intensive creation of annotated features or knowledge bases or computationally demanding domain-specific pretraining. This study provides a starting point towards investigating deep contextualized language models for biomedical text summarization. © 2019","Biomedical text mining; Clustering; Contextualized embeddings; Deep learning, domain knowledge; Text summarization","Cluster analysis; Computational linguistics; Deep learning; Embeddings; Hybrid systems; Text processing; Biomedical text minings; Clustering; Clustering methods; Domain independents; Domain knowledge; NAtural language processing; State of the art; Text summarization; Natural language processing systems; article; deep learning; embedding; human; human experiment; knowledge base; mining; natural language processing; algorithm; data mining; medical informatics; natural language processing; procedures; semantics; Unified Medical Language System; Algorithms; Data Mining; Deep Learning; Humans; Medical Informatics; Natural Language Processing; Semantics; Unified Medical Language System",,,,,,Article,"Final","",Scopus,2-s2.0-85073186155
"Jin N., Wu J., Ma X., Yan K., Mo Y.","55413069200;57215127976;57199215239;56263145300;13408519600;","Multi-task learning model based on multi-scale CNN and LSTM for sentiment classification",2020,"IEEE Access","8",,"9076160","77060","77072",,16,"10.1109/ACCESS.2020.2989428","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084802469&doi=10.1109%2fACCESS.2020.2989428&partnerID=40&md5=55a412503d97d6826417c901a06ba356","Sentiment classification is an interesting and crucial research topic in the field of natural language processing (NLP). Data-driven methods, including machine learning and deep learning techniques, provide one direct and effective solution to solve the sentiment classification problem. However, the classification performance declines when the input includes review comments for multiple tasks. The most appropriate way of constructing a sentiment classification model under multi-tasking circumstances remains questionable in the related field. In this study, aiming at the multi-tasking sentiment classification problem, we propose a multi-task learning model based on a multi-scale convolutional neural network (CNN) and long short term memory (LSTM) for multi-task multi-scale sentiment classification (MTL-MSCNN-LSTM). The model comprehensively utilizes and properly handles global features and local features of different scales of text to model and represent sentences. The multi-task learning framework improves the encoder quality, simultaneously improving the results of emotion classification. Six different types of commodity review datasets were employed in the experiment. Using accuracy and F1-score as the metrics to evaluate the performance of the proposed model, comparing with methods such as single-task learning and LSTM encoder, the proposed MTL-MSCNN-LSTM model outperforms most of the existing methods. © 2013 IEEE.","long short term memory; multi-scale convolutional neural network; multi-task learning model; Sentiment classification","Convolutional neural networks; Deep learning; Learning algorithms; Learning systems; Linearization; Multi-task learning; Multitasking; Natural language processing systems; Signal encoding; Classification performance; Data-driven methods; Effective solution; Emotion classification; Learning techniques; NAtural language processing; Sentiment classification; Single task learning; Long short-term memory",,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85084802469
"Wang H., Focke C., Sylvester R., Mishra N., Wang W.","","Fine-tune bert for docred with two-step process",2019,"Fine-tune Bert for DocRED with Two-step Process",,,,"","",,16,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85082534152
"Wang J., Wu N., Zhao W.X., Peng F., Lin X.","36111994400;57210639832;36769862300;57210645276;57213000478;","Empowering A* search algorithms with neural networks for personalized route recommendation",2019,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"539","547",,16,"10.1145/3292500.3330824","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071178917&doi=10.1145%2f3292500.3330824&partnerID=40&md5=9b76a546430dcf49408b03ec6fa8f221","Personalized Route Recommendation (PRR) aims to generate user-specific route suggestions in response to users' route queries. Early studies cast the PRR task as a pathfinding problem on graphs, and adopt adapted search algorithms by integrating heuristic strategies. Although these methods are effective to some extent, they require setting the cost functions with heuristics. In addition, it is difficult to utilize useful context information in the search procedure. To address these issues, we propose using neural networks to automatically learn the cost functions of a classic heuristic algorithm, namely A* algorithm, for the PRR task. Our model consists of two components. First, we employ attention-based Recurrent Neural Networks (RNN) to model the cost from the source to the candidate location by incorporating useful context information. Instead of learning a single cost value, the RNN component is able to learn a time-varying vectorized representation for the moving state of a user. Second, we propose to use a value network for estimating the cost from a candidate location to the destination. For capturing structural characteristics, the value network is built on top of improved graph attention networks by incorporating the moving state of a user and other context information. The two components are integrated in a principled way for deriving a more accurate cost of a candidate location. Extensive experiment results on three real-world datasets have shown the effectiveness and robustness of the proposed model. © 2019 Association for Computing Machinery.","A* Search; Attention; Neural Networks; Personalized Recommendation; Route Recommendation","Cost functions; Data mining; Heuristic algorithms; Heuristic methods; Learning algorithms; Location; Neural networks; Attention; Candidate locations; Context information; Personalized recommendation; Real-world datasets; Recurrent neural network (RNN); Route Recommendation; Structural characteristics; Recurrent neural networks","ACM SIGKDD;ACM SIGMOD","25th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2019","4 August 2019 through 8 August 2019",,149966,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85071178917
"Hu B., Shi C., Zhao W.X., Yang T.","57194042110;55447999200;36769862300;57204945425;","Local and global information fusion for top-n recommendation in heterogeneous information network",2018,"International Conference on Information and Knowledge Management, Proceedings",,,,"1683","1686",,16,"10.1145/3269206.3269278","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85058034036&doi=10.1145%2f3269206.3269278&partnerID=40&md5=49f888bd7aeaca1cef1b0d10d1287128","Since heterogeneous information network (HIN) is able to integrate complex information and contain rich semantics, there is a surge of HIN based recommendation in recent years. Although existing methods have achieved performance improvement to some extent, they still face the following problems: how to extensively exploit and comprehensively explore the local and global information in HIN for recommendation. To address these issues, we propose a unified model LGRec to fuse local and global information for top-N recommendation in HIN. We firstly model most informative local neighbor information for users and items respectively with a co-attention mechanism. In addition, our model learns effective relation representations between users and items to capture rich information in HIN by optimizing a multi-label classification problem. Finally, we combine the two parts into an unified model for top-N recommendation. Extensive experiments on four real-world datasets demonstrate the effectiveness of the proposed model. © 2018 Association for Computing Machinery.","Attention Mechanism; Heterogeneous Information Network; Local and Global Information; Recommender System","Information services; Knowledge management; Recommender systems; Semantics; Attention mechanisms; Complex information; Following problem; Global informations; Heterogeneous information; Multi label classification; Performance improvements; Real-world datasets; Classification (of information)","ACM SIGIR;ACM SIGWEB","27th ACM International Conference on Information and Knowledge Management, CIKM 2018","22 October 2018 through 26 October 2018",,142310,Conference Paper,"Final","",Scopus,2-s2.0-85058034036
"Valkov L., Chaudhari D., Sutton C., Srivastava A., Chaudhuri S.","57202057345;57214342248;57204256039;57205843834;8727948900;","Houdini: Lifelong learning as program synthesis",2018,"Advances in Neural Information Processing Systems","2018-December",,,"8687","8698",,16,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064803205&partnerID=40&md5=52647caa786ccac55201af962b1c1afe","We present a neurosymbolic framework for the lifelong learning of algorithmic tasks that mix perception and procedural reasoning. Reusing high-level concepts across domains and learning complex procedures are key challenges in lifelong learning. We show that a program synthesis approach that combines gradient descent with combinatorial search over programs can be a more effective response to these challenges than purely neural methods. Our framework, called HOUDINI, represents neural networks as strongly typed, differentiable functional programs that use symbolic higher-order combinators to compose a library of neural functions. Our learning algorithm consists of: (1) a symbolic program synthesizer that performs a type-directed search over parameterized programs, and decides on the library functions to reuse, and the architectures to combine them, while learning a sequence of tasks; and (2) a neural module that trains these programs using stochastic gradient descent. We evaluate HOUDINI on three benchmarks that combine perception with the algorithmic tasks of counting, summing, and shortest-path computation. Our experiments show that HOUDINI transfers high-level concepts more effectively than traditional transfer learning and progressive neural networks, and that the typed representation of networks significantly accelerates the search. © 2018 Curran Associates Inc.All rights reserved.",,"Computer software reusability; Stochastic systems; Combinatorial search; Complex procedure; Directed searches; Functional programs; Library functions; Life long learning; Shortest path computations; Stochastic gradient descent; Learning algorithms",,"32nd Conference on Neural Information Processing Systems, NeurIPS 2018","2 December 2018 through 8 December 2018",,147412,Conference Paper,"Final","",Scopus,2-s2.0-85064803205
"Broumi S., Bakali A., Talea M., Smarandache F., Dey A., Sonf L.H.","55919041000;57192556455;55542641500;6506230265;57201837722;57201672276;","Spanning tree problem with neutrosophic edge weights",2018,"Procedia Computer Science","127",,,"190","199",,16,"10.1016/j.procs.2018.01.114","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85045676559&doi=10.1016%2fj.procs.2018.01.114&partnerID=40&md5=7a60f65e37637beaaacf2058a5da626e","Neutrosophic set and neutrosophic logic theory are renowned theories to deal with complex, not clearly explained and uncertain real life problems, in which classical fuzzy sets/models may fail to model properly. This paper introduces an algorithm for finding minimum spanning tree (MST) of an undirected neutrosophic weighted connected graph (abbr. UNWCG) where the arc/edge lengths are represented by a single valued neutrosophic numbers. To build the MST of UNWCG, a new algorithm based on matrix approach has been introduced. The proposed algorithm is compared to other existing methods and finally a numerical example is provided. © 2018 The Authors. Published by Elsevier B.V.","Minimum spanning tree problem; Neutrosophic matrix; Score function; Single valued neutrosophic sets","Clustering algorithms; Computation theory; Data Science; Graph algorithms; Intelligent computing; Matrix algebra; Numerical methods; Connected graph; Minimum Spanning Tree problem; Minimum spanning trees; Neutrosophic logic; Neutrosophic sets; Real-life problems; Score function; Spanning tree problems; Trees (mathematics)","INNS-Morocco Regional Chapter;International Neural Network Society (INNS)","1st International Conference on Intelligent Computing in Data Sciences, ICDS 2017","18 December 2017 through 19 December 2017",,135687,Conference Paper,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85045676559
"Myroniv B., Wu C.-W., Ren Y., Christian A.B., Bajo E., Tseng Y.-C.","","Analyzing user emotions via physiology signals",2017,"Data Sci. Pattern Recogn.","1","2",,"11","25",,16,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85046772970
"Kordopatis-Zilos G., Papadopoulos S., Kompatsiaris I.","56411991900;23095370800;7004756014;","Geotagging Text Content with Language Models and Feature Mining",2017,"Proceedings of the IEEE","105","10","7998610","1971","1986",,16,"10.1109/JPROC.2017.2688799","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028966057&doi=10.1109%2fJPROC.2017.2688799&partnerID=40&md5=79f22b847c2b2ef8bb6272a2505b1132","The large-scale availability of user-generated content in social media platforms has recently opened up new possibilities for studying and understanding the geospatial aspects of real-world phenomena and events. Yet, the large majority of user-generated content lacks proper geographic information (in the form of latitude and longitude coordinates). As a result, the problem of multimedia geotagging, i.e., extracting location information from user-generated text items when this is not explicitly available, has attracted increasing research interest. Here, we present a highly accurate geotagging approach for estimating the locations alluded by text annotations based on refined language models that are learned from massive corpora of social media annotations. We further explore the impact of different feature selection and weighting techniques on the performance of the approach. In terms of evaluation, we employ a large benchmark collection from the MediaEval Placing Task over several years. We demonstrate the consistently superior geotagging accuracy and low median distance error of the proposed approach using various data sets and comparing it against a number of state-of-the-art systems. © 1963-2012 IEEE.","Feature selection; geolocation; geotagging; language model; location estimation","Computational linguistics; Feature extraction; Location; Social networking (online); Feature selection and weighting; Geo-tagging; Geographic information; Geolocations; Language model; Location estimation; Social media platforms; User-generated content; Geographic information systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85028966057
"Ko Y.","8082612400;","A new term-weighting scheme for text classification using the odds of positive and negative class probabilities",2015,"Journal of the Association for Information Science and Technology","66","12",,"2553","2565",,16,"10.1002/asi.23338","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84956935883&doi=10.1002%2fasi.23338&partnerID=40&md5=1f2ea1936b353eb66a51151e1bcda0e6","Text classification (TC) is a core technique for text mining and information retrieval. It has been applied to many applications in many different research and industrial areas. Term-weighting schemes assign an appropriate weight to each term to obtain a high TC performance. Although term weighting is one of the important modules for TC and TC has different peculiarities from those in information retrieval, many term-weighting schemes used in information retrieval, such as term frequency-inverse document frequency (tf-idf), have been used in TC in the same manner. The peculiarity of TC that differs most from information retrieval is the existence of class information. This article proposes a new term-weighting scheme that uses class information using positive and negative class distributions. As a result, the proposed scheme, log tf-TRR, consistently performs better than do other schemes using class information as well as traditional schemes such as tf-idf. © 2015 ASIS&T.","automatic categorization; automatic classification; text mining","Classification (of information); Data mining; Industrial research; Information retrieval; Information use; Automatic categorization; Automatic classification; Class distributions; Class probabilities; Term frequencyinverse document frequency (TF-IDF); Term weighting scheme; Text classification; Text mining; Text processing; classification; industrial area; information retrieval; mining",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84956935883
"Chen Y.","","Convolutional Neural Network for Sentence Classification",2015,"Convolutional Neural Network for Sentence Classification",,,,"","",,16,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84996788005
"Bender M., Pelzer B., Schon C.","55788871500;22635244800;53881954300;","System description: E-KRHyper 1.4 - Extensions for unique names and description logic",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","7898 LNAI",,,"126","134",,16,"10.1007/978-3-642-38574-2_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84879985484&doi=10.1007%2f978-3-642-38574-2_8&partnerID=40&md5=4beca4c27765f18695638fed6f5e4600","Formal ontologies may go beyond first-order logic (FOL) in their expressivity, hindering the usage of common automated theorem provers (ATP) for ontology reasoning. The Unique Name Assumption (UNA) is an extension to FOL that is valuable for ontology specification, allowing the definition of distinct objects. Likewise, the Description Logic SHIQ is a popular language for knowledge representation (KR). This system description provides details on the extension of the prover E-KRHyper by the ability to handle both the UNA and SHIQ. This ATP was developed for embedding in KR applications and hence already equipped with special features and extensions to FOL, making it natural to add the new capabilities in E-KRHyper version 1.4. We report on the theory, the implementation and also the evaluation results of the new features. © 2013 Springer-Verlag.",,"Automated theorem prover; Description logic; Evaluation results; First order logic; Formal ontology; Ontology reasonings; System description; Automata theory; Automation; Formal languages; Knowledge representation; Theorem proving; Data description","CADE Inc.","24th International Conference on Automated Deduction, CADE 2013","9 June 2013 through 14 June 2013","Lake Placid, NY",97770,Conference Paper,"Final","",Scopus,2-s2.0-84879985484
"Howes C., Healey P., Purver M.","","Tracking lexical and syntactic alignment in conversation",2010,"Proceedings of the 20th Annual Conference of the Cognitive Science Society",,,,"2004","2009",,16,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84861779451
"Graja M., Jaoua M., Belguith L.H.","54402703400;35791938700;35791744800;","Statistical framework with knowledge base integration for robust speech understanding of the Tunisian dialect",2015,"IEEE Transactions on Audio, Speech and Language Processing","23","12","23","2311","2321",,15,"10.1109/TASLP.2015.2464687","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84954325308&doi=10.1109%2fTASLP.2015.2464687&partnerID=40&md5=28151a52548ad9af18a3985b231ea3fa","In this paper, we propose a hybrid method for the spoken Tunisian dialect understanding within a limited task. This method couples a discriminative statistical method with a domain ontology. The statistical method is based on conditional random field (CRF) models learned from a little size corpus to perform conceptual labeling task. These models are able to detect the semantic dependency between words. However, the domain ontology is used to add prior knowledge about the task. Our experiments are based on a real spoken Tunisian dialect corpus. The obtained results show that the proposed method is able to improve the performance of CRF models for speech understanding by the integration of the domain ontology. Our method can be exploited for under-resourced languages and Arabic dialects to overcome the lack of linguistic resources. Copyright © 2015 IEEE.","Conditional random field (CRF); Domain ontology; Knowledge base; Speech understanding; Statistical models; Tunisian dialect (TD)","Computational linguistics; Knowledge based systems; Semantics; Statistical methods; Conditional random field; Domain ontologies; Knowledge base; Speech understanding; Tunisian dialect (TD); Random processes",,,,,,Article,"Final","",Scopus,2-s2.0-84954325308
"Speer R., Havasi C.","","ConceptNet 5: A large semantic network for relational knowledge",2012,"Theory and Applications of Natural Language Processing",,,,"","",,15,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84876932893
"Starczewski J.T.","",[No title available],2012,"Advanced concepts in fuzzy logic and systems with membership uncertainty","284",,,"","",,15,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85012169353
"Funke J.","","On the psychology of creativity",2009,"Milieus of creativity: An interdisciplinary approach to spatiality of creativity","2",,,"11","23",,15,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84952714748
"Farkaš I., Crocker M.W.","7007056931;7004995455;","Syntactic systematicity in sentence processing with a recurrent self-organizing network",2008,"Neurocomputing","71","7-9",,"1172","1179",,15,"10.1016/j.neucom.2007.11.025","https://www.scopus.com/inward/record.uri?eid=2-s2.0-40649098781&doi=10.1016%2fj.neucom.2007.11.025&partnerID=40&md5=1cee5504382649747b65f8a046fe80ce","As potential candidates for explaining human cognition, connectionist models of sentence processing must demonstrate their ability to behave systematically, generalizing from a small training set. It has recently been shown that simple recurrent networks and, to a greater extent, echo-state networks possess some ability to generalize in artificial language learning tasks. We investigate this capacity for a recently introduced model that consists of separately trained modules: a recursive self-organizing module for learning temporal context representations and a feedforward two-layer perceptron module for next-word prediction. We show that the performance of this architecture is comparable with echo-state networks. Taken together, these results weaken the criticism of connectionist approaches, showing that various general recursive connectionist architectures share the potential of behaving systematically. © 2008 Elsevier B.V. All rights reserved.","Next-word prediction; Recurrent neural network; Self-organization; Systematicity","Data processing; Feedforward neural networks; Learning algorithms; Self organizing maps; Syntactics; Artificial language learning; Next-word prediction; Syntactic systematicity; Recurrent neural networks; accuracy; article; artificial neural network; automatic speech recognition; computer language; computer model; controlled study; information processing; intermethod comparison; network learning; perceptron; prediction; priority journal; process control; speech articulation; system analysis",,,,,,Article,"Final","",Scopus,2-s2.0-40649098781
"Chen Y., Martins R., Feng Y.","57210928384;36188587200;57026668300;","Maximal multi-layer specification synthesis",2019,"ESEC/FSE 2019 - Proceedings of the 2019 27th ACM Joint Meeting European Software Engineering Conference and Symposium on the Foundations of Software Engineering",,,,"602","612",,14,"10.1145/3338906.3338951","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071951198&doi=10.1145%2f3338906.3338951&partnerID=40&md5=bd476df005937fd424e4111b1e03e498","There has been a significant interest in applying programming-by-example to automate repetitive and tedious tasks. However, due to the incomplete nature of input-output examples, a synthesizer may generate programs that pass the examples but do not match the user intent. In this paper, we propose MARS, a novel synthesis framework that takes as input a multi-layer specification composed by input-output examples, textual description, and partial code snippets that capture the user intent. To accurately capture the user intent from the noisy and ambiguous description, we propose a hybrid model that combines the power of an LSTM-based sequence-to-sequence model with the apriori algorithm for mining association rules through unsupervised learning. We reduce the problem of solving a multi-layer specification synthesis to a Max-SMT problem, where hard constraints encode well-typed concrete programs and soft constraints encode the user intent learned by the hybrid model. We instantiate our hybrid model to the data wrangling domain and compare its performance against Morpheus, a state-of-the-art synthesizer for data wrangling tasks. Our experiments demonstrate that our approach outperforms MORPHEUS in terms of running time and solved benchmarks. For challenging benchmarks, our approach can suggest candidates with rankings that are an order of magnitude better than MORPHEUS which leads to running times that are 15x faster than MORPHEUS. © 2019 ACM.","Machine learning; Max-SMT; Neural networks; Program synthesis","Encoding (symbols); Input output programs; Learning systems; Machine learning; Neural networks; Specifications; Ambiguous description; Apriori algorithms; Hard constraints; Mining associations; Program synthesis; Programming by Example; Sequence modeling; Textual description; Long short-term memory","ACM SIGSOFT","27th ACM Joint European Software Engineering Conference and Symposium on the Foundations of Software Engineering, ESEC/FSE 2019","26 August 2019 through 30 August 2019",,150532,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85071951198
"Peng H., Bao M., Li J., Bhuiyan M.Z.A., Liu Y., He Y., Yang E.","57190014707;57194435374;55720560100;35184906400;57194433649;57194429723;23993299800;","Incremental term representation learning for social network analysis",2018,"Future Generation Computer Systems","86",,,"1503","1512",,14,"10.1016/j.future.2017.05.020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85020123700&doi=10.1016%2fj.future.2017.05.020&partnerID=40&md5=0db4eb983c22b8feb1713f6ddcd41539","Term representation methods as computable and semantic tools have been widely applied in social network analysis. This paper provides a new perspective that can incrementally factorize co-occurrence matrix to query latest semantic vectors. We divide the streaming social network data into old and updated training tasks respectively, and factorize the training objective function based on stochastic gradient methods to update vectors. We prove that the incremental objective function is convergent. Experimental results demonstrate that our incremental factorizing can save a substantial amount of time by speeding up training convergence. The smaller the updated data is, the faster the update factorizing process can be, even 30 times faster than existing methods in certain cases. To evaluate the correctness of incremental representation, social text similarity/relatedness, linguistic tasks, network event detection, social user multi-label classification and user clustering for social network analysis are employed as benchmarks in this paper. © 2017 Elsevier B.V.","GloVe model; Incremental learning; Social network analysis; Term representation","Gradient methods; Knowledge representation; Semantics; Social networking (online); Stochastic systems; Text processing; Co-occurrence-matrix; Incremental learning; Multi label classification; Objective functions; Semantic vectors; Stochastic gradient methods; Term representation; User clustering; Classification (of information)",,,,,,Article,"Final","",Scopus,2-s2.0-85020123700
"Kumari U., Sharma A.K., Soni D.","57202987747;56956516000;57200527300;","Sentiment analysis of smart phone product review using SVM classification technique",2018,"2017 International Conference on Energy, Communication, Data Analytics and Soft Computing, ICECDS 2017",,,,"1469","1474",,14,"10.1109/ICECDS.2017.8389689","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85050108754&doi=10.1109%2fICECDS.2017.8389689&partnerID=40&md5=0035da230ded12b47daa57da9fbba566","There is a massive increase in number of people who access various social networking and micro-blogging websites that gives new shape to the impression of today's generation. Several reviews for a specific product, brand, individual personality, forum sand movies etc. are very helpful in directing the perception of people. Hence the analysts are commenced to create algorithms to automate the classification of distinctive reviews on the basis of their polarities particularly: Positive, Negative and Neutral. This automated classification mechanism is referred as Sentiment Analysis. The ultimate aim of this paper is to apply Support Vector Machine (SVM) classification technique to classify the sentiment sand texts for smart phone product review that analyses different datasets used for classification of sentiments and texts. Furthermore, various data sets have been utilized for training as well as testing and implemented using Support Vector Machine (SVM) to investigate polarity of the ambiguous tweets. The experimental work includes three performance features such as Precision, Recall and F-measure. On the basis of these features, the accuracy of the different products has been computed. The obtained result approves high accuracy as predicted on the basis of smart phone reviews. © 2017 IEEE.","Clustering; Sentiment Analysis; SVM; Twitter Reviews","Classification (of information); Data mining; Sentiment analysis; Smartphones; Soft computing; Telephone sets; Automated classification; Clustering; High-accuracy; Micro blogging; Number of peoples; Product reviews; Support vector machine classification techniques; SVM classification; Support vector machines","","2017 International Conference on Energy, Communication, Data Analytics and Soft Computing, ICECDS 2017","1 August 2017 through 2 August 2017",,137325,Conference Paper,"Final","",Scopus,2-s2.0-85050108754
"Malon Christopher","","Team papelo: Transformer networks at fever",2018,"Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)",,,,"109","113",,14,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85084074495
"Cabessa J., Villa A.E.P.","24400916700;7201597625;","The super-turing computational power of interactive evolving recurrent neural networks",2013,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","8131 LNCS",,,"58","65",,14,"10.1007/978-3-642-40728-4_8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84884914245&doi=10.1007%2f978-3-642-40728-4_8&partnerID=40&md5=c1111a1dff899adf5a2d5b74e0f91a9f","Understanding the dynamical and computational capabilities of neural models represents an issue of central importance. Here, we consider a model of first-order recurrent neural networks provided with the possibility to evolve over time and involved in a basic interactive and memory active computational paradigm. In this context, we prove that the so-called interactive evolving recurrent neural networks are computationally equivalent to interactive Turing machines with advice, hence capable of super-Turing potentialities. We further provide a precise characterisation of the ω-translations realised by these networks. Therefore, the consideration of evolving capabilities in a first-order neural model provides the potentiality to break the Turing barrier. © 2013 Springer-Verlag Berlin Heidelberg.","analog computation; interactive computation; neural computation; recurrent neural networks; super-Turing; Turing machines with advice","Analog computation; Computational capability; Computational paradigm; Computational power; Interactive computation; Neural computations; super-Turing; Turing barriers; Analog computers; Machinery; Turing machines; Recurrent neural networks",,"23rd International Conference on Artificial Neural Networks, ICANN 2013","10 September 2013 through 13 September 2013","Sofia",99717,Conference Paper,"Final","",Scopus,2-s2.0-84884914245
"Lloret E., Palomar M.","","Analyzing the use of word graphs for abstractive text summarization",2011,"Proceedings of the First International Conference on Advances in Information Mining and Management (IMMM)",,,,"61","66",,14,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84869767046
"Le Q.V., Smola A.J., Gärtner T.","12140898300;6701849799;36883129600;","Simpler knowledge-based support vector machines",2006,"ICML 2006 - Proceedings of the 23rd International Conference on Machine Learning","2006",,,"521","528",,14,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-33749267288&partnerID=40&md5=3eacc831b1f2978556f66fdbe8915191","If appropriately used, prior knowledge can significantly improve the predictive accuracy of learning algorithms or reduce the amount of training data needed. In this paper we introduce a simple method to incorporate prior knowledge in support vector machines by modifying the hypothesis space rather than the optimization problem. The optimization problem is amenable to solution by the constrained concave convex procedure, which finds a local optimum. The paper discusses different kinds of prior knowledge and demonstrates the applicability of the approach in some characteristic experiments.",,"Data reduction; Knowledge based systems; Learning algorithms; Optimization; Predictive control systems; Problem solving; Constrained concave convex procedure; Optimization problems; Support vector machines; Learning systems","Carnegie Mellon University;Google Inc.;Microsoft;IBM Research;The Boeing Company","ICML 2006: 23rd International Conference on Machine Learning","25 June 2006 through 29 June 2006","Pittsburgh, PA",68215,Conference Paper,"Final","",Scopus,2-s2.0-33749267288
"Graben P.B., Jurish B., Saddy D., Frisch S.","23110284000;16637043100;6603176933;7006533679;","Language processing by dynamical systems",2004,"International Journal of Bifurcation and Chaos in Applied Sciences and Engineering","14","2",,"599","621",,14,"10.1142/S0218127404009326","https://www.scopus.com/inward/record.uri?eid=2-s2.0-13844297730&doi=10.1142%2fS0218127404009326&partnerID=40&md5=9a235cfefd53f402a99a0d43ec9b59df","We describe a part of the stimulus sentences of a German language processing ERP experiment using a context-free grammar and represent different processing preferences by its unambiguous partitions. The processing is modeled by deterministic pushdown automata. Using a theorem proven by Moore, we map these automata onto discrete time dynamical systems acting at the unit square, where the processing preferences are represented by a control parameter. The actual states of the automata are rectangles lying in the unit square that can be interpreted as cylinder sets in the context of symbolic dynamics theory. We show that applying a wrong processing preference to a certain input string leads to an unwanted invariant set in the parsers dynamics. Then, syntactic reanalysis and repair can be modeled by a switching of the control parameter - in analogy to phase transitions observed in brain dynamics. We argue that ERP components are indicators of these bifurcations and propose an ERP-like measure of the parsing model.","Ambiguity resolution; Cylinder sets; Entropy; Gödel codes; Language processing; Local ambiguity; Pushdown automata; Symbolic dynamics","Automata theory; Brain; Brain models; Codes (symbols); Entropy; Parameter estimation; Phase transitions; Theorem proving; Ambiguity resolution; Cylinder sets; Gödel codes; Language processing; Local ambiguity; Pushdown automata; Symbolic dynamics; Natural language processing systems",,,,,,Conference Paper,"Final","",Scopus,2-s2.0-13844297730
"Kim Byeongchang, Ahn Jaewoo, Kim Gunhee","","Sequential latent knowledge selection for knowledge-grounded dialogue",2020,"International Conference on Learning Representations",,,,"","",,13,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85094399274
"Gupta Nitish, Lin Kevin, Roth Dan, Singh Sameer, Gardner Matt","","Neural module networks for reasoning over text",2020,"International Conference on Learning Representations",,,,"","",,13,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85094990858
"Liao Y., Bing L., Li P., Shi S., Lam W., Zhang T.","56898379600;24314897600;36440053900;7402200449;57203073460;7404373332;","QuaSE: Sequence editing under quantifiable guidance",2020,"Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018",,,,"3855","3864",,13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081744937&partnerID=40&md5=843c65457248efd54df89b8e67e1929b","We propose the task of Quantifiable Sequence Editing (QuaSE): editing an input sequence to generate an output sequence that satisfies a given numerical outcome value measuring a certain property of the sequence, with the requirement of keeping the main content of the input sequence. For example, an input sequence could be a word sequence, such as review sentence and advertisement text. For a review sentence, the outcome could be the review rating; for an advertisement, the outcome could be the click-through rate. The major challenge in performing QuaSE is how to perceive the outcome-related wordings, and only edit them to change the outcome. In this paper, the proposed framework contains two latent factors, namely, outcome factor and content factor, disentangled from the input sentence to allow convenient editing to change the outcome and keep the content. Our framework explores the pseudo-parallel sentences by modeling their content similarity and outcome differences to enable a better disentanglement of the latent factors, which allows generating an output to better satisfy the desired outcome and keep the content. The dual reconstruction structure further enhances the capability of generating expected output by exploiting the couplings of latent factors of pseudo-parallel sentences. For evaluation, we prepared a dataset of Yelp review sentences with the ratings as outcome. Extensive experimental results are reported and discussed to elaborate the peculiarities of our framework. 1 © 2018 Association for Computational Linguistics",,"Click-through rate; Content similarity; Input sequence; Latent factor; Numerical outcomes; Output sequences; Pseudo parallels; Natural language processing systems","Apple;Bloomberg;et al.;Facebook;Google;salesforce","2018 Conference on Empirical Methods in Natural Language Processing, EMNLP 2018","31 October 2018 through 4 November 2018",,158085,Conference Paper,"Final","",Scopus,2-s2.0-85081744937
"Jain A., Jain V.","","Sentiment classification of twitter data belonging to renewable energy using machine learning",2019,"Journal Information and Optimization Sciences","40","2",,"521","533",,13,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85083682080
"Odmaa B., Yunfei Y., Zhifang S., Damai D., Baobao C., Sujian L., Hongying Z.","","Preliminary study on the construction of Chinese medical knowledge graph",2019,"Journal of Chinese Information Processing","33","10",,"1","9",,13,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85086883684
"Jing K., Xu J., He B.","",[No title available],2019,"A Survey on Neural Network Language Models",,,,"","",,13,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091780888
"Palangi H., Smolensky P., He X., Deng L.","26647742200;6602511111;37085932700;36071490500;","Question-answering with grammatically-interpretable representations",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"5350","5357",,13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040641381&partnerID=40&md5=c8e53d38964f5dbb9e3d4b363906cb17","We introduce an architecture, the Tensor Product Recurrent Network (TPRN). In our application of TPRN, internal representations-learned by end-to-end optimization in a deep neural network performing a textual question-answering (QA) task-can be interpreted using basic concepts from linguistic theory. No performance penalty need be paid for this increased interpretability: the proposed model performs comparably to a state-of-the-art system on the SQuAD QA task. The internal representation which is interpreted is a Tensor Product Representation: for each input word, the model selects a symbol to encode the word, and a role in which to place the symbol, and binds the two together. The selection is via soft attention. The overall interpretation is built from interpretations of the symbols, as recruited by the trained model, and interpretations of the roles as used by the model. We find support for our initial hypothesis that symbols can be interpreted as lexical-semantic word meanings, while roles can be interpreted as approximations of grammatical roles (or categories) such as subject, wh-word, determiner, etc. Fine-grained analysis reveals specific correspondences between the learned roles and parts of speech as assigned by a standard tagger (Toutanova et al. 2003), and finds several discrepancies in the model's favor. In this sense, the model learns significant aspects of grammar, after having been exposed solely to linguistically unannotated text, questions, and answers: no prior linguistic knowledge is given to the model. What is given is the means to build representations using symbols and roles, with an inductive bias favoring use of these in an approximately discrete manner. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Semantics; Syntactics; Tensors; Fine-grained analysis; Initial hypothesis; Internal representation; Interpretable representation; Linguistic knowledge; Performance penalties; Recurrent networks; State-of-the-art system; Deep neural networks","Association for the Advancement of Artificial Intelligence","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","2 February 2018 through 7 February 2018",,143510,Conference Paper,"Final","",Scopus,2-s2.0-85040641381
"Jiang J., Li X., Zhao C., Guan Y., Yu Q.","56376858400;57196471472;57189217850;7202924009;56359402300;","Learning and inference in knowledge-based probabilistic model for medical diagnosis",2017,"Knowledge-Based Systems","138",,,"58","68",,13,"10.1016/j.knosys.2017.09.030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85033491118&doi=10.1016%2fj.knosys.2017.09.030&partnerID=40&md5=2779a352ea62c5ce9f70baee6dfd57c4","Based on a weighted knowledge graph to represent first-order knowledge and combining it with a probabilistic model, we propose a methodology for creating a medical knowledge network (MKN) in medical diagnosis. When a set of evidence is activated for a specific patient, we can generate a ground medical knowledge network that is composed of evidence nodes and potential disease nodes. By incorporating a Boltzmann machine into the potential function of a Markov network, we investigated the joint probability distribution of the MKN. To consider numerical evidence, a multivariate inference model is presented that uses conditional probability. In addition, the weights for the knowledge graph are efficiently learned from manually annotated Chinese Electronic Medical Records (CEMRs) and Blood Examination Records (BERs). In our experiments, we found numerically that an improved expression of evidence variables is necessary for medical diagnosis. Our experimental results comparing a Markov logic network and six kinds of classic machine learning algorithms on the actual CEMR database and BER database indicate that our method holds promise and that MKN can facilitate studies of intelligent diagnosis. © 2017 Elsevier B.V.","First-order knowledge; Gradient descent; Markov logic network; Markov network; Probabilistic model","Computer circuits; Knowledge based systems; Learning algorithms; Learning systems; Markov processes; Medical computing; Probabilistic logics; Probability distributions; First order; Gradient descent; Markov logic networks; Markov networks; Probabilistic modeling; Diagnosis",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85033491118
"Wang X., Dillig I., Singh R.","","Synthesis of data completion scripts using finite tree automata",2017,"Proceedings of the ACM on Programming Languages",,,,"1","62",,13,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85069857984
"Fernández-Garćia S., Desroches M., Krupa M., Clément F.","36617411500;23990353700;7003885195;7102518220;","A multiple time scale coupling of piecewise linear oscillators. Application to a neuroendocrine system",2015,"SIAM Journal on Applied Dynamical Systems","14","2",,"643","673",,13,"10.1137/140984464","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937898515&doi=10.1137%2f140984464&partnerID=40&md5=3b3ad6078778ff434585b715e204c494","We analyze a four-dimensional slow-fast piecewise linear system consisting of two coupled McKean caricatures of the FitzHugh-Nagumo system. Each oscillator is a continuous slow-fast piecewise linear system with three zones of linearity. The coupling is one-way, that is, one subsystem evolves independently and is forcing the other subsystem. In contrast to the original FitzHugh-Nagumo system, we consider a negative slope of the linear nullcline in both the forcing and the forced system. In the forcing system, this lets us, by just changing one parameter, pass from a system having one equilibrium and a relaxation cycle to a system with three equilibria keeping the relaxation cycle. Thus, we can easily control the changes in the oscillation frequency of the forced system. The case with three equilibria and a linear slow nullcline is a new configuration of the McKean caricature, where the existence of the relaxation cycle was not studied previously. We also consider a negative slope of the y-nullcline in the forced system that enables us to reproduce a quasi-steady state called the surge. We analyze not only the qualitative behavior of the four-dimensional system, but also quantitative aspects such as the period, frequency, and amplitude of the oscillations. The system is used to reproduce all the features endowed in a former smooth model and reproduce the secretion pattern of the hypothalamic neurohormone GnRH along the ovarian cycle in different species. © 2015 Society for Industrial and Applied Mathematics.","Coupled oscillators; GnRH secretion; Piecewise linear systems; Relaxation oscillations; Slow-fast dynamics","Linear systems; Oscillators (electronic); Oscillators (mechanical); Physiology; Relaxation oscillators; Coupled oscillators; GnRH secretion; Piece-wise linear systems; Relaxation oscillation; Slow-fast dynamics; Piecewise linear techniques",,,,,,Article,"Final","",Scopus,2-s2.0-84937898515
"Aharrane N., El Moutaouakil K., Satori K.","56705494000;35329465400;35976902500;","A comparison of supervised classification methods for a statistical set of features: Application: Amazigh OCR",2015,"2015 Intelligent Systems and Computer Vision, ISCV 2015",,,"7106171","","",,13,"10.1109/ISACV.2015.7106171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84934324252&doi=10.1109%2fISACV.2015.7106171&partnerID=40&md5=b4fafb21d1ee271223259a15620c25e4","This paper is devoted to the study of supervised learning methods as part of pattern recognition and especially the Amazigh Characters Recognition. The goal is to compare a partial list of the popular automatic classification methods, and test the performance of the proposed features set extracted from isolated characters using statistical methods with these different classifiers. In Experimental evaluation, several runs have been conducted for the different algorithms and the best accuracy observed is for the multilayer perceptron with a recognition rate about 96,47% which is very satisfactory. © 2015 IEEE.","Amazigh; features extraction; intelligent data analysis; Machine learning; OCR; supervised classification","Intelligent systems; Learning systems; Optical character recognition; Pattern recognition; Amazigh; Automatic classification; Experimental evaluation; Features extraction; Intelligent data analysis; Recognition rates; Supervised classification; Supervised learning methods; Supervised learning",,"1st International Conference on Intelligent Systems and Computer Vision, ISCV 2015","25 March 2015 through 26 March 2015",,112332,Conference Paper,"Final","",Scopus,2-s2.0-84934324252
"Kanayama H., Miyao Y., Prager J.","24605030600;8967499500;55942563100;","Answering yes/no questions via question inversion",2012,"24th International Conference on Computational Linguistics - Proceedings of COLING 2012: Technical Papers",,,,"1377","1392",,13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84876795239&partnerID=40&md5=d46b619fed6290b12ebae6a093ce50b1","This paper investigates a solution to yes/no question answering, which can be mapped to the task of determining the correctness of a given proposition. Generally it is hard to obtain explicit evidence to conclude a proposition is false from an information source, so we convert this task to a set of factoid-style questions and use an existing question answering system as a subsystem. By aggregating the answers and confidence values from a factoid-style question answering system we can determine the correctness of the entire proposition or the substitutions that make the proposition false. We evaluated the system on multiple-choice questions from a university admission test on world history, and found it to be highly accurate. © 2012 The COLING.","Facts validation; Question answering; Question inversion; Yes/no question","Facts validation; Information sources; Multiple-choice questions; Question Answering; Question answering systems; Question inversion; University admissions; Yes/no question; Computational linguistics","Tata Consultancy Services;Linguistic Data Consortium for Indian Languages (LDC-IL);Microsoft Research;Beijing Baidu Netcon Science Technology Co. Ltd.;IBM, India Private Limited","24th International Conference on Computational Linguistics, COLING 2012","8 December 2012 through 15 December 2012","Mumbai",96421,Conference Paper,"Final","",Scopus,2-s2.0-84876795239
"Gayler R.W.","12784638500;","Vector symbolic architectures are a viable alternative for Jackendoff's challenges",2006,"Behavioral and Brain Sciences","29","1",,"78","79",,13,"10.1017/S0140525X06309028","https://www.scopus.com/inward/record.uri?eid=2-s2.0-33644992342&doi=10.1017%2fS0140525X06309028&partnerID=40&md5=29b6514de20072aafb794bff0919691d","The authors, on the basis of brief arguments, have dismissed tensor networks as a viable response to Jackendoff's challenges. However, there are reasons to believe that connectionist approaches descended from tensor networks are actually very well suited to answering Jackendoff's challenges. I rebut their arguments for dismissing tensor networks and briefly compare the approaches. © 2006 Cambridge University Press.",,"article; artificial neural network; automated pattern recognition; computer language; computer memory; computer network; computer program; intermethod comparison; mathematical computing; theoretical study",,,,,,Article,"Final","",Scopus,2-s2.0-33644992342
"Bard J.F.","","Practical Bilevel Optimization: Algorithms and Applications",1999,"Nonconvex Optimization and Its Applications","30",,,"","",,13,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0010822868
"Sipser M.","",[No title available],1996,"Introduction to the Theory of Computation",,,,"","",,13,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85008245895
"Rappl G.","36796635400;","On Linear Convergence of a Class of Random Search Algorithms",1989,"ZAMM ‐ Journal of Applied Mathematics and Mechanics / Zeitschrift für Angewandte Mathematik und Mechanik","69","1",,"37","45",,13,"10.1002/zamm.19890690119","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84984028909&doi=10.1002%2fzamm.19890690119&partnerID=40&md5=1d09ee075a8561a81e26d6f2c1aab514","The paper deals with a class of random‐search‐algorithms representing a generalization of the deterministic gradient algorithm in such a way that the gradient direction is replaced by the direction of a random vector uniformly distributed on the unit hypersphere. It is shown that under weak assumptions on the objective function the linear convergence rate of the gradient method can be transferred to these stochastic algorithms. Copyright © 1989 WILEY‐VCH Verlag GmbH & Co. KGaA, Weinheim",,,,,,,,Article,"Final","",Scopus,2-s2.0-84984028909
"Doyle Jon","7403327410;","INS AND OUTS OF REASON MAINTENANCE.",1983,,"1",,,"349","351",,13,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0020909766&partnerID=40&md5=4544490d6fea34124300c72f63b8e56b",[No abstract available],,"ALGORITHMS; DATA ELEMENTS; REASON MAINTENANCE SYSTEMS; STATE DOMAINS; SYSTEMS SCIENCE AND CYBERNETICS","Int Joint Conferences on Artificial Intelligence Inc;Gesellschaft fuer Informatik, Ger;Soc for the Study of Artificial Intelligence & the Simulation;Nederlandse Vereniging voor Kunstmatige Intelligentie, Neth;British Telecom, Engl;et al","Proceedings of the 8th International Joint Conference on Artificial Intelligence. Distributed by William Kaufmann Inc",,"Karlsruhe, W Ger",3703,Conference Paper,"Final","",Scopus,2-s2.0-0020909766
"Brockschmidt M., Allamanis M., Gaunt A.L., Polozov O.","","Generative code modeling with graphs",2018,"Generative Code Modeling with Graphs",,,,"","",,12,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073430719
"Wehrmann J., Cerri R., Barros R.C.","57192664396;56277443600;35247802600;","Hierarchical multi-label classification networks",2018,"35th International Conference on Machine Learning, ICML 2018","12",,,"8321","8330",,12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057303988&partnerID=40&md5=34f6d8135665f8919cb2b5ccdfb58783","One of the most challenging machine learning problems is a particular case of data classification in which classes are hierarchically structured and objects can be assigned to multiple paths of the class hierarchy at the same time. This task is known as hierarchical multi-label classification (HMC), with applications in text classification, image annotation, and in bioinformatics problems such as protein function prediction. In this paper, we propose novel neural network architectures for HMC called HMCN, capable of simultaneously optimizing local and global loss functions for discovering local hierarchical class-relationships and global information from the entire class hierarchy while penalizing hierarchical violations. We evaluate its performance in 21 datasets from four distinct domains, and we compare it against the current HMC state-of-the-art approaches. Results show that HMCN substantially outperforms all baselines with statistical significance, arising as the novel state-of-the-art for HMC. © 35th International Conference on Machine Learning, ICML 2018.All Rights Reserved.",,"Computer aided diagnosis; Learning systems; Network architecture; Neural networks; Text processing; Global informations; Hierarchical multi-label classifications; Machine learning problem; Novel neural network; Protein function prediction; State-of-the-art approach; Statistical significance; Text classification; Classification (of information)",,"35th International Conference on Machine Learning, ICML 2018","10 July 2018 through 15 July 2018",,141700,Conference Paper,"Final","",Scopus,2-s2.0-85057303988
"Mullai M., Broumi S., Stephen A.","","Shortest path problem by minimal spanning tree algorithm using bipolar neutrosophic numbers",2017,"Int. J. Math. Trends Technol.","46","N2",,"80","87",,12,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85044095583
"Sathe J.B., Mali M.P.","57193579936;57193580217;","A hybrid Sentiment Classification method using Neural Network and Fuzzy Logic",2017,"Proceedings of 2017 11th International Conference on Intelligent Systems and Control, ISCO 2017",,,"7855960","93","96",,12,"10.1109/ISCO.2017.7855960","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85015023600&doi=10.1109%2fISCO.2017.7855960&partnerID=40&md5=c4157070e91f80f4d69606f5d161b098","Neural Network(NN) and fuzzy systems are suitable for determining the input-output relationships. NN contend with numeric and quantitative information whereas fuzzy systems can handle symbolic and qualitative information. Coupling of Neural Network and Fuzzy Logic results in an intelligent crossbreed system widely referred to as Neuro-fuzzy system (NFS) that exploits the most effective qualities of these two approaches expeditiously. The coupled system combines the human alike logical reasoning of fuzzy systems with the training and connectedness structure of neural network. In this paper, we propose a method for performing Sentiment Classification using an NN and fuzzy set theory. In this method input reviews are fuzzified by using Gaussian membership function and fuzzification matrix is build. This matrix is transposed and passed to Multilayer Perceptron Backpropagation Network(MLPBPN). © 2017 IEEE.","neuro-fuzzy; sentiment classification; soft computing","Computation theory; Computer circuits; Fuzzy inference; Fuzzy neural networks; Fuzzy set theory; Fuzzy systems; Intelligent systems; Membership functions; Soft computing; Backpropagation network; Gaussian membership function; Logical reasoning; Neural network (nn); Neuro-Fuzzy; Qualitative information; Quantitative information; Sentiment classification; Fuzzy logic","","2017 11th International Conference on Intelligent Systems and Control, ISCO 2017","5 January 2017 through 6 January 2017",,126306,Conference Paper,"Final","",Scopus,2-s2.0-85015023600
"Maslan N., Roemmele M., Gordon A.S.","","One hundred challenge problems for logical formalizations of commonsense psychology",2015,"Twelfth International Symposium on Logical Formalizations of Commonsense Reasoning",,,,"","",,12,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84985977109
"Anagha M., Kumar R.R., Sreetha K., Raj P.C.R.","57208954283;56723137400;56724214600;6507179566;","Fuzzy logic based hybrid approach for sentiment analysisl of Malayalam movie reviews",2015,"2015 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems, SPICES 2015",,,"7091512","","",,12,"10.1109/SPICES.2015.7091512","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84937128310&doi=10.1109%2fSPICES.2015.7091512&partnerID=40&md5=3bd3e880037f931f403d8f4bc42f432a","In this paper, Sentence level Sentiment Analysis of Malayalam movie reviews is done by classifying the polarity of opinions obtained from the user as positive, negative and neutral. Sentiment analysis is an application of Natural Language Processing and text analysis which helps to identify the emotions in a given context. In this work a hybrid approach for Sentiment Analysis is used in which Machine Learning method is used for tagging and Fuzzy Logic is used to find the membership of the review in each sentiment class. Fuzzy logic can be used to handle the vagueness in natural language. Manually tagged data are trained using TnT and Fuzzy rules are incorporated for identifying and classifying the emotions. Certain other rules are also incorporated to handle certain special cases. The fuzzy rules yield output that varies in degree between 0 and 1. © 2015 IEEE.","Fuzzy logic; Sentiment Analysis; TnT tagger","Artificial intelligence; Computational linguistics; Data mining; Food products; Fuzzy inference; Fuzzy rules; Information science; Learning algorithms; Learning systems; Natural language processing systems; Signal processing; Hybrid approach; Machine learning methods; NAtural language processing; Natural languages; Sentence level; Sentiment analysis; Text analysis; TnT tagger; Fuzzy logic","","2015 IEEE International Conference on Signal Processing, Informatics, Communication and Energy Systems, SPICES 2015","19 February 2015 through 21 February 2015",,112047,Conference Paper,"Final","",Scopus,2-s2.0-84937128310
"Mansour R., Hady M.F.A., Hosam E., Amr H., Ashour A.","55967785000;26325610100;56875432600;56875371200;55412965000;","Feature selection for twitter sentiment analysis: An experimental study",2015,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9042",,,"92","103",,12,"10.1007/978-3-319-18117-2_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84942508773&doi=10.1007%2f978-3-319-18117-2_7&partnerID=40&md5=5de80c2a5a0ce74cc859a7a47c924627","Feature selection is an important problem for any pattern classification task. In this paper, we developed an ensemble of two Maximum Entropy classifiers for Twitter sentiment analysis: one for subjectivity and the other for polarity classification. Our ensemble employs surface-form, semantic and sentiment features. The classification complexity of this ensemble of linear models is linear with respect to the number of features. Our goal is to select a compact feature subset from the exhaustive list of extracted features in order to reduce the computational complexity without scarifying the classification accuracy. We evaluate the performance on two benchmark datasets, CrowdScale and SemEval. Our selected 20K features have shown very similar results in subjectivity classification to the NRC state-of-the-art system with 4 million features that has ranked first in 2013 SemEval competition. Also, our selected features have shown a relative performance gain in the ensemble classification over the baseline of uni-gram and bi-gram features of 9.9% on CrowdScale and 11.9% on SemEval. © Springer International Publishing Switzerland 2015.",,"Benchmarking; Classification (of information); Computational linguistics; Data mining; Linguistics; Maximum entropy methods; Semantics; Social networking (online); Text processing; Benchmark datasets; Classification accuracy; Ensemble classification; Polarity classification; Relative performance; Sentiment features; State-of-the-art system; Subjectivity classifications; Feature extraction","","16th Annual Conference on Intelligent Text Processing and Computational Linguistics, CICLing 2015","14 April 2015 through 20 April 2015",,142389,Conference Paper,"Final","",Scopus,2-s2.0-84942508773
"Yamangil E., Shieber S.M.","35090647800;6603019922;","Bayesian synchronous tree-substitution grammar induction and its application to sentence compression",2010,"ACL 2010 - 48th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",,,,"937","947",,12,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-79960613531&partnerID=40&md5=48bc0373867f7807364f42274d6c20be","We describe our experiments with training algorithms for tree-to-tree synchronous tree-substitution grammar (STSG) for monolingual translation tasks such as sentence compression and paraphrasing. These translation tasks are characterized by the relative ability to commit to parallel parse trees and availability of word alignments, yet the unavailability of large-scale data, calling for a Bayesian tree-to-tree formalism. We formalize nonparametric Bayesian STSG with epsilon alignment in full generality, and provide a Gibbs sampling algorithm for posterior inference tailored to the task of extractive sentence compression. We achieve improvements against a number of baselines, including expectation maximization and variational Bayes training, illustrating the merits of nonparametric inference over the space of grammars as opposed to sparse parametric inference with a fixed grammar. © 2010 Association for Computational Linguistics.",,"Expectation Maximization; Gibbs sampling; Grammar induction; Non-parametric Bayesian; Nonparametric inference; Parametric inference; Parse trees; Sentence compression; Training algorithms; Variational bayes; Word alignment; Algorithms; Alignment; Computational linguistics; Inference engines; Forestry; Algorithms; Computation; Optimization; Parametric Equations",,"48th Annual Meeting of the Association for Computational Linguistics, ACL 2010","11 July 2010 through 16 July 2010","Uppsala",89384,Conference Paper,"Final","",Scopus,2-s2.0-79960613531
"Saggion H.","","A classification algorithm for predicting the structure of summaries",2009,"Proceedings of the 2009 Workshop on Language Generation and Summarisation (UCNLG+Sum 2009)",,,,"31","38",,12,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-79959662357
"Nenkova A.","","Entity-driven rewrite for multidocument summarization",2008,"Proceedings of the Third International Joint Conference on Natural Language Processing",,,,"118","125",,12,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-79960612461
"Khosrovian K., Pfahl D., Garousi V.","24338277400;6603033193;13408954200;","GENSIM 2.0: A customizable process simulation model for software process evaluation",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5007 LNCS",,,"294","306",,12,"10.1007/978-3-540-79588-9_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-44649163405&doi=10.1007%2f978-3-540-79588-9_26&partnerID=40&md5=dfedf01560fd20611dbf2884a9e4bd4f","Software process analysis and improvement relies heavily on empirical research. Empirical research requires measurement, experimentation, and modeling. Moreover, whatever evidence is gained via empirical research is strongly context dependent. Thus, it is hard to combine results and capitalize upon them in order to improve software development processes in evolving development environments. The process simulation model GENSIM 2.0 addresses these challenges. Compared to existing process simulation models in the literature, the novelty of GENSIM 2.0 is twofold: (1) The model structure is customizable to organization-specific processes. This is achieved by using a limited set of macro-patterns. (2) Model parameters can be easily calibrated to available empirical data and expert knowledge. This is achieved by making the internal model structures explicit and by providing guidance on how to calibrate model parameters. This paper outlines the structure of GENSIM 2.0, shows examples of how to calibrate the simulator to available empirical data, and demonstrates its usefulness through two application scenarios. In those scenarios, GENSIM 2.0 is used to rank feasible combinations of verification and validation (V&V) techniques with regards to their impact on project duration, product quality and resource consumption. Though results confirm the expectation that doing more V&V earlier is generally beneficial to all project performance dimensions, the exact rankings are sensitive to project context. © 2008 Springer-Verlag Berlin Heidelberg.",,"Computer simulation; Mathematical models; Societies and institutions; Software engineering; Model parameters; Organization-specific processes; Process control","International Software Process Association (ISPA);Chinese Academy of Sciences, Institute of Software (ISCAS);ISCAS Laboratory for Internet Software Technologies (iTechs)","International Conference on Software Process, ICSP 2008","10 May 2008 through 11 May 2008","Leipzig",72135,Conference Paper,"Final","",Scopus,2-s2.0-44649163405
"Tabor W.","6603814753;","Learning exponential state-growth languages by hill climbing",2003,"IEEE Transactions on Neural Networks","14","2",,"444","446",,12,"10.1109/TNN.2003.809421","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0037360272&doi=10.1109%2fTNN.2003.809421&partnerID=40&md5=69a81cff682e7a3a2c4632adfff4fb8f","Training recurrent neural networks on infinite state languages has been successful with languages in which the minimal number of machine states grows linearly wilh sentence length, but has faired poorly with exponential state-growth languages. A new architecture learns several exponential state-growth languages nearly perfectly by hill climbing.",,"Automata theory; Computer simulation; Constraint theory; Context free languages; Fractals; Learning systems; Natural language processing systems; Probability distributions; Radial basis function networks; Fractal learning neural network; Gaussian activation function; Pushdown dynamical automata; Recurrent neural networks",,,,,,Letter,"Final","",Scopus,2-s2.0-0037360272
"Wang Q., Hao Y., Cao J.","57216212246;55197931300;57192160183;","ADRL: An attention-based deep reinforcement learning framework for knowledge graph reasoning",2020,"Knowledge-Based Systems","197",,"105910","","",,11,"10.1016/j.knosys.2020.105910","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083306400&doi=10.1016%2fj.knosys.2020.105910&partnerID=40&md5=de8d6ab87c63ac28eca6420214134478","Knowledge graph reasoning is one of the key technologies for knowledge graph construction, which plays an important part in application scenarios such as vertical search and intelligent question answering. It is intended to infer the desired entity from the entities and relations that already exist in the knowledge graph. Most current methods for reasoning, such as embedding-based methods, globally embed all entities and relations, and then use the similarity of vectors to infer relations between entities or whether given triples are true. However, in real application scenarios, we require a clear and interpretable target entity as the output answer. In this paper, we propose a novel attention-based deep reinforcement learning framework (ADRL) for learning multi-hop relational paths, which improves the efficiency, generalization capacity, and interpretability of conventional approaches through the structured perception of deep learning and relational reasoning of reinforcement learning. We define the entire process of reasoning as a Markov decision process. First, we employ CNN to map the knowledge graph to a low-dimensional space, and a message-passing mechanism to sense neighbor entities at each level, and then employ LSTM to memorize and generate a sequence of historical trajectories to form a policy and value functions. We design a relational module that includes a self-attention mechanism that can infer and share the weights of neighborhood entity vectors and relation vectors. Finally, we employ the actor–critic algorithm to optimize the entire framework. Experiments confirm the effectiveness and efficiency of our method on several benchmark data sets. © 2020 Elsevier B.V.","Attention; Deep learning; Knowledge graph; Knowledge reasoning; Reinforcement learning","Deep learning; Efficiency; Long short-term memory; Markov processes; Message passing; Application scenario; Attention mechanisms; Conventional approach; Effectiveness and efficiencies; Generalization capacity; Low-dimensional spaces; Markov Decision Processes; Relational reasoning; Reinforcement learning",,,,,,Article,"Final","",Scopus,2-s2.0-85083306400
"Lan Y., Wang S., Jiang J.","57191379533;57191852679;56196028200;","Knowledge Base Question Answering with a Matching-Aggregation Model and Question-Specific Contextual Relations",2019,"IEEE/ACM Transactions on Audio Speech and Language Processing","27","10","8752379","1629","1638",,11,"10.1109/TASLP.2019.2926125","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069962310&doi=10.1109%2fTASLP.2019.2926125&partnerID=40&md5=847142e342248c27afe38d0df2da1aa7","Making use of knowledge bases to answer questions (KBQA) is a key direction in question answering systems. Researchers have developed a diverse range of methods to address this problem, but there are still some limitations with the existing methods. Specifically, the existing neural network-based methods for KBQA have not taken advantage of the recent 'matching-Aggregation' framework for the sequence matching, and when representing a candidate answer entity, they may not choose the most useful context of the candidate for matching. In this paper, we explore the use of a 'matching-Aggregation' framework to match candidate answers with questions. We further make use of question-specific contextual relations to enhance the representations of candidate answer entities. Our complete method is able to achieve state-of-The-Art performance on two benchmark datasets: WebQuestions and SimpleQuestions. © 2014 IEEE.","Artificial intelligence; knowledge base question answering; natural language processing","Artificial intelligence; Benchmarking; Knowledge based systems; Aggregation model; Benchmark datasets; Knowledge basis; NAtural language processing; Question Answering; Question answering systems; Sequence matching; State-of-the-art performance; Natural language processing systems",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85069962310
"Xu L., Zhou Q., Gong K., Liang X., Tang J., Lin L.","57218925767;57220842036;57201377057;55926362100;57216618053;15061363400;","End-to-end knowledge-routed relational dialogue system for automatic diagnosis",2019,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",,,,"7346","7353",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090800757&partnerID=40&md5=24ff8efb588e4b4dfd9fb6388c963755","Beyond current conversational chatbots or task-oriented dialogue systems that have attracted increasing attention, we move forward to develop a dialogue system for automatic medical diagnosis that converses with patients to collect additional symptoms beyond their self-reports and automatically makes a diagnosis. Besides the challenges for conversational dialogue systems (e.g. topic transition coherency and question understanding), automatic medical diagnosis further poses more critical requirements for the dialogue rationality in the context of medical knowledge and symptom-disease relations. Existing dialogue systems (Madotto, Wu, and Fung 2018; Wei et al. 2018; Li et al. 2017) mostly rely on data-driven learning and cannot be able to encode extra expert knowledge graph. In this work, we propose an End-to-End Knowledge-routed Relational Dialogue System (KR-DS) that seamlessly incorporates rich medical knowledge graph into the topic transition in dialogue management, and makes it cooperative with natural language understanding and natural language generation. A novel Knowledge-routed Deep Q-network (KR-DQN) is introduced to manage topic transitions, which integrates a relational refinement branch for encoding relations among different symptoms and symptom-disease pairs, and a knowledge-routed graph branch for topic decision-making. Extensive experiments on a public medical dialogue dataset show our KR-DS significantly beats state-of-the-art methods (by more than 8% in diagnosis accuracy). We further show the superiority of our KR-DS on a newly collected medical dialogue system dataset, which is more challenging retaining original self-reports and conversational data between patients and doctors. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Behavioral research; Decision making; Encoding (symbols); Knowledge representation; Natural language processing systems; Speech processing; Automatic diagnosis; Dialogue management; Dialogue systems; Expert knowledge; Medical knowledge; Natural language generation; Natural language understanding; State-of-the-art methods; Diagnosis","Association for the Advancement of Artificial Intelligence","33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019","27 January 2019 through 1 February 2019",,160302,Conference Paper,"Final","",Scopus,2-s2.0-85090800757
"Liu L., Mu F., Li P., Mu X., Tang J., Ai X., Fu R., Wang L., Zhou X.","57216615154;57216612031;57216614588;57216610713;57216618105;57216614518;57216617242;57216616774;57216623499;","Neural classifier: An Open-source neural hierarchical multi-label text classification toolkit",2019,"ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of System Demonstrations",,,,"87","92",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081050040&partnerID=40&md5=35b6b43ad54ce3269dab6576c57258e4","In this paper, we introduce NeuralClassifier, a toolkit for neural hierarchical multi-label text classification. NeuralClassifier is designed for quick implementation of neural models for hierarchical multi-label classification task, which is more challenging and common in real-world scenarios. A salient feature is that NeuralClassifier currently provides a variety of text encoders, such as FastText, TextCNN, TextRNN, RCNN, VDCNN, DPCNN, DRNN, AttentiveConvNet and Transformer encoder, etc. It also supports other text classification scenarios, including binary-class and multiclass classification. Built on PyTorch1, the core operations are calculated in batch, making the toolkit efficient with the acceleration of GPU. Experiments show that models built in our toolkit achieve comparable performance with reported results in the literature. © 2019 All rights reserved.",,"Classification (of information); Computational linguistics; Signal encoding; Hierarchical multi-label classifications; Multi-class classification; Multi-label text classification; Neural classifiers; Neural models; Real-world scenario; Salient features; Text classification; Text processing",,"57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, ACL 2019","28 July 2019 through 2 August 2019",,159207,Conference Paper,"Final","",Scopus,2-s2.0-85081050040
"Ismail H.M., Belkhouche B., Zaki N.","57191893775;6603878472;16044448200;","Semantic Twitter sentiment analysis based on a fuzzy thesaurus",2018,"Soft Computing","22","18",,"6011","6024",,11,"10.1007/s00500-017-2994-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040057848&doi=10.1007%2fs00500-017-2994-8&partnerID=40&md5=f4ed1e98ab345a3d97875b30974cf303","We define a new, fully automated and domain-independent method for building feature vectors from Twitter text corpus for machine learning sentiment analysis based on a fuzzy thesaurus and sentiment replacement. The proposed method measures the semantic similarity of Tweets with features in the feature space instead of using terms’ presence or frequency feature vectors. Thus, we account for the sentiment of the context instead of just counting sentiment words. We use sentiment replacement to reduce the dimensionality of the feature space and a fuzzy thesaurus to incorporate semantics. Experimental results show that sentiment replacement yields up to 35% reduction in the dimensionality of the feature space. Moreover, feature vectors developed based on a fuzzy thesaurus show improvement of sentiment classification performance with multinomial naïve Bayes and support vector machine classifiers with accuracies of 83 and 85%, respectively, on the Stanford testing dataset. Incorporating the fuzzy thesaurus resulted in the best accuracy compared to the baselines with an increase greater than 3%. Comparable results were obtained with a larger dataset, the STS-Gold, indicating the robustness of the proposed method. Furthermore, comparison of results with previous work shows that the proposed method outperforms other methods reported in the literature using the same benchmark data. © 2018, Springer-Verlag GmbH Germany, part of Springer Nature.","Fuzzy thesaurus; Semantic analysis; Text context; Text mining; Twitter sentiment analysis","Classification (of information); Data mining; Natural language processing systems; Semantics; Sentiment analysis; Social networking (online); Statistical tests; Support vector machines; Text processing; Thesauri; Vectors; Domain independents; Fuzzy thesauri; Semantic analysis; Semantic similarity; Sentiment classification; Support vector machine classifiers; Text context; Text mining; Vector spaces",,,,,,Article,"Final","",Scopus,2-s2.0-85040057848
"Hanselowski A., Zhang H., Li Z., Sorokin D., Schiller B., Schulz C., Gurevych I.","",[No title available],2018,"Ukp-Athene: Multi-Sentence Textual Entailment for Claim Verification",,,,"","",,11,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075550869
"Li Y., Shen B.","57202893848;57051936000;","Research on sentiment analysis of microblogging based on LSA and TF-IDF",2018,"2017 3rd IEEE International Conference on Computer and Communications, ICCC 2017","2018-January",,,"2584","2588",,11,"10.1109/CompComm.2017.8323002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85049689803&doi=10.1109%2fCompComm.2017.8323002&partnerID=40&md5=7eecd5d97e33bdff2738424dc292352b","As a typical social network application, the impact of microblogging on people has penetrated into all aspects, which attracts more and more scholars to carry out in-depth study on microblogging. The sentiment analysis of microblogging text is the hot studying field now. Feature selection and extraction is one of the core parts of microblogging text sentiment analysis, TF-IDF algorithm is the most widely used method in selecting features. Although the TF-IDF algorithm is simple to use, there is still a problem of semantic deletion on it, that is, it ignores the semantic information contained in the text. To solve the problem, LSA is introduced in this paper. Firstly, the eigenvectors generated by TF-IDF algorithm is decomposed by singular value. Then, calculating the cosine value between the row vectors of the decomposition results to identify the similarity between the words, which realizes the feature extraction and makes up for the deficiency of TF-IDF. Finally, the extracted features are applied into four classification algorithms to verify the effectiveness of the proposed method. The experimental results show that the introcuction of LSA can make improvements of microblogging text classification in accuracy, recall and F value. © 2017 IEEE.","eigenvectors; LSA; microblogging; sentiment analysis; TF-IDF","Classification (of information); Data mining; Eigenvalues and eigenfunctions; Extraction; Feature extraction; Semantics; Sentiment analysis; Classification algorithm; Feature selection and extractions; Micro-blogging; Network applications; Semantic information; Text classification; TF-IDF; TF-IDF algorithms; Text processing","IEEE;Sichuan Institute of Electronics","3rd IEEE International Conference on Computer and Communications, ICCC 2017","13 December 2017 through 16 December 2017",,135473,Conference Paper,"Final","",Scopus,2-s2.0-85049689803
"Schlag I., Schmidhuber J.","57201861913;7003514621;","Learning to reason with third-order tensor products",2018,"Advances in Neural Information Processing Systems","2018-December",,,"9981","9993",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85064826748&partnerID=40&md5=09ec3e65d70a4e724be5264bcc9f201e","We combine Recurrent Neural Networks with Tensor Product Representations to learn combinatorial representations of sequential data. This improves symbolic interpretation and systematic generalisation. Our architecture is trained end-to-end through gradient descent on a variety of simple natural language reasoning tasks, significantly outperforming the latest state-of-the-art models in single-task and all-tasks settings. We also augment a subset of the data such that training and test data exhibit large systematic differences and show that our approach generalises better than the previous state-of-the-art. © 2018 Curran Associates Inc.All rights reserved.",,"Tensors; Gradient descent; Natural languages; Reasoning tasks; Sequential data; State of the art; Symbolic interpretation; Tensor products; Third-order tensors; Recurrent neural networks",,"32nd Conference on Neural Information Processing Systems, NeurIPS 2018","2 December 2018 through 8 December 2018",,147412,Conference Paper,"Final","",Scopus,2-s2.0-85064826748
"Feng X., Liang Y., Shi X., Xu D., Wang X., Guan R.","8540390200;55584054800;7402953420;7404074295;57192590819;25721966400;","Overfitting reduction of text classification based on AdaBELM",2017,"Entropy","19","7","330","","",,11,"10.3390/e19070330","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022182795&doi=10.3390%2fe19070330&partnerID=40&md5=67cab24b698940936e128a7285e18bbb","Overfitting is an important problem in machine learning. Several algorithms, such as the extreme learning machine (ELM), suffer from this issue when facing high-dimensional sparse data, e.g., in text classification. One common issue is that the extent of overfitting is not well quantified. In this paper, we propose a quantitative measure of overfitting referred to as the rate of overfitting (RO) and a novel model, named AdaBELM, to reduce the overfitting. With RO, the overfitting problem can be quantitatively measured and identified. The newly proposed model can achieve high performance on multi-class text classification. To evaluate the generalizability of the new model, we designed experiments based on three datasets, i.e., the 20 Newsgroups, Reuters-21578, and BioMed corpora, which represent balanced, unbalanced, and real application data, respectively. Experiment results demonstrate that AdaBELM can reduce overfitting and outperform classical ELM, decision tree, random forests, and AdaBoost on all three text-classification datasets; for example, it can achieve 62.2% higher accuracy than ELM. Therefore, the proposed model has a good generalizability. © 2017 by the authors.","AdaBoost; Extreme learning machine; Feedforward neural network; Machine learning; Overfitting",,,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-85022182795
"Basile V., Cabrio E., Schon C.","","KNEWS: Using logical and lexical semantics to extract knowledge from natural language",2016,"Proceedings of the European Conference on Artificial Intelligence (ECAI) 2016 Conference",,,,"","",,11,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85033444430
"Liu Y., Li S., Wei F., Ji H.","57206819031;53984734300;23995914700;35240121900;","Relation classification via modeling augmented dependency paths",2016,"IEEE/ACM Transactions on Audio Speech and Language Processing","24","9",,"1589","1598",,11,"10.1109/TASLP.2016.2573050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84979995722&doi=10.1109%2fTASLP.2016.2573050&partnerID=40&md5=0ec5e6c9775ecf4ec0524b36694b44e2","Previous research on relation classification has verified the effectiveness of using dependency shortest paths or dependency subtrees. How to efficiently unify these two kinds of dependency information in relation classification is still an open problem. In this paper, we propose a novel structure, termed augmented dependency path (ADP), which is composed of the shortest dependency path between two entities and the subtrees attached to the shortest path. To exploit the semantic representation behind the ADP structure, we develop the dependency-based neural networks (DepNN) model which combines the advantages of the recursive neural network (RNN) and the convolutional neural network (CNN). In DepNN, RNN is designed to model the dependency subtrees since it is good at capturing the hierarchical structures. Then, the semantic representation in subtrees is passed to the nodes on the shortest path and CNN is used to get the most important features on the ADP. Experiments on the SemEval-2010 dataset show that the ADP structure including both the shortest dependency path and the attached subtrees is helpful to classify the semantic relations between two entities and our proposed method can achieve the state-of-the-art performance. © 2014 IEEE.","Convolution neural network; dependency subtree; recursive neural network; relation classification; shortest dependency path","Convolution; Graph theory; Neural networks; Semantics; Convolution neural network; Recursive neural networks; Relation classifications; shortest dependency path; Sub trees; Classification (of information)",,,,,,Article,"Final","",Scopus,2-s2.0-84979995722
"Liu G., Xu X., Deng B., Chen S., Li L.","56541907400;56413183300;57190031818;57190030701;56150413500;","A hybrid method for bilingual text sentiment classification based on deep learning",2016,"2016 IEEE/ACIS 17th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2016",,,"7515884","93","98",,11,"10.1109/SNPD.2016.7515884","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84983372744&doi=10.1109%2fSNPD.2016.7515884&partnerID=40&md5=cf653b9050232ef1f2564299359b7cd2","Text sentiment classification has occupied a pivotal position in sentiment analysis research, it offers important opinion mining functions. Nowadays, with explosion of information, many researchers are focusing on sentiment classification research on massive amounts of data. However, the traditional machine learning methods cannot acquire text semantic information and most research achievements are about single language, in this paper, a hybrid method which integrates the deep learning features and shallow learning features is proposed. The hybrid method can not only realize single language text sentiment classification but realize bilingual text sentiment classification as well. Models such as recurrent neural networks (RNNs) with long short term memory(LSTM), Naïve Bayes Support Vector Machine (NB-SVM), word vectors and bag-of-words are explored. Firstly, these models are studied separately in sentiment classification task. The paper then integrates the above methods as a whole to complete the task. Different combination strategies are discussed regarding the contribution of each method. The experiments show that the accuracy can reach 89% and the hybrid method performs much better than any other method individually. The proposed method achieves a performance close to the state-of-the-art methods based on the had-engineered features. What's more, the hybrid model can learn more linguistic phenomena with the growth of the accuracy of emotional tendency discrimination when more background knowledge is available. © 2016 IEEE.","deep learning; neural network; opinion mining; sentiment analysis; text sentiment classification","Artificial intelligence; Computational linguistics; Data mining; Learning systems; Neural networks; Recurrent neural networks; Semantics; Software engineering; Support vector machines; Text processing; Combination strategies; Deep learning; Machine learning methods; Opinion mining; Recurrent neural network (RNNs); Sentiment analysis; Sentiment classification; State-of-the-art methods; Classification (of information)","IEEE Computer Society;International Association for Computer and Information Science (ACIS)","17th IEEE/ACIS International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing, SNPD 2016","30 May 2016 through 1 June 2016",,122820,Conference Paper,"Final","",Scopus,2-s2.0-84983372744
"Lee M., He X., Yih W.-T., Gao J., Deng L., Smolensky P.","56576568500;37085932700;23010913500;55702627000;36071490500;6602511111;","Reasoning in vector space: An exploratory study of question answering",2016,"4th International Conference on Learning Representations, ICLR 2016 - Conference Track Proceedings",,,,"","",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083951918&partnerID=40&md5=d5f9c36e5ca3b5fed5d9938d79189c1d","Question answering tasks have shown remarkable progress with distributed vector representation. In this paper, we investigate the recently proposed Facebook bAbI tasks which consist of twenty different categories of questions that require complex reasoning. Because the previous work on bAbI are all end-to-end models, errors could come from either an imperfect understanding of semantics or in certain steps of the reasoning. For clearer analysis, we propose two vector space models inspired by Tensor Product Representation (TPR) to perform knowledge encoding and logical reasoning based on common-sense inference. They together achieve near-perfect accuracy on all categories including positional reasoning and path finding that have proved difficult for most of the previous approaches. We hypothesize that the difficulties in these categories are due to the multi-relations in contrast to uni-relational characteristic of other categories. Our exploration sheds light on designing more sophisticated dataset and moving one step toward integrating transparent and interpretable formalism of TPR into existing learning paradigms. © ICLR 2016: San Juan, Puerto Rico. All Rights Reserved.",,"Semantics; End-to-end models; Exploratory studies; Learning paradigms; Logical reasoning; Question Answering; Question Answering Task; Vector representations; Vector space models; Vector spaces",,"4th International Conference on Learning Representations, ICLR 2016","2 May 2016 through 4 May 2016",,149803,Conference Paper,"Final","",Scopus,2-s2.0-85083951918
"Mihǎescu M.C.","25825215100;","Classification of learners using linear regression",2011,"2011 Federated Conference on Computer Science and Information Systems, FedCSIS 2011",,,"6078214","717","721",,11,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-83155190335&partnerID=40&md5=dcbce58b273c9b19f99e79e3d1299d06","Proper classification of learners is one of the key aspects in e-Learning environments. This paper uses linear regression for modeling the quantity of accumulated knowledge in relationship with variables representing the performed activity. Within the modeling process there are used the experiences performed by students for which it is known the level of accumulated knowledge. The classification of learners is performed at concept level. The outcome is computed as a percentage representing the concept covering in knowledge. © 2011 Polish Info Processing Soc.","e-learning; learner classification; linear regression","Concept levels; E-learning environment; Modeling process; Computer science; E-learning; Information systems; Linear regression",,"2011 Federated Conference on Computer Science and Information Systems, FedCSIS 2011","18 September 2011 through 21 September 2011","Szczecin",87614,Conference Paper,"Final","",Scopus,2-s2.0-83155190335
"Kuiper K.","","On the linguistic properties of formulaic speech",2000,"Oral Tradition","15","2",,"279","305",,11,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0742276749
"Sun R.","","A two-level hybrid architecture for structuring knowledge for commonsense reasoning",1995,"Computational Architectures Integrating Neural and Symbolic Processing",,,,"247","282",,11,,,[No abstract available],,,,,,,,,,"",Scopus,2-s2.0-0010721465
"Wang Q., Hao Y.","57216212246;55197931300;","ALSTM: An attention-based long short-term memory framework for knowledge base reasoning",2020,"Neurocomputing","399",,,"342","351",,10,"10.1016/j.neucom.2020.02.065","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85081931785&doi=10.1016%2fj.neucom.2020.02.065&partnerID=40&md5=1cbeb4e9a1aa7b61a3f9f20e5f3cd26e","Knowledge Graphs (KGs) have been applied to various application scenarios including Web searching, Q&A, recommendation system, natural language processing and so on. However, the vast majority of Knowledge Bases (KBs) are incomplete, necessitating a demand for KB completion (KBC). Methods of KBC used in the mainstream current knowledge base include the latent factor model, the random walk model and recent popular methods based on reinforcement learning, which performs well in their respective areas of expertise. Recurrent neural network (RNN) and its variants model temporal data by remembering information for long periods, however, whether they also have the ability to use the information they have already remembered to achieve complex reasoning in the knowledge graph. In this paper, we produce a novel framework (ALSTM) based on the Attention mechanism and Long Short-Term Memory (LSTM), which associates structure learning with parameter learning of first-order logical rules in an end-to-end differentiable neural networks model. In this framework, we designed a memory system and employed a multi-head dot product attention (MHDPA) to interact and update the memories embedded in the memory system for reasoning purposes. This is also consistent with the process of human cognition and reasoning, looking for enlightenment for the future in historical memory. In addition, we explored the use of inductive bias in deep learning to facilitate learning of entities, relations, and rules. Experiments establish the efficiency and effectiveness of our model and show that our method achieves better performance in tasks which include fact prediction and link prediction than baseline models on several benchmark datasets such as WN18RR, FB15K-237 and NELL-995. © 2020 Elsevier Ltd","Attention; Deep learning; Knowledge base; Logical rule; LSTM; Memory","Benchmarking; Brain; Data storage equipment; Deep learning; Formal logic; Knowledge based systems; Learning systems; Natural language processing systems; Reinforcement learning; Attention; Knowledge base; Knowledge basis (KBs); Logical rules; LSTM; NAtural language processing; Neural networks model; Recurrent neural network (RNN); Long short-term memory; article; attention; deep learning; human; human experiment; knowledge base; prediction; reasoning; recurrent neural network; short term memory",,,,,,Article,"Final","",Scopus,2-s2.0-85081931785
"Chen Q., Wang X., Ye X., Durrett G., Dillig I.","57217225561;57026634100;57217225613;6504004627;22936636100;","Multi-modal synthesis of regular expressions",2020,"Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",,,,"487","502",,10,"10.1145/3385412.3385988","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086823111&doi=10.1145%2f3385412.3385988&partnerID=40&md5=f7442b384ecf9aaff389b499ec97243a","In this paper, we propose a multi-modal synthesis technique for automatically constructing regular expressions (regexes) from a combination of examples and natural language. Using multiple modalities is useful in this context because natural language alone is often highly ambiguous, whereas examples in isolation are often not sufficient for conveying user intent. Our proposed technique first parses the English description into a so-called hierarchical sketch that guides our programming-by-example (PBE) engine. Since the hierarchical sketch captures crucial hints, the PBE engine can leverage this information to both prioritize the search as well as make useful deductions for pruning the search space. We have implemented the proposed technique in a tool called Regel and evaluate it on over three hundred regexes. Our evaluation shows that Regel achieves 80 % accuracy whereas the NLP-only and PBE-only baselines achieve 43 % and 26 % respectively. We also compare our proposed PBE engine against an adaptation of AlphaRegex, a state-of-the-art regex synthesis tool, and show that our proposed PBE engine is an order of magnitude faster, even if we adapt the search algorithm of AlphaRegex to leverage the sketch. Finally, we conduct a user study involving 20 participants and show that users are twice as likely to successfully come up with the desired regex using Regel compared to without it. © 2020 ACM.","Program Synthesis; Programming by Example; Programming by Natural Languages; Regular Expression","Engines; Modal analysis; Pattern matching; Search engines; Multiple modalities; Natural languages; Programming by Example; Regular expressions; Search Algorithms; Search spaces; State of the art; Synthesis tool; Computer programming languages","ACM SIGPLAN","41st ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2020","15 June 2020 through 20 June 2020",,160874,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85086823111
"Kaura P., Singhb M., Josanc G.S.","","Classification and prediction based data mining algorithms to predict slow learners in education sector",2020,"3Rd International Conference on Recent Trends in Computing 2015(ICRTC-2015)",,,,"","",,10,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85054683986
"Tuan Y.-L., Chen Y.-N., Lee H.-Y.","57204047867;47061006000;34969292900;","Dykgchat: Benchmarking dialogue generation grounding on dynamic knowledge graphs",2020,"EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference",,,,"1855","1865",,10,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084288031&partnerID=40&md5=e4262146741d134ceff7c12ef1ab3d43","Data-driven, knowledge-grounded neural conversation models are capable of generating more informative responses. However, these models have not yet demonstrated that they can zero-shot adapt to updated, unseen knowledge graphs. This paper proposes a new task about how to apply dynamic knowledge graphs in neural conversation model and presents a novel TV series conversation corpus (DyKgChat) for the task. Our new task and corpus aids in understanding the influence of dynamic knowledge graphs on responses generation. Also, we propose a preliminary model that selects an output from two networks at each time step: a sequence-to-sequence model (Seq2Seq) and a multi-hop reasoning model, in order to support dynamic knowledge graphs. To benchmark this new task and evaluate the capability of adaptation, we introduce several evaluation metrics and the experiments show that our proposed approach outperforms previous knowledge-grounded conversation models. The proposed corpus and model can motivate the future research directions1. © 2019 Association for Computational Linguistics",,"Graphic methods; Knowledge representation; Linguistics; Dialogue generations; Evaluation metrics; Future research directions; Knowledge graphs; ON dynamics; Preliminary model; Reasoning models; Sequence modeling; Natural language processing systems","Apple;ASAPP;et al.;Facebook;Google;salesforce","2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019","3 November 2019 through 7 November 2019",,159367,Conference Paper,"Final","",Scopus,2-s2.0-85084288031
"Mao Q., Li J., Wang S., Zhang Y., Peng H., He M., Wang L.","57207455023;55720560100;56424441500;57211756758;57190014707;57211758516;57007714500;","Aspect-based sentiment classification with attentive neural turing machines",2019,"IJCAI International Joint Conference on Artificial Intelligence","2019-August",,,"5139","5145",,10,"10.24963/ijcai.2019/714","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85074917337&doi=10.24963%2fijcai.2019%2f714&partnerID=40&md5=5411396c907068007573bf0f5f8cf04c","Aspect-based sentiment classification aims to identify sentiment polarity expressed towards a given opinion target in a sentence. The sentiment polarity of the target is not only highly determined by sentiment semantic context but also correlated with the concerned opinion target. Existing works cannot effectively capture and store the inter-dependence between the opinion target and its context. To solve this issue, we propose a novel model of Attentive Neural Turing Machines (ANTM). Via interactive read-write operations between an external memory storage and a recurrent controller, ANTM can learn the dependable correlation of the opinion target to context and concentrate on crucial sentiment information. Specifically, ANTM separates the information of storage and computation, which extends the capabilities of the controller to learn and store sequential features. The read and write operations enable ANTM to adaptively keep track of the interactive attention history between memory content and controller state. Moreover, we append target entity embeddings into both input and output of the controller in order to augment the integration of target information. We evaluate our model on SemEval2014 dataset which contains reviews of Laptop and Restaurant domains and Twitter review dataset. Experimental results verify that our model achieves state-of-the-art performance on aspect-based sentiment classification. © 2019 International Joint Conferences on Artificial Intelligence. All rights reserved.",,,"Baidu;et al.;Huawei;International Joint Conferences on Artifical Intelligence (IJCAI);Sony;Xiao-i","28th International Joint Conference on Artificial Intelligence, IJCAI 2019","10 August 2019 through 16 August 2019",,153611,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85074917337
"Bounabi M., El Moutaouakil K., Satori K.","57195492422;35329465400;35976902500;","A comparison of text classification methods using different stemming techniques",2019,"International Journal of Computer Applications in Technology","60","4",,"298","306",,10,"10.1504/IJCAT.2019.101171","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85069942172&doi=10.1504%2fIJCAT.2019.101171&partnerID=40&md5=b057ffc1342ed0437a61d3b5c71fcefb","In the retrieval of information, two factors have an important impact on the performance of systems: the extract features and the matching process. In this work, we compare three well-known stemming techniques: Lovins stemmer, iterated Lovins and snowball stemmer. Concerning the classification phase, we compare, experimentally, six methods: BNET, NBMU, CNB, RF, SLogicF, and SVM. Basing on this comparison, we propose a new retrieval system by calling the voting method, as a matching tool, to improve the performance of the classical systems. In this paper, we use the TF-IDF algorithm to extract features. The envisaged systems are tested on two databases: BBCNEWS and BBCSPORT. The systems based on Lovins stemmers and on the voting technique give the best results. In fact, for the first databases, the best accuracy observed is for the system Lovins + Vote with a recognition rate of 97%. Concerning the second database, the system snowball +Vote gives us 99% as recognition rate. Copyright © 2019 Inderscience Enterprises Ltd.","Classification; CNB; NB; NBMU; RF; SLogiF; Stemmer; SVM; Term-weighting; Voting technique","Classification (of information); Database systems; Natural language processing systems; Niobium; Rayon yarn; Support vector machines; Text processing; NBMU; SLogiF; Stemmer; Term weighting; Voting techniques; Search engines",,,,,,Article,"Final","",Scopus,2-s2.0-85069942172
"Zimmer M., Doncieux S.","57191836800;14833739000;","Bootstrapping Q-Learning for Robotics from Neuro-Evolution Results",2018,"IEEE Transactions on Cognitive and Developmental Systems","10","1",,"102","119",,10,"10.1109/TCDS.2016.2628817","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85043772084&doi=10.1109%2fTCDS.2016.2628817&partnerID=40&md5=2db5e021a3c8d92602a39fb199e278ad","Reinforcement learning (RL) problems are hard to solve in a robotics context as classical algorithms rely on discrete representations of actions and states, but in robotics both are continuous. A discrete set of actions and states can be defined, but it requires an expertise that may not be available, in particular in open environments. It is proposed to define a process to make a robot build its own representation for an RL algorithm. The principle is to first use a direct policy search in the sensori-motor space, i.e., with no predefined discrete sets of states nor actions, and then extract from the corresponding learning traces discrete actions and identify the relevant dimensions of the state to estimate the value function. Once this is done, the robot can apply RL: 1) to be more robust to new domains and, if required and 2) to learn faster than a direct policy search. This approach allows to take the best of both worlds: first learning in a continuous space to avoid the need of a specific representation, but at a price of a long learning process and a poor generalization, and then learning with an adapted representation to be faster and more robust. © 2016 IEEE.","Generation of representation during development; robots with development and learning skills; transfer learning","Robotics; Robots; Continuous spaces; Direct policy search; Generation of representation during development; Learning process; Learning skills; Neuro evolutions; Open environment; Transfer learning; Reinforcement learning",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85043772084
"Dorronsoro B., Pinel F.","55880832100;35729032500;","Combining machine learning and genetic algorithms to solve the independent tasks scheduling problem",2017,"2017 3rd IEEE International Conference on Cybernetics, CYBCONF 2017 - Proceedings",,,"7985766","","",,10,"10.1109/CYBConf.2017.7985766","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027860081&doi=10.1109%2fCYBConf.2017.7985766&partnerID=40&md5=a70592bd950897f4f1e13c82a52c612a","We propose a new accurate and fast memetic parallel optimization algorithm for the independent tasks scheduling problem. The new technique combines the Virtual Savant (VS) with a parallel genetic algorithm (called PA-CGA). VS is an optimization framework based on machine learning that learns from a reference set of (pseudo-)optimal solutions how to solve the problem, providing accurate results in extremely low run times. We propose in this work the use of VS to generate a highly accurate initial population for the PA-CGA. Results show how initializing the population with VS (we test two versions of VS, differing on its training process) significantly increases the accuracy of the PA-CGA, compared to two other population initialization techniques: Random and using a state-of-the-art heuristic. © 2017 IEEE.",,"Artificial intelligence; Cybernetics; Genetic algorithms; Learning systems; Optimization; Scheduling; Independent tasks scheduling; Initial population; Optimal solutions; Optimization framework; Parallel genetic algorithms; Parallel optimization; Population initializations; State of the art; Problem solving","","3rd IEEE International Conference on Cybernetics, CYBCONF 2017","21 June 2017 through 23 June 2017",,129381,Conference Paper,"Final","",Scopus,2-s2.0-85027860081
"Heath D., Norton D., Ventura D.","55354928800;57196523811;24402340500;","Conveying semantics through visual metaphor",2014,"ACM Transactions on Intelligent Systems and Technology","5","2","31","","",,10,"10.1145/2589483","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84899714809&doi=10.1145%2f2589483&partnerID=40&md5=ea2fe2fb545f2b8ab6a3604b4d2cffeb","In the field of visual art, metaphor is a way to communicate meaning to the viewer. We present a computational system for communicating visual metaphor that can identify adjectives for describing an image based on a low-level visual feature representation of the image. We show that the system can use this visuallinguistic association to render source images that convey the meaning of adjectives in a way consistent with human understanding. Our conclusions are based on a detailed analysis of how the system's artifacts cluster, how these clusters correspond to the semantic relationships of adjectives as documented in WordNet, and how these clusters correspond to human opinion. © 2014 ACM.","Clustering; Evolutionary art; Neural networks; Visual metaphor","Neural networks; Clustering; Computational system; Evolutionary arts; Human understanding; Semantic relationships; Source images; Visual feature; Visual metaphor; Semantics",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84899714809
"Norrick N.R.","","Proverbs as set phrases",2007,"Phraseology. An International Handbook of Contemporary Research","1",,,"381","393",,10,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84861768028
"Weir D.J.","57214547520;","LINEAR ITERATED PUSHDOWNS",1994,"Computational Intelligence","10","4",,"431","439",,10,"10.1111/j.1467-8640.1994.tb00007.x","https://www.scopus.com/inward/record.uri?eid=2-s2.0-0028549737&doi=10.1111%2fj.1467-8640.1994.tb00007.x&partnerID=40&md5=b3e158f74fa2a36dc6d3fdd0e5edd9e2","This paper discusses variants of nondeterministic one‐way S‐automata and context‐free S‐grammars where S is a storage type. The framework that these systems provide can be used to give alternative formulations of embedded pushdown automata and linear indexed grammars. The embedded pushdown automata is obtained by means of a linear version of a class of storage types called iterated pushdowns. Linear indexed grammar is obtained by using the pushdown storage type and restricting the way in which the grammar uses its storage. Copyright © 1994, Wiley Blackwell. All rights reserved","formal language theory; grammar formalisms; mathematical linguistics; string automata","Computational linguistics; Context free grammars; Formal languages; Iterative methods; Mathematical models; Linear indexed grammar; Pushdown automata; Tree adjoining languages; Automata theory",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-0028549737
"Jiang S., Baumgartner S., Ittycheriah A., Yu C.","57203234897;57216893922;57225269593;35222733100;","Factoring Fact-Checks: Structured Information Extraction from Fact-Checking Articles",2020,"The Web Conference 2020 - Proceedings of the World Wide Web Conference, WWW 2020",,,,"1592","1603",,9,"10.1145/3366423.3380231","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086589255&doi=10.1145%2f3366423.3380231&partnerID=40&md5=8dfe0fe7c1bde51803d20f7834cb1e58","Fact-checking, which investigates claims made in public to arrive at a verdict supported by evidence and logical reasoning, has long been a significant form of journalism to combat misinformation in the news ecosystem. Most of the fact-checks share common structured information (called factors) such as claim, claimant, and verdict. In recent years, the emergence of ClaimReview as the standard schema for annotating those factors within fact-checking articles has led to wide adoption of fact-checking features by online platforms (e.g., Google, Bing). However, annotating fact-checks is a tedious process for fact-checkers and distracts them from their core job of investigating claims. As a result, less than half of the fact-checkers worldwide have adopted ClaimReview as of mid-2019. In this paper, we propose the task of factoring fact-checks for automatically extracting structured information from fact-checking articles. Exploring a public dataset of fact-checks, we empirically show that factoring fact-checks is a challenging task, especially for fact-checkers that are under-represented in the existing dataset. We then formulate the task as a sequence tagging problem and fine-tune the pre-trained BERT models with a modification made from our observations to approach the problem. Through extensive experiments, we demonstrate the performance of our models for well-known fact-checkers and promising initial results for under-represented fact-checkers. © 2020 ACM.","BERT; ClaimReview; computational journalism; fact-checking; information extraction; misinformation; sequence tagging","Logical reasoning; Online platforms; Public dataset; Structured information; Tagging problem; Under-represented; World Wide Web","Chunghwa Telecom;et al.;Microsoft;Quanta Computer;Taiwan Mobile;ZOOM","29th International World Wide Web Conference, WWW 2020","20 April 2020 through 24 April 2020",,160505,Conference Paper,"Final","",Scopus,2-s2.0-85086589255
"Sharma D., Sabharwal M., Goyal V., Vij M.","57211339572;57192422156;57211339705;57193791396;","Sentiment analysis techniques for social media data: A review",2020,"Advances in Intelligent Systems and Computing","1045",,,"75","90",,9,"10.1007/978-981-15-0029-9_7","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85076848234&doi=10.1007%2f978-981-15-0029-9_7&partnerID=40&md5=477d9c897a7cc459afd75b459d42dd69","The world is going to digitize day by day. A lot of data generated by the social website users that play an essential role in decision-making. It is impossible to read the whole text, so sentiment analysis make it easy by providing the polarity to the text and classify text into positive and negative classes. Classification task can be performed by using different algorithms results in a different level of accuracy. The purpose of the survey is to provide an overview of various methods that deal with sentiment analysis. The review also presented a comparative analysis of various sentimental analysis techniques with their performance measurement. © Springer Nature Singapore Pte Ltd. 2020","Decision-making; Opinion mining; Sentiment analysis","Artificial intelligence; Decision making; Analysis techniques; Classification tasks; Comparative analysis; Opinion mining; Performance measurements; Social media datum; Sentiment analysis",,"1st International Conference on Sustainable Technologies for Computational Intelligence, ICTSCI 2019","29 March 2019 through 30 March 2019",,234729,Conference Paper,"Final","",Scopus,2-s2.0-85076848234
"Baldini Soares L., FitzGerald N., Ling J., Kwiatkowski T.","",[No title available],2019,"Matching the Blanks: Distributional Similarity for Relation Learning",,,,"2895","2905",,9,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85092513335
"Gijsbers P., Vanschoren J.","","GAMA: Genetic automated machine learning assistant",2019,"Journal of Open Source Software","4","33",,"","",,9,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85083463709
"Denysiuk R., Gaspar-Cunha A., Delbem A.C.B.","55346440000;6506838748;8575424400;","Neuroevolution for solving multiobjective knapsack problems",2019,"Expert Systems with Applications","116",,,"65","77",,9,"10.1016/j.eswa.2018.09.004","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85053206729&doi=10.1016%2fj.eswa.2018.09.004&partnerID=40&md5=99308b0b47dee04204c6aee334e8b9e0","The multiobjective knapsack problem (MOKP) is an important combinatorial problem that arises in various applications, including resource allocation, computer science and finance. When tackling this problem by evolutionary multiobjective optimization algorithms (EMOAs), it has been demonstrated that traditional recombination operators acting on binary solution representations are susceptible to a loss of diversity and poor scalability. To address those issues, we propose to use artificial neural networks for generating solutions by performing a binary classification of items using the information about their profits and weights. As gradient-based learning cannot be used when target values are unknown, neuroevolution is adapted to adjust the neural network parameters. The main contribution of this study resides in developing a solution encoding and genotype-phenotype mapping for EMOAs to solve MOKPs. The proposal is implemented within a state-of-the-art EMOA and benchmarked against traditional variation operators based on binary crossovers. The obtained experimental results indicate a superior performance of the proposed approach. Furthermore, it is advantageous in terms of scalability and can be readily incorporated into different EMOAs. © 2018 Elsevier Ltd","Evolutionary computation; Multiobjective knapsack problem; Neuroevolution","Classification (of information); Combinatorial optimization; Evolutionary algorithms; Multiobjective optimization; Neural networks; Scalability; Combinatorial problem; Evolutionary multiobjective optimization algorithm (EMOAs); Genotype-phenotype mapping; Gradient-based learning; Knapsack problems; Neural network parameters; Neuro evolutions; Recombination operators; Problem solving",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85053206729
"Amini A., Gabriel S., Lin S., Koncel-Kedziorski R., Choi Y., Hajishirzi H.","57204957087;57215561709;57216965928;57188303100;36172231400;23008126600;","MathQA: Towards interpretable math word problem solving with operation-based formalisms",2019,"NAACL HLT 2019 - 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference","1",,,"2357","2367",,9,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085560883&partnerID=40&md5=b6680533f6f7ae38a37d9eff29deee1a","We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA datasets. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https://math-qa.github.io/math-QA/. © 2019 Association for Computational Linguistics",,"Computational linguistics; Human performance; Interpretability; Large-scale dataset; Operation programs; Operational projects; Program modeling; Representation languages; Word problem solving; Large dataset","Amazon;ASAPP;Bloomberg Engineering;et al.;facebook;Google","2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2019","2 June 2019 through 7 June 2019",,159851,Conference Paper,"Final","",Scopus,2-s2.0-85085560883
"Minervini P., Bosnjak M., Rocktaschel T., Riedel S.","",[No title available],2018,"Towards Neural Theorem Proving at Scale",,,,"","",,9,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078352789
"Sodhro A.H., Sangaiah A.K., Sodhro G.H., Sekhari A., Ouzrout Y., Pirbhulal S.","55389831000;55616335800;57201320740;26640679500;6506918347;56660655500;","Energy-efficiency of tools and applications on internet",2018,"Computational Intelligence for Multimedia Big Data on the Cloud with Engineering Applications",,,,"297","318",,9,"10.1016/B978-0-12-813314-9.00014-1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060684145&doi=10.1016%2fB978-0-12-813314-9.00014-1&partnerID=40&md5=2e3a65296d5957092c46f068a7638ac2","This chapter presents the comparative analysis of the various tools and applications in terms of energy-efficiency. Due to increasing demand of Green technology it is very challenging to minimize the energy consumption of software and hardware components. In addition, the use of applications or software on our computers consumes energy and it also affects the energy drain of several hardware components and system resources. Consequently, running web browsers, media players, file transfer protocols, wired and wireless security protocol applications will utilize the considerable amount of energy. In this remarkable research, we have run different types of experiments which contain the use of several measuring tools. Firstly, joulemeter and powertop (pTop) are used to monitor and calculate the energy drain of hardware and software while running web-based and stand-alone applications with Windows 7 and Linux (i.e. Ubuntu 16.04) operating systems on desktop and laptop computers. Secondly, it is presented that how much energy is consumed by an ordinary citizen on typical things of everyday use on the web. © 2018 Elsevier Inc. All rights reserved.","Application; Energy-efficiency; Joulemeter; Powertop; Tools",,,,,,,Book Chapter,"Final","",Scopus,2-s2.0-85060684145
"Chaturvedi I., Ragusa E., Gastaldo P., Zunino R., Cambria E.","","Bayesian network based extreme learning machine for subjectivity detection",2017,"J Franklin Inst",,,,"","",,9,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85042347247
"Smolensky P., Lee M., He X., Yih W., Gao J., Deng L.","",[No title available],2016,"Basic Reasoning with Tensor Product Representations",,,,"","",,9,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85045249255
"Chasins S., Barman S., Gulwani S., Bodik R.","54395086400;35733236000;55901318200;6701821028;","Browser record and replay as a building block for end-user web automation tools",2015,"WWW 2015 Companion - Proceedings of the 24th International Conference on World Wide Web",,,,"179","182",,9,"10.1145/2740908.2742849","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84968654887&doi=10.1145%2f2740908.2742849&partnerID=40&md5=86e55488f75df3168dbf4d957c0e47c4","To build a programming by demonstration (PBD) web scraping tool for end users, one needs two central components: a list finder, and a record and replay tool. A list finder extracts logical tables from a webpage. A record and replay (R+R) system records a user's interactions with a webpage, and replays them programmatically. The research community has invested substantial work in list finding - variously called wrapper induction, structured data extraction, and template detection. In contrast, researchers largely considered the browser R+R problem solved until recently, when webpage complexity and interactivity began to rise. We argue that the increase in interactivity necessitates the use of new, more robust R+R approaches, which will facilitate the PBD web tools of the future. Because robust R+R is difficult to build and understand, we argue that tool developers need an R+R layer that they can treat as a black box. We have designed an easy-to-use API that allows programmers to use and even customize R+R, without having to understand R+R internals. We have instantiated our API in Ringer, our robust R+R tool. We use the API to implement WebCombine, a PBD scraping tool. A WebCombine user demonstrates how to collect the first row of a relational dataset, and the tool collects all remaining rows. WebCombine uses the Ringer API to handle navigation between pages, enabling users to scrape from modern, interaction-heavy pages. We demonstrate WebCombine by collecting a 3,787,146 row dataset from Google Scholar that allows us to explore the relationship between researchers' years of experience and their papers' citation counts.","Automation; End-User Programming; Programming by Demonstration; Record and Replay","Automation; Budget control; Computer programming; Websites; World Wide Web; Building blockes; Central component; End user programming; Programming by demon-stration; Record-and-replay; Research communities; Template detection; Wrapper induction; Human computer interaction","International World Wide Web Conference Steering Committee (IW3C2)","24th International Conference on World Wide Web, WWW 2015","18 May 2015 through 22 May 2015",,119451,Conference Paper,"Final","",Scopus,2-s2.0-84968654887
"Calì A., Gottlob G., Pieris A.","57204263871;7005068491;24336135100;","Query answering under expressive entity-relationship schemata",2010,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","6412 LNCS",,,"347","361",,9,"10.1007/978-3-642-16373-9_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78649938489&doi=10.1007%2f978-3-642-16373-9_25&partnerID=40&md5=46b4ca114c22629c2458c9a54f0551bc","We address the problem of answering conjunctive queries under constraints representing schemata expressed in an extended version of the Entity- Relationship model. This extended model, called ER+ model, comprises is-a constraints among entities and relationships, plus functional and mandatory participation constraints. In particular, it allows arbitrary permutations of the roles in is-a among relationships. A key notion that ensures high tractability in ER+ schemata is separability, i.e., the absence of interaction between the functional participation constraints and the other constructs of ER+.We provide a precise syntactic characterization of separable ER+ schemata, called ER± schemata, by means of a necessary and sufficient condition. We present a complete complexity analysis of the conjunctive query answering problem under ER± schemata. We show that the addition of so-called negative constraints does not increase the complexity of query answering. With such constraints, our model properly generalizes the most widely-adopted tractable ontology languages. © 2010 Springer-Verlag.",,"Arbitrary permutations; Complexity analysis; Conjunctive queries; Entity-relationship model; Entity-Relationship schema; Extended model; Extended versions; Negative constraints; Ontology language; Query answering; Sufficient conditions; Syntactic characterization; Ontology; Query languages","The ER Institute;Sauder School of Business;Xerox Canada Limited","29th International Conference on Conceptual Modeling, ER 2010","1 November 2010 through 4 November 2010","Vancouver, BC",82677,Conference Paper,"Final","",Scopus,2-s2.0-78649938489
"Domingos P., Sumner M.","",[No title available],2010,"The Alchemy Tutorial",,,,"","",,9,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-79957486677
"Yan P., Yong T., Luxian L., Yemin L.","7403340386;34969635100;57212936066;25825878900;","Question classification with semantic tree kernel",2008,"ACM SIGIR 2008 - 31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Proceedings",,,,"837","838",,9,"10.1145/1390334.1390530","https://www.scopus.com/inward/record.uri?eid=2-s2.0-57349165324&doi=10.1145%2f1390334.1390530&partnerID=40&md5=aad442aa9653b0c7141260f197cc6a57","Question Classification plays an important role in most Question Answering systems. In this paper, we exploit semantic features in Support Vector Machines (SVMs) for Question Classification. We propose a semantic tree kernel to incorporate semantic similarity information. A diverse set of semantic features is evaluated. Experimental results show that SVMs with semantic features, especially semantic classes, can significantly outperform the state-of-the-art systems.","Machine learning; Question answering; Question classification; Semantic class; Support vector machines; Tree kernel","Content based retrieval; Image retrieval; Information retrieval; Information services; Learning systems; Natural language processing systems; Research and development management; Semantics; Support vector machines; Vectors; Machine learning; Question answering; Question classification; Semantic class; Tree kernel; Information theory",,"31st Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, ACM SIGIR 2008","20 July 2008 through 24 July 2008","Singapore",74470,Conference Paper,"Final","",Scopus,2-s2.0-57349165324
"Cumby C.M., Roth D.","7801359589;7401669040;","Learning with feature description logics",2003,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","2583",,,"32","47",,9,"10.1007/3-540-36468-4_3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-7044224298&doi=10.1007%2f3-540-36468-4_3&partnerID=40&md5=575e5631fe895cea92afa67a0abe197f","We present a paradigm for efficient learning and inference with relational data using propositional means. The paradigm utilizes description logics and concepts graphs in the service of learning relational models using efficient propositional learning algorithms.We introduce a Feature Description Logic (FDL) - a relational (frame based) language that supports efficient inference, along with a generation function that uses inference with descriptions in the FDL to produce features suitable for use by learning algorithms. These are used within a learning framework that is shown to learn efficiently and accurately relational representations in terms of the FDL descriptions. The paradigm was designed to support learning in domains that are relational but where the amount of data and size of representation learned are very large; we exemplify it here, for clarity, on the classical ILP tasks of learning family relations and mutagenesis. This paradigm provides a natural solution to the problem of learning and representing relational data; it extends and unifies several lines of works in KRR and Machine Learning in ways that provide hope for a coherent usage of learning and reasoning methods in large scale intelligent inference. © Springer-Verlag Berlin Heidelberg 2003.",,"Artificial intelligence; Data description; Formal languages; Inference engines; Learning algorithms; Learning systems; Inductive logic programming (ILP); Description logic; Efficient learning; Feature description; Learning frameworks; Reasoning methods; Relational Model; Relational representations; Support learning; Inductive logic programming (ILP); Learning algorithms",,"12th International Conference on Inductive Logic Programming, ILP 2002","9 July 2002 through 11 July 2002",,119269,Conference Paper,"Final","",Scopus,2-s2.0-7044224298
"Shastri L.","","Neurally motivated constraints on the working memory capacity of a production system for parallel processing",1992,"Proceedings of the Fourteenth Conference of the Cognitive Science Society",,,,"159","164",,9,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-0343757829
"Du N., Chen K., Kannan A., Tran L., Chen Y., Shafran I.","55697607100;57304936500;57190983493;56231034600;57216623226;13907300200;","Extracting symptoms and their status from clinical conversations",2020,"ACL 2019 - 57th Annual Meeting of the Association for Computational Linguistics, Proceedings of the Conference",,,,"915","925",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084044773&partnerID=40&md5=7fe57c364a9313e1614a8e3790395e27","This paper describes novel models tailored for a new application, that of extracting the symptoms mentioned in clinical conversations along with their status. Lack of any publicly available corpus in this privacy-sensitive domain led us to develop our own corpus, consisting of about 3K conversations annotated by professional medical scribes. We propose two novel deep learning approaches to infer the symptom names and their status: (1) a new hierarchical span-attribute tagging (SA-T) model, trained using curriculum learning, and (2) a variant of sequence-to-sequence model which decodes the symptoms and their status from a few speaker turns within a sliding window over the conversation. This task stems from a realistic application of assisting medical providers in capturing symptoms mentioned by patients from their clinical conversations. To reflect this application, we define multiple metrics. From inter-rater agreement, we find that the task is inherently difficult. We conduct comprehensive evaluations on several contrasting conditions and observe that the performance of the models range from an F-score of 0.5 to 0.8 depending on the condition. Our analysis not only reveals the inherent challenges of the task, but also provides useful directions to improve the models. © 2019 Association for Computational Linguistics",,"Computational linguistics; Comprehensive evaluation; Inter-rater agreements; Learning approach; Medical providers; New applications; Realistic applications; Sequence modeling; Sliding Window; Deep learning","Apple;ASAPP;Bloomberg Engineering;BOSCH;et al.;Expedia","57th Annual Meeting of the Association for Computational Linguistics, ACL 2019","28 July 2019 through 2 August 2019",,159206,Conference Paper,"Final","",Scopus,2-s2.0-85084044773
"Huang K.W., Lin C.C., Lee Y.M., Wu Z.X.","","A deep learning and image recognition system for image recognition",2019,"Data Sci Pattern Recognit","3","2",,"1","11",,8,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85081045682
"Zhong Wanjun, Xu Jingjing, Tang Duyu, Xu Zenan, Duan Nan, Zhou Ming, Wang Jiahai, Yin Jian","",[No title available],2019,"Reasoning over semantic-level graph for fact checking",,,,"","",,8,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85084173266
"Huang Q., Smolensky P., He X., Deng L., Wu D.","55233543400;6602511111;37085932700;36071490500;12807474900;","Tensor product generation networks for deep nlp modeling",2018,"NAACL HLT 2018 - 2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies - Proceedings of the Conference","1",,,"1263","1273",,8,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85066460500&partnerID=40&md5=4aa1b56408bea4ca5736245eabc07418","We present a new approach to the design of deep networks for natural language processing (NLP), based on the general technique of Tensor Product Representations (TPRs) for encoding and processing symbol structures in distributed neural networks. A network architecture - the Tensor Product Generation Network (TPGN) - is proposed which is capable in principle of carrying out TPR computation, but which uses unconstrained deep learning to design its internal representations. Instantiated in a model for image-caption generation, TPGN outperforms LSTM baselines when evaluated on the COCO dataset. The TPR-capable structure enables interpretation of internal representations and operations, which prove to contain considerable grammatical content. Our caption-generation model can be interpreted as generating sequences of grammatical categories and retrieving words by their categories from a plan encoded as a distributed representation. © 2018 The Association for Computational Linguistics.",,"Computational linguistics; Deep learning; Natural language processing systems; Network architecture; Product design; Tensors; Distributed neural networks; Distributed representation; Grammatical category; Image caption; Internal representation; NAtural language processing; New approaches; Tensor products; Long short-term memory","Amazon;Atoutiao AI Lab;Bloomberg;ByteDance;et al.;Google","2018 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, NAACL HLT 2018","1 June 2018 through 6 June 2018",,158892,Conference Paper,"Final","",Scopus,2-s2.0-85066460500
"Bounabi M., Moutaouakil K.E., Satori K.","57195492422;35329465400;35976902500;","A probabilistic vector representation and neural network for text classification",2018,"Communications in Computer and Information Science","872",,,"343","355",,8,"10.1007/978-3-319-96292-4_27","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063738781&doi=10.1007%2f978-3-319-96292-4_27&partnerID=40&md5=3c88943481bc451ba2e1044a705c1a32","The increasing of the textual databases and its representation in large spaces prevents the automation of the treatment of these great masses and the extraction of knowledge. In order to address the challenges of high dimensionality which using the methods and technics of the text mining. Where the term frequency-inverse document frequency (TF-IDF), weighting method, is the most required approach to represent the document. Unfortunately, TF-IDF produces descriptors of large sizes (generally greater than 1000), which requires models with great complexity. However, the texts classification systems based on these models suffer from the overfitting phenomenon and are very slow. Therefore, to overcome these problems, we use the select attributes methods; by giving the deterministic aspect of this latter, we risk to lose huge information. Thus, to recover from this loss, we propose a probabilistic vector representation of each document, based on the relevant terms selected previously. Then, we associate a set of features to each document composed by local and global probabilistic coefficients basing on the selected terms. More specifically and precisely, the components formulas are composed by the frequency of each descriptor, the length of each document and the size of the corpus. To show the performance of this treatment we propose comparative studies between TF-IDF representation and the new probabilistic representation, to classify the BBCSPORT corpus. Moreover, in the classification phase, we use several versions of Bayesian Network and Multilayer Perceptron. The obtained results are satisfied, where the neural network classifier, multilayer perceptron, gives 100% as a recognition rate, using the new representation and 94.69%, using the simple TF-IDF weighting. © Springer Nature Switzerland AG 2018.","Bayes Net; Feature selection; Multilayer perceptron; Probabilistic representation; Select attributes; Text classification","Bayesian networks; Classification (of information); Data mining; Feature extraction; Inverse problems; Multilayer neural networks; Multilayers; Bayes net; Classification system; Neural network classifier; Probabilistic representation; Probabilistic vector; Select attributes; Term frequencyinverse document frequency (TF-IDF); Text classification; Text processing",,,,,,Article,"Final","",Scopus,2-s2.0-85063738781
"Chasins Sarah, Bodik Rastislav","","Skip blocks: Reusing execution history to accelerate web scripts",2017,"Proceedings of the ACM on Programming Languages","1",,,"1","28",,8,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85056824098
"Forbus K.D., Liang C., Rabkina I.","7003585370;57149458400;57194632893;","Representation and Computation in Cognitive Models",2017,"Topics in Cognitive Science","9","3",,"694","718",,8,"10.1111/tops.12277","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85021324802&doi=10.1111%2ftops.12277&partnerID=40&md5=c3e0b2d59cc2ae1e283bc2530ca7ad23","One of the central issues in cognitive science is the nature of human representations. We argue that symbolic representations are essential for capturing human cognitive capabilities. We start by examining some common misconceptions found in discussions of representations and models. Next we examine evidence that symbolic representations are essential for capturing human cognitive capabilities, drawing on the analogy literature. Then we examine fundamental limitations of feature vectors and other distributed representations that, despite their recent successes on various practical problems, suggest that they are insufficient to capture many aspects of human cognition. After that, we describe the implications for cognitive architecture of our view that analogy is central, and we speculate on roles for hybrid approaches. We close with an analogy that might help bridge the gap. Copyright © 2017 Cognitive Science Society, Inc.","Analogy; Computational modeling; Learning; Machine learning; Relational representations; Representation; Symbolic modeling","cognition; human; learning; psychology; theoretical model; Cognition; Cognitive Science; Humans; Learning; Models, Theoretical",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-85021324802
"Gaunt Alexander L, Brockschmidt Marc, Kushman Nate, Tarlow Daniel","","Differentiable programs with neural libraries",2017,"Proceedings of the 34th International Conference on Machine Learning",,,,"1213","1222",,8,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85071156970
"Graben P.B., Rodrigues S.","23110284000;57202415938;","A biophysical observation model for field potentials of networks of leaky integrate-and-fire neurons",2013,"Frontiers in Computational Neuroscience",,"JAN",,"1","13",,8,"10.3389/fncom.2012.00100","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84872080384&doi=10.3389%2ffncom.2012.00100&partnerID=40&md5=255ed6d32e5cd81b7de8738607095d14","We present a biophysical approach for the coupling of neural network activity as resulting from proper dipole currents of cortical pyramidal neurons to the electric field in extracellular fluid. Starting from a reduced three-compartment model of a single pyramidal neuron, we derive an observation model for dendritic dipole currents in extracellular space and thereby for the dendritic field potential (DFP) that contributes to the local field potential (LFP) of a neural population. This work aligns and satisfies the widespread dipole assumption that is motivated by the ""open-field"" configuration of the DFP around cortical pyramidal cells. Our reduced three-compartment scheme allows to derive networks of leaky integrate-and-fire (LIF) models, which facilitates comparison with existing neural network and observation models. In particular, by means of numerical simulations we compare our approach with an ad hoc model by Mazzoni et al. (2008), and conclude that our biophysically motivated approach yields substantial improvement. © 2013 beim Graben and Rodrigues.","Biophysics; Current dipoles; Extracellular medium; Field potentials; Leaky integrate-and-fire neuron; Neural networks","Current dipoles; Dipole currents; Extracellular fluid; Extracellular medium; Extracellular space; Field potential; Integrate-and-fire model; Integrate-and-fire neurons; Local field potentials; Network activities; Neural populations; Observation model; Pyramidal cell; Pyramidal neuron; Biophysics; Body fluids; Electric fields; Electrophysiology; Independent component analysis; Neurons; Neural networks; article; biophysics; brain electrophysiology; dendritic dipole current; dendritic field potential; extracellular space; local field potential; nerve cell network; pyramidal nerve cell; simulation",,,,,,Article,"Final","All Open Access, Gold, Green",Scopus,2-s2.0-84872080384
"Wang Q., Ji Y., Hao Y., Cao J.","57216212246;56017430500;55197931300;57192160183;","GRL: Knowledge graph completion with GAN-based reinforcement learning",2020,"Knowledge-Based Systems","209",,"106421","","",,7,"10.1016/j.knosys.2020.106421","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091653381&doi=10.1016%2fj.knosys.2020.106421&partnerID=40&md5=61c72f18d6f85c67d95b32717a8ebef9","Knowledge graph completion intends to infer the entities that need to be queried through the entities and relations known in the knowledge graphs. It is used in many applications, such as question and answer systems, and searching engines. As the completion process can be represented as a Markov process, existing works would solve this problem with reinforcement learning. However, there are three issues blocking them from achieving high accuracy, which are reward sparsity, missing specific domain rules, and ignoring the generation of knowledge graphs. In this paper, we design a generative adversarial net (GAN)-based reinforcement learning model, named GRL, for knowledge graph completion. First, GRL employs the graph convolutional network to embed the knowledge graphs into the low-dimensional space. Second, GRL employs both GAN and long short-term memory (LSTM) to record trajectory sequences obtained by the agent from traversing the knowledge graph and generate new trajectory sequences if needed. At the same time, GRL applies domain-specific rules accordingly. Finally, GRL employs the deep deterministic policy gradient method to optimize both rewards and adversarial loss. The experiments show that GRL is able to both generate better policies and outperform traditional methods for several tasks. © 2020 Elsevier B.V.","Deep learning; Knowledge graph; Knowledge graph completion; Reinforcement learning","Convolutional neural networks; Gradient methods; Knowledge representation; Long short-term memory; Markov processes; Convolutional networks; Domain specific; Knowledge graphs; Low-dimensional spaces; Policy gradient methods; Question and answer system; Reinforcement learning models; Searching engine; Reinforcement learning",,,,,,Article,"Final","",Scopus,2-s2.0-85091653381
"Lin Y., Li J., Yang L., Xu K., Lin H.","56399483900;57216838178;55733088000;48762086600;24468572400;","Sentiment Analysis with Comparison Enhanced Deep Neural Network",2020,"IEEE Access","8",,"9076035","78378","78384",,7,"10.1109/ACCESS.2020.2989424","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084928560&doi=10.1109%2fACCESS.2020.2989424&partnerID=40&md5=5d1f755718247721353a7215b34e3966","Sentiment analysis is a significant task in Natural Language Processing. It refers to classification based on the emotional tendency in text by extracting text features. The existing results show that models based on RNN and CNN have good performance. In order to improve the performance of text sentiment analysis, we reformulate the classification task as a comparing problem, and propose Comparison Enhanced Bi-LSTM with Multi-Head Attention (CE-B-MHA). In fact, it is efficient to classify by comparison mechanism instead of doing complex calculation. In this model, bidirectional LSTM is used for initial feature extraction, and valuable information is extracted from different dimensions and representation subspaces by Multi-Head Attention. The comparison mechanism aims to score the feature vectors by comparing with the labeled vectors. The experimental results show that CE-B-MHA has better performance than many existing models on three sentiment analysis datasets. © 2013 IEEE.","machine learning; neural networks; Sentiment analysis","Long short-term memory; Sentiment analysis; Classification tasks; Feature vectors; NAtural language processing; Text feature; Deep neural networks",,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85084928560
"Es-Sabery F., Hair A.","57216687150;6603926985;","An Improved ID3 Classification Algorithm Based on Correlation Function and Weighted Attribute",2019,"Proceedings - 2019 International Conference on Intelligent Systems and Advanced Computing Sciences, ISACS 2019",,,"9068891","","",,7,"10.1109/ISACS48493.2019.9068891","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084278134&doi=10.1109%2fISACS48493.2019.9068891&partnerID=40&md5=41e1909682a8b585d3f68c7dc6c1d895","ID3 decision tree algorithm is a supervised learning model based on calculating the information gain to select the best splitting attribute, which is the main factor to construct a decision tree. The process of calculating gain takes into consideration only a current condition attribute and decision attribute, and the other condition attributes cannot be used to measure the attribute importance. Because of the above problem, an improved ID3 takes into consideration the connection between the current condition attribute and the other conditions attributes. An experiment is presented to compare our improved algorithm with the traditional ID3 algorithm. Experiment results show that our improved algorithm provides a decision tree with less number of leaves and higher predictive accuracy. © 2019 IEEE.","Classification; Data Mining; Decision Tree; ID3; Information gain; Weighted Modified Information gain","Decision trees; Intelligent systems; Attribute importance; Classification algorithm; Condition attributes; Correlation function; Decision attribute; Decision-tree algorithm; Predictive accuracy; Weighted attributes; Trees (mathematics)","Association of Research Professors and Officials of the Taza FP;IEEE Morocco Section;Polydisciplinary Faculty of Taza","3rd International Conference on Intelligent Systems and Advanced Computing Sciences, ISACS 2019","26 December 2019 through 27 December 2019",,159407,Conference Paper,"Final","",Scopus,2-s2.0-85084278134
"Misra R., Arora P.","",[No title available],2019,"Sarcasm Detection Using Hybrid Neural Network",,,,"","",,7,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85081261033
"Lee K., Palangi H., Chen X., Hu H., Gao J.","",[No title available],2019,"Learning visual relation priors for image-text matching and image captioning with neural scene graph generators",,,,"","",,7,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85089805438
"Wu Y., Luo R., Leung H.C.M., Ting H.-F., Lam T.-W.","57208683314;35763988800;35233742700;7005654198;7202523165;","RENET: A Deep Learning Approach for Extracting Gene-Disease Associations from Literature",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11467 LNBI",,,"272","284",,7,"10.1007/978-3-030-17083-7_17","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85065526380&doi=10.1007%2f978-3-030-17083-7_17&partnerID=40&md5=e851f1ff9e8bd196474a687a8ade8d12","Over one million new biomedical articles are published every year. Efficient and accurate text-mining tools are urgently needed to automatically extract knowledge from these articles to support research and genetic testing. In particular, the extraction of gene-disease associations is mostly studied. However, existing text-mining tools for extracting gene-disease associations have limited capacity, as each sentence is considered separately. Our experiments show that the best existing tools, such as BeFree and DTMiner, achieve a precision of 48% and recall rate of 78% at most. In this study, we designed and implemented a deep learning approach, named RENET, which considers the correlation between the sentences in an article to extract gene-disease associations. Our method has significantly improved the precision and recall rate to 85.2% and 81.8%, respectively. The source code of RENET is available at https://bitbucket.org/alexwuhkucs/gda-extraction/src/master/. © 2019, Springer Nature Switzerland AG.","Deep learning; Gene-disease association; Literature mining; Relation Extraction","Data mining; Extraction; Genes; Molecular biology; Gene-disease associations; Genetic testing; Learning approach; Limited capacity; Literature mining; Precision and recall; Relation extraction; Source codes; Deep learning","Akamai Technologies;Computation (MDPI journal);et al.;Natera;Springer;The George Washington University","23rd International Conference on Research in Computational Molecular Biology, RECOMB 2019","5 May 2019 through 8 May 2019",,225459,Conference Paper,"Final","",Scopus,2-s2.0-85065526380
"Hidey C., Diab M.","","Team sweeper: Joint sentence extraction and fact checking with pointer networks",2018,"Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)",,,,"150","155",,7,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85076463779
"Arif M.H., Li J., Iqbal M., Peng H.","57210526777;55720560100;57202609807;57190014707;","Optimizing XCSR for Text Classification",2017,"Proceedings - 11th IEEE International Symposium on Service-Oriented System Engineering, SOSE 2017",,,"7943296","86","95",,7,"10.1109/SOSE.2017.9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85022181309&doi=10.1109%2fSOSE.2017.9&partnerID=40&md5=5a5be053d006f8adddf51413a98d037e","XCS, an evolutionary computing technique, can classify data using both bit strings and real valued representations. 'Real valued XCS' (XCSR) commonly uses the min max interval based representation (MMR) for continuous valued data sets. Text data sets can be represented using bag of words based real valued representation, e.g. term frequency inverse document frequency of features. In this work we classify social media short informal text messages using XCSR, for the first time, from two major domains, i.e. spam detection and sentiment analysis. We perform spam detection of SMS and Email messages, and sentiment analysis of reviews and tweets. Feature vectors extracted from short text messages are very sparse and XCSR with MMR representation can not handle sparse data sets very well. We proposed XCSR# that uses MMR representation with explicit 'don't care' intervals to handle sparse social media data sets. The experimental results indicate that introduction of the explicit 'don't care' intervals improved the performance and created a statistically significant impact, specifically in the spam detection data sets. Further, it is observed that XCSR# produced more accurate and general rules than XCSR. © 2017 IEEE.","Sentiment Analysis; Spam Detection; Text Classification; XCSR","Data mining; Social networking (online); Systems engineering; Text processing; Evolutionary computing; Real-valued representations; Sentiment analysis; Short text messages; Spam detection; Term frequency-inverse document frequencies; Text classification; XCSR; Classification (of information)","IEEE;IEEE Computer Society;San Jose State University","11th IEEE International Symposium on Service-Oriented System Engineering, SOSE 2017","6 April 2017 through 9 April 2017",,128270,Conference Paper,"Final","",Scopus,2-s2.0-85022181309
"Evans V.","","What's in a concept? Analog versus parametric concepts in LCCM Theory",2015,"The Conceptual Mind: New Directions in the Study of Concepts",,,,"251","290",,7,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84925474291
"Stewart T.C., Choo X., Eliasmith C.","","Sentence processing in spiking neurons: A biologically plausible leftcorner parser",2014,"36th Annual Conference of the Cognitive Science Society",,,,"1533","1538",,7,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84960545023
"Kushman N., Zettlemoyer L., Barzilay R., Artzi Y.","","Learning to automatically solve algebra word problems",2014,"ACL",,,,"","",,7,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85007150187
"Tabor W., Cho P.W., Szkudlarek E.","6603814753;55771113000;55646983300;","Fractal Analysis Illuminates the Form of Connectionist Structural Gradualness",2013,"Topics in Cognitive Science","5","3",,"634","667",,7,"10.1111/tops.12036","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84880331465&doi=10.1111%2ftops.12036&partnerID=40&md5=fe8912f3abdc2a6548227961f90c0086","We examine two connectionist networks-a fractal learning neural network (FLNN) and a Simple Recurrent Network (SRN)-that are trained to process center-embedded symbol sequences. Previous work provides evidence that connectionist networks trained on infinite-state languages tend to form fractal encodings. Most such work focuses on simple counting recursion cases (e.g., anbn), which are not comparable to the complex recursive patterns seen in natural language syntax. Here, we consider exponential state growth cases (including mirror recursion), describe a new training scheme that seems to facilitate learning, and note that the connectionist learning of these cases has a continuous metamorphosis property that looks very different from what is achievable with symbolic encodings. We identify a property-ragged progressive generalization-which helps make this difference clearer. We suggest two conclusions. First, the fractal analysis of these more complex learning cases reveals the possibility of comparing connectionist networks and symbolic models of grammatical structure in a principled way-this helps remove the black box character of connectionist networks and indicates how the theory they support is different from symbolic approaches. Second, the findings indicate the value of future, linked mathematical and empirical work on these models-something that is more possible now than it was 10 years ago. © 2013 Cognitive Science Society, Inc.","Dynamical systems; Fractal grammars; Generalization; Neural (connectionist) networks; Recursion; Self-organization; Simple Recurrent Network (SRN); Symbolic models","algorithm; article; artificial neural network; computer simulation; Dynamical systems; fractal analysis; Fractal grammars; generalization; human; language; learning; Neural (connectionist) networks; Recursion; self-organization; Simple Recurrent Network (SRN); Symbolic models; theoretical model; Dynamical systems; Fractal grammars; Generalization; Neural (connectionist) networks; Recursion; Self-organization; Simple Recurrent Network (SRN); Symbolic models; Algorithms; Computer Simulation; Fractals; Generalization (Psychology); Humans; Language; Models, Theoretical; Neural Networks (Computer)",,,,,,Article,"Final","All Open Access, Bronze",Scopus,2-s2.0-84880331465
"Kumar M., Das D., Agarwal S., Rudnicky A.","","Non-textual event summarization by applying machine learning to template-based language generation",2009,"Proceedings of the 2009 Workshop on Language Generation and Summarisation",,,,"67","71",,7,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-79959652444
"Ropero J., Gómez A., Leon C., Carrasco A.","8856865200;57213449257;24724405600;23395682000;","Term weighting: Novel fuzzy logic based method vs. classical TF-IDF method for web information extraction",2009,"ICEIS 2009 - 11th International Conference on Enterprise Information Systems, Proceedings","AIDSS",,,"130","137",,7,"10.5220/0001982901300137","https://www.scopus.com/inward/record.uri?eid=2-s2.0-74549167331&doi=10.5220%2f0001982901300137&partnerID=40&md5=a8907ae6fd07d5bf6283866c139a555c","Solving Term Weighting problem is one of the most important tasks for Information Retrieval and Information Extraction. Tipically, the TF-IDF method have been widely used for determining the weight of a term. In this paper, we propose a novel alternative fuzzy logic based method. The main advantage for the proposed method is the obtention of better results, especially in terms of extracting not only the most suitable information but also related information. This method will be used for the design of a Web Intelligent Agent which will soon start to work for the University of Seville web page.","Fuzzy logic; Information extraction; Information retrieval; Intelligent agent; Term weighting; TF-IDF; Vector space model","Computer circuits; Information retrieval; Information systems; Information use; Intelligent agents; Vector spaces; Websites; Seville; Term weighting; TF-IDF; Vector space models; Web information extraction; Fuzzy logic",,"ICEIS 2009 - 11th International Conference on Enterprise Information Systems","6 May 2009 through 10 May 2009","Milan",79065,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-74549167331
"Amaral C., Cassan A., Figueira H., Martins A., Mendes A., Mendes P., Pinto C., Vidal D.","14832184400;23392245200;23392597200;55937372200;23393151700;57197289932;23393942600;23394239800;","Priberam's question answering system in QA@CLEF 2007",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5152 LNCS",,,"364","371",,7,"10.1007/978-3-540-85760-0_46","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349832162&doi=10.1007%2f978-3-540-85760-0_46&partnerID=40&md5=be684898c902a6b844b989774d481f99","This paper accounts for Priberam's participation in the monolingual question answering (QA) track of CLEF 2007. In previous participations, Priberam's QA system obtained encouraging results both in monolingual and cross-language tasks. This year we endowed the system with syntactical processing, in order to capture the syntactic structure of the question. The main goal was to obtain a more tuned question categorisation and consequently a more precise answer extraction. Besides this, we provided our system with the ability to handle topic-related questions and to use encyclopaedic sources like Wikipedia. The paper provides a description of the improvements made in the system, followed by the discussion of the results obtained in Portuguese and Spanish monolingual runs. © 2008 Springer-Verlag Berlin Heidelberg.",,"Syntactics; Answer extraction; Cross languages; QA system; Question Answering; Question answering systems; Syntactic structure; Wikipedia; Natural language processing systems",,"8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007","19 September 2007 through 21 September 2007","Budapest",77494,Conference Paper,"Final","",Scopus,2-s2.0-70349832162
"Hadelich K., Branigan H., Pickering M., Crocker M.","","Alignment in Dialogue: Effects of Visual versus Verbal-feedback",2004,"Proceedings of the 8th Workshop on the Semantics and Pragmatics of Dialogue, Catalog'04",,,,"35","40",,7,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-33749041963
"Zhong Y.-X.","7401809029;","Study on information-knowledge-intelligence transformation",2004,"Tien Tzu Hsueh Pao/Acta Electronica Sinica","32","4",,"601","605",,7,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-3142731912&partnerID=40&md5=555bfc34e77b4761c53e4adbaddb5ecd","Resources are the bases of human life. Energy is the characteristic resource in industrial age and therefore, the law of energy transform and conservation became the solid foundation of science and technology in modern time. Information is the characteristic resource in information age and thus the transformational and unified theory of information, knowledge, strategy and its execution will be the soul of science and technology development in information age. The concepts and the mechanism of information, knowledge, strategy and its execution will be explained and the unified theory of information, knowledge, strategy and its execution can thus be established within which the information theory has yet greatly gone beyond Shannon theory. The knowledge theory has also dramatically extended from knowledge engineering. The three existing approaches of intelligence theory, the neural networks approach, the expert systems approach, and the sensor-motor approach, are integrated into a uniform system. The unified theory will provide new vision and new vigor for the further development of intelligence science.","Comprehensive information theory (CIT); Knowledge; Knowledge theory; Strategy and execution; Unified theory of information","Expert systems; Information theory; Knowledge engineering; Neural networks; Strategic planning; Comprehensive information theory; Knowledge; Knowledge theory; Strategy and execution; Unified theory of information; Artificial intelligence",,,,,,Article,"Final","",Scopus,2-s2.0-3142731912
"Lan Y., Hao Y., Xia K., Qian B., Li C.","57207795264;57188804372;57216587623;36601594000;56564807100;","Stacked Residual Recurrent Neural Networks with Cross-Layer Attention for Text Classification",2020,"IEEE Access","8",,"9063530","70401","70410",,6,"10.1109/ACCESS.2020.2987101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083963945&doi=10.1109%2fACCESS.2020.2987101&partnerID=40&md5=372440643f910231d6bba2bf2dbe29f0","Text classification is a fundamental task in natural language processing and is essential for many tasks like sentiment analysis and question classification etc. As we all know, different NLP tasks require different linguistic features. Tasks such as text classification requires more semantic features than other tasks such as dependency parsing requiring more syntactic features. Most existing methods focus on improving performance by mixing and calibrating features, without distinguishing the types of features and corresponding effects. In this paper, we propose a stacked residual recurrent neural networks with cross-layer attention model to filter more semantic features for text classification, which named SRCLA. Firstly, we build a stacked network structure to filter different types of linguistic features, and then propose a novel cross-layer attention mechanism that exploits higher-level features to supervise the lower-level features to refine the filtering process. Based on this, more semantic features can be selected for text classification. We conduct experiments on eight text classification tasks, including sentiment analysis, question classification and subjectivity classification and compare with a broad range of baselines. Experimental results show that the proposed approaches achieve the state-of-the-art results on 5 out of 8 tasks. © 2020 IEEE.","bidirectional long short-term memory; Cross-layer attention; feature filtering; stacked residual neural network; text classification","Multilayer neural networks; Network layers; Recurrent neural networks; Semantics; Sentiment analysis; Syntactics; Attention mechanisms; Improving performance; Linguistic features; NAtural language processing; Network structures; Question classification; Subjectivity classifications; Text classification; Classification (of information)",,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85083963945
"Hajj N., Rizk Y., Awad M.","55497496700;56028080500;35177145900;","A subjectivity classification framework for sports articles using improved cortical algorithms",2019,"Neural Computing and Applications","31","11",,"8069","8085",,6,"10.1007/s00521-018-3549-3","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051077042&doi=10.1007%2fs00521-018-3549-3&partnerID=40&md5=fb417fcde85240dd29c33900183a8067","The enormous number of articles published daily on the Internet, by a diverse array of authors, often offers misleading or unwanted information, rendering activities such as sports betting riskier. As a result, extracting meaningful and reliable information from these sources becomes a time-consuming and near impossible task. In this context, labeling articles as objective or subjective is not a simple natural language processing task because subjectivity can take several forms. With the rise of online sports betting due to the revolution in Internet and mobile technology, an automated system capable of sifting through all these data and finding relevant sources in a reasonable amount of time presents itself as a desirable and marketable product. In this work, we present a framework for the classification of sports articles composed of three stages: The first stage extracts articles from web pages using text extraction libraries, parses the text and then tags words using Stanford’s parts of speech tagger; the second stage extracts unique syntactic and semantic features, and reduces them using our modified cortical algorithm (CA)—hereafter CA*—while the third stage classifies these texts as objective or subjective. Our framework was tested on a database containing 1000 articles, manually labeled using Amazon’s crowdsourcing tool, Mechanical Turk; and results using CA, CA*, support vector machines and one of its soft computing variants (LMSVM) as classifiers were reported. A testing accuracy of 85.6% was achieved on a fourfold cross-validation with a 40% reduction in features using CA* that was trained using an entropy weight update rule and a cross-entropy cost function. © 2018, The Natural Computing Applications Forum.","Cortical algorithm; Feature reduction; Natural language processing; Subjectivity analysis; Support vector machines","Automation; Cost functions; Entropy; Semantics; Soft computing; Sports; Support vector machines; Syntactics; Text processing; Websites; Automated systems; Feature reduction; Mechanical turks; Mobile Technology; Semantic features; Subjectivity analysis; Subjectivity classifications; Testing accuracy; Natural language processing systems",,,,,,Article,"Final","",Scopus,2-s2.0-85051077042
"Li L., Wang J., Li J., Ma Q., Wei J.","57209507651;55959923500;57209506792;56717086000;57203496589;","Relation Classification via Keyword-Attentive Sentence Mechanism and Synthetic Stimulation Loss",2019,"IEEE/ACM Transactions on Audio Speech and Language Processing","27","9","8733064","1392","1404",,6,"10.1109/TASLP.2019.2921726","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067957904&doi=10.1109%2fTASLP.2019.2921726&partnerID=40&md5=bfc44ff29528c03a796879a3bbe5f799","Previous studies have shown that attention mechanisms and shortest dependency paths have a positive effect on relation classification. In this paper, a keyword-attentive sentence mechanism is proposed to effectively combine the two methods. Furthermore, to effectively handle the imbalanced classification problem, this paper proposes a new loss function called the synthetic stimulation loss, which uses a modulating factor to allow the model to focus on hard-to-classify samples. The proposed two methods are integrated into a bidirectional gated recurrent unit (BiGRU). As a single model is not strong in noise immunity, this paper applies the mutual learning method to our model and forces the networks to teach each other. Therefore, we call the final model SSL-KAS-MuBiGRU. Experiments on the SemEval-2010 Task 8 data set and the TAC40 data set demonstrate that the keyword-attentive sentence mechanism and synthetic stimulation loss are useful for relation classification, and our model achieves state-of-the-art results. © 2014 IEEE.","attention mechanism; bidirectional gated recurrent unit; imbalanced classification; loss function; mutual learning; Relation classification; shortest dependency path","Attention mechanisms; bidirectional gated recurrent unit; Imbalanced classification; Loss functions; Mutual learning; Relation classifications; shortest dependency path; Classification (of information)",,,,,,Article,"Final","",Scopus,2-s2.0-85067957904
"Škrlj B., Repar A., Pollak S.","57191625180;57210335940;55543643800;","RaKUn: Rank-based Keyword Extraction via Unsupervised Learning and Meta Vertex Aggregation",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11816 LNAI",,,"311","323",,6,"10.1007/978-3-030-31372-2_26","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85075881689&doi=10.1007%2f978-3-030-31372-2_26&partnerID=40&md5=17496ce4bf64e5708b1e7e4f8ffb1700","Keyword extraction is used for summarizing the content of a document and supports efficient document retrieval, and is as such an indispensable part of modern text-based systems. We explore how load centrality, a graph-theoretic measure applied to graphs derived from a given text can be used to efficiently identify and rank keywords. Introducing meta vertices (aggregates of existing vertices) and systematic redundancy filters, the proposed method performs on par with state-of-the-art for the keyword extraction task on 14 diverse datasets. The proposed method is unsupervised, interpretable and can also be used for document visualization. © 2019, Springer Nature Switzerland AG.","Graph applications; Information retrieval; Keyword extraction; Load centrality; Vertex ranking","Graph theory; Information retrieval; Speech processing; Document Retrieval; Document visualization; Graph-theoretic; Keyword extraction; State of the art; Text-based systems; Vertex-ranking; Extraction",,"7th International Conference on Statistical Language and Speech Processing, SLSP 2019","14 October 2019 through 16 October 2019",,232699,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85075881689
"Hu Q., D’Antoni L.","57195073024;52863449200;","Syntax-guided synthesis with quantitative syntactic objectives",2018,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10981 LNCS",,,"386","403",,6,"10.1007/978-3-319-96145-3_21","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85051131373&doi=10.1007%2f978-3-319-96145-3_21&partnerID=40&md5=86b85897f295256208eae284a2b67685","Automatic program synthesis promises to increase the productivity of programmers and end-users of computing devices by automating tedious and error-prone tasks. Despite the practical successes of program synthesis, we still do not have systematic frameworks to synthesize programs that are “good” according to certain metrics—e.g., produce programs of reasonable sizes or with good runtime—and to understand when synthesis can result in such good programs. In this paper, we propose QSyGuS, a unifying framework for describing syntax-guided synthesis problems with quantitative objectives over the syntax of the synthesized programs. QSyGuS builds on weighted (tree) grammars, a clean and foundational formalism that provides flexible support for different quantitative objectives, useful closure properties, and practical decision procedures. We then present an algorithm for solving QSyGuS. Our algorithm leverages closure properties of weighted grammars to generate intermediate problems that can be solved using non-quantitative SyGuS solvers. Finally, we implement our algorithm in a tool, QuaSi, and evaluate it on 26 quantitative extensions of existing SyGuS benchmarks. QuaSi can synthesize optimal solutions in 15/26 benchmarks with times comparable to those needed to find an arbitrary solution. © The Author(s) 2018.",,"Computation theory; Computer aided analysis; Syntactics; Algorithm for solving; Automatic programs; Computing devices; Decision procedure; Error prone tasks; Quantitative objectives; Synthesis problems; Systematic framework; Computer circuits","","30th International Conference on Computer Aided Verification, CAV 2018 Held as Part of the Federated Logic Conference, FloC 2018","14 July 2018 through 17 July 2018",,216429,Conference Paper,"Final","",Scopus,2-s2.0-85051131373
"Sodhro A.H., Fortino G., Pirbhulal S., Muhammad Lodro M., Shah M.A.","","Energy-efficiency in wireless body sensor networks",2017,"Energy-efficiency in Wireless Body Sensor Networks"", Book Title: Networks of the Future Architectures, Technologies, and Implementations",,,,"","",,6,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85064108789
"Bounabi M., El Moutaouakil K., Satori Kh.","","A comparison of Text Classification methods Method of weighted terms selected by different Stemming Techniques",2017,"Proceedings of the 2nd International Conference on Big Data, Cloud and Applications",,,,"","",,6,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078337028
"Pilát M., Křen T., Neruda R.","22433278900;56315454900;6701660464;","Asynchronous evolution of data mining workflow schemes by strongly typed genetic programming",2017,"Proceedings - 2016 IEEE 28th International Conference on Tools with Artificial Intelligence, ICTAI 2016",,,"7814654","577","584",,6,"10.1109/ICTAI.2016.91","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85013657562&doi=10.1109%2fICTAI.2016.91&partnerID=40&md5=5c75a2b8487acac57d826dd02d22cd4d","This paper describes an algorithm for the automated design of whole machine learning workflows, including preprocessing of the data and automatic creation of several types of ensembles. The algorithm is based on strongly typed genetic programming which ensures the validity of the workflows. The evolution of the individuals in the population is asynchronous in order to improve the utilization of computational resources. The approach is validated on four data sets from the UCI machine learning repository. © 2016 IEEE.",,"Artificial intelligence; Data mining; Genetic algorithms; Learning systems; Automated design; Automatic creations; Computational resources; Data mining workflow; Strongly-typed genetic programming; UCI machine learning repository; Whole machine; Work-flows; Genetic programming","Biological and Artificial Intelligence Foundation (BAIF);IEEE Computer Society","28th IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2016","6 November 2016 through 8 November 2016",,126023,Conference Paper,"Final","",Scopus,2-s2.0-85013657562
"Bisio F., Gastaldo P., Zunino R., Decherchi S.","55547779800;35612596100;7006338311;23090052400;","Semi-supervised machine learning approach for unknown malicious software detection",2014,"INISTA 2014 - IEEE International Symposium on Innovations in Intelligent Systems and Applications, Proceedings",,,"6873597","52","59",,6,"10.1109/INISTA.2014.6873597","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84906696338&doi=10.1109%2fINISTA.2014.6873597&partnerID=40&md5=08c1cb42312fe6e5af5c4b68962c9086","Inductive bias represents an important factor in learning theory, as it can shape the generalization properties of a learning machine. This paper shows that biased regularization can be used as inductive bias to effectively tackle the semi-supervised classification problem. Thus, semi-supervised learning is formalized as a supervised learning problem biased by an unsupervised reference solution. The proposed framework has been tested on a malware-detection problem. Experimental results confirmed the effectiveness of the semi-supervised methodology presented in this paper. © 2014 IEEE.","biased regularization; malware detection; semi-supervised; SVM","Computer crime; Intelligent systems; Malware; biased regularization; Machine learning approaches; Malicious software detections; Malware detection; Semi-supervised; Semi-supervised classification; Supervised learning problems; SVM; Supervised learning","IEEE Italy Section","2014 IEEE International Symposium on Innovations in Intelligent Systems and Applications, INISTA 2014","23 June 2014 through 25 June 2014","Alberobello",107095,Conference Paper,"Final","",Scopus,2-s2.0-84906696338
"Yathongchai C., Angskun T., Yathongchai W., Angskun J.","55538499700;9734958900;55538639600;35758375800;","Learner classification based on learning behavior and performance",2013,"2013 IEEE Conference on Open Systems, ICOS 2013",,,"6735050","66","70",,6,"10.1109/ICOS.2013.6735050","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84897694975&doi=10.1109%2fICOS.2013.6735050&partnerID=40&md5=abc1cf00104e2fbd5867117ca09a95ec","A learner classification is an important process in providing online lessons to suit each individual learner. In this paper, the concept of learner classification is considered on learning behavior and performance. There are two main processes for generating the classification model as follows: 1) Applying K-means clustering to analyze learning behaviors of each learner based on learner's profile from e-learning system; and 2) Applying a decision tree classifier to generate the learner classification model based on the learning behaviors and student's performance. The experimental results show that the learner classification model is achieved in 83.8% of precision, 85.4% of recall and 85.5% of F-measure. © 2013 IEEE.","Behavior profile; Learner classification; Learner performance","Decision trees; E-learning; Mathematical models; Behavior profile; Classification models; Decision tree classifiers; E-learning systems; K-means clustering; Learner performance; Learner's profile; Student's performance; Learning systems","","2013 IEEE Conference on Open Systems, ICOS 2013","2 December 2013 through 4 December 2013","Kuching, Sarawak",103067,Conference Paper,"Final","",Scopus,2-s2.0-84897694975
"Blutner R.","55999893600;","Taking a broader view: Abstraction and idealization1",2011,"Theoretical Linguistics","37","1-2",,"27","35",,6,"10.1515/THLI.2011.002","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80655123402&doi=10.1515%2fTHLI.2011.002&partnerID=40&md5=52fe61a175e6bf1834f58118963ac42f",[No abstract available],,,,,,,,Note,"Final","All Open Access, Green",Scopus,2-s2.0-80655123402
"Tabor W.","","Recursion and recursion-like structure in ensembles of neural elements",2011,"Unifying Themes in Complex Systems. Proceedings of the VIII International Conference on Complex Systems",,,,"1494","1508",,6,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84880331266
"Mendes A., Coheur L., Mamede N.J., Ribeiro R., Batista F., De Matos D.M.","25031944200;8075360000;6602527446;13609216700;15041642100;54959008500;","QA@L2F, first steps at QA@CLEF",2008,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5152 LNCS",,,"356","363",,6,"10.1007/978-3-540-85760-0_45","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349795176&doi=10.1007%2f978-3-540-85760-0_45&partnerID=40&md5=095aaa17b9c42d403ba68b60705617d1","This paper presents QA@L2F, the question-answering system developed at L2F, INESC-ID. QA@L2F follows different strategies according with the question type, and relies strongly on named entity recognition and on the pre-detection of linguistic patterns. Each question type is mapped into a single strategy; however, if no answer is found, the system proceeds and tries to find an answer using one of the other strategies. © 2008 Springer-Verlag Berlin Heidelberg.",,"Natural language processing systems; Pattern recognition; Linguistic patterns; Named entity recognition; Question answering systems; Question type; Linguistics",,"8th Workshop of the Cross-Language Evaluation Forum, CLEF 2007","19 September 2007 through 21 September 2007","Budapest",77494,Conference Paper,"Final","",Scopus,2-s2.0-70349795176
"Čermák F.","","Idioms and morphology",2007,"Phraseology: An International Handbook of Contemporary Research","1",,,"20","26",,6,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84855934092
"Ding Liya","7403133122;","Neural prolog - the concepts, construction and mechanism -",1995,"Proceedings of the IEEE International Conference on Systems, Man and Cybernetics","4",,,"3603","3608",,6,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-0029543122&partnerID=40&md5=8c839de6a739754485b3b1063116f09e","The research under the name of Neural Logic Networks (NLN) is an attempt to integrate connectionist models and logic reasoning towards realization of future real world computing. Neural Prolog, a Prolog-like inference system based on Neural Logic (NL) is a part of the research. In this system, a programming form which is similar with Prolog in the form but with a rich set of operations and treated by a different inference mechanism exists. The Neural Prolog system has two major components, the inference engine and the knowledge network.",,"Artificial intelligence; Inference engines; Knowledge representation; Logic programming; PROLOG (programming language); Real time systems; Recursive functions; Knowledge construction; Knowledge network; Neural logic; Neural prolog; Prolog like inference systems; Uncertainty; Neural networks","IEEE","Proceedings of the 1995 IEEE International Conference on Systems, Man and Cybernetics. Part 2 (of 5)","22 October 1995 through 25 October 1995","Vancouver, BC, Can",44222,Conference Paper,"Final","",Scopus,2-s2.0-0029543122
"Yousuf H., Salloum S.","57216269926;57195670894;","Survey analysis: Enhancing the security of vectorization by using word2vec and CryptDB",2020,"Advances in Science, Technology and Engineering Systems","5","4",,"374","380",,5,"10.25046/aj050443","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091027234&doi=10.25046%2faj050443&partnerID=40&md5=06dc12ef07f1e1dcb2c8c7e5b65a1a0c","Vectorization is extracting data from strings through Natural Language Processing by using different approaches; one of the best approaches used in vectorization is word2vec. To make the vectorized data secure, we must apply a security method, which will be CryptDB. The paper is analyzing the survey, which is created to interview security engineers through the SPSS tool. By analyzing the responses from software security engineers, it is seen that both word2vec and CryptDB works significantly. Word2vec is an effective vectorization approach, while CryptDB is an effective, secure database. In future work, we will be developing a secure vectorization using both approaches. © 2020 ASTES Publishers. All rights reserved.","CryptDB; Natural Language Processing; Vectorisation; Word2vec",,,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85091027234
"Krause B., Kahembwe E., Murray I., Renals S.","",[No title available],2019,"Dynamic evaluation of transformer language models",,,,"","",,5,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85087520288
"Zhou J., Han X., Yang C., Liu Z., Wang L., Li C., Sun M.","",[No title available],2019,"GEAR: Graph-based evidence aggregating and reasoning for fact verification",,,,"","",,5,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85090640622
"Iyer A., Jonnalagedda M., Parthasarathy S., Radhakrishna A., Rajamani S.K.","57209514733;49963940200;57192061291;36188746400;7004184325;","Synthesis and machine learning for heterogeneous extraction",2019,"Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation (PLDI)",,,,"301","315",,5,"10.1145/3314221.3322485","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067684348&doi=10.1145%2f3314221.3322485&partnerID=40&md5=92f32b1750b4cd877b6b0effe3203e17","We present a way to combine techniques from the program synthesis and machine learning communities to extract structured information from heterogeneous data. Such problems arise in several situations such as extracting attributes from web pages, machine-generated emails, or from data obtained from multiple sources. Our goal is to extract a set of structured attributes from such data. We use machine learning models (“ML models”) such as conditional random fields to get an initial labeling of potential attribute values. However, such models are typically not interpretable, and the noise produced by such models is hard to manage or debug. We use (noisy) labels produced by such ML models as inputs to program synthesis, and generate interpretable programs that cover the input space. We also employ type specifications (called “field constraints”) to certify well-formedness of extracted values. Using synthesized programs and field constraints, we re-train the ML models with improved confidence on the labels. We then use these improved labels to re-synthesize a better set of programs. We iterate the process of re-synthesizing the programs and re-training the ML models, and find that such an iterative process improves the quality of the extraction process. This iterative approach, called HDEF, is novel, not only the in way it combines the ML models with program synthesis, but also in the way it adapts program synthesis to deal with noise and heterogeneity. More broadly, our approach points to ways by which machine learning and programming language techniques can be combined to get the best of both worlds - handling noise, transferring signals from one context to another using ML, producing interpretable programs using PL, and minimizing user intervention. © 2019 Copyright held by the owner/author(s). Publication rights licensed to ACM.","Data extraction; Heterogeneous data; Machine Learning; Program synthesis","Ada (programming language); Extraction; Iterative methods; Learning systems; Machine learning; Websites; Conditional random field; Data extraction; Heterogeneous data; Iterative approach; Machine learning communities; Machine learning models; Program synthesis; Structured information; Data mining","ACM SIGPLAN","40th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI 2019","22 June 2019 through 26 June 2019",,148581,Conference Paper,"Final","",Scopus,2-s2.0-85067684348
"Uban A.-S., Dinu L.P.","57003736200;6602512834;","On Transfer Learning for Detecting Abusive Language Online",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11506 LNCS",,,"688","700",,5,"10.1007/978-3-030-20521-8_57","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85067445781&doi=10.1007%2f978-3-030-20521-8_57&partnerID=40&md5=3b02dd3d9f942f3e121cd57451368878","Abusive language online has become a growing social issue in our age of social media. Given the massive amounts of data being generated daily on social platforms, manually detecting and regulating such behavior has become unfeasible, so automatic solutions are necessary, and tasks related to identifying abusive language, in its various forms, from hate speech to bullying, have come into the focus of the natural language processing research community. In this paper, we focus on two subtypes of abusive language: aggressive language and offensive language, for which we implement a deep learning model based on convolutional neural networks. We further propose a new approach using transfer learning to boost performance of abusive language detection by leveraging data annotated with a different type of label, related to sentiment. We show how transferring knowledge between these tasks affects performance of detecting abusive language, offering insights into how these tasks are related, and how the more traditional task of sentiment analysis can be leveraged to help with solving the newer and less data rich task of abusive language detection. © 2019, Springer Nature Switzerland AG.","Abusive language; Convolutional neural network; Deep learning; Sentiment analysis; Transfer learning; Tweet","Convolution; Data mining; E-learning; Knowledge management; Neural networks; Sentiment analysis; Abusive language; Convolutional neural network; Language detection; NAtural language processing; Offensive languages; Research communities; Transfer learning; Tweet; Deep learning",,"15th International Work-Conference on Artificial Neural Networks, IWANN 2019","12 June 2019 through 14 June 2019",,226879,Conference Paper,"Final","",Scopus,2-s2.0-85067445781
"Zhong Y.","","Mechanism-based artificial intelligence theory",2018,"CAAI Trans. Intell. Syst.","13","1",,"2","18",,5,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85056467338
"Luken Jackson, Jiang Nanjiang, de Marneffe Marie-Catherine","","QED: A fact verification system for the FEVER shared task",2018,"Proceedings of the First Workshop on Fact Extraction and VERification (FEVER)",,,,"156","160",,5,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85084052217
"Peng H., Li J., Gong Q., Wang S., Ning Y., Yu P.S.","",[No title available],2018,"Graph Convolutional Neural Networks Via Motif-Based Attention",,,,"","",,5,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85074953973
"Kuta M., Morawiec M., Kitowski J.","6506508740;57195536815;8963078200;","Sentiment analysis with tree-structured gated recurrent units",2017,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","10415 LNAI",,,"74","82",,5,"10.1007/978-3-319-64206-2_9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85028673833&doi=10.1007%2f978-3-319-64206-2_9&partnerID=40&md5=bb1aa6380b2720c890b74e23fe80c74b","Advances in neural network models and deep learning mark great impact on sentiment analysis, where models based on recursive or convolutional neural networks show state-of-the-art results leaving behind non-neural models like SVM or traditional lexicon-based approaches. We present Tree-Structured Gated Recurrent Unit network, which exhibits greater simplicity in comparison to the current state of the art in sentiment analysis, Tree-Structured LSTM model. © Springer International Publishing AG 2017.","Gated Recurrent Unit; Long Short-Term Memory; Recursive neural network; Sentiment analysis; Tree-Structured GRU","Forestry; Long short-term memory; Neural networks; Convolutional neural network; Gated Recurrent Unit; Neural models; Neural network model; Recursive neural networks; Sentiment analysis; State of the art; Tree-structured; Data mining","","20th International Conference on Text, Speech and Dialogue, TSD 2017","27 August 2017 through 31 August 2017",,196699,Conference Paper,"Final","",Scopus,2-s2.0-85028673833
"Chelba C., Mikolov T., Schuster M., Ge Q., Brants T., Koehn P.","","One billion word benchmark for measuring progress in statistical language modeling",2013,"INTERSPEECH",,,,"","",,5,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85076501349
"Rifon L.E.A., Rodriguez A.C., Roris V.M.A., Gago J.M.S., Iglesias M.J.F.","25927482900;55821743600;54082682400;35408895800;6602607569;","A recommender system for educational resources in specific learning contexts",2013,"Proceedings of the 8th International Conference on Computer Science and Education, ICCSE 2013",,,"6553940","371","376",,5,"10.1109/ICCSE.2013.6553940","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84881495793&doi=10.1109%2fICCSE.2013.6553940&partnerID=40&md5=c5c8a246d762209cea5d9f3e639dc722","This paper presents the algorithms that drive the behaviour of a recommendation system for educational resources in specific learning contexts. The criteria taken into account to provide these recommendations is not based on the preferences or previous behaviour of an individual (personalization). Instead of that, we need to consider the particular characteristics of a learning context: subject, language, tools available, age range, etc. We present three different multi-criteria recommendation algorithms depending upon the type of resource to be recommended: tools, events and contributors (experts, parents and other external potential contributors to a learning activity). These algorithms have been designed as a join effort between technical and pedagogical experts within a large European project: innovative Technologies for an Engaging Classroom (iTEC). © 2013 IEEE.","Learning resources; Multi-criteria analysis; Recommender systems; Semantic technologies","Educational resource; External potential; Innovative technology; Learning resource; Multi Criteria Analysis; Pedagogical experts; Recommendation algorithms; Semantic technologies; Computer science; Education computing; Learning algorithms; Recommender systems; Semantics; Teaching; Tools; Engineering education",,"8th International Conference on Computer Science and Education, ICCSE 2013","26 August 2013 through 28 August 2013","Colombo",98480,Conference Paper,"Final","",Scopus,2-s2.0-84881495793
"Komendantskaya E.","19640466600;","Unification neural networks: Unification by error-correction learning",2011,"Logic Journal of the IGPL","19","6","jzq012","821","847",,5,"10.1093/jigpal/jzq012","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054760691&doi=10.1093%2fjigpal%2fjzq012&partnerID=40&md5=08ae75df457b533b9d4e06c93b32c573","We show that the conventional first-order algorithm of unification can be simulated by finite artificial neural networks with one layer of neurons. In these unification neural networks, the unification algorithm is performed by error-correction learning. Each time-step of adaptation of the network corresponds to a single iteration of the unification algorithm. We present this result together with the library of learning functions and examples fully formalised in MATLAB Neural Network Toolbox. © The Author 2010. Published by Oxford University Press. All rights reserved.","Connectionism; Error-correction Learning; Hybrid networks; Neural network learning; Neuro-symbolic networks; Unification",,,,,,,Article,"Final","",Scopus,2-s2.0-80054760691
"Floridi L.","","Philosophical conceptions of information",2009,"Formal Theories of Information",,,,"","",,5,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-80053195602
"Hasler L.","","From extracts to abstracts: human summary production operations for computer-aided summarisation",2007,"From Extracts to Abstracts: Human Summary Production Operations for Computer-aided Summarisation",,,,"","",,5,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84887161526
"Lavrač N., Škrlj B., Robnik-Šikonja M.","7004388979;57191625180;55900495300;","Propositionalization and embeddings: two sides of the same coin",2020,"Machine Learning","109","7",,"1465","1507",,4,"10.1007/s10994-020-05890-8","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087028636&doi=10.1007%2fs10994-020-05890-8&partnerID=40&md5=3a7b38489dcaac3e5b5b7fff571226bb","Data preprocessing is an important component of machine learning pipelines, which requires ample time and resources. An integral part of preprocessing is data transformation into the format required by a given learning algorithm. This paper outlines some of the modern data processing techniques used in relational learning that enable data fusion from different input data types and formats into a single table data representation, focusing on the propositionalization and embedding data transformation approaches. While both approaches aim at transforming data into tabular data format, they use different terminology and task definitions, are perceived to address different goals, and are used in different contexts. This paper contributes a unifying framework that allows for improved understanding of these two data transformation techniques by presenting their unified definitions, and by explaining the similarities and differences between the two approaches as variants of a unified complex data transformation task. In addition to the unifying framework, the novelty of this paper is a unifying methodology combining propositionalization and embeddings, which benefits from the advantages of both in solving complex data transformation and learning tasks. We present two efficient implementations of the unifying methodology: an instance-based PropDRM approach, and a feature-based PropStar approach to data transformation and learning, together with their empirical evaluation on several relational problems. The results show that the new algorithms can outperform existing relational learners and can solve much larger problems. © 2020, The Author(s).","Embeddings; Inductive logic programming; Knowledge graphs; Propositionalization; Relational learning","Data fusion; Data handling; Embeddings; Learning algorithms; Machine learning; Data preprocessing; Data processing techniques; Data representations; Data transformation; Efficient implementation; Empirical evaluations; Propositionalization; Relational learning; Metadata",,,,,,Article,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85087028636
"Raza M., Gulwani S.","56394011400;55901318200;","Web Data Extraction using Hybrid Program Synthesis: A Combination of Top-down and Bottom-up Inference",2020,"Proceedings of the ACM SIGMOD International Conference on Management of Data",,,,"1967","1978",,4,"10.1145/3318464.3380608","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086271456&doi=10.1145%2f3318464.3380608&partnerID=40&md5=466ca7c3ee35a05c47b021228f672ef7","Automatic synthesis of web data extraction programs has been explored in a variety of settings, but in practice there remain various robustness and usability challenges. In this work we present a novel program synthesis approach which combines the benefits of deductive and enumerative synthesis strategies, yielding a semi-supervised technique with which concise programs expressible in standard languages can be synthesized from very few examples. We demonstrate improvement over existing techniques in terms of overall accuracy, number of examples required, and program complexity. Our method has been deployed as a web extraction feature in the mass market Microsoft Power BI product. © 2020 Association for Computing Machinery.","program synthesis; web data extraction; wrapper induction","Extraction; Automatic synthesis; Overall accuracies; Program complexity; Program synthesis; Semi-supervised; Synthesis strategy; Web data extraction; Web extraction; Data mining","ACM SIGMOD","2020 ACM SIGMOD International Conference on Management of Data, SIGMOD 2020","14 June 2020 through 19 June 2020",,160493,Conference Paper,"Final","All Open Access, Bronze",Scopus,2-s2.0-85086271456
"Es-Sabery F., Hair A.","57216687150;6603926985;","Big Data Solutions Proposed for Cluster Computing Systems Challenges: A survey",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"","",,4,"10.1145/3386723.3387826","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85117539501&doi=10.1145%2f3386723.3387826&partnerID=40&md5=09c7e48cdfcd55fc9eb7f596b8bbfd2e","CCS (Cluster Computing System) is coming to solve the problems of standard technology. Whose, objective is to improve the performance/power efficiency of a single processor for storing and mining the large data sets, using the parallel programming to read and process the massive data sets on multiple disks and CPUs. The thing which makes these systems somewhat performant than the standard technology is the physical organization of computing nodes in the cluster. Currently, this kind of cluster does not entirely solve the problem because it comes with its challenges, which are Node failures, Computations, Network Bottleneck, and Distributed programming. All these problems are coming when we are mining and storing the massive volume of data using cluster computing. To solve these challenges, Google invented a new Big Data framework of data processing called MapReduce, to manage large scale data processing across large clusters of commodity servers. The paper outlines the running of CCS and presents its challenges in this era of Big Data. Moreover, it introduces the most popular Big Data solutions proposed to overcome the CCS challenges. Also, it shows how Big Data technologies solve CCS issues. Generally, the main goal of this work is to provide a better understanding of the challenges of CCS and identify the essential big data solutions in this increasingly important area. © 2020 ACM.","Big Data; CCS; Challenges; Distributed File System; MapReduce","Big data; Computer architecture; Data handling; Information systems; Information use; Parallel programming; Program processors; Cluster computing system; Data technologies; Distributed programming; Large-scale data processing; Massive data sets; Network bottlenecks; Single processors; Standard technology; Cluster computing",,"3rd International Conference on Networking, Information Systems and Security, NISS 2020","31 March 2020 through 2 April 2020",,160124,Conference Paper,"Final","",Scopus,2-s2.0-85117539501
"Anderson G., Verma A., Dillig I., Chaudhuri S.","57203282495;57204815118;22936636100;8727948900;","Neurosymbolic reinforcement learning with formally verified exploration",2020,"Advances in Neural Information Processing Systems","2020-December",,,"","",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106121315&partnerID=40&md5=e33881b92533cf315ae1c923fca93742","We present REVEL, a partially neural reinforcement learning (RL) framework for provably safe exploration in continuous state and action spaces. A key challenge for provably safe deep RL is that repeatedly verifying neural networks within a learning loop is computationally infeasible. We address this challenge using two policy classes: a general, neurosymbolic class with approximate gradients and a more restricted class of symbolic policies that allows efficient verification. Our learning algorithm is a mirror descent over policies: in each iteration, it safely lifts a symbolic policy into the neurosymbolic space, performs safe gradient updates to the resulting policy, and projects the updated policy into the safe symbolic subset, all without requiring explicit verification of neural networks. Our empirical results show that REVEL enforces safe exploration in many scenarios in which Constrained Policy Optimization does not, and that it can discover policies that outperform those learned through prior approaches to verified exploration. © 2020 Neural information processing systems foundation. All rights reserved.",,"Constrained optimization; Iterative methods; Neural networks; Reinforcement learning; Action spaces; Approximate gradient; Continuous state; Policy optimization; Learning algorithms","Apple;et al.;Microsoft;PDT Partners;Sony;Tenstorrent","34th Conference on Neural Information Processing Systems, NeurIPS 2020","6 December 2020 through 12 December 2020",,169463,Conference Paper,"Final","",Scopus,2-s2.0-85106121315
"Qin Y., Qi F., Ouyang S., Liu Z., Yang C., Wang Y., Liu Q., Sun M.","57218918509;57215717685;57218919984;57191691341;57001472900;57211255040;56181387900;7403180987;","Improving Sequence Modeling Ability of Recurrent Neural Networks via Sememes",2020,"IEEE/ACM Transactions on Audio Speech and Language Processing","28",,"9149672","2364","2373",,4,"10.1109/TASLP.2020.3012060","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090764179&doi=10.1109%2fTASLP.2020.3012060&partnerID=40&md5=cf7c9430eb7a6d972b9a50fa0835df74","Sememes, the minimum semantic units of human languages, have been successfully utilized in various natural language processing applications. However, most existing studies exploit sememes in specific tasks and few efforts are made to utilize sememes more fundamentally. In this paper, we propose to incorporate sememes into recurrent neural networks (RNNs) to improve their sequence modeling ability, which is beneficial to all kinds of downstream tasks. We design three different sememe incorporation methods and employ them in typical RNNs including LSTM, GRU and their bidirectional variants. In evaluation, we use several benchmark datasets involving PTB and WikiText-2 for language modeling, SNLI for natural language inference and another two datasets for sentiment analysis and paraphrase detection. Experimental results show evident and consistent improvement of our sememe-incorporated models compared with vanilla RNNs, which proves the effectiveness of our sememe incorporation methods. Moreover, we find the sememe-incorporated models have higher robustness and outperform adversarial training in defending adversarial attack. All the code and data of this work can be obtained at https://github.com/thunlp/SememeRNN. © 2014 IEEE.","recurrent neural network; Sememe","Modeling languages; Semantics; Sentiment analysis; Benchmark datasets; Human language; Incorporation method; Language model; Natural language processing applications; Natural languages; Recurrent neural network (RNNs); Sequence modeling; Long short-term memory",,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85090764179
"Du N., Wang M., Tran L., Li G., Shafran I.","55697607100;57216689340;56231034600;57216696118;13907300200;","Learning to infer entities, properties and their relations from clinical conversations",2020,"EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference",,,,"4979","4990",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084313347&partnerID=40&md5=8684045bd2c48cd290b967a8f6952b43","Recently we proposed the Span Attribute Tagging (SAT) Model (Du et al., 2019) to infer clinical entities (e.g., symptoms) and their properties (e.g., duration). It tackles the challenge of large label space and limited training data using a hierarchical two-stage approach that identifies the span of interest in a tagging step and assigns labels to the span in a classification step. We extend the SAT model to jointly infer not only entities and their properties but also relations between them. Most relation extraction models restrict inferring relations between tokens within a few neighboring sentences, mainly to avoid high computational complexity. In contrast, our proposed Relation-SAT (R-SAT) model is computationally efficient and can infer relations over the entire conversation, spanning an average duration of 10 minutes. We evaluate our model on a corpus of clinical conversations. When the entities are given, the R-SAT outperforms baselines in identifying relations between symptoms and their properties by about 32% (0.82 vs 0.62 F-score) and by about 50% (0.60 vs 0.41 F-score) on medications and their properties. On the more difficult task of jointly inferring entities and relations, the R-SAT model achieves a performance of 0.34 and 0.45 for symptoms and medications respectively, which is significantly better than 0.18 and 0.35 for the baseline model. The contributions of different components of the model are quantified using ablation analysis. © 2019 Association for Computational Linguistics",,"Classification (of information); 10 minutes; Baseline models; Computationally efficient; Label space; Limited training data; Relation extraction; SAT model; Two stage approach; Natural language processing systems","Apple;ASAPP;et al.;Facebook;Google;salesforce","2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019","3 November 2019 through 7 November 2019",,159367,Conference Paper,"Final","",Scopus,2-s2.0-85084313347
"Singh S.N., Sarraf T.","56434702500;57216592340;","Sentiment analysis of a product based on user reviews using random forests algorithm",2020,"Proceedings of the Confluence 2020 - 10th International Conference on Cloud Computing, Data Science and Engineering",,,"9058128","112","116",,4,"10.1109/Confluence47617.2020.9058128","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85083991802&doi=10.1109%2fConfluence47617.2020.9058128&partnerID=40&md5=1f723e7b352f4568769e000d254ff167","After many sentiment analysis as well as many types of methods classify the reviews that is based on test data and reviewer's ratings which uses training., after reading reviews it is seen that star rating of reviewer do not always give a precise measure of his sentiment. This paper primarily focuses on analyzing customer reviews from the e-commerce space. Upon surveying popular e-commerce websites it can be observed that in several instances the product rating given by a customer is not consistent with the product review written by him/her. The problem is made complex by the fact that there is no standard scale to measure the rating that the user gives and the rating of the product are instinctive to the customers' view. In several cases it is seen that a product is rated 4 out of 5. However, the reviews detail that the customer's experience with the product is not favourable. Indeed, text reviews are a true picture of the product. To get rid of this problem, the stated system will give a boolean result i.e. whether the product is good or bad and the user does not need to read all the reviews to analyze the product. © 2020 IEEE.","Bag-of-words; Product reviews; Random forest classifier; Sentiment analysis","Cloud computing; Decision trees; Electronic commerce; Random forests; Sentiment analysis; Customer review; E-commerce websites; Product ratings; Product reviews; Standard scale; Star ratings; Test data; User reviews; Sales","IEEE","10th International Conference on Cloud Computing, Data Science and Engineering, Confluence 2020","29 January 2020 through 31 January 2020",,159162,Conference Paper,"Final","",Scopus,2-s2.0-85083991802
"Lakshmi Devi B., Varaswathi Bai V., Ramasubbareddy S., Govinda K.","57215004472;57214988713;57201195237;36570310500;","Sentiment analysis on movie reviews",2020,"Advances in Intelligent Systems and Computing","1054",,,"321","328",,4,"10.1007/978-981-15-0135-7_31","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85079664314&doi=10.1007%2f978-981-15-0135-7_31&partnerID=40&md5=e910a85053111c388f3fb24cc4ddd9e6","Movie reviews help users decide if the movie is worth their time. A summary of all reviews for a movie can help users make this decision by not wasting their time reading all reviews. Movie-rating websites are often used by critics to post comments and rate movies which help viewers decide if the movie is worth watching. Sentiment analysis can determine the attitude of critics depending on their reviews. Sentiment analysis of a movie review can rate how positive or negative a movie review is and hence the overall rating for a movie. Therefore, the process of understanding if a review is positive or negative can be automated as the machine learns through training and testing the data. This project aims to rate reviews using two classifiers and compare which gives better and more accurate results. Classification is a data mining methodology that assigns classes to a collection of data in order to help in more accurate predictions and analysis. Naïve Bayes and decision tree classifications will be used and the results of sentiment analysis compared. © Springer Nature Singapore Pte Ltd 2020.","Decision tree; Movie reviews; Naive Bayes; Prediction; SLIQ","Data mining; Decision trees; Forecasting; Sentiment analysis; Accurate prediction; Decision tree classification; Movie ratings; Movie reviews; Naive bayes; SLIQ; Training and testing; Motion pictures",,"2nd International Conference on Computing, Communications and Data Engineering, CCODE 2019","1 February 2020 through 2 February 2020",,237309,Conference Paper,"Final","",Scopus,2-s2.0-85079664314
"Liu Y., Peng H., Li J., Song Y., Li X.","","Event detection and evolution in multi-lingual social streams",2019,"Frontiers of Computer Science",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85074940755
"Kamath Aishwarya, Das Rajarshi","","A survey on semantic parsing",2019,"AKBC'19",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85093195060
"Smart P., Jones C., Twaroch F.","","Multi-source toponym data integration and mediation for a meta-gazetteer service",2019,"Lecture Notes in Computer Science",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84855408290
"Unxos GmbH.","",[No title available],2019,"GeoNames",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85084488219
"Es-sabery F., Hair A.","57216687150;6603926985;","A MapReduce C4.5 Decision Tree Algorithm Based on Fuzzy Rule-Based System",2019,"Fuzzy Information and Engineering","11","4",,"446","473",,4,"10.1080/16168658.2020.1756099","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85087368253&doi=10.1080%2f16168658.2020.1756099&partnerID=40&md5=d3566a1ea00b0a95d17d9a875a06cb53","Decision tree is the most efficient and fast technology of data mining that is frequently used in data analysis and prediction. According to the development in science and technology in the last years, the data is growing faster, and the principle of the decision tree algorithms become not efficient in respect runtime and speed-up ratio. In view of the above problem, we propose a new method of classification based on framework Hadoop and Fuzzy logic. Our proposed hybrid approach is designed to propose a new C4.5 decision tree algorithm using fuzzy logic and fuzzy set theory to handle uncertainty and imprecision in data, and Hadoop framework (MapReduce + HDFS) to parallelize our work. This combination of big data technologies, fuzzy systems and C4.5 decision tree algorithm has produced a parallel fuzzy decision tree model, which takes advantage of these three techniques (hadoop + fuzzy logic + C4.5) to produce a decision tree with higher predictive accuracy. In this paper, an experiment is presented to compare our approach with other approaches from the literature. Experiments were carried out using three datasets, and the results show that our new method outperforms the other approaches in terms of accuracy and execution time. © 2020 The Author(s). Published by Informa UK Limited, trading as Taylor & Francis Group.","BigData; decision tree; fuzzy logic; Hadoop; HDFS; MapReduce",,,,,,,Article,"Final","All Open Access, Gold",Scopus,2-s2.0-85087368253
"Huang Q., Deng L., Wu D., Liu C., He X.","55233543400;36071490500;12807474900;55873082700;37085932700;","Attentive tensor product learning",2019,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",,,,"1344","1351",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85080593464&partnerID=40&md5=1476fefc849931de96f874e88fa203a9","This paper proposes a novel neural architecture - Attentive Tensor Product Learning (ATPL) - to represent grammatical structures of natural language in deep learning models. ATPL exploits Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, to integrate deep learning with explicit natural language structures and rules. The key ideas of ATPL are: 1) unsupervised learning of role-unbinding vectors of words via the TPR-based deep neural network; 2) the use of attention modules to compute TPR; and 3) the integration of TPR with typical deep learning architectures including long short-term memory and feedforward neural networks. The novelty of our approach lies in its ability to extract the grammatical structure of a sentence by using role-unbinding vectors, which are obtained in an unsupervised manner. Our ATPL approach is applied to 1) image captioning, 2) part of speech (POS) tagging, and 3) constituency parsing of a natural language sentence. The experimental results demonstrate the effectiveness of the proposed approach in all these three natural language processing tasks. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Deep neural networks; Feedforward neural networks; Learning systems; Natural language processing systems; Network architecture; Syntactics; Tensors; Cognitive science; Grammatical structure; Image captioning; Learning architectures; NAtural language processing; Natural languages; Neural architectures; Part of speech tagging; Deep learning","Association for the Advancement of Artificial Intelligence","33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019","27 January 2019 through 1 February 2019",,160302,Conference Paper,"Final","",Scopus,2-s2.0-85080593464
"Hu J., Guo C., Yang B., Jensen C.S., Chen L.","",[No title available],2018,"Recurrent Multi-Graph Neural Networks for Travel Cost Prediction",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075467351
"Shen D., Zhang X., Henao R., Carin L.","",[No title available],2018,"Improved Semantic-Aware Network Embedding with Fine-Grained Word Alignment",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85064846564
"Jiang J., Xie J., Zhao C., Su J., Guan Y., Yu Q.","56376858400;57200535026;57189217850;57189214933;7202924009;56359402300;","Max-margin weight learning for medical knowledge network",2018,"Computer Methods and Programs in Biomedicine","156",,,"179","190",,4,"10.1016/j.cmpb.2018.01.005","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85041550100&doi=10.1016%2fj.cmpb.2018.01.005&partnerID=40&md5=b51dda43144e3194deeef2a8e1241012","Background and objective: The application of medical knowledge strongly affects the performance of intelligent diagnosis, and method of learning the weights of medical knowledge plays a substantial role in probabilistic graphical models (PGMs). The purpose of this study is to investigate a discriminative weight-learning method based on a medical knowledge network (MKN). Methods: We propose a training model called the maximum margin medical knowledge network (M3KN), which is strictly derived for calculating the weight of medical knowledge. Using the definition of a reasonable margin, the weight learning can be transformed into a margin optimization problem. To solve the optimization problem, we adopt a sequential minimal optimization (SMO) algorithm and the clique property of a Markov network. Ultimately, M3KN not only incorporates the inference ability of PGMs but also deals with high-dimensional logic knowledge. Results: The experimental results indicate that M3KN obtains a higher F-measure score than the maximum likelihood learning algorithm of MKN for both Chinese Electronic Medical Records (CEMRs) and Blood Examination Records (BERs). Furthermore, the proposed approach is obviously superior to some classical machine learning algorithms for medical diagnosis. To adequately manifest the importance of domain knowledge, we numerically verify that the diagnostic accuracy of M3KN is gradually improved as the number of learned CEMRs increase, which contain important medical knowledge. Conclusions: Our experimental results show that the proposed method performs reliably for learning the weights of medical knowledge. M3KN outperforms other existing methods by achieving an F-measure of 0.731 for CEMRs and 0.4538 for BERs. This further illustrates that M3KN can facilitate the investigations of intelligent healthcare. © 2018 Elsevier B.V.","Electronic medical records; Markov logic network; Medical knowledge network; Weight learning","Computer circuits; Diagnosis; Hospital data processing; Learning systems; Markov processes; Maximum likelihood; Medical computing; Optimization; Speech recognition; Electronic medical record; Intelligent diagnosis; Markov logic networks; Maximum likelihood learning; Medical knowledge; Probabilistic graphical models; Sequential minimal optimization algorithms; Weight learning; Learning algorithms; article; blood examination; diagnosis; diagnostic accuracy; diagnostic test accuracy study; electronic medical record; human; human tissue; learning algorithm; logic; maximum likelihood method; algorithm; artificial neural network; China; computer assisted diagnosis; computer graphics; electronic health record; machine learning; Markov chain; procedures; reproducibility; signal processing; statistical model; theoretical model; Algorithms; China; Computer Graphics; Diagnosis, Computer-Assisted; Electronic Health Records; Humans; Likelihood Functions; Machine Learning; Markov Chains; Models, Statistical; Models, Theoretical; Neural Networks (Computer); Reproducibility of Results; Signal Processing, Computer-Assisted",,,,,,Article,"Final","",Scopus,2-s2.0-85041550100
"Yin W., Yaghoobzadeh Y., Schütze H.","","Recurrent one-hop predictions for reasoning over knowledge graphs",2018,"Proceedings of the 27th International Conference on Computational Linguistics, COLING",,,,"2369","2378",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85079053011
"Biltawi M., Etaiwi W., Tedmori S., Shaout A.","56181974300;26531155200;26768244100;7003399563;","Fuzzy based sentiment classification in the Arabic language",2018,"Advances in Intelligent Systems and Computing","868",,,"579","591",,4,"10.1007/978-3-030-01054-6_42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85057102916&doi=10.1007%2f978-3-030-01054-6_42&partnerID=40&md5=4e31da1acd04c72c7cbdf6da8897f3bb","Sentiment Analysis is the task of identifying individuals’ positive and negative opinions, emotions and evaluations concerning a specific object. Fuzzy logic in the field of sentiment analysis can be employed to classify the polarity of sentences or documents. Although some efforts have been made by researchers who applied fuzzy logic for Sentiment Analysis on English texts, to the best of the authors’ knowledge, no efforts have been made targeting Arabic texts. This paper proposes a lexicon based approach to extract sentiment polarity from Arabic text using a fuzzy logic approach. The proposed approach consists of two main phases. In the first phase, Arabic text is assigned weights, while in the second phase fuzzy logic is employed to assign the polarity to the inputted sentence. Experiments were conducted on Large Scale Arabic Book Reviews Dataset (LABR), and the results showed 94.87%, 84.04%, 80.59% and 89.13% for recall, precision, accuracy, and F1-measure, respectively. © Springer Nature Switzerland AG 2019.","Arabic fuzzy logic; Arabic natural language processing; Arabic sentiment analysis; Linguistic variables","Classification (of information); Computer circuits; Intelligent systems; Large dataset; Reviews; Sentiment analysis; Arabic languages; Arabic natural language processing; Book reviews; Fuzzy logic approach; Lexicon-based; Linguistic variable; Second phase; Sentiment classification; Fuzzy logic",,"Intelligent Systems Conference, IntelliSys 2018","6 September 2018 through 7 September 2018",,220949,Conference Paper,"Final","",Scopus,2-s2.0-85057102916
"Hupkes D., Zuidema W.","57192945585;6603324777;","Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure",2018,"IJCAI International Joint Conference on Artificial Intelligence","2018-July",,,"5617","5621",,4,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85055686445&partnerID=40&md5=de437c821677707e0536e44752e78888","In this paper, we investigate how recurrent neural networks can learn and process languages with hierarchical, compositional semantics. To this end, we define the artificial task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. We find that simple recurrent networks cannot find a gen-eralising solution to this task, but gated recurrent neural networks perform surprisingly well: networks learn to predict the outcome of the arithmetic expressions with high accuracy, although performance deteriorates somewhat with increasing length. We test multiple hypotheses on the information that is encoded and processed by the networks using a method called diagnostic classification. In this method, simple neural classifiers are used to test sequences of predictions about features of the hidden state representations at each time step. Our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. This, in turn, shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. © 2018 International Joint Conferences on Artificial Intelligence.All right reserved.",,"Classification (of information); Deterioration; Semantics; Arithmetic expression; Compositional semantics; Hierarchical structures; Multiple hypothesis; Neural classifiers; Process languages; Recursive neural networks; Simple recurrent networks; Recurrent neural networks","International Joint Conferences on Artifical Intelligence (IJCAI)","27th International Joint Conference on Artificial Intelligence, IJCAI 2018","13 July 2018 through 19 July 2018",,140653,Conference Paper,"Final","",Scopus,2-s2.0-85055686445
"Sequiera R., Baruah G., Tu Z., Mohammed S., Rao J., Zhang H., Lin J.","",[No title available],2017,"Exploring The Effectiveness of Convolutional Neural Networks for Answer Selection in End-to-End Question Answering",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85051506809
"Tamari T.","15137509500;","The Phenomenology of Architecture: A Short Introduction to Juhani Pallasmaa",2017,"Body and Society","23","1",,"91","95",,4,"10.1177/1357034X16676540","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85012186683&doi=10.1177%2f1357034X16676540&partnerID=40&md5=cac41df4ea0a2cf0f9cff7daecadf86b","This piece focuses on the work of Juhani Pallasmaa who introduces phenomenological aspects of kinesthetic and multisensory perception of the human body into architecture theory. He argues that hand-drawing is a vital spatial and haptic exercise in facilitating architectural design. Through this process, architecture can emerge as the very ‘material’ existence of human embodied ‘immaterial’ emotion, feelings and wisdom. Hence, for Pallasmaa, architecture can be seen as an artistic practice, which entails multisensory and embodied thought in order to establish the sense of being in the world. © 2017, © The Author(s) 2017.","architecture; embodiment; Juhani Pallasmaa; kinesthetic; phenomenology; senses",,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85012186683
"Weissenborn Dirk","",[No title available],2016,"Separating Answers from Queries for Neural Reading Comprehension",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85071037411
"Barman S., Chasins S., Bodik R., Gulwani S.","35733236000;54395086400;6701821028;55901318200;","Ringer: web automation by demonstration",2016,"ACM SIGPLAN Notices","51","10",,"748","764",,4,"10.1145/2983990.2984020","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084556461&doi=10.1145%2f2983990.2984020&partnerID=40&md5=cd1c1cc1110e186a88cd155fd111615f","With increasing amounts of data available on the web and a diverse range of users interested in programmatically accessing that data, web automation must become easier. Automation helps users complete many tedious interactions, such as scraping data, completing forms, or transferring data between websites. However, writing web automation scripts typically requires an expert programmer because the writer must be able to reverse engineer the target webpage. We have built a record and replay tool, Ringer, that makes web automation accessible to non-coders. Ringer takes a user demonstration as input and creates a script that interacts with the page as a user would. This approach makes Ringer scripts more robust to webpage changes because user-facing interfaces remain relatively stable compared to the underlying webpage implementations. We evaluated our approach on benchmarks recorded on real webpages and found that it replayed 4x more benchmarks than a state-of-the-art replay tool. © 2016 ACM.","Automation; Browser; Javascript; Record-Replay","User interfaces; Websites; Diverse range; Expert programmers; Record-and-replay; State of the art; Web automation; Automation",,,,,,Article,"Final","",Scopus,2-s2.0-85084556461
"Wang X., Gulwani S., Singh R.","57026634100;55901318200;55496964900;","FIDEX: filtering spreadsheet data using examples",2016,"ACM SIGPLAN Notices","51","10",,"195","213",,4,"10.1145/2983990.2984030","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084536686&doi=10.1145%2f2983990.2984030&partnerID=40&md5=cdd8853d6b3517677205b0cc2ccbff3d","Data filtering in spreadsheets is a common problem faced by millions of end-users. The task of data filtering requires a computational model that can separate intended positive and negative string instances. We present a system, FIDEX, that can efficiently learn desired data filtering expressions from a small set of positive and negative string examples. There are two key ideas of our approach. First, we design an expressive DSL to represent disjunctive filter expressions needed for several real-world data filtering tasks. Second, we develop an efficient synthesis algorithm for incrementally learning consistent filter expressions in the DSL from very few positive and negative examples. A DAG-based data structure is used to succinctly represent a large number of filter expressions, and two corresponding operators are defined for algorithmically handling positive and negative examples, namely, the intersection and subtraction operators. FIDEX is able to learn data filters for 452 out of 460 real-world data filtering tasks in real time (0.22s), using only 2.2 positive string instances and 2.7 negative string instances on average. © 2016 ACM.","Data Filtering; Program Synthesis; Programming By Examples; Regular Expressions","Digital subscriber lines; Computational model; Data filter; Data filtering; Desired datum; Efficient synthesis; End users; Negative examples; Real-world; Spreadsheets",,,,,,Article,"Final","",Scopus,2-s2.0-85084536686
"Spithourakis G.P., Augenstein I., Riedel S.","55800661200;55236320300;36562438800;","Numerically grounded language models for semantic error correction",2016,"EMNLP 2016 - Conference on Empirical Methods in Natural Language Processing, Proceedings",,,,"987","992",,4,"10.18653/v1/d16-1101","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85063098121&doi=10.18653%2fv1%2fd16-1101&partnerID=40&md5=341562cd2256b58671f1b8bf3819e66a","Semantic error detection and correction is an important task for applications such as fact checking, speech-to-text or grammatical error correction. Current approaches generally focus on relatively shallow semantics and do not account for numeric quantities. Our approach uses language models grounded in numbers within the text. Such groundings are easily achieved for recurrent neural language model architectures, which can be further conditioned on incomplete background knowledge bases. Our evaluation on clinical reports shows that numerical grounding improves perplexity by 33% and F1 for semantic error correction by 5 points when compared to ungrounded approaches. Conditioning on a knowledge base yields further improvements. © 2016 Association for Computational Linguistics",,"Computational linguistics; Error correction; Knowledge based systems; Semantics; Back-ground knowledge; Grammatical errors; Knowledge base; Language model; Semantic errors; Natural language processing systems","Amazon.com;Baidu;et al.;Google;Grammarly;Microsoft","2016 Conference on Empirical Methods in Natural Language Processing, EMNLP 2016","1 November 2016 through 5 November 2016",,150070,Conference Paper,"Final","All Open Access, Hybrid Gold, Green",Scopus,2-s2.0-85063098121
"Furbach U., Schon C.","6603357096;53881954300;","Commonsense reasoning meets theorem proving",2016,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","9872 LNAI",,,"3","17",,4,"10.1007/978-3-319-45889-2_1","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84988672424&doi=10.1007%2f978-3-319-45889-2_1&partnerID=40&md5=d5a595d717ed4b50bc75ed46251fbc0f","The area of commonsense reasoning aims at the creation of systems able to simulate the human way of rational thinking. This paper describes the use of automated reasoning methods for tackling commonsense reasoning benchmarks. For this we use a benchmark suite introduced in literature. Our goal is to use general purpose background knowledge without domain specific hand coding of axioms, such that the approach and the result can be used as well for other domains in mathematics and science. Furthermore, we discuss the modeling of normative statements in commonsense reasoning and in robot ethics (This paper is an extended version of the informal proceedings [9] and [10]). © Springer International Publishing Switzerland 2016.",,"Theorem proving; Automated reasoning; Back-ground knowledge; Benchmark suites; Commonsense reasoning; Domain specific; Extended versions; Hand coding; Robot ethics; Multi agent systems","","14th German Conference on Multiagent System Technologies, MATES 2016","27 September 2016 through 30 September 2016",,181629,Conference Paper,"Final","",Scopus,2-s2.0-84988672424
"Ganascia J.-G.","6603992288;","Abstraction of levels of abstraction",2015,"Journal of Experimental and Theoretical Artificial Intelligence","27","1",,"23","35",,4,"10.1080/0952813X.2014.940685","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85027930148&doi=10.1080%2f0952813X.2014.940685&partnerID=40&md5=2591077b9c1ae72d5b1e3122afeed4cd","The notion of level of abstraction (LoA) is one of the foundations of the Floridi's Philosophy of Information. It also serves for many practical purposes as in information ethics. But the notion of abstraction is not new; it has been given many different meanings in various fields, especially in scientific disciplines and, in particular, in computer science. Our purpose here is to examine the use of abstraction in Floridi's works in conjunction with some of the meanings of abstraction in computer science. The article is divided into five sections. After a general introduction to the Floridi's method of abstraction (MoA) in Section 1, Section 2 revisits Floridi's definition of abstraction and Section 3 gives the different senses of abstraction in computer science. The Section 4 compares them with the Floridi's LoAs and proposes to generalise the Floridi's approach to abstraction using an abstraction of the LoAs, while Section 5 concludes on what we think to be some new arguments in favour of MoA and LoA. © 2014 Taylor & Francis.","abstract data type; abstract interpretation; abstraction; knowledge level; level of abstraction; method of abstraction; philosophy of information; software engineering","Abstract data types; Birds; Ontology; Software engineering; Abstract interpretations; abstraction; Knowledge level; Level of abstraction; method of abstraction; Philosophy of information; Abstracting",,,,,,Article,"Final","",Scopus,2-s2.0-85027930148
"Woodsend K., Lapata M.","","Wikisimple: Automatic simplification of wikipedia articles",2011,"AAAI",,,,"","",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84939551020
"Hermjakob U., Hovy E.H., Lin C.-Y.","","Automated question answering in webclopedia-A demonstration",2002,"Proceedings of ACL-02",,,,"370","371",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-33750728197
"Derrida J.","",[No title available],1967,"De la Grammatologie editions de Minuit",,,,"145","445",,4,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-33847501133
"Lin B.Y., Sheng Y., Vo N., Tata S.","57205548667;57203391985;57220035060;14833670500;","FreeDOM: A Transferable Neural Architecture for Structured Information Extraction on Web Documents",2020,"Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data Mining",,,,"1092","1102",,3,"10.1145/3394486.3403153","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090401556&doi=10.1145%2f3394486.3403153&partnerID=40&md5=53e82d43b845f2ffefa5ad71ed6a0dd0","Extracting structured data from HTML documents is a long-studied problem with a broad range of applications like augmenting knowledge bases, supporting faceted search, and providing domain-specific experiences for key verticals like shopping and movies. Previous approaches have either required a small number of examples for each target site or relied on carefully handcrafted heuristics built over visual renderings of websites. In this paper, we present a novel two-stage neural approach, named FreeDOM, which overcomes both these limitations. The first stage learns a representation for each DOM node in the page by combining both the text and markup information. The second stage captures longer range distance and semantic relatedness using a relational neural network. By combining these stages, FreeDOM is able to generalize to unseen sites after training on a small number of seed sites from that vertical without requiring expensive hand-crafted features over visual renderings of the page. Through experiments on a public dataset with 8 different verticals, we show that FreeDOM beats the previous state of the art by nearly 3.7 F1 points on average without requiring features over rendered pages or expensive hand-crafted features. © 2020 Owner/Author.","structured data extraction; web information extraction","Electronic document exchange; Natural language processing systems; Rendering (computer graphics); Semantics; Domain specific; Knowledge basis; Neural architectures; Relational neural networks; Semantic relatedness; State of the art; Structured data; Structured information; Data mining","ACM SIGKDD;ACM SIGMOD","26th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD 2020","23 August 2020 through 27 August 2020",,162480,Conference Paper,"Final","All Open Access, Bronze, Green",Scopus,2-s2.0-85090401556
"Gangadharan V., Gupta D., Amritha L., Athira A.T.","57198996470;25821943200;57218672116;57220281027;","Paraphrase Detection Using Deep Neural Network Based Word Embedding Techniques",2020,"Proceedings of the 4th International Conference on Trends in Electronics and Informatics, ICOEI 2020",,,"9142877","517","521",,3,"10.1109/ICOEI48184.2020.9142877","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85089995670&doi=10.1109%2fICOEI48184.2020.9142877&partnerID=40&md5=075d8b700d5c14c66849d7453b30629d","This paper focuses on detecting paraphrase in sentences using different word vectorization techniques and finding which vectorization method is more efficient. Word vectorization is a technique which is used to retrieve information from large collection of textual data like corpus or documents by associating each word as a vector. As the textual data are massive, the problem with the text data is that it need to defined in a form of numbers for solving mathematical problems. There are elementary to composite methods to solve this problem. In this paper we are comparing different word vectorization techniques and they are, Count Vectorizer,Hashing Vectorizer, TF-IDF Vectorizer, fastText, ELMo, GloVe, BERT. © 2020 IEEE.","BERT; Count Vectorizer; ELMo; fastText; GloVe; Hashing Vectorizer; TF-IDF Vectorizer","Neural networks; Composite method; Embedding technique; Mathematical problems; Text data; Textual data; Vectorization; Vectorization techniques; Vectorizer; Deep neural networks",,"4th International Conference on Trends in Electronics and Informatics, ICOEI 2020","15 June 2020 through 17 June 2020",,161905,Conference Paper,"Final","",Scopus,2-s2.0-85089995670
"Bounabi M., El Moutaouakil K., Satori K.","57195492422;57214147181;35976902500;","Association Models to Select the Best Rules for Fuzzy Inference System",2020,"Advances in Intelligent Systems and Computing","1076",,,"349","357",,3,"10.1007/978-981-15-0947-6_33","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85085171190&doi=10.1007%2f978-981-15-0947-6_33&partnerID=40&md5=308886ffc78859306af608b220af116e","In the past decade, the fuzzy inference system (FIS) has seen explosive growth in popularity among the researchers and industrialists. The fuzzy classification system is based on the most knowing process of the FIS in order to map features as inputs to outputs classes. Generally, to make a fuzzy classifier decision it is necessary to select manually the best rules that require the intervention of experts. In the current paper, we propose an automatic method to select the best rules using associations models: Apriority and Filter Associations. The proposed method process in five steps: preprocessing (feature reduction), determining membership function for every input and output, nominal representation of the database basing on the membership functions, call for the association model to select the most important rules, and finally post-processing of the obtained rules. In this work, we lead to appropriate rules set tested on the iris data for the classification task. In addition, the proposed method is highly simple to be implemented to control the even complex system. Our system achieved promising results, which demonstrate the effectiveness of the proposed approach. © Springer Nature Singapore Pte Ltd. 2020.","Association models; Fuzzy classifier; Fuzzy inference system; Fuzzy logic; Inference rules","Artificial intelligence; Embedded systems; Fuzzy sets; Membership functions; Association models; Classification tasks; Explosive growth; Feature reduction; Fuzzy classification systems; Fuzzy classifiers; Fuzzy inference systems; Input and outputs; Fuzzy inference",,"1st International Conference on Embedded Systems and Artificial Intelligence, ESAI 2019","2 May 2019 through 3 May 2019",,239769,Conference Paper,"Final","",Scopus,2-s2.0-85085171190
"Bose R., Dey R.K., Roy S., Sarddar D.","56735693500;57205211568;56591063600;36500143100;","Sentiment Analysis on Online Product Reviews",2020,"Advances in Intelligent Systems and Computing","933",,,"559","569",,3,"10.1007/978-981-13-7166-0_56","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85068195855&doi=10.1007%2f978-981-13-7166-0_56&partnerID=40&md5=29903296944e182ffec48064b382b7a0","Today, people are exchanging their thoughts through online Web forums, blogs, and different social media platforms. Sometimes, they are giving reviews and opinions on different products, brand, and their services. Their reviews toward a product not only improve the product quality but also influence purchase decisions of the consumers. Thus, product review analysis is a widely accepted platform where consumer can easily aware of their requirements. In this experiment, we track 568,454 fine food reviews of 74,258 products and 256,059 users on Amazon over a period of ten years. To analyze the result, we select six most popular products and users based on the plain text review, and NRC emotion lexicon is used which can be categorized eight basic emotions and two sentiments. Word cloud also help our research to make comparisons between the eight emotion categories. Our results show that how sentiment analysis will help to identify the consumers’ behaviors and overcome those risks to meet the consumers’ satisfaction. © Springer Nature Singapore Pte Ltd. 2020.","Amazon’s customer reviews; Digital marketing; Electronic word-of-mouth (e-WOM); Sentiment analysis; Unstructured data","Risk assessment; Sentiment analysis; Customer review; Digital marketing; Electronic word of mouths; Online product reviews; Product review analysis; Purchase decision; Social media platforms; Unstructured data; Consumer behavior",,,,,,Book Chapter,"Final","",Scopus,2-s2.0-85068195855
"Vassilev A.","",[No title available],2019,"BowTie-A Deep Learning Feedforward Neural Network for Sentiment Analysis",,,,"","",,3,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85106340847
"Yu W., Zhou J., Yu W., Liang X., Xiao N.","57211681215;57218716563;57204472270;55926362100;7006517717;","Heterogeneous graph learning for visual commonsense reasoning",2019,"Advances in Neural Information Processing Systems","32",,,"","",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090177668&partnerID=40&md5=ff253676affe10ea608305f3bb94d7bd","Visual commonsense reasoning task aims at leading the research field into solving cognition-level reasoning with the ability of predicting correct answers and meanwhile providing convincing reasoning paths, resulting in three sub-tasks i.e., Q?A, QA?R and Q?AR. It poses great challenges over the proper semantic alignment between vision and linguistic domains and knowledge reasoning to generate persuasive reasoning paths. Existing works either resort to a powerful end-to-end network that cannot produce interpretable reasoning paths or solely explore intra-relationship of visual objects (homogeneous graph) while ignoring the cross-domain semantic alignment among visual concepts and linguistic words. In this paper, we propose a new Heterogeneous Graph Learning (HGL) framework for seamlessly integrating the intra-graph and inter-graph reasoning in order to bridge vision and language domain. Our HGL consists of a primal vision-to-answer heterogeneous graph (VAHG) module and a dual question-to-answer heterogeneous graph (QAHG) module to interactively refine reasoning paths for semantic agreement. Moreover, our HGL integrates a contextual voting module to exploit long-range visual context for better global reasoning. Experiments on the large-scale Visual Commonsense Reasoning benchmark demonstrate the superior performance of our proposed modules on three tasks (improving 5% accuracy on Q?A, 3.5% on QA?R, 5.8% on Q?AR)2,. © 2019 Neural information processing systems foundation. All rights reserved.",,"Alignment; Benchmarking; Semantics; Commonsense reasoning; End-to-end network; Heterogeneous graph; Knowledge reasoning; Research fields; Semantic alignments; Visual concept; Visual context; Graph theory","Citadel;Doc.AI;et al.;Lambda;Lyft;Microsoft Research","33rd Annual Conference on Neural Information Processing Systems, NeurIPS 2019","8 December 2019 through 14 December 2019",,161263,Conference Paper,"Final","",Scopus,2-s2.0-85090177668
"Siebert S., Schon C., Stolzenburg F.","57194032036;53881954300;55887001400;","Commonsense Reasoning Using Theorem Proving and Machine Learning",2019,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","11713 LNCS",,,"395","413",,3,"10.1007/978-3-030-29726-8_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072865702&doi=10.1007%2f978-3-030-29726-8_25&partnerID=40&md5=e6446e6bf8a07c025f5f072379d4a8f2","Commonsense reasoning is a difficult task for a computer to handle. Current algorithms score around 80% on benchmarks. Usually these approaches use machine learning which lacks explainability, however. Therefore, we propose a combination with automated theorem proving here. Automated theorem proving allows us to derive new knowledge in an explainable way, but suffers from the inevitable incompleteness of existing background knowledge. We alleviate this problem by using machine learning. In this paper, we present our approach which uses an automatic theorem prover, large existing ontologies with background knowledge, and machine learning. We present first experimental results and identify an insufficient amount of training data and lack of background knowledge as causes for our system not to stand out much from the baseline. © IFIP International Federation for Information Processing 2019.","Causal reasoning; Commonsense reasoning; Large background knowledge; Machine learning; Theorem proving","Data mining; Extraction; Learning systems; Machine learning; Automated theorem proving; Back-ground knowledge; Causal reasoning; Commonsense reasoning; Theorem provers; Training data; Theorem proving",,"3rd IFIP Cross Domain Conference for Machine Learning and Knowledge Extraction, CD-MAKE 2019","26 August 2019 through 29 August 2019",,231589,Conference Paper,"Final","",Scopus,2-s2.0-85072865702
"Kao C.-C., Chang J.-W., Wang T.-I., Huang Y.-M., Chiu P.-S.","56599657200;55570534700;7405564492;8630348700;24823976000;","Design and Development of the Sentence-based Collocation Recommender with Error Detection for Academic Writing",2019,"Journal of Internet Technology","20","1",,"229","236",,3,"10.3966/160792642019012001021","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071197515&doi=10.3966%2f160792642019012001021&partnerID=40&md5=05c8b8f21c98a0afa91bb47b447e1d44","Appropriate collocations in writing of research papers in English can make the context smoother and expression of ideas more precise. In consequence, it is easier for the reader to understand and the purpose of sharing the outcome of research is accomplished. However, for nonnative English speakers, the choice and use of collocations is very difficult. For this reason, this study is intended to refer to a large amount of high-quality academic literature to establish a collocation corpus and adopt natural language processing techniques and statistical methods to develop a collocation recommendation system. The system will allow users to enter sentences, automatically detect the locations and types of collocations and recommend synonymous collocations in accordance with the semantics and frequency of use. The fitness of collocations in the system for beginning sentences achieves 73.1%. Writers of academic papers can use the system to select appropriate collocations, reduce erroneous use of collocations and improve the quality of their papers. © 2019 Taiwan Academic Network Management Committee. All rights reserved.","Academic writing assistance; Collocation corpus; Collocation recommender","Semantics; Academic literature; Academic writings; Collocation corpus; Collocation recommender; Design and Development; Frequency of use; NAtural language processing; Research papers; Natural language processing systems",,,,,,Article,"Final","",Scopus,2-s2.0-85071197515
"Spithourakis Georgios P, Riedel Sebastian","",[No title available],2018,"Numeracy for language models: Evaluating and improving their ability to predict numbers",,,,"","",,3,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85093273025
"Chen K., Forbus K.D.","57205545957;7003585370;","Action recognition from skeleton data via analogical generalization over qualitative representations",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"638","645",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060442252&partnerID=40&md5=6d676e262ea0f217c16f15487e371721","Human action recognition remains a difficult problem for AI. Traditional machine learning techniques can have high recognition accuracy, but they are typically black boxes whose internal models are not inspectable and whose results are not explainable. This paper describes a new pipeline for recognizing human actions from skeleton data via analogical generalization. Specifically, starting with Kinect data, we segment each human action by temporal regions where the motion is qualitatively uniform, creating a sketch graph that provides a form of qualitative representation of the behavior that is easy to visualize. Models are learned from sketch graphs via analogical generalization, which are then used for classification via analogical retrieval. The retrieval process also produces links between the new example and components of the model that provide explanations. To improve recognition accuracy, we implement dynamic feature selection to pick reasonable relational features. We show the explanation advantage of our approach by example, and results on three public datasets illustrate its utility. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Learning systems; Musculoskeletal system; Action recognition; Dynamic feature selections; Human-action recognition; Machine learning techniques; Qualitative representation; Recognition accuracy; Relational features; Retrieval process; Artificial intelligence","Association for the Advancement of Artificial Intelligence","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","2 February 2018 through 7 February 2018",,143510,Conference Paper,"Final","",Scopus,2-s2.0-85060442252
"Sarwar S., García-Castro R., Qayyum Z.U., Safyan M., Munir R.F.","26668090800;13404965900;36538522000;26422483100;54384067100;","Ontology-based learner categorization through case based reasoning and fuzzy logic",2017,"Proceedings of the International Conference on E-Learning, EL 2017 - Part of the Multi Conference on Computer Science and Information Systems 2017",,,,"159","163",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85040105509&partnerID=40&md5=6e15a5eb3d962b1d832e64707c1b63f6","Learner categorization has a pivotal role in making e-learning systems a success. However, learner characteristics exploited at abstract level of granularity by contemporary techniques cannot categorize the learners effectively. In this paper, an architecture of e-learning framework has been presented that exploits the machine learning based techniques for learner categorization taking into account the cognitive and inclinatory attributes of learners at finer level of granularity. Learner attributes are subjected to a pre-processing mechanism for taking into account the most important ones out of initial attribute set. Subsequently, couple of machine learning techniques namely Fuzzy Logic and Case Based Reasoning was employed on attributes selected for learner categorization. To best of our knowledge, these techniques have not been employed so far in learner categorization with quality of data and adaptivity while targeting semantic web.","Case Based Reasoning; E-Learning; Fuzzy Logic; Learner Categorization","Artificial intelligence; Case based reasoning; Computer circuits; E-learning; Fuzzy logic; Ontology; Abstract levels; Attribute sets; Contemporary techniques; Learner Categorization; Machine learning techniques; Ontology-based; Pre-processing; Quality of data; Learning systems","","2017 International Conference on E-Learning, EL 2017","20 July 2017 through 22 July 2017",,132157,Conference Paper,"Final","",Scopus,2-s2.0-85040105509
"Tambe S.S., Kadam G.V.","","""An Efficient framework for ELearning Recommendation system using fuzzy Logic and Ontology""",2016,"International research journal of engineering and technology","3","6",,"2062","2067",,3,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85061028136
"Furbach U., Schon C., Stolzenburg F., Weis K.-H., Wirth C.-P.","6603357096;53881954300;55887001400;55586110700;56187739300;","The RatioLog Project: Rational Extensions of Logical Reasoning",2015,"KI - Kunstliche Intelligenz","29","3",,"271","277",,3,"10.1007/s13218-015-0377-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85072861875&doi=10.1007%2fs13218-015-0377-9&partnerID=40&md5=c5095ffbeeaf0de5e98788136f7cf0d4","Higher-level cognition includes logical reasoning and the ability of question answering with common sense. The RatioLog project addresses the problem of rational reasoning in deep question answering by methods from automated deduction and cognitive computing. In a first phase, we combine techniques from information retrieval and machine learning to find appropriate answer candidates from the huge amount of text in the German version of the free encyclopedia “Wikipedia”. In a second phase, an automated theorem prover tries to verify the answer candidates on the basis of their logical representations. In a third phase—because the knowledge may be incomplete and inconsistent—we consider extensions of logical reasoning to improve the results. In this context, we work toward the application of techniques from human reasoning: We employ defeasible reasoning to compare the answers w.r.t. specificity, deontic logic, normative reasoning, and model construction. Moreover, we use integrated case-based reasoning and machine learning techniques on the basis of the semantic structure of the questions and answer candidates to learn giving the right answers. © 2015, Springer-Verlag Berlin Heidelberg.","Automated deduction; Case-based reasoning; Common-sense reasoning; Defeasible reasoning; Deontic logic; Question answering; Specificity",,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-85072861875
"Ventura D.","","The computational creativity complex",2015,"Computational Creativity Research: Towards Creative Machines",,,,"65","92",,3,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85080690920
"Heath D., Norton D., Ventura D.","55354928800;57196523811;24402340500;","Autonomously communicating conceptual knowledge through visual art",2013,"Proceedings of the 4th International Conference on Computational Creativity, ICCC 2013",,,,"97","104",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84949423196&partnerID=40&md5=e13508fdff5cef0269bd7d57bd4a7dc1","In visual art, the communication of meaning or intent is an important part of eliciting an aesthetic experience in the viewer. Building on previous work, we present three additions to DARCI that enhances its ability to communicate concepts through the images it creates. The first addition is a model of semantic memory based on word associations for providing meaning to concepts. The second addition composes universal icons into a single image and renders the image to match an associated adjective. The third addition is a similarity metric that maintains recognizability while allowing for the introduction of artistic elements. We use an online survey to show that the system is successful at creating images that communicate concepts to human viewers. © 2013 Proceedings of the 4th International Conference on Computational Creativity, ICCC 2013. All rights reserved.",,"Artificial intelligence; Arts computing; Conceptual knowledge; Online surveys; Semantic memory; Similarity metrics; Single images; Visual arts; Word association; Image enhancement",,"4th International Conference on Computational Creativity, ICCC 2013","12 June 2013 through 14 June 2013",,159704,Conference Paper,"Final","",Scopus,2-s2.0-84949423196
"Huang M., Shi X., Jin F., Zhu X.","7404260571;55444959800;57214366191;7406185137;","Using first-order logic to compress sentences",2012,"Proceedings of the National Conference on Artificial Intelligence","2",,,"1657","1663",,3,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-84868280217&partnerID=40&md5=cb5b271d4a9f8b1fab7361d5aaa26b9b","Sentence compression is one of the most challenging tasks in natural language processing, which may be of increasing interest to many applications such as abstractive summarization and text simplification for mobile devices. In this paper, we present a novel sentence compression model based on first-order logic, using Markov Logic Network. Sentence compression is formulated as a word/phrase deletion problem in this model. By taking advantage of first-order logic, the proposed method is able to incorporate local linguistic features and to capture global dependencies between word deletion operations. Experiments on both written and spoken corpora show that our approach produces competitive performance against the state-of-the-art methods in terms of manual evaluation measures such as importance, grammaticality, and overall quality. Copyright © 2012, Association for the Advancement of Artificial Intelligence. All rights reserved.",,"Evaluation measures; First order logic; Linguistic features; Markov logic networks; NAtural language processing; Overall quality; Sentence compression; State-of-the-art methods; Artificial intelligence; Formal logic; Natural language processing systems; Mobile devices","Association for the Advancement of Artificial Intelligence (AAAI);AI Journal;Steven Kuhn, Pine River Capital;National Science Foundation;Microsoft Research","26th AAAI Conference on Artificial Intelligence and the 24th Innovative Applications of Artificial Intelligence Conference, AAAI-12 / IAAI-12","22 July 2012 through 26 July 2012","Toronto, ON",93437,Conference Paper,"Final","",Scopus,2-s2.0-84868280217
"Wu B., Li C., Wang B.","56449782000;57204644301;7405918429;","Event detection and evolution based on entity separation",2011,"Proceedings - 2011 8th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2011","3",,"6019835","1803","1806",,3,"10.1109/FSKD.2011.6019835","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80053427384&doi=10.1109%2fFSKD.2011.6019835&partnerID=40&md5=b065ee1f2ae3e9ab0793cb00315fff95","By computing the relevance of follow-up stories, the traditional topic tracking approaches could track the stories. However, subtopics derived from one topic and the evolution process of these subtopics could not be identified with traditional approaches. A method is proposed to detect event and subtopics and get the evolution process of event from media data. Firstly creating entity vectors with entities in the media data and computing similarity between two entity vectors to separate single event. Then creating full vectors with all keywords in the dataset of an event and computing similarity between two full vectors to get several subtopics of an event and the evolution process of the event. Experiments show that this method can get events and subtopics of them effectively. © 2011 IEEE.","entity; evolution; similarity; subtopic","Data sets; entity; Event detection; evolution; Evolution process; similarity; Single event; subtopic; Topic tracking; Fuzzy systems; Information retrieval systems; Separation; Vectors","Coll. Inf. Sci. Technol. Donghua Univ.","2011 8th International Conference on Fuzzy Systems and Knowledge Discovery, FSKD 2011, Jointly with the 2011 7th International Conference on Natural Computation, ICNC'11","26 July 2011 through 28 July 2011","Shanghai",86748,Conference Paper,"Final","",Scopus,2-s2.0-80053427384
"Healey P.G.T., Howes C., Purver M.","","Does structural priming occur in ordinary conversation?",2010,"In Proc. of conference on linguistic evidence.",,,,"","",,3,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84861779450
"Valetta Weiß M.P., Pfeiffer T., Schaffranietz G., Rickheit G.","","Coordination in dialog: Alignment of object naming in the jigsaw map game",2008,"Proceedings of the 8Th Annual Meeting of the Cognitive Science Society of Germany","4",,,"1","17",,3,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-80054719452
"Brick I.","","The gist of creativity",1997,"The Complexity of Creativity",,,,"","",,3,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85082895205
"Modha S., Mandl T., Majumder P., Patel D.","","Tracking hate in social media: Evaluation, challenges and approaches",2020,"SN Comput. Sci.","1",,,"1","16",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85102940491
"Baik C., Jin Z., Cafarella M., Jagadish H.V.","57203024225;57158059800;8425432800;35242128300;","Duoquest: A Dual-Specification System for Expressive SQL Queries",2020,"Proceedings of the ACM SIGMOD International Conference on Management of Data",,,,"2319","2329",,2,"10.1145/3318464.3389776","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85086260631&doi=10.1145%2f3318464.3389776&partnerID=40&md5=1005048fc607c06c08b9c0ad31715e31","Querying a relational database is difficult because it requires users to be familiar with both the SQL language and the schema. However, many users possess enough domain expertise to describe their desired queries by alternative means. For such users, two major alternatives to writing SQL are natural language interfaces (NLIs) and programming-by-example (PBE). Both of these alternatives face certain pitfalls: natural language queries (NLQs) are often ambiguous, even for human interpreters, while current PBE approaches limit functionality to be tractable. Consequently, we propose dual-specification query synthesis, which consumes both a NLQ and an optional PBE-like table sketch query that enables users to express varied levels of domain knowledge. We introduce the novel dual-specification Duoquest system, which leverages guided partial query enumeration to efficiently explore the space of possible queries. We present results from user studies in which Duoquest demonstrates a 62.5% absolute increase in query construction accuracy over a state-of-the-art NLI and comparable accuracy to a PBE system on a limited workload supported by the PBE system. In a simulation study on the Spider benchmark, Duoquest demonstrates a >2x increase in top-1 accuracy over both NLI and PBE. © 2020 Association for Computing Machinery.","database usability; dual-specification interface; program synthesis; query by example; SQL","Natural language processing systems; Specifications; Domain knowledge; Natural language interfaces; Natural language queries; Programming by Example; Query construction; Relational Database; Simulation studies; State of the art; Query processing","ACM SIGMOD","2020 ACM SIGMOD International Conference on Management of Data, SIGMOD 2020","14 June 2020 through 19 June 2020",,160493,Conference Paper,"Final","All Open Access, Green",Scopus,2-s2.0-85086260631
"Shi X., Hu H., Che W., Sun Z., Liu T., Huang J.","57223977642;57223982261;57204332760;57211841896;57199476645;57195386755;","Understanding medical conversations with scattered keyword attention and weak supervision from responses",2020,"AAAI 2020 - 34th AAAI Conference on Artificial Intelligence",,,,"8838","8845",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85102974214&partnerID=40&md5=383a6e8b1455ca735aa3850d4a6b3768","In this work, we consider the medical slot filling problem, i.e., the problem of converting medical queries into structured representations which is a challenging task. We analyze the effectiveness of two points: scattered keywords in user utterances and weak supervision with responses. We approach the medical slot filling as a multi-label classification problem with label-embedding attentive model to pay more attention to scattered medical keywords and learn the classification models by weak-supervision from responses. To evaluate the approaches, we annotate a medical slot filling data and collect a large scale unlabeled data. The experiments demonstrate that these two points are promising to improve the task. Copyright © 2020, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Classification (of information); Filling; Classification models; Multi label classification; Two-point; Unlabeled data; Artificial intelligence","Association for the Advancement of Artificial Intelligence","34th AAAI Conference on Artificial Intelligence, AAAI 2020","7 February 2020 through 12 February 2020",,166426,Conference Paper,"Final","",Scopus,2-s2.0-85102974214
"Chen Q., Lin X., He X., Tou H., Chen T., Wei Z.","56482442600;57216694791;57216694923;57201547336;57207880261;51666060600;","Enhancing dialogue symptom diagnosis with global attention and symptom graph",2020,"EMNLP-IJCNLP 2019 - 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, Proceedings of the Conference",,,,"5033","5042",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85084308242&partnerID=40&md5=b7848a48355f4d126db266e899a10b60","Symptom diagnosis is a challenging yet profound problem in natural language processing. Most previous research focus on investigating the standard electronic medical records for symptom diagnosis, while the dialogues between doctors and patients that contain more rich information are not well studied. In this paper, we first construct a dialogue symptom diagnosis dataset based on an online medical forum with a large amount of dialogues between patients and doctors. Then, we provide some benchmark models on this dataset to boost the research of dialogue symptom diagnosis. In order to further enhance the performance of symptom diagnosis over dialogues, we propose a global attention mechanism to capture more symptom related information, and build a symptom graph to model the associations between symptoms rather than treating each symptom independently. Experimental results show that both the global attention and symptom graph are effective to boost dialogue symptom diagnosis. In particular, our proposed model achieves the state-of-the-art performance on the constructed dataset. © 2019 Association for Computational Linguistics",,"Diagnosis; Large dataset; Medical computing; Attention mechanisms; Benchmark models; Electronic medical record; Large amounts; NAtural language processing; Research focus; State-of-the-art performance; Natural language processing systems","Apple;ASAPP;et al.;Facebook;Google;salesforce","2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019","3 November 2019 through 7 November 2019",,159367,Conference Paper,"Final","",Scopus,2-s2.0-85084308242
"Gassert H.","","Operators on Fuzzy Sets: Zadeh and Einstein ations on Fuzzy Sets Properties of T-Norms and T-Conorms",2019,"Operators on Fuzzy Sets: Zadeh and Einsteinations on Fuzzy Sets Properties of T-Norms and T-Conorms",,,,"","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85061116372
"Bednarek Jakub, Piaskowski Karol, Krawiec Krzysztof","",[No title available],2019,"Ain't nobody got time for coding: Structure-aware program synthesis from natural language",,,,"","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85093307292
"Frank J., Cooper K.","","Multiobjective feature selection: Classification using educational datasets in an ensemble validation scheme",2019,"Data Sci. Pattern Recogn","3","1",,"9","34",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85081052615
"Hanselowski A., Stab C., Schulz C., Li Z., Gurevych I.","",[No title available],2019,"A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking",,,,"","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091196244
"Abdul-Jaleel M., Ali Y.H., Ibrahim N.J.","57211395215;56267313700;57211394169;","Fuzzy logic and Genetic Algorithm based Text Classification Twitter",2019,"SCCS 2019 - 2019 2nd Scientific Conference of Computer Sciences",,,"8852607","93","98",,2,"10.1109/SCCS.2019.8852607","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85073755657&doi=10.1109%2fSCCS.2019.8852607&partnerID=40&md5=cd60f9ed161424b20414f17dca579f6c","Social media are a modern web-based application for communication and interaction between humans through audio messages, written messages, and video messages. These devices build and activate living communities around the world. People share their interests and activities with these Applications. Twitter is a social media site, where people communicate through tweets. A service that enables users to communicate in touch through the exchange of frequent tweets and quick. People publish their tweets on their profile and send their followers to express their thoughts and opinions about events in this world. It is crucial to study and categorize these tweets. This work is based on fuzzy logic and genetic algorithm to solve the problem of text classification based on the relevance degree. The Inputs for this classification system are a set of features extracted from a tweet, and the output of this system is a decision of classification for a tweet, which is a degree of relevance for each tweet to an appointed event where the degree of relevance to the desired event iftweet irrelevant or relevant. The results are compared with a method of keyword search and fuzzy logic, which is based on the method incremental rate and correction rate. In the incremental rate, the proposed system is able to extract tweets more than this method, wherein dataset 1, the number of the tweets that are extracted by the proposed system is 160 tweets, but the number of the tweets that extracted by the other one are 98 and141. The correction rate of the proposed system is (98.75), but the correction rates of this method are (97.9) and (95.7). © 2019 IEEE.","A Social media; Fuzzy logic; Genetic algorithm; Text classification","Classification (of information); Computer circuits; Genetic algorithms; Search engines; Social networking (online); Tellurium compounds; Text processing; Classification system; Communication and interaction; Degree of relevance; Incremental rates; Living communities; Social media; Text classification; Web-based applications; Fuzzy logic","IEEE IRAQ Section","2nd Scientific Conference of Computer Sciences, SCCS 2019","27 March 2019 through 28 March 2019",,152297,Conference Paper,"Final","",Scopus,2-s2.0-85073755657
"Chen K., Rabkina I., McLure M.D., Forbus K.D.","57205545957;57194632893;57151001700;7003585370;","Human-like sketch object recognition v ia analogical learning",2019,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",,,,"1336","1343",,2,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85090804194&partnerID=40&md5=ebc0bf1bb6653f10635c031141e2dc9b","Deep learning systems can perform well on some image recognition tasks. However, they have serious limitations, including requiring far more training data than humans do and being fooled by adversarial examples. By contrast, analogical learning over relational representations tends to be far more data-efficient, requiring only human-like amounts of training data. This paper introduces an approach that combines automatically constructed qualitative visual representations with analogical learning to tackle a hard computer vision problem, object recognition from sketches. Results from the MNIST dataset and a novel dataset, the Coloring Book Objects dataset, are provided. Comparison to existing approaches indicates that analogical generalization can be used to identify sketched objects from these datasets with several orders of magnitude fewer examples than deep learning systems require. © 2019, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Deep learning; Image recognition; Object recognition; Computer vision problems; Human like; Orders of magnitude; Relational representations; Training data; Visual representations; Learning systems","Association for the Advancement of Artificial Intelligence","33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019","27 January 2019 through 1 February 2019",,160302,Conference Paper,"Final","",Scopus,2-s2.0-85090804194
"Ezzat M.","57210670847;","A comprehensive proposition of urbanism: With potential applications on users’ urban cognitive-mapping users’ generated urban designs",2019,"Smart Innovation, Systems and Technologies","100",,,"433","443",,2,"10.1007/978-3-319-92099-3_49","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85048014306&doi=10.1007%2f978-3-319-92099-3_49&partnerID=40&md5=35a04f40401ffc8accb6dbd76344db21","“Urbanism could be comprehended using three perspectives labelled Visual, Emotional, and Rational” is the comprehensive proposition the paper introduces, asserts its validity, and analyses its potential utilizations. The proposition assures not only that there are three relativistic perspectives that could sufficiently help in having a proper understanding of Urbanism, but it also assures that these three relativistic perspectives are contrasting. By contrasting we mean that having an understanding of one perspective would entail a closer understanding of the others. Urbanism is comprehensible by perceiving its immense notions using the three perspectives, the repository of these perceived notions is accumulated by a companion comprehensive model. There is a desperate need for such comprehensive, highly abstract, proposition rather than relying solely on fragmented concrete low-level propositions for the analysis of complex subject like urban identity. Holistic abstraction is indispensable for proper understanding and prediction of users’ urban cognition depending on their urban identity, and hence for developing socially and culturally sustainable urban designs. For instance, the successfulness of any empirical computational simulation of the users’ urban Cognition-mapping is highly dependent on such comprehensive proposition. Additionally, using the comprehensive model as a standard monolithic tool of analysis would enrich the social comparative studies. A main task of the paper is to examine the validity of the comprehensive proposition by analysing the exhaustive understanding of important urban notions using the three perspectives. During that examination, characteristics of the three perspectives would be disclosed and accumulated using the companion comprehensive model. Our findings conform to theoretical and philosophical ontological foundations. © Springer International Publishing AG, part of Springer Nature 2019.","A unifying model of urbanism; Emotionally driven urbanism; Rationally driven urbanism; Visually driven urbanism","Mapping; Philosophical aspects; A unifying model of urbanism; Comprehensive model; Computational simulation; Emotionally driven urbanism; Ontological foundation; Rationally driven urbanism; Sustainable urban designs; Visually driven urbanism; Regional planning","","3rd International New Metropolitan Perspectives. Local Knowledge and Innovation dynamics towards territory attractiveness through the implementation of Horizon/Europe2020/Agenda2030, 2018","22 May 2018 through 25 May 2018",,213639,Conference Paper,"Final","",Scopus,2-s2.0-85048014306
"Jin H.","",[No title available],2018,"Incorporating Chinese Characters of Words for Lexical Sememe Prediction. Arxiv Preprint Arxiv","1806",,,"06349","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075568686
"Lai T., Bui T., Li S., Lipka N.","","A simple end-to-end question answering model for product information",2018,"Proceedings of the First Workshop on Economics and Natural Language Processing",,,,"38","43",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85062221445
"Ali T., Asghar S.","57197215794;35068957300;","Multi-label scientific document classification",2018,"Journal of Internet Technology","19",,,"1707","1716",,2,"10.3966/160792642018111906008","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060703467&doi=10.3966%2f160792642018111906008&partnerID=40&md5=6941dd8df7c93d939fcf74d8351d759a","Scientific document label identification is a significant research area having numerous applications like digital libraries. The author assigns a category or categories to their document manually. Likewise, categories are structured in taxonomy in the form of tree such as ACM CCS. The dilemma becomes more complex when a document belongs to multiple categories. The problem of manual assignment becomes more complicated when the number of expected labels increases. Moreover, the accession schemes are insufficient for solutions with higher accuracy on real scientific document datasets. One way to handle the multi-label classification is to change the problem into a single-label classification. Another way is the variation of the algorithm to handle multi-label classification. The focus of our research is on conversion. Moreover, we propose a solution stimulated from the particle swarm optimization algorithm that can consign a label from the taxonomy. A set of similarity measures is evaluated as well for documentation relatedness that are used in the proposed approach. The designed solution is evaluated on two documents dataset that are retrieved from J. UCS and ACM with an average accuracy of 77 percent as compared to the state of the art algorithms. © 2018 Taiwan Academic Network Management Committee. All rights reserved.","Digital libraries; Multi-label classification; PSO; Text similarity","Digital libraries; Particle swarm optimization (PSO); Taxonomies; Multi label classification; Multi-label; Particle swarm optimization algorithm; Scientific documents; Similarity measure; State-of-the-art algorithms; Text similarity; Text processing",,,,,,Article,"Final","",Scopus,2-s2.0-85060703467
"Ergen T., Kozat S.S.","57195223149;6603217246;","Neural networks based online learning [Sinir Aǧlari Merkezli Cevrimici Öǧrenim]",2017,"2017 25th Signal Processing and Communications Applications Conference, SIU 2017",,,"7960218","","",,2,"10.1109/SIU.2017.7960218","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85026325393&doi=10.1109%2fSIU.2017.7960218&partnerID=40&md5=902747a9f9909bdd7f6a5eca6c24515d","In this paper, we investigate online nonlinear regression and introduce novel algorithms based on the long short term memory (LSTM) networks. We first put the underlying architecture in a nonlinear state space form and introduce highly efficient particle filtering (PF) based updates, as well as, extended Kalman filter (EKF) based updates. Our PF based training method guarantees convergence to the optimal parameter estimation under certain assumptions. We achieve this performance with a computational complexity in the order of the first order gradient based methods by controlling the number of particles. The experimental results illustrate significant performance improvements achieved by the introduced algorithms with respect to the conventional methods. © 2017 IEEE.","Kalman filtering; long short term memory network; online learning; particle filtering","Complex networks; Education; Extended Kalman filters; Kalman filters; Monte Carlo methods; Signal filtering and prediction; Signal processing; State space methods; Gradient-based method; Kalman-filtering; Non-linear regression; Nonlinear state space; Online learning; Optimal parameter estimation; Particle Filtering; Short term memory; E-learning","","25th Signal Processing and Communications Applications Conference, SIU 2017","15 May 2017 through 18 May 2017",,128703,Conference Paper,"Final","",Scopus,2-s2.0-85026325393
"Wirth C.-P., Stolzenburg F.","56187739300;55887001400;","A series of revisions of David Poole’s specificity",2016,"Annals of Mathematics and Artificial Intelligence","78","3-4",,"205","258",,2,"10.1007/s10472-015-9471-9","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84944909121&doi=10.1007%2fs10472-015-9471-9&partnerID=40&md5=5339af8b8799612008ead00d2ae05a63","In the middle of the 1980s, David Poole introduced a semantic, model-theoretic notion of specificity to the artificial-intelligence community. Since then it has found further applications in non-monotonic reasoning, in particular in defeasible reasoning. Poole tried to approximate the intuitive human concept of specificity, which seems to be essential for reasoning in everyday life with its partial and inconsistent information. His notion, however, turns out to be intricate and problematic, which — as we show — can be overcome to some extent by a closer approximation of the intuitive human concept of specificity. Besides the intuitive advantages of our novel specificity orderings over Poole’s specificity relation in the classical examples of the literature, we also report some hard mathematical facts: Contrary to what was claimed before, we show that Poole’s relation is not transitive in general. The first of our specificity orderings (CP1) captures Poole’s original intuition as close as we could get after the correction of its technical flaws. The second one (CP2) is a variation of CP1 and presents a step toward similar notions that may eventually solve the intractability problem of Poole-style specificity relations. The present means toward deciding our novel specificity relations, however, show only slight improvements over the known ones for Poole’s relation; therefore, we suggest a more efficient workaround for applications in practice. © 2015, Springer International Publishing Switzerland.","Artificial intelligence; Defeasible reasoning; Non-monotonic reasoning; Positive-conditional specification; Specificity",,,,,,,Article,"Final","",Scopus,2-s2.0-84944909121
"Hamon D.","",[No title available],2016,"System and method providing a binary representation of a web page",,,,"","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091456976
"Bouquet P., Molinari A.","","Using Semantic Technologies in E-Learning Platforms: A Case Study",2016,"International Journal of Information and Education Technology","6",,,"","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073820020
"Rocktschel T., Singh S., Riedel S.","","Injecting logical background knowledge into embeddings for relation extraction",2014,"NAACL",,,,"648","664",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049368596
"West D.M., Learning M.","","Transforming Education, Engaging Students and Improving Outcomes",2013,"International Journal of ICT, E-Management and E-Learning","4",,,"","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073834345
"Beim Graben P., Drenhaus H.","23110284000;6504775573;","Computationelle Neurolinguistik",2012,"Zeitschrift fur Germanistische Linguistik","40","1",,"97","125",,2,"10.1515/zgl-2012-0006","https://www.scopus.com/inward/record.uri?eid=2-s2.0-84870269238&doi=10.1515%2fzgl-2012-0006&partnerID=40&md5=83b4c532236e71e9abacad562e06184e","Computational neurolinguistics integrates methods from computational (psycho-)linguistics and computational neuroscience in order to model neural correlates of linguistic behavior. We illustrate these techniques using an example of the language processing of German negative polarity items (NPI) in the eventrelated brain potential (ERP) paradigm. To that aim, we first describe the syntactic and semantic licensing conditions of NPIs by means of slightly modified minimalist grammars. In a second step we use dynamic cognitive modeling (DCM) to map the state descriptions of a minimalist parser onto activation patterns of a neural network. Thirdly, the network's synaptic weights are trained with the correct parse of NPI constructions. Using these weights we calculate neural harmony measures for correct and for ungrammatical NPI constructions. In a final step we correlate the harmonies of the dynamical model with experimentally obtained ERP amplitudes by means of a simple statistical model.",,,,,,,,Article,"Final","All Open Access, Green",Scopus,2-s2.0-84870269238
"David P.","","A Design Requirements Framework for Distance Learning Environments",2007,"J Comput","2","4",,"99","113",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073833673
"Richardson M., Domingos P.","","Markov logic networks",2006,"Machine Learning","63","2",,"","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-34748853447
"Bhagat R., Leuski A., Hovy E.","","Shallow semantic parsing despite little training data",2005,"Proc. ACL/SIGPARSE 9th Int. Workshop on Parsing Technologies",,,,"","",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-70549100459
"Gigley H.","","Computational neurolinguistics: What is it all about?",1985,"Proceedings IJCAI-85","1",,,"260","266",,2,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84994376644
"Raghavan S.","",[No title available],2020,"AI Predictions from IBM Research (2019).",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85096544020
"Liu B., Zhou Y., Sun W.","","Character-level hybrid convolutional and recurrent neural network for fast-text categorization",2020,"Proc. ELM, J. Cao, C. M. Vong, Y. Miche, and A. Lendasse, Eds. Cham, Switzerland: Springer",,,,"108","117",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85106843489
[No author name available],[No author id available],[No title available],2020,"The Most Popular Research, Guides, News and More in Artificial Intelli-gence",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85106842826
[No author name available],[No author id available],[No title available],2020,"COVID-19-Sentiments India[20/03/20-31/05/20]",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85106824007
[No author name available],[No author id available],"Transformers: State-of-the-art natural language processing. In: Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations (pp. 38–45). Association for Computational Linguistics, Online",2020,"Https://Doi.Org/10.18653/V1/2020.Emnlp-Demos.6",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85104644277
"Lan Z., Sharma P.","",[No title available],2020,"Albert: A LITE BERT FOR SELF-SUPERVISED LEARNING OF LANGUAGE REPRESENTATIONS","1-17",,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098721331
"Yu T., Nwet K.T.","57213140700;54393960900;","Comparing SVM and KNN algorithms for myanmar news sentiment analysis system",2020,"PervasiveHealth: Pervasive Computing Technologies for Healthcare",,,,"65","69",,1,"10.1145/3379247.3379293","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85082175294&doi=10.1145%2f3379247.3379293&partnerID=40&md5=d59fde1e19f54c25f4ccac5202604804","Sentiment analysis is one of the natural language processing research fields that identify the polarity and subjectivity of documents. With the increasing of web technology, large volumes of data are available from many resources. Sentiment analysis applications are widely applied in many domains from news, financial, election, blogs, and post. News is very important and provides valuable information for society. News is tagged as positive and negative in this system. News is collected from many websites and ALT tree bank. N-gram and TF-IDF feature extraction and selection methods are used in this system to get more performance. Supervised machine learning algorithm is a classification algorithm that uses labeled data. K nearest neighbors is a simple algorithm that stores all available features and classifies new features based on a similarity measure. SVM is one of the classifier that has higher accuracy. This paper shows comparison of performance results for sentiment analysis system by using support vector machine (SVM) and K nearest neighbor (KNN) algorithms. © 2020 ACM International Conference Proceeding Series. All rights reserved.","KNN; N-gram; Sentiment Analysis; SVM; TF-IDF","Computational linguistics; Feature extraction; Motion compensation; Nearest neighbor search; Sentiment analysis; Support vector machines; Classification algorithm; Comparison of performance; Feature extraction and selection; K nearest neighbor algorithm; N-grams; NAtural language processing; Supervised machine learning; TF-IDF; Learning algorithms",,"6th International Conference on Computing and Data Engineering, ICCDE 2020 and 2020 International Conference on Artificial Intelligence in Electronics Engineering, AIEE 2020","4 January 2020 through 6 January 2020",,158230,Conference Paper,"Final","",Scopus,2-s2.0-85082175294
"Inala J.P., Yang Y., Paulos J., Pu Y., Bastani O., Kumar V., Rinard M., Solar-Lezama A.","57189510407;57219528909;36876561300;54417881700;56786340300;57202530488;7003321126;12141220900;","Neurosymbolic transformers for multi-agent communication",2020,"Advances in Neural Information Processing Systems","2020-December",,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106449100&partnerID=40&md5=30302ac3ea220492f5a15dee0a461ccc","We study the problem of inferring communication structures that can solve cooperative multi-agent planning problems while minimizing the amount of communication. We quantify the amount of communication as the maximum degree of the communication graph; this metric captures settings where agents have limited bandwidth. Minimizing communication is challenging due to the combinatorial nature of both the decision space and the objective; for instance, we cannot solve this problem by training neural networks using gradient descent. We propose a novel algorithm that synthesizes a control policy that combines a programmatic communication policy used to generate the communication graph with a transformer policy network used to choose actions. Our algorithm first trains the transformer policy, which implicitly generates a “soft” communication graph; then, it synthesizes a programmatic communication policy that “hardens” this graph, forming a neurosymbolic transformer. Our experiments demonstrate how our approach can synthesize policies that generate low-degree communication graphs while maintaining near-optimal performance. © 2020 Neural information processing systems foundation. All rights reserved.",,"Gradient methods; Graph algorithms; Multi agent systems; Communication graphs; Communication policies; Communication structures; Gradient descent; Limited bandwidth; Multi-agent communications; Multi-agent planning; Near-optimal performance; Cooperative communication","Apple;et al.;Microsoft;PDT Partners;Sony;Tenstorrent","34th Conference on Neural Information Processing Systems, NeurIPS 2020","6 December 2020 through 12 December 2020",,169463,Conference Paper,"Final","",Scopus,2-s2.0-85106449100
"Shah A., Zhan E., Sun J.J., Verma A., Yue Y., Chaudhuri S.","57210635031;57210636592;57205393900;57204815118;23013485900;8727948900;","Learning differentiable programs with admissible neural heuristics",2020,"Advances in Neural Information Processing Systems","2020-December",,,"","",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85106404203&partnerID=40&md5=8e70531277c2416b2d051e71417edfdd","We study the problem of learning differentiable functions expressed as programs in a domain-specific language. Such programmatic models can offer benefits such as composability and interpretability; however, learning them requires optimizing over a combinatorial space of program “architectures”. We frame this optimization problem as a search in a weighted graph whose paths encode top-down derivations of program syntax. Our key innovation is to view various classes of neural networks as continuous relaxations over the space of programs, which can then be used to complete any partial program. This relaxed program is differentiable and can be trained end-to-end, and the resulting training loss is an approximately admissible heuristic that can guide the combinatorial search. We instantiate our approach on top of the A* algorithm and an iteratively deepened branch-and-bound search, and use these algorithms to learn programmatic classifiers in three sequence classification tasks. Our experiments show that the algorithms outperform state-of-the-art methods for program learning, and that they discover programmatic classifiers that yield natural interpretations and achieve competitive accuracy. © 2020 Neural information processing systems foundation. All rights reserved.",,"Classification (of information); Iterative methods; Optimization; Problem oriented languages; Branch and bound search; Combinatorial search; Continuous relaxation; Differentiable functions; Domain specific languages; Optimization problems; Sequence classification; State-of-the-art methods; Learning systems","Apple;et al.;Microsoft;PDT Partners;Sony;Tenstorrent","34th Conference on Neural Information Processing Systems, NeurIPS 2020","6 December 2020 through 12 December 2020",,169463,Conference Paper,"Final","",Scopus,2-s2.0-85106404203
"Ezzat M.","57210670847;","A Framework for a Comprehensive Conceptualization of Urban Constructs",2020,"RE: Anthropocene, Design in the Age of Humans - Proceedings of the 25th International Conference on Computer-Aided Architectural Design Research in Asia, CAADRIA 2020","2",,,"111","120",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85091286980&partnerID=40&md5=6481b239274ce5a522c91a20a662588e","Analogy is thought to be foundational for designing and for design creativity. Nonetheless, practicing analogical reasoning needs a knowledge-base. The paper proposes a framework for constructing a knowledge-base of urban constructs that builds on an ontology of urbanism. The framework is composed of two modules that are responsible for representing either the concepts or the features of any urban constructs' materialization. The concepts are represented as a knowledge graph (KG) named SpatialNet, while the physical features are represented by a deep neural network (DNN) called SpatialFeaturesNet. For structuring SpatialNet, as a KG that comprehensively conceptualizes spatial qualities, deep learning applied to natural language processing (NLP) is employed. The comprehensive concepts of SpatialNet are firstly discovered using semantic analyses of nine English lingual corpora and then structured using the urban ontology. The goal of the framework is to map the spatial features to the plethora of their matching concepts. The granularity ànd the coherence of the proposed framework is expected to sustain or substitute other known analogical, knowledge-based, inspirational design approaches such as case-based reasoning (CBR) and its analogical application on architectural design (CBD). © 2020 and published by the Association for Computer-Aided Architectural Design Research in Asia (CAADRIA), Hong Kong.","Case-based reasoning (cbr) and case-based design (cbd); Deep neural network for structuring kg; Domain-specific knowledge graph of urban qualities; Natural language processing and comprehensive understanding of urban constructs; Urban cognition and design creativity","Case based reasoning; Deep learning; Deep neural networks; Knowledge based systems; Knowledge representation; Natural language processing systems; Ontology; Regional planning; Semantics; Analogical reasoning; Casebased reasonings (CBR); Design approaches; Design creativities; NAtural language processing; Physical features; Semantic analysis; Spatial features; Architectural design",,"25th International Conference on Computer-Aided Architectural Design Research in Asia, CAADRIA 2020","5 August 2020 through 6 August 2020",,162806,Conference Paper,"Final","",Scopus,2-s2.0-85091286980
"Smolensky P.","",[No title available],2019,"Next-generation architectures bridge gap between neural and symbolic representations with neural symbols",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85095340804
"Young Halley, Bastani Osbert, Naik Mayur","",[No title available],2019,"Learning neurosymbolic generative models via program synthesis",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85095155856
"Cai Deng, Lam Wai","",[No title available],2019,"Core semantic first: A top-down approach for amr parsing",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85093347305
"Zhao C., Xiong C., Rosset C., Song X., Bennett P., Tiwary S.","","Transformer-XH: Multi-evidence reasoning with extra hop attention",2019,"Proc. Int. Conf. Learn. Represent.",,,,"1","16",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091239823
"Roads B., Love B.","",[No title available],2019,"Learning as the unsupervised alignment of conceptual systems",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85105210164
"Madrid J.","",[No title available],2019,"Autotext: Automl for Text Classification",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85104654034
"Revanasiddappa M.B., Harish B.S.","","A novel text representation model to categorize text documents using convolution neural network",2019,"Int. J. Intell. Syst. Appl.","11","5",,"36","45",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85081050420
[No author name available],[No author id available],[No title available],2019,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098730426
"Dong L., Zhao H.","57215880403;55613085900;","Hierarchical feature selection with orthogonal transfer",2019,"Journal of Internet Technology","20","4",,"1205","1212",,1,"10.3966/160792642019072004019","https://www.scopus.com/inward/record.uri?eid=2-s2.0-85071732168&doi=10.3966%2f160792642019072004019&partnerID=40&md5=82f23b125363ed6f730bd7c2bae328df","Feature selection is an indispensable preprocessing step in high-dimensional data classification, which has an effect on both the running time and the result quality of the subsequent classification processing steps. Most existing approaches use flat strategies, which treat each category or class separately and ignore hierarchical structure. In this paper, we propose a hierarchical feature selection algorithm with orthogonal transfer. We first compute the weight of the feature to the category by hierarchical SVM with orthogonal transfer. More specifically, we use an objective that is a convex function of the normal vectors to compute the weight. Then, we select features using the weight and predict the class label for a test sample according to classifier. Finally, extensive experimental results on various real-life datasets have demonstrated the superiority of the proposed algorithm. © 2019 Taiwan Academic Network Management Committee. All rights reserved.","Feature selection; Hierarchical classification; Orthogonal transfer","Clustering algorithms; Feature extraction; Functions; Support vector machines; Convex functions; Hierarchical classification; Hierarchical features; Hierarchical structures; High dimensional data; Orthogonal transfer; Pre-processing step; Real life datasets; Classification (of information)",,,,,,Article,"Final","",Scopus,2-s2.0-85071732168
"Sarwar S., Ul Qayyum Z., Castro R.G., Safyan M.","","Ontology based E-learning Systems: A Step towards context aware content recommendation",2018,"International Journal of Information and Educational Technology","8","10",,"10","19",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075767501
"Koolhaas R.","",[No title available],2018,"Harvard Graduate School of Design, Trüby, S., Westcott, J., Petermann, S.: Rem Koolhaas. Elements of Architecture",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091306751
"Ezzat M.","","A computational tool for mapping the users’ urban cognition-a framework and a representation for the evolutionary optimization of the fuzzy binary relation between the urban conceptions of “us” and “others",2018,"The 36th eCAADe Conference",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091286818
[No author name available],[No author id available],[No title available],2018,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078361727
[No author name available],[No author id available],[No title available],2018,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098721284
[No author name available],[No author id available],[No title available],2018,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098687348
"Crouse M., McFate C., Forbus K.","57205545442;55150285200;7003585370;","Learning from unannotated QA pairs to analogically disambiguate and answer questions",2018,"32nd AAAI Conference on Artificial Intelligence, AAAI 2018",,,,"654","662",,1,,"https://www.scopus.com/inward/record.uri?eid=2-s2.0-85060496073&partnerID=40&md5=cb0107c2148c309e08591fb265fa99df","Creating systems that can learn to answer natural language questions has been a longstanding challenge for artificial intelligence. Most prior approaches focused on producing a specialized language system for a particular domain and dataset, and they required training on a large corpus manually annotated with logical forms. This paper introduces an analogy-based approach that instead adapts an existing general purpose semantic parser to answer questions in a novel domain by jointly learning disambiguation heuristics and query construction templates from purely textual question-answer pairs. Our technique uses possible semantic interpretations of the natural language questions and answers to constrain a query-generation procedure, producing cases during training that are subsequently reused via analogical retrieval and composed to answer test questions. Bootstrapping an existing semantic parser in this way significantly reduces the number of training examples needed to accurately answer questions. We demonstrate the efficacy of our technique using the Geoquery corpus, on which it approaches state of the art performance using 10-fold cross validation, shows little decrease in performance with 2-folds, and achieves above 50% accuracy with as few as 10 examples. Copyright © 2018, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.",,"Artificial intelligence; Computational linguistics; Semantics; 10-fold cross-validation; Natural language questions; Query construction; Query generation; Question-answer pairs; Semantic interpretation; State-of-the-art performance; Training example; Query processing","Association for the Advancement of Artificial Intelligence","32nd AAAI Conference on Artificial Intelligence, AAAI 2018","2 February 2018 through 7 February 2018",,143510,Conference Paper,"Final","",Scopus,2-s2.0-85060496073
"Safyan M., Qayyum Z., Sarwar S.","",[No title available],2017,"Context-Aware Personalized Activity Modeling in Concurrent Environment",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075770896
"Kansky K., Silver T., Mély D.A., Eldawy M., Lázaro-Gredilla M., Lou X.","",[No title available],2017,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049866650
"Abdon Miranda-Correa J., Khomami M., Sebe N., Patras I.","",[No title available],2017,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85065492363
[No author name available],[No author id available],[No title available],2017,"D.",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85104643054
"Martinc M., Škrjanec I., Zupan K., Pollak S.","",[No title available],2017,"Pan",,,,"2017","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85104625546
"Akhtar N., Qureshi M.N., Ahamad M.V.","","An improved clustering method for text documents using neutrosophic logic",2017,"Applications of Soft Computing for the Web",,,,"167","179",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85103915355
"Liang W.E.I.","","Exploration on translation of the literary term ambiguity",2017,"China Terminology",,"4",,"9","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85103886759
[No author name available],[No author id available],[No title available],2017,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098721970
[No author name available],[No author id available],[No title available],2017,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098710632
[No author name available],[No author id available],[No title available],2017,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098703325
"Tang Kai-Fu, Kao Hao-Cheng, Chou Chun-Nan, Chang Edward Y.","",[No title available],2016,"Inquire and Diagnose: Neural Symptom Checking Ensemble using Deep Reinforcement Learning",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85094518459
"Sarwar S., Ul Qayyum Z., Safyan M., Munir F.","",[No title available],2016,"Ontology Based Adaptive, Semantic E-Learning Framework (OASEF)",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073812441
"Supriya B.N., Kallimani V., Prakash S., Akki C.B.","","Twitter sentiment analysis using binary classification technique",2016,"International Conference on Nature of Computation and Communication ICTCC 2016: Nature of Computation and Communication",,,,"91","396",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85088846768
"Luo Z., Sha Y., Zhu K.Q.","","Won Hwang S, Wang Z Commonsense causal reasoning between short texts",2016,"Proceeding of 15Th Int. Conf. on Principles of Knowledge Representation and Reasonging (KR’2016",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85088012579
"Gulcehre C., Chandar S., Cho K., Bengio Y.","",[No title available],2016,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049861018
"Ahn S., Choi H., Pärnamaa T., Bengio Y.","",[No title available],2016,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049858092
"Serafini L., D'Avila Garcez A.S.","","Logic tensor networks: Deep learning and logical reasoning from data and knowledge",2016,"Proc. 11th Int. Workshop Neural-Symbolic Learn. Reasoning NeSy",,,,"1","12",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078348967
[No author name available],[No author id available],[No title available],2016,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098711569
[No author name available],[No author id available],[No title available],2016,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098701520
[No author name available],[No author id available],[No title available],2015,"OOP Quizes",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075769068
"Carmantini G.S.","",[No title available],2015,"Turing neural networks. GitHub repository",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84994376619
"Carmantini G.S., Beim Graben P., Desroches M., Rodrigues S.","","Turing computation with recurrent artificial neural networks",2015,"Proceedings of the NIPS workshop on cognitive computation: integrating neural and symbolic approaches",,,,"5","13",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84994333207
"Jahankhani H., Tawil R.","","Adaptive E-learning Approach based on Semantic Web Technology",2015,"International Journal of Webology","10","2",,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073825313
"Weston J., Bordes A., Chopra S., Rush A.M., van Merriënboer B., Joulin A.","",[No title available],2015,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049851198
"He J., Chen J., He X., Gao J., Li L., Deng L.","",[No title available],2015,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049837753
[No author name available],[No author id available],[No title available],2015,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098716087
[No author name available],[No author id available],[No title available],2015,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098710926
[No author name available],[No author id available],[No title available],2015,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098688453
[No author name available],[No author id available],[No title available],2015,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098682798
"Mohammad T.Z., Mahmoud A.M.","","Classification Model of English Course e-Learning System for Slow Learners “Recent Advances in Information Science",2014,"International Journal of Computer Science (IIJCS",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075770924
"Peckham A., Schmiedeknecht T.","",[No title available],2014,"The Rationalist Reader",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091314941
"Cariani F., Grossi D., Meheus J., Parent X.","",[No title available],2014,"Deontic Logic and Normative systems—12th International Conference, DEON 2014, Ghent, Belgium, Proceedings",,,,"8554","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85088022063
"Graves A., Wayne G., Danihelka I.","",[No title available],2014,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049830861
[No author name available],[No author id available],[No title available],2014,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098738751
[No author name available],[No author id available],[No title available],2014,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098706198
[No author name available],[No author id available],[No title available],2014,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098695950
"Sarma Cakula M.S.","","Development of Personalized e-learning model",2013,"ICTE in Regional Development","26","4",,"113","120",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073811425
"Stojanovski T.","",[No title available],2013,"City Information Modeling (CIM) and urbanism: blocks, connections, territories, people and situations",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091267832
[No author name available],[No author id available],[No title available],2013,"Second Joint Conference on Lexical and Computational Semantics (*SEM)",,,,"312","320",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85104638705
"Chen D., Socher R., Manning C.D.","","Learning new facts from knowledge bases with neural tensor networks and semantic word vectors",2013,"Comput Sci",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078152104
"Fellbaum C.","","Word net",2012,"the Encyclopedia of Applied Linguistics. Oxford",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091206985
"Mehler A., Lücking A., Menke P.","13907942400;55918401600;37007738900;","From neural activation to symbolic alignment: A network-based approach to the formation of dialogue lexica",2011,"Proceedings of the International Joint Conference on Neural Networks",,,"6033266","527","536",,1,"10.1109/IJCNN.2011.6033266","https://www.scopus.com/inward/record.uri?eid=2-s2.0-80054754266&doi=10.1109%2fIJCNN.2011.6033266&partnerID=40&md5=540bfadfc2b6460e510593fa93e9eb06","We present a lexical network model, called TiTAN, that captures the formation and the structure of natural language dialogue lexica. The model creates a bridge between neural connectionist networks and symbolic architectures: On the one hand, TiTAN is driven by the neural motor of lexical alignment, namely priming. On the other hand, TiTAN accounts for observed symbolic output of interlocutors, namely uttered words. The TiTAN series update is driven by the dialogue inherent dynamics of turns and incorporates a measure of the structural similarity of graphs. This allows to apply and evaluate the model: TiTAN is tested classifying 55 experimental dialogue data according to their alignment status. The trade-off between precision and recall of the classification results in an F-score of 0.92. © 2011 IEEE.",,"Classification results; Connectionist networks; F-score; Lexical networks; Natural language dialogue; Network-based approach; Neural activation; Precision and recall; Structural similarity; Alignment; Neural networks","International Neural Network Society (INNS);IEEE Computational Intelligence Society (CIS);National Science Foundation (NSF);Cognimem Technologies, Inc.;Univ. Cincinnati Coll. Eng. Appl. Sci.","2011 International Joint Conference on Neural Network, IJCNN 2011","31 July 2011 through 5 August 2011","San Jose, CA",86979,Conference Paper,"Final","",Scopus,2-s2.0-80054754266
"Mehler A., Lücking A., Menke P.","","Modelling lexical alignment in spontaneous direction dialogue data by means of a lexicon network model",2011,,"2",,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84861766372
"WangSmarandacheZhang H., Sunderraman R.","",[No title available],2010,"Single valued neutrosophic sets. Infinite study",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85103908011
[No author name available],[No author id available],[No title available],2010,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098722762
"Gayler R.W., Levy S.D., Bod R.","12784638500;7402774486;12784000100;","Explanatory aspirations and the scandal of cognitive neuroscience",2010,"Frontiers in Artificial Intelligence and Applications","221",,,"42","51",,1,"10.3233/978-1-60750-661-4-42","https://www.scopus.com/inward/record.uri?eid=2-s2.0-78149384084&doi=10.3233%2f978-1-60750-661-4-42&partnerID=40&md5=f8a68be87fdd98b42a6e2e4a3ba89789","In this position paper we argue that BICA must simultaneously be compatible with the explanation of human cognition and support the human design of artificial cognitive systems. Most cognitive neuroscience models fail to provide a basis for implementation because they neglect necessary levels of functional organisation in jumping directly from physical phenomena to cognitive behaviour. Of those models that do attempt to include the intervening levels, most either fail to implement the required cognitive functionality or do not scale adequately. We argue that these problems of functionality and scaling arise because of identifying computational entities with physical resources such as neurons and synapses. This issue can be avoided by introducing appropriate virtual machines. We propose a tool stack that introduces such virtual machines and supports design of cognitive architectures by simplifying the design task through vertical modularity. © 2010 The authors and IOS Press. All rights reserved.","cognitive neuroscience; Data-Oriented Processing; Jackendoff's challenges; Neural Engineering Framework; Vector Symbolic Architecture","Architecture; Data handling; Neurology; Cognitive architectures; Cognitive neurosciences; Computational entities; Data-oriented processing; Jackendoff's challenges; Neural engineering; Physical phenomena; Physical resources; Cognitive systems",,,,,,Conference Paper,"Final","",Scopus,2-s2.0-78149384084
"Zanda M., Brown G.","23092621900;7406467765;","A study of semi-supervised generative ensembles",2009,"Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)","5519 LNCS",,,"242","251",,1,"10.1007/978-3-642-02326-2_25","https://www.scopus.com/inward/record.uri?eid=2-s2.0-70349338926&doi=10.1007%2f978-3-642-02326-2_25&partnerID=40&md5=093eac38c10c2bdb029b37149742a205","Machine Learning can be divided into two schools of thought: generative model learning and discriminative model learning. While the MCS community has been focused mainly on the latter, our paper is concerned with questions that arise from ensembles of generative models. Generative models provide us with neat ways of thinking about two interesting learning issues: model selection and semi-supervised learning. Preliminary results show that for semi-supervised low-variance generative models, traditional MCS techniques like Bagging and Random Subspace Method (RSM) do not outperform the single classifier approach. However, RSM introduces diversity between base classifiers. This starting point suggests that diversity between base components has to lie within the structure of the base classifier, and not in the dataset, and it highlights the need for novel generative ensemble learning techniques. © 2009 Springer Berlin Heidelberg.",,"Base classifiers; Base components; Data sets; Discriminative models; Ensemble learning; Generative model; Machine-learning; Model Selection; Random subspace method; Semi-supervised; Semi-supervised learning; Ways of thinking; Classifiers; Image retrieval; Knowledge engineering; Learning algorithms; Supervised learning; Education","Int. Assoc. Pattern Recogn. Tech. Comm. TC1:;Stat. Tech. Pattern Recogn.;IEEE Geosci. Remote Sens. Soc., IEEE Icel. Sect.;University of Iceland;University of Cagliari;University of Surrey","8th International Workshop on Multiple Classifier Systems, MCS 2009","10 June 2009 through 12 June 2009","Reykjavik",77272,Conference Paper,"Final","",Scopus,2-s2.0-70349338926
"Lin C.C.","","A case study on SCORM – based eLearning in computer aided drafting course with user satisfaction survey",2008,"WSEAS Trans Inf Sci Appl","5","10",,"1416","1427",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073814933
"Richter K., Donath D.","",[No title available],2006,"eCAADe",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85091285516
"Shen L., Shen R.","","Ontology based Content Recommendation",2005,"International Journal of Continued Engineering and Education","15","1",,"13","26",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85073833077
"Sharada B.A., Girish P.M.","",[No title available],2004,"Wordnet has No 'Recycle Bin'",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-79956107406
"Liao K., Liu Q., Wei Z., Peng B., Chen Q., Sun W., Huang X.","",[No title available],2004,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85102974130
[No author name available],[No author id available],[No title available],2003,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85103911573
[No author name available],[No author id available],"A. S. d'Avila Garcez*K. Broda*D. M. Gabbay",2002,"Neural-Symbolic Learning Systems: Foundations and Applications",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078327952
"Church K.W.","",[No title available],2000,,,,,"180","186",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-84861786672
"Beer C.","","Fuzzy thinking: The new science of fuzzy logic. Bart Kosko",1995,"Quart. Rev. Biol.","70","2",,"210","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85106798603
[No author name available],[No author id available],[No title available],1810,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098720541
"Balduzzi D.","",[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85038598414
[No author name available],[No author id available],[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85075767599
"Lee M., Yih H.X.","",[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049860707
"Fatemi M., El Asri L., Schulz H., Suleman H.J.","",[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85049853457
"Gavran I., Darulova E., Majumdar R.","","Interactive synthesis of temporal specifcations from examples and natural language",0000,"Proc. Acm Program. Lang. 4, Oopsla","2020",,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85108917487
"Luo R., Xu J., Zhang Y., Ren X., Sun X.","",[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85102976331
"Peng H., Du B., Ma H., Bhuiyan M.Z.A.B., Jianwei L., Wang L., Yu P.S.","","Spatial temporal incidence dynamic graph neural networks for traffic flow forecasting",0000,"Inf. Sci.",,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85081053195
[No author name available],[No author id available],[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078149327
[No author name available],[No author id available],[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078118156
[No author name available],[No author id available],[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078104932
[No author name available],[No author id available],[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85078081821
[No author name available],[No author id available],[No title available],0000,,,,,"","",,1,,,[No abstract available],,,,,,,,,"Final","",Scopus,2-s2.0-85098684772
