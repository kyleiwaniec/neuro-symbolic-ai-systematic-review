Doc type,Researcher,Title,Scopus Hash ID,DOI,Journal/Conference,Year,Business Use Case,Technical Application,Type of Learning,Key-intake,Contribution,Authors Definition of NeSy,Symbolic terms,Neural terms,NeSy Category,Kautz category,Datasets,Model description,Evaluation Metrics,Reported Score,Study Quality,Comments,MISC,,,
cp,K,A simple neural approach to spatial role labelling,2-s2.0-85064856751,10.1007/978-3-030-15719-7_13,"41st European Conference on Information Retrieval, ECIR 2019",2019,Text classification,classification,supervised,Context surrounding spacial roles is helpful in classifying these roles,"two-step approach of generating context vectors and relation identification based on the learned context vectors, outperforms the state-of-the-art results on tasks of SemEval 2012 and SemEval 2013.",," deduction,
dependency parsing,
rule based",recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,IAPR TC-12 image benchmark,Glove to generate word embeddings -> BiLSTM to generate context vectors. Rule based parser of dependency tree to generate candidate relations (returns a list of relation triples) -> feed candidates concatenated with context to NN classifier.,F1,better than SOTA,66.67%,"It is difficult to understand exactly the procedure. In short, a NN learns context embeddings, a rule based parser generates candidate relations, and these two items are concatenated and fed to a NN classifier.",,,,
cp,A,Semantic Fake News Detection: A Machine Learning Perspective,2-s2.0-85067488804,10.1007/978-3-030-20521-8_54,"15th International Work-Conference on Artificial Neural Networks, IWANN 2019",2019,Fact verification,classification,supervised,,"Semantic features (sentiment analysis, named entities and relations) will be added to a set of syntactic features (POS - part-of-speech and NPs - Noun Phrases) and to the features of the original input dataset. On the resulted augmented dataset various classifiers, such as LSTM, CNN, and Capsule Networks are applied. ",,semantic enrichement,recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic, Liar data set,1. Data augmentation,Accuracy,64%,72.22%,"Data is preprocessed to add various semantic and syntax level details. Metadata contains : sentiment (mostly aspect-based and sentence level), named entities (extracted from NLP library) and named entity links (wrappers on top of DBPedia spotlight). Relation extraction contains :  POS tags ( extracted directly from news sentence), triples (extracted from DBPedia). Experiments were conduscted using pretrained embeddings (Glove, fastText and word2vec). Both classical and DL based models were implemented to compare impact of added relations.  ",,,,
cp,K,Jointly Learning to Detect Emotions and Predict Facebook Reactions,2-s2.0-85072850367,10.1007/978-3-030-30490-4_16,"28th International Conference on Artificial Neural Networks, ICANN 2019",2019,emotion detection,classification,semi-supervised,The provided experimental analysis has shown that bridging these two tasks (emotion detection and reaction prediction) by means of FOL-based constraints leads to improvements in the prediction quality that clearly goes beyond more naive approaches in which artificial labels are generated in the data preprocessing stage.,a neural network-based model to jointly learn the task of emotion detection and the task of predicting Facebook reactions.,,first order logic (FOL),recurrent neural network (RNN),compiled,5. Neuro_Symbolic,"AffectiveText, ISEAR, and Fairy Tales, Facebook posts",text -> LSTM (word embeddings ) -> 2 MLPs (one for task) with logical constraints ,F1,on par or better than SOTA in some intances,77.78%,The model jointly predicts the reactions and the emotions of facebook posts by using FOL as constraints to the optimization problem. The FOL formulas are mapped using t-norm functions and added to the Loss function for each reaction and emotion. The system is inspired by Learning from Constraints. The procedure doesn't require an emotion lexicon which other SOTA systems employ.,,,,
cp,K,Complementing logical reasoning with sub-symbolic commonsense,2-s2.0-85075592929,10.1007/978-3-030-31095-0_11,"3rd International Joint Conference on Rules and Reasoning, RuleML+RR 2019",2019,KG Completion / link prediction,reasoning,supervised,"the major contribution of this work is to show that combining commonsense knowledge under the form of text-based entity embeddings with LTNs is not only simple, but it is also promising","In this paper, we have shown that the combination of sub-symoblic commonsense representations, under the form of entity embeddings generated from text, and logical reasoning in vector spaces is flexible and can be used to solve completion tasks. Since LTNs are based on Neural Networks, they reach similar results while also achieving high explainability due to the fact that they ground first-order logic. The real advantage comes from the fact that LTNs allow us to get the best of both the symbolic and connective worlds and to easily integrate additional knowledge like sub-symbolic commonsense knowledge.
",Neuro-symbolic integration is a current field of investigation in which symbolic approaches are combined with deep learning ones.,"first order logic (FOL),
knowledge graph (KG),
logic tensor network (LTN)",logic tensor network (LTN),compiled,4. Neuro: Symbolic → Neuro,DBpedia,word embeddings -> logic tensor network -> reaoning,F1,on par with SOTA,94.44%,"quantifiers make LTNs computationally inefficient.
LTN_EE can be used for inference after training, while DNN cannot.",https://github.com/vinid/logical commonsense.,,,
cp,K,A Framework for a Comprehensive Conceptualization of Urban Constructs,2-s2.0-85091286980,10.1007/978-3-030-48279-4_55,"25th International Conference on Computer-Aided Architectural Design Research in Asia, CAADRIA 2020",2020,"KG Completion / link prediction,
Textual reasoning,
Analogical reasoning",information extraction,supervised,"The paper proposes a framework for constructing a knowledge-base of urban constructs that builds on an ontology of urbanism. The system is made of two modules:  concepts are represented as a knowledge graph (KG) named SpatialNet, while the physical features are represented by a deep neural network (DNN) called SpatialFeaturesNet. SpatialNet is constructed using NLP using semantic analyses of nine English lingual corpora and then structured using the urban ontology.

","The paper proposes a framework for constructing a knowledge-base of urban constructs that builds on an ontology of urbanism. The system is made of two modules:  concepts are represented as a knowledge graph (KG) named SpatialNet, while the physical features are represented by a deep neural network (DNN) called SpatialFeaturesNet. SpatialNet is constructed using NLP using semantic analyses of nine English lingual corpora and then structured using the urban ontology.

",,"ontology,
knowledge graph (KG)","recurrent neural network (RNN),
convolutional neural network (CNN)",sequential,1. symbolic Neuro symbolic,NOT SPECIFIED,KG + DNN,NA,NA,33.33%,"Associating the materialized spatial symbols with their meanings is called symbol grounding, alternatively known as symbol conceptualization. The two components of the system are SPatialFeaturesNet, an application of computer vision, and SpatialNet, an application of natural language processing.
This is a system similar to CBD (case based reasoning), in that it keeps a knowledge base from which the user can draw ideas. It surpasses the traditional CBD because the KG is built from big data, using NLP/DL. The KG is built on an existing ontology of urbanism. The purpose of the system is to aid in the creative process of urban design. Unfortunately the authors do not provide any evaluation of their solution.
",,,,
cp,A,Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases,2-s2.0-85096590382,10.1007/978-3-030-61609-0_51,"29th International Conference on Artificial Neural Networks, ICANN 2020",2020,KG Completion / link prediction,classification,supervised,"Trains a model to reason directly over graphs. enabling reasoning over
many paths at once.",,,semantic enrichement,graph neural network (GNN),cooperative,3. Neuro; Symbolic,"ClueWeb, Freebase",GNN to learn embeddings followed by MLP for prediction,"Mean Average Precision, MAP)",Upto 92%,72.22%,"Authors have created 3 models, GNN-Relation, GNN-Mean and GNN-sum ; GNN relation considers only relations where as other 2 models considers both entity and relations to store/compute embeddings. These embeddings are fed into DL model (GNN ; LSTM cells and Relu activation function)",,,,
cp,A,Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information,2-s2.0-85113718164,10.1007/978-3-030-82147-0_34,"14th International Conference on Knowledge Science, Engineering and Management, KSEM 2021",2021,"Text classification,
KG Completion / link prediction",classification,supervised,"Rather than using TransE, TransR etc they have proposed a new embedding model using pretrained BERT.","sememes-based framework for knowledge graph to streamline the semantic space of entities. Replace entity descriptions with a finite set of semantics and encode the sememe labels of entities using a pre-trained Bert model, and finally jointly learning the symbolic triples and sememe labels",,sememe,neural network (NN),sequential,1. symbolic Neuro symbolic,"FB15K, WN18","Sememes-based framework for knowledge graph embedding architecture, where grammatical sememes and pragmatic sememes  co-represent entity’s comprehensive-information. Pretrained BERT is used to generate embeddings by considering pragmatic and grammatical sememes seperately. One-vs-rest training strategy is used to train the logistic regression model to make multi-label predictions. Multi-label classification task with 50 classes, which means that for each entity, the method should provide a set of types instead of a single type.  Semantic enrichment -> BERT -> LR ?","Mean Average Precision, MAP)",Upto 95%,55.56%,Embeddings are generated by classifying text into grammatical and pragmatic sememes.  Pretrained BERT is used to classify of grammatical and pragmatic sememes. They have translated english words back to chinese to compare their models with others.  ,"A sememe is an indivisible semantic unit for human languages defined bylinguists. I see this as a NN approach enriched by additonal information about each node (in the form of sememes). One could argue that this additional knowledge is symbolic, and conclude that the neuro-symbolic approach outperforms SOTA NN link prediction, such asTRANS-E. ",,,
ar,A,From symbolic to sub-symbolic information in question classification,2FCUJH2G,10.1007/s10462-010-9188-4,Artificial Intelligence Review,2011,Question answering,classification,supervised,rule-based question classifier that partially founds its performance in the detection of the question headword and in its mapping into the target category through the use of WordNet.,,,"rule based,
semantic enrichement",SVM,sequential,1. symbolic Neuro symbolic,"UIUC dataset,",Rule based system  + SVM,"Accuracy, Precision/recall",Upto 88%,66.67%,"rules based results are given as features to SVM. The rule-based classifier ttriggers a set of 60 manually built patterns, that are  matched against each question. If the match is successful, a category is returned and the question is classified; otherwise the  classifier searches for the question headword and extracts it. Then, the headword hypernyms are followed until one is associated with a possible question category using wordnet.",,,,
ar,K,autoBOT: evolving neuro-symbolic representations for explainable low resource text classification,JN4IZETG,10.1007/s10994-021-05968-x,Machine Learning,2021,Text classification,classification,supervised,"autoBOT is explainable, efficient, and performs on par with language models like BERT.","a novel approach for text classification with limited data and resources, but comparable in erformance to BERT. Hyperparameters are optimized using an evolutionary algorithm. Novel feature types: document keywords, relational features, and first order features (based on grounded relations from ConceptNet)",NA,"symbolic learner,
symbolic representations,
ConceptNet KG","transformer,
neuroevolution (NE)",compiled,4. Neuro: Symbolic → Neuro,"semeval2019, semeval2019,
mbti,
Fox,
BBC,
etc..","autoBOT (automatic Bags-Of-Tokens),
The proposed approach consists of an evolutionary algorithm that jointly optimizes various sparse representations of a given text (including word, subword, POS tag, keyword-based, knowledge graph-based and relational features) and two types of document embeddings (non-sparse representations). The key idea of autoBOT is that, instead of evolving at the learner level, evolution is conducted at the representation level.
",F1,up to 99% on some tasks ,100.00%,"autoBot is a genetic algorithm for text data that learns both the representations and the models for classification jointly. The inclusion of novel symbolic features improves performance over traditional linear models such as SVM and Logistic Regression, and performs on par with large language models such as BERT. The advantage of autoBot is its explainability (feature importances), and efficiency in a low resource setting (small data/feature sparsity, and limited compute power). What makes AutoBOT neuro-symbolic is the combination of symbolic and non-symbolic features. The study was conducted using off the shelf implementations and a very basic evolutionary algorithm, thus leaving plenty of room for improvements. Classification is performed at the document level, but it would be worth exploring this pipeline for sentence level and span level classification on for example, the semeval2020 task, which already has a leaderboard with BERT based solutions for comaprison.","Does not fall into any of Kautz's categories. However, the use of linear classifiers (ie symbolic learners) in combination with sub-symbolic representations qualifies for the neuro-symbolic moniker.",,,
ar,A,"Ontology based E-learning framework: A personalized, adaptive and context aware model",PD2A2ZVV,10.1007/s11042-019-08125-8,Multimedia Tools and Applications,2019,Recommendation model,recommendation,supervised,Recommendation via ANN using CBR,hybrid of two machine learning techniques named CBR and (ANN),,ontology,neural network (NN),compiled,4. Neuro: Symbolic → Neuro,Private,The content is annotated with domain ontology which is used to train a ANN (only one hidden layer)  with  small dynamic dataset (cases which are extracted from CBR. ),Accuracy,Upto 80%,50.00%,"Using CBR n ANN, learners are categorised as novice, expert etc. Ontology is modeled to hold information of users and the subect (Course ontology). Content to study are recommended using if-then rules. ",not text data,,,
ar,A,Solving the twitter sentiment analysis problem based on a machine learning-based approach,4EF4BXD9,10.1007/s12065-019-00301-x,Evolutionary Intelligence,2020,Sentiment analysis,classification,supervised,Combined 4 methods of feature selection which is given as input to GA which optimizes the features. Einstien conorm is used to combine values from 4 methods,,,semantic enrichement,SVM,sequential,1. symbolic Neuro symbolic,"Stanford Twitter Sentiment (STS), STS-Gold, Strict Obama-McCain Debate, Obama-McCain Debate",feature selection -> SV\M,"Precision/recall, Accuracy, F1",Upto 85%,61.11%,"Twitter sentiment analysis is perfomed using support vector machine and multinomial
naïve Bayes classifcation algorithms. Vector space is computed by ",VERY POOR QUALITY PAPER,,,
ar,K,The CoRg Project: Cognitive Reasoning,7MMJY5BM,10.1007/s13218-019-00601-5,KI - Kunstliche Intelligenz,2019,Question answering,reasoning,supervised,"Authors argue that combining logic reasoning with ML improves performance and gerates explainable results. The challenge is in finding an appropriate logics for cognitive (human-like) reasoning. The objective of CoRg is to develop a system for cognitive computing.
","So far we implemented a first prototype of the CoRg system, using KNEWS, WordNet, Hyper, ConceptNet Numberbatch and neural network techniques and evaluated it on COPA.",,logic,recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,COPA benchmark set (Choice of Plausible Alternatives),raw text -> FOL (using KNEWS) -> augment with bg knwledge bases -> theorem prover -> NN,,,44.44%,"The study explores using a variety of logic systems for Cognitive Reasoning. The authors posit that combining logic programming with NN should produce not only more accurate results, but also provide expalainbility. It is a work in progress. Results are not reported. ",,,,
ar,K,Semantic-based regularization for learning and inference,7Q5JRVK2,10.1016/j.artint.2015.08.011,Artificial Intelligence,2015,Topic modeling / categorization,classification,semi-supervised,adding prior knowledge in the form of grounded knowledge as constraints significantly improves accuracy.,"Semantic Based Regularization (SBR),",,"statistical relational learning (SLR, RML),
fuzzy logic,
FOL,
Transductive learning,
grounding,
propositionalization,
semantic enrichment",SVM,compiled,5. Neuro_Symbolic,CORA,"""Semantic Based Regularization (SBR) builds a multi-layer architecture having kernel machines at the input layer. The output of the kernel machines is fed to the higher layers implementing a fuzzy generalization of the FOL knowledge. The resulting model is continuous with respect to the feature values. Therefore, the high-level semantic inference provided by the logic can be back-propagated down to the kernel machines using any gradient-based schema.""
","Area Under the Curve (AUC),
F1",0.48 - 0.66,88.89%,"Prior knowledge in the form of grounded FOL rules is used as constraints to optimization. To translate logical expressions, t-norm functions are defined for connectives AND and NOT, as well as universal and existential quatifiers, the results of which mimic that of fuzzy logic (ie., continuous in the range [0,1]). While the performance of the proposed architecture is a significant improvement on the chosen benchmarks (SVM, MLN, and variants thereof), the absolute accuracy scores (on the CORA dataset) are still relatively low (ie., not too far off random chance). The limitation appears to be the use of SVM (seemingly because it lends itself to a transductive (semi-supervised) setting - ie., where there is little labeled data, transduction is used to assign labels to data points in close promiximty to labeled points, a bit like using clustering). This is a classification task in a setting with little labeled data. Future experiments could be conducted with more recent NN approaches in place of the SVM. In addition, other NLP architectures may be better benchmarks for this particular task.",,,,
ar,K,Medical knowledge embedding based on recursive neural network for multi-disease diagnosis,JGU2SECC,10.1016/j.artmed.2019.101772,Artificial Intelligence in Medicine,2020,Decision making,"classification,
multi-label (371 labels) probalities",supervised,Using Huffman trees to build symbols and logic into a Recursive NN which can be optimized with GD.,"Knowledge-based neural network, in which nerve cells in a hidden layer consist of first-order logic.
For neural network parameter learning, a complete theoretical framework based on back-propagation was established to minimize the cross-entropy error.
",,"FOL,
Huffman tree","recursive neural knowledge network (RNKN),
recursive neural network (RNN)",compiled,4. Neuro: Symbolic → Neuro,CEMR,"text -> KG triples -> Huffman tree -> logic functions as nerve cells -> softmax,
optimized using GD/backProp","p@k,
Discounted Cummulative Gain (DCG)",[0.36 - 0.72],72.22%,"The study encodes knowledge in the form of huffman trees made of triples, and logic expressions, in order to jointly learn embeddings and model weights. The first layer consists of entities, the second layer consists of relations (x-> y). Higher layers compute logic rules. The root node is the final embedding representing a document (in this case a single health record). Softmax is used to calculate class probabilities. back propagation is used for optimization. Sub-symbolic representations are learned from symbolic features and rules iteratively.",interesting approach that would be worth trying for propaganda detection,,,
ar,K,Concept generalization and fusion for abstractive sentence generation,7YX447XS,10.1016/j.eswa.2016.01.007,Expert Systems with Applications,2016,Text summarization,generative,supervised,"Dependency parsing and wordNet provide input to a ML clasification model, the output of which is use to generate sentences based on rules.",we have addressed the problem of concepts fusion and generalization for abstractive sentence generation.,,"semantic enrichement,
dependency parsing",SVM,nested,3. Neuro; Symbolic,NLTK,sentences -> dependency parse -> generate candidates (versions) -> reduce search space using rules -> SVM to select best candidate -> generate sentence using rules,F1,0.808,77.78%,"Uses external data sources to semantically enrich text summarization. It is the application text fusion. The system benefits from an internal machine learnign module. Comparisons with other ML text summarization systems are given, however, these baselines appear to be very weak. There is no comparison with NN, or neural language models. To reasses the approach, more modern ML/DL techniques should be be considered.",,,,
ar,K,Semi-supervised learning for big social data analysis,GS3TRUYZ,10.1016/j.neucom.2017.10.010,Neurocomputing,2018,Sentiment analysis,classification,semi-supervised,The addtion of prior knowledge to bias the classification model improves the accuracy of sentiment predictions.,"AffectNet: combined ConceptNet and WNA - commonsense and emotional knowledge are melded together.
AffectNet2: AffectNet with reduced dimentionality.
a flexible framework for semisupervised learning comprising of two parts: clustering and regularized classification.",,"commonsense knowledge,
graph represenatation",SVM,cooperative,5. Neuro_Symbolic,"Pang and Lee,
AffectNet benchmark",clutering -> classification regularized by output of clustering,Accuracy,88.50%,77.78%,"The study embeds prior knowledge in the form of a graph representing concepts and relations from natural language. These are projected into multi-dimentional space and a clustering algorithm is employed to generate the solution space. Subsequently, a regularized classification algorithm is employed where the output of the clustering stage is used in the regularization term. I have classified this as Kautz category 5. However, it differs from the cat5 defition in that the regularization term is not a logic rule, but prior knowledge.",,,,
ar,K,Deepening the IDA* algorithm for knowledge graph reasoning through neural network architecture,KTEHK4MZ,10.1016/j.neucom.2020.12.040,Neurocomputing,2021,KG Completion / link prediction,classification,reinforcement ,Two phased approach combining NN and traditional path-finding techniques improves KG reasoning performance.,Deep-IDA* framework that integrates the traditional path searching algorithms IDA* and deep neural networks for KG reasoning (link prediction).,,Markov Decision Process (MDP),reinforcement learning (RL),cooperative,2. Symbolic[Neuro],"NELL-995,
WN18RR,
FB15-237
","fusion of various models that do path-finding and path-reasoning. Pathfinding combines Deepening DFS and A*, the result of which is fed to a GNN for path reasoning. There are many neural modules inside this framework including BERT, LSTM, GRU, and GCN.  ","Mean Reciprocal Rank (MRR),
Hits@K,
Mean Average Precision (MAP)",Better than all benchmarks (see tables in paper),66.67%,"While the results reported exceed SOTA, the system lacks parsimony. The computation times are not reported.",not text data,,,
ar,A,Heterogeneous graph reasoning for knowledge-grounded medical dialogue system,2PNFHS7L,10.1016/j.neucom.2021.02.021,Neurocomputing,2021,Dialog system,classification,supervised,RNN (encoder) + GAT + RNN (decoder) ,heterogeneous graph reasoning (HGR) model to unify the dialogue context understanding and entity-correlation reasoning.,,knowledge graph (KG),recurrent neural network (RNN),compiled,4. Neuro: Symbolic → Neuro,"MDG-C, MDG-D",RNN -> GAT -> RNN,"F1, Precision/recall, Bleu score",Upto 44%,77.78%,"Though it is medical application, might be useful. Heterogeneous graph reasoning model to efficiently incorporate the contextual information and domain knowledge for entity reasoning in medical dialogue. ",,,,
ar,A,Assessing cognitive alignment in different types of dialog by means of a network model,6DWCY3EC,10.1016/j.neunet.2012.02.013,Neural Networks,2012,Relation extraction,classification,semi-supervised,dialog alignment.,,,"graph representation, 
position reasoning",neural network (NN),cooperative,3. Neuro; Symbolic,"Jigsaw Map Game, Speechand  Gesture  Alignment corpus", priming and spreading activation are modeledi n terms of two layered time aligned networking,F1,Upto 91%,44.44%,"Alignment is important for idioms like (learn by hearts, ready-mades etc). But sometimes speakers create their own linguistic representations. . Whether the class of aligned dialogs can be identified across different types of dialog. A successful classification indicates a structural fingerprint left by alignment, regardless of the underlying dialog type.","dialog, 
conversation, 
graph representations of communication units as input to machine learning ",,,
ar,K,Learning to activate logic rules for textual reasoning,NB39QA35,10.1016/j.neunet.2018.06.012,Neural Networks,2018,Question answering,reasoning,reinforcement ,"Image Scheme (IS) is acquired from early life experience, which is also our hypothesis in this work
our work aims at precise reasoning, good interpretability of models, and perfect performances on small dataset","We propose a novel memory-augmented neural network framework making use of logic rules inducted from existing theories of Image Schema and Cognitive Model.
We redefine textual reasoning tasks as interactive-feedback process with human working memory. Under certain assumptions, we jointly solve two main problems: variable binding and relation activating, via deep reinforcement learning.
Our experimental results show the existence of common properties among real-world relations, and the probability to partially re-construct human logic system to boost performances on language comprehension tasks.
Variable-Relation Reasoning Machine (VRRM)

",NA,"Sequencial decision making,
predicate logic,
relational logic,
rule based,
rule induction,
cognitive linguistics","reinforcement learning (RL),
Memory Network,
neural network (NN)
",cooperative,3. Neuro; Symbolic,bAbI-20,"We redefine textual reasoning as a sequential decision-making problem, and solve it with reinforcement learning (RL).",Test error rate,0.70%,66.67%,"This work makes the hypothesis that human reasoning can be modeled by Image Schemas (IS). Schemas are made up of logical rules on (Entity1,Relation,Entitity2) tuples, such as transitivity, or inversion. Input sentences and questions are encoded as the concatenation of individual word embeddings with zero paddings. An MLP module is used to learn a set of tuples by performing simulations and utilizing Reinforcement Learning. This work is limited by the nature of the synthetic data set. It is not shown how well it might generalize to a real world scenario. There is also a limited number of schemas, and as the authors point out, future work should include additional schemas more representative of human reasoning. (See also https://arxiv.org/pdf/1706.01427.pdf from DeepMind 2017)",,,,
ar,K,Fuzzy commonsense reasoning for multimodal sentiment analysis,B47SSE6P,10.1016/j.patrec.2019.04.024,Pattern Recognition Letters,2019,Sentiment analysis,classification,supervised,multi-modal sentiment analysis. The computational cost is reduced by projecting the feature embeddings onto a 4 dimentional affective space. Accuracy is increased by the use of fuzzy rules.,"Convolutional Fuzzy Sentiment Classifier (CFSN). in this paper we use a low-dimensional RNN to classify concepts learned via deep learning. By projecting temporal features onto AffectiveSpace, we are able to interpret the features learned. Lastly, we take into account that most sentences have mixed emotions such as sarcasm that can only be modeled effectively using fuzzy membership functions. Hence, we predict the final accuracy of the classifier using fuzzy blending over each pair of simple emotions",,"fuzzy logic,
fuzzy neural network (FNN),
logical connectives","convolutional neural network (CNN),
recurrent neural network (RNN),
deep belief network (DBN)",nested,2. Symbolic[Neuro],"multiple, multi-modal",text -> NN -> n-grams -> low-dimenstion ->  RNN -> CDBN (convolutional deep belief network) -> Fuzzy Logic calssification ,Accuracy,10-20% accuracy improvement over baselines,77.78%,"Not strictly text input. Text is only part of the input, and has been converted using pretrained embeddings. sentiment analysis. includes text datasets. This pipeline includes selection of logical connectives at the input phase, and a fuzzy logic classifier at the output phase.",,,,
ar,A,A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case,ACDMDN8W,10.1108/IJWIS-11-2020-0067,International Journal of Web Information Systems,2021,Text classification,classification,supervised,Neutrosophic reasoning to analyze and generate weights for terms in natural languages.,"a new term weighting method, which uses the term frequencies as components to define the relevance and the ambiguity of term;",,"fuzzy logic,
Neutrosophy",neural network (NN),sequential,1. symbolic Neuro symbolic,"BBC News database, BBC SPORTS database, Kaagle news ","SVM, feedforward network",Area Under the Curve (AUC),Upto 97%,77.78%,"Instead of directly using TF-IDF, a new weighting method is proposed. This combines the advantages of the neutrosophic reasoning and ambiguity membership function to determine the weight for a given term from a document. ",,,,eliminated
ar,A,Question Answering Systems with Deep Learning-Based Symbolic Processing,EGI547RA,10.1109/ACCESS.2019.2948081,IEEE Access,2019,Text classification,classification,supervised,Question answering = NMT ; Embedding for symbolic ; NMT -> Seq2Seq model and Transformer ; ,Prolog-like processing system using deep learning,,symbolic processing,transformer,sequential,1. symbolic Neuro symbolic,"Kinsources, Geoquery",Tranformers,"Accuracy, Rate",Upto 99%,55.56%,They tried to build prolog like system (in terms of unification and inference) using deep learning and achieved the same using NMT. Embedding is generated with the help of word2vec n graycode. Unification is also done using word2vec.  Data is reconsutrcuted using augmentation (proper nouns  are replicated).  ,,,,
ar,K,Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification,3YFVRRKE,10.1109/ACCESS.2020.2972751,IEEE Access,2020,Text classification,classification,supervised,Text -> Graph -> embedding ; attention mechanism at various levels to capture semantics,"graph-based document modeling, hierarchical transformer encoder architecture for feature extraction, and weight-directed loss for label classification.",,semantic net,"convolutional neural network (CNN),
recurrent neural network (RNN)",sequential,1. symbolic Neuro symbolic,"Reuters Corpus Volume 1, RCV1-2K, AmazonCat-13K",Graph embeddings ; CNN for feature extraction ; Transformer LSTM,"NDCG@K, p@k",Upto 95%,83.33%,"combination of graph-based document modeling and the hierarchical transformer. By capturing the logical structure in the text, performance is improved.",,,,
ar,A,Robust reasoning over heterogeneous textual information for fact verification,74QZV8X9,10.1109/ACCESS.2020.3019586,IEEE Access,2020,Fact verification,classification,supervised,KG + GNN ,Double-graph-based reasoning approach for claim verification.,,knowledge graph (KG),graph neural network (GNN),nested,2. Symbolic[Neuro],"FEVER, UKP Snopes Corpus","XLNet, Combination of knowledge graph (KG) and graph neural network (GNN), XLNet, Combination of knowledge graph (KG) and graph neural network (GNN) -> GCN + GAT ; ","F1, Accuracy",Upto 78%,72.22%,"Fact verification using pretrained embedding models ; CONTEXTUAL SEMANTIC GRAPH CONSTRUCTION G1 using AllenNLP(converts sentences into Tuples) finetuned using Wordnet -> CONTEXTUAL KNOWLEDGE GRAPH CONSTRUCTION G2 using conceptnet -> CONTEXTUAL COLLABORATIVE KNOWLEDGE GRAPH CONSTRUCTION modeled using XLNet, TransR (input - G1 + G2 output - prob )",VERY POOR QUALITY PAPER,,,
ar,K,Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier,BAMGPUCX,10.1109/ACCESS.2021.3053917,IEEE Access,2021,Sentiment analysis,classification,supervised,"Data cleaning and preprocessing, as well as the addtion of the fuzzy logic component (MFS) significantly improves performance on sentiment analysis tasks on Twitter datsets compared to SOTA. In addition, parallelization using hadoop map reduce significantly reduces computation time.","we develop a new hybrid fuzzy-deep learning approach, that basically integrates the CNN, FFNN deep learning networks with the MFS fuzzy logic system.",NA,"fuzzy logic,
fuzzy rules,
mamdani fuzzy system (MFS)","convolutional neural network (CNN),
neural network (NN)",sequential,1. symbolic Neuro symbolic,"sentiment140, sentiment140,
COVID-19_Sentiments","TREC,
SST-1,
MR,
SST-2","F1,
Accuracy,
Precision/Recall,
True Positive Rate (TPR), True Negative Rate (TNR) or Specificity, False Positive Rate (FPR), False Negative Rate (FNR), Error Rate (ER), Precision (PR), Classification Rate or Accuracy (AC), Kappa Statistic (KS), F1-score (FS) and Time Consumption (TC)",upwards of 99% F1 and accuracy,88.89%,"A pipeline is proposed for sentence level sentiment analysis on two Twitter datasets. The pipeline comprises of data cleaning and preprocessing (this effort alone reduces the error rate by an order of magnitude), learning embeddings, CNN+FFNN for classification, and a MFS (Mandani Fuzzy System) classification step. The algorithms are implemented on hadoop map reduce. The best models achieve F1 and accuracy scores above the SOTA, as well as improve computational efficiency and cost complexity. For the MFS, the choices of linguistic terms and fuzzy rules are made manually relying on domain expertise.  It would be interesting to see how well this system performs on less emotionally charged datasets. A future neural-symbolic direction might be to learn these ""parameters"" automatically with a view to a generalizable system. 
The system doesn't fall into any of the Kautz categories, as it's not strictly speaking an integration of neuro-symnolic techniques, but rather a sequential pipeline.",,,,
cp,K,Cases without borders: Automating knowledge acquisition approach using deep autoencoders and siamese networks in case-based reasoning,2-s2.0-85081087069,10.1109/ICTAI.2019.00027,"31st IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2019",2019,Question answering,recommendation,unsupervised,"This paper presents an enhanced version of DeepKAF, where a hybrid CBR approach performs automatic feature extractions and defines similarity measures across the entire textual case base.","enhancements in the DeepKAF architecture for hybrid Textual Case-based Reasoning. Using Skip-thought + MaLSTM (Manhatan siamese network).
",,case based reasoning (CBR),recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,Private,Skipthought & MaLSTM -> similarities across all cases.,p@k,,83.33%,"Improvemnet of the overall framework performance in terms of less training time, faster retrieval and more accurate results. The unsupervised approach of generating text representations beat the supervised approach. SImilarities between the learned representations and cases were more accurate. AE helped speeding up the overall model training process of and improved the overall retrieval time as well.
","I'm not sure if this can be considered Neuro-symbolic. The case base could be considered symbolic, but it's a stretch since all we are doing is comparing sub-symbolic representations.",,,
cp,K,Leveraging Recursive Processing for Neural-Symbolic Affect-Target Associations,2-s2.0-85073214635,10.1109/IJCNN.2019.8851875,"2019 International Joint Conference on Neural Networks, IJCNN 2019",2019,Sentiment analysis,classification,unsupervised,Using a rules to identify subtrees in a dependency parse so that each subtree can be evaluated using its own local context. Subtrees are fed into a BiLSTM for classification.,a novel approach of combining the recursive and structured sentiment parse provided by the recursive Tree-LSTM with symbolic rules to perform targeted affect labeling.,,rule based,recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,SemEval 2016,Dependency parse using Spacy -> rules based subtree extraction -> BiLSTM classification,Accuracy,67%,33.33%,"As far as I can make out, the symbolic component comprises of rules to extract relevant subtrees and to identify affective targets from a dependency parse. These subtrees are used as input to a BiLSTM classification model. For each subtree (target), a sentiment label is predicted. The result is interpretable given the rules based approach to the identification of targets and local context.","The paper is written with lots of jargon and run-on sentences and as such, it is unnecesarily difficult to understand. As in many other papers, the term eraoning is bandied about in a hand-wavy fashion, even though no actual reasoning is performed by the system. In other words, the papaer is padded with keywords such as 'commonsense', 'reasonning', and 'neuro-symbolic', but these are buzzwords rather than actual techniques used.",,,
cp,A,Causal relation classification using convolutional neural networks and grammar tags,2-s2.0-85083026315,10.1109/INDICON47234.2019.9028985,"16th IEEE India Council International Conference, INDICON 2019",2019,Cause-effect Identification,classification,supervised,Without using wordnet and word2vec it is possible to identify cause-effect pair with the help of grammar tags. ,,,constituency parsing,convolutional neural network (CNN),sequential,1. symbolic Neuro symbolic,SemEval-2010,The input is grammar tags of three sections and the parse tree distance between two nominals. Fully connected CNN produces binary value which indicated the presence of cause-effect relation in a given sentence.  ,F1,86%,66.67%,"A novel approach using only the grammar tags has been proposed for the classification of a cause-effect pair.  CNNs are trained iwth three different sections of a sentence. These sections are generated using parse tree in which 1st and 2nd nominal words are identified (before NW, B/w NW and after NW). The output layer has one output which gives binary value representing the presence of cause-effect relation in a given sentence.",,,,
cp,A,Using Deep Learning and an External Knowledge Base to Develop Human-Robot Dialogues,2-s2.0-85062227737,10.1109/SMC.2018.00628,"2018 IEEE International Conference on Systems, Man, and Cybernetics, SMC 2018",2019,Dialog system,reasoning,,,,,case based reasoning (CBR),recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,"Private, Personal Finance & Money site of the StackExchange service,
Insurance Library site,
Quora Question Pairs is retrieved from the Kaggle site
","LSTM -> question embedding; LSTM -> answer embedding; cos(question, answer)",prediction rate,not properly reported.,44.44%,,VERY POOR QUALITY PAPER,,,
ar,K,Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text,RDSQSBN7,10.1109/TASLP.2021.3082295,IEEE/ACM Transactions on Audio Speech and Language Processing,2021,Relation extraction,classification,supervised,the combination of 4 different modules improves overall performance for this task and supercedes the SOTA,a novel end-to-end model for the novel dialogue-based relation extraction task with code.,NA,"graph representation,
graph reasoning","attention network (BERT),
graph neural network (GNN)",cooperative,3. Neuro; Symbolic,DialogRE,the context encoder -> attention layer -> graph layer -> classifier,F1,64.89%,77.78%,"Attention using BERT followed by graph ""reasoning"" with GNN. The system exploits the structure of the data to extract relations. It is not clear what is actually being learned by the graph module, but the authors do provide a code repo. The representations learned by the first 3 modules facilitate the final classification task.",,,,
cp,A,Automated ontology-based annotation of scientific literature using deep learning,2-s2.0-85087103850,10.1145/3391274.3393636,"2020 International Workshop on on Semantic Big Data, SBD 2020 - In conjunction with the 2020 ACM SIGMOD/PODS Conference",2020,Annotation,classification,supervised,"Inclusion of ontology semantics via subsumption reasoning. Even with a small training dataset, the domain specific embeddings performed better than large pretrained language models",domain-specic embedding model trained on the CRAFT,,ontology,recurrent neural network (RNN),nested,4. Neuro: Symbolic → Neuro,CRAFT,"Three types of inputs are used - character, word, and part-of-speech (POS) embeddings. Character embeddings after a 30% dropout layer are passed through a 150 unit GRU model with a 50% recurrent dropout. The output from this GRU model is concatenated with word and part-of-speech embeddings is passed to a 200 unit bi-directional GRU unit.","F1,
Precision/recall,
Jaccard semantic similarity
",0.79 F1,61.11%,They have created their own embeddings based on CRAFT (Colorado Richly Annotated Full Text). Trained GRU and LSTM with multiple embedding techniques. As per theory semantically enriched.,"I did not understand the significance. 
I think the unique approach here is to encode the ontology heirarchy. However, it did not result in better performance. They provide some future directions to improve how this heirarchy is encoded.
I'm not sure I understand how they measured the accuracy when the top two predictions were considered. Did they pick one out of these two, or did they just score a hit if any of the two were a hit. If the latter, then it is not surprising that the accuracy has increased, and therefore I do not understand why this is a ""surprising"" result.",,,
cp,K,Web question answering with neurosymbolic program synthesis,2-s2.0-85108908770,10.1145/3453483.3454047,"42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2021",2021,Question answering,information extraction,semi-supervised,leverages the structure of HTML and Large pretrained language models to significantly outperform all the baselines ,new technique for web question answering that is based on optimal neurosymbolic program synthesis. a DSL for web information extraction that combines pre-trained NLP models with traditional language constructs for string manipulation and tree traversal.,,"domain specific language (DSL),
transductive learning,
program synthesis",neural network (NN),nested,2. Symbolic[Neuro],Private,NN -> DSL -> informatin extraction,"F1, Precision/recall",[0.7 - 0.9] - varies per task. much higher than baselines,88.89%,"the sytem is made up of existing NLP models (like BERT), for tasks such as keyword extractiion and NER, and a custom DSL for traversing the HTML structure of web pages. The DSL is selected via transductive learning on unlabeled web pages.",,,,
cp,K,A novel NLP-fuzzy system prototype for information extraction from medical guidelines,2-s2.0-85070277710,10.23919/MIPRO.2019.8756929,"42nd International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2019",2019,Information extraction,classification,supervised,"Our NLP-FUZZY system prototype performs a semantic extraction of a medical guideline using Bi-directional Long short-term memory (bi-LSTM) in the first step. In the second step, using the extracted semantic classes, the NLP-FUZZY system creates fuzzy rules and a database that recognizes a new case while predicting a priority of a recommendation.",Our NLP-FUZZY system prototype allows the classification of recommendations by semantic meaning and prediction of the recommendation grade.,,"fuzzy logic,
fuzzy rule",recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,"Medical Guideline Central, word2vec -> lstm (5 class prediction for each word) -> 125 successfully obtained fuzzy rules in the fuzzy rules database in MATLAB -> defuzzification
",LSTM -> Fuzzy logic,F1,12 points above SOTA LSTM only system.,66.67%,"First a NN predicts 1 of 5 classes for each word, then a fuzzy rule set is generated broadly based on word frequencies as far as I can understand. The defuzzification process results in a score for each recommendation.",,,,
cp,K,"The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",2-s2.0-85083954234,,"7th International Conference on Learning Representations, ICLR 2019",2019,Question answering,reasoning,reinforcement ,joint learning of vision and natural language (semantic query representations) with zero annotations.,"neuro-symbolic concept learner (NS-CL), which jointly learns visual perception, words, and semantic language parsing from images and question-answer pairs",,domain specific language (DSL),"curriculum learning,
recurrent neural network (RNN),
reinforcement learning (RL)",cooperative,3. Neuro; Symbolic,CLEVR,Object detection to latent representations -> semantic parsing of question using DSL (GRU) -> Quasi symbolic program executor,Accuracy,almost perfect,83.33%,"This work acheives SOTA results without explicit annotations or class labels. It also does so even on a small subset of the data (10%), where other approaches degrade in accuracy by 30% or more. The limitation is the hand crafted DSL and no improvements in accuracy when the data is non-trivial, such as images from real life (as opposed to the synthetic world of CLEVR which contains only basic 3d shapes and colors).",,,,
cp,A,Team SVMrank: Leveraging feature-rich support vector machines for ranking explanations to elementary science questions,2-s2.0-85085038180,,"13th Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs 2019, in conjunction with the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",2019,Question answering,classification,supervised,Ranking of explantions for answers ; Dataset used is highschool/primary science questions. Model is built using SVM which is again refined by rule based technique.,,,rule based,SVM,sequential,1. symbolic Neuro symbolic,MIER-19 ,"SVM -> Rules ; Model is trained by considering (Q, A) as features and E as label. To rank the answers, rule based technique is used.","Mean Average Precision, MAP)",39%,55.56%,"The MIER-19 task focused on computing a ranked list of explanation sentences for a question and its correct answer (QA) from an unordered collection of explanation sentences. Ranking is computed using features such as lexical, relations (from OpenIE) and semantics (ConceptNet). Later pairwise relations are learned and model is built as binary classification model. ",Part of Multi-Hop  Inference for Explanation Regeneration (MIER) 2019 challenge. ,,,
cp,K,Attentive tensor product learning,2-s2.0-85080593464,,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",2019,Image captioning,generative,supervised,"ATPL exploits Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, to integrate deep learning with explicit language structures and rules.",a novel attention-based neural network architecture for learning the unbinding vectors ut to serve the core at ATPL,,constituency parsing,"recursive neural network (RNN),
Tensor Product Representation",compiled,4. Neuro: Symbolic → Neuro,"COCO, Penn TreeBank",TPR combined with LSTM with attention mechanism,"BLEU, METEOR, CIDEr",improves on LSTM,77.78%,"The purpose of ATPR is to replace the LSTM core layers. Future work will incorporate this into better performing systems. Our findings in this paper show great promise of TPRs. The model has a novel architecture based on a rationale derived from the use of Tensor Product Representations for encoding and processing symbolic structure through neural network computation.

",,,,
cp,K,Hybrid deep neural networks to predict socio-moral reasoning skills,2-s2.0-85081092649,,"12th International Conference on Educational Data Mining, EDM 2019",2019,Text classification,classification,supervised,combine a priori knowledge with deep learning architecture using the attentional mechanism,incorporating expert knowledge via an attention mechanism into a DNN,,semantic enrichment,"recurrent neural network (RNN),
convolutional neural network (CNN),
attention network",compiled,4. Neuro: Symbolic → Neuro,SoMoral,text -> LSTM (with attention which holds the expert knowledge) -> prediction,F1,0.73,44.44%,"The authors combine expert knowledge - a vector of a priori weights or each class. This vector is used to weight the importance of the DNN hidden layers. The main improvement over SOTA is due to the fact that there are separate models learned for each class in addition to the expert knowledge, while the attention mechanism provides only a minimal improvement. Also, this study is self referencial. The results are only compared with the authors' own prior research, and are not put into a braoder context of research.",,,,
cp,K,Graph enhanced cross-domain text-to-SQL generation,2-s2.0-85085049747,,"13th Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs 2019, in conjunction with the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",2019,Translation,generative,unsupervised,transaltion of text to sql queries. A database graph is constructed and a graph neural network is used to encode the columns.,"a novel column embedding technique that additionally includes a graph of the tables, connected through shared column names.",,"symbolic node embedding,
graph representation,
semantic parsing",graph neural network (GNN),compiled,4. Neuro: Symbolic → Neuro,Spider,"create graph from column names, table names, and column types from multiple databases -> GNN learn node representations -> feed into SyntaxSQLNet",Accuracy,57%,72.22%,"Our model is based on the framework of SyntaxSQLNet (Yu et al., 2018b) but extends it with our approach for generating column embeddings. Our idea is to use a graph to connect the column names across all tables and databases and compute representations of them by using a graph neural network. In this way, the information of different domains is passed to each other, so that one can learn a better column embedding that generalizes across domains.  (I am calssifying this as unsupervised because the part of the system which this study describes is unsupervised. However, the system as a whole is supervised).
The results show that the graph representations outperform vanilla SyntaxSQLNet, but do not perform as well as a data-augmentation technique. However, the data augmentation technique relies on expensive hand-annotated data, whereas the graph technique does not have this overhead.
","I am not sure if this is neuro-symbolic. The symbolic part is the graph. Embeddings are learnded which capture the relationships between nodes in a sub-symbolic representation. However, there is no reasoning component.",,,
cp,K,Mapping natural-language problems to formal-language solutions using structured neural representations,2-s2.0-85105153943,,"37th International Conference on Machine Learning, ICML 2020",2020,Textual reasoning,"reasoning,
generative",supervised,"Encoder binding, decoder unbinding. State-of-the-art performance on two recently developed N2F tasks shows that the TPN2F model has significant structure learning ability on tasks requiring symbolic reasoning through program synthesis.","a new scheme for neural-symbolic relational representations and a new architecture, TP-N2F, for formal-language generation from natural-language descriptions.",,"Tensor Product Representation,
binding problem",recurrent neural network (RNN),compiled,4. Neuro: Symbolic → Neuro,MathQA,natural language to TPR -> LSTM encoder ( binding) -> LSTM decoder (unbinding) -> formal language,Accuracy,71.89%,72.22%,"Natural- to formal-language generation. Using tensor product representations in an emcoder-decoder architecture.
",,,,
cp,K,Just Add Functions: A Neural-Symbolic Language Model,2-s2.0-85106687657,,"34th AAAI Conference on Artificial Intelligence, AAAI 2020",2020,Textual reasoning,generative,supervised,"adding explicit logic functions to a language model improves performance especially on small datasets. It does this by enhancing the inductive bias of NNLM. In other words, it makes the assumption that ""you are the company you keep"" (refered to as the distributional hypothesis of language) even stronger.","NSLM - neuro symbolic language model. A method for embedding domain specific logic and/or knowledge into a language model, thus enhancing the models ability to predict words in said domain. The method can be extended to any domain as long as the architect can find appropriate logical expressions for her domain. A Neural-Symbolic Language Model (NSLM) is a hierarchical NNLM that incorporates simple functions to enhance inductive bias.",,"rule based,
semantic enrichement",recurrent neural network (RNN),nested,4. Neuro: Symbolic → Neuro,Wikitext,"1. Identify a class C, which has an inductive bias that can be encoded as a simple mathematical or logical expression. 
2. generate vocabulary for class C 
3. Add/delete candidate metric functions fc(w(c) t , w(c) t−1) and PDFs to be considered. 
4. Train a hierarchical NNLM on words and class labels and select a {metric function,PDF} pair.
",perplexity,outperforms NHLM but not LLM.,66.67%,"We seek to establish a general method to tune language models for domain-specific tasks by enhancing the inductive bias with symbolic expressions that encode domain knowledge (Grounding with External Knowledge). 
A NNLM is finetuned with a ""micro-model"" which is a function (learned by grid search) over a class distribution obtained from an external knowledge base.
It's a language model, so it can be used for sentence completion, measuring similarity, and other downstream tasks. ",,,,