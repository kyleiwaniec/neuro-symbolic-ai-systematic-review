Doc type,citation,Title,Abstract,Scopus Hash ID,DOI,Journal/Conference,Year,Business Use Case,Technical Application,Type of Learning,Key-intake,Contribution,Authors Definition of NeSy,Symbolic terms,Neural terms,NeSy Category,Kautz category,Datasets,Model description,Evaluation Metrics,Reported Score,Study Quality,Comments,MISC,leverages natural language structure,leverages relational structure,Knowledge representation,implicit vs explicit,reasoning,out-of-distribution generalization,interpretability,reduced data,transferability,NeSy (check if the authors label their work as NeSy),total,,,
ar,\cite{Pacheco_Goldwasser_2021},Modeling Content and Context with Deep Relational Learning,"Building models for realistic natural language tasks requires dealing with long texts and accounting for complicated structural dependencies. Neural-symbolic representations have emerged as a way to combine the reasoning capabilities of symbolic methods, with the expressiveness of neural networks. However, most of the existing frameworks for combining neural and symbolic representations have been designed for classic relational learning tasks that work over a universe of symbolic entities and relations. In this paper, we present DRaiL, an open-source declarative framework for specifying deep relational models, designed to support a variety of NLP scenarios. Our framework supports easy integration with expressive language encoders, and provides an interface to study the interactions between representation, inference and learning.",https://aclanthology.org/2021.tacl-1.7,10.1162/tacl_a_00357,Transactions of the Association for Computational Linguistics,2021,Language modeling,classification,supervised,"DRAIL uses a declarative language for defining deep relational models. It allows users to inject their knowledge by specifying dependencies between decisions using first-order logic rules, which are later compiled into a factor graph with neural potentials. In addition to probabilistic inference, DRAIL also models dependencies using a distributed knowledge representation, denoted RELNETS, which provides a shared representation space for entities and their relations, trained using a relational multi-task learning approach.","The main difference between DRAIL and these languages is that, in addition to graphical models, it uses distributed knowledge representations to represent dependencies.",,"first order logic (FOL),
Grounding,
factor graph","long short term memory (LSTM),
TransE,
transformer,
recurrent neural network (RNN)",nested,2. Symbolic[Neuro],debate.org,,F1,a little better than baselines,1,this was a tough read.,"they have a nice summary of NeSy architectures. Model exmplified on Stance prediction, Argumentation mining",FALSE,TRUE,logical,explicit,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,2,,,
cp,\cite{Chen_Gao_Moss_2021},{N}eural{L}og: Natural Language Inference with Joint Neural and Logical Reasoning,"Deep learning (DL) based language models achieve high performance on various benchmarks for Natural Language Inference (NLI). And at this time, symbolic approaches to NLI are receiving less attention. Both approaches (symbolic and DL) have their advantages and weaknesses. However, currently, no method combines them in a system to solve the task of NLI. To merge symbolic and deep learning methods, we propose an inference framework called NeuralLog, which utilizes both a monotonicity-based logical inference engine and a neural network language model for phrase alignment. Our framework models the NLI task as a classic search problem and uses the beam search algorithm to search for optimal inference paths. Experiments show that our joint logic and neural inference system improves accuracy on the NLI task and can achieve state-of-art accuracy on the SICK and MED datasets.",https://aclanthology.org/2021.starsem-1.7,10.18653/v1/2021.starsem-1.7,,2021,Reading comprehension,generative,supervised,"Universal dependecy parser: Stanza,","First, we propose a new framework for combining logic-based inference with deeplearning-based network inference for better performance on conducting natural language inference. We model an NLI task as a path-searching problem between the premises and the hypothesis. We use beam-search to find an optimal path that can transform a premise to a hypothesis through a series of  inference steps.",,dependency parsing,transformer,nested,2. Symbolic[Neuro],SICK,"Sentence-BERT, BERT-large model
pre-trained on STS-B. ALBERT, ALBERT-base model
pretrained on MRPC from transformers. For word
alignment in the controller, Gensim framework to calculate
word similarity from pre-trained word embedding.",Precision/recall,Upto 90,0.6666666667,"Main goal is to convert a premise into hypothesis (NLI). Monotonicity (polarity) of each sentence is marked by Udep2mono, followed by various sentences are inferred by lexical (comparing polarised words with conceptnet and wordnet), phrasal (generation of trees) and syntactical (using transformer models). This search space is searched by beam search (sentence bert) to identify a optimal inference. Sentence inference controller (sentence representation graph, graph alignment, recommendation engine) helps to select the most appropriate inference.",We first pass the chunk pair to ALBERT to obtain the logits. Then we apply a softmax function to the logits to get the final probability.,FALSE,FALSE,frames,explicit,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,4,,,
cp,\cite{Saveleva_Petukhova_Mosbach_Klakow_2021},Graph-based Argument Quality Assessment,"The paper presents a novel discourse-based approach to argument quality assessment defined as a graph classification task, where the depth of reasoning (argumentation) is evident from the number and type of detected discourse units and relations between them. We successfully applied state-of-the-art discourse parsers and machine learning models to reconstruct argument graphs with the identified and classified discourse units as nodes and relations between them as edges. Then Graph Neural Networks were trained to predict the argument quality assessing its acceptability, relevance, sufficiency and overall cogency. The obtained accuracy ranges from 74.5{\%} to 85.0{\%} and indicates that discourse-based argument structures reflect qualitative properties of natural language arguments. The results open many interesting prospects for future research in the field of argumentation mining.",https://aclanthology.org/2021.ranlp-1.143,,,2021,Argumentation mining,classification,supervised,Argument graph reconstruction involves: (1) segmentation of a text into EDUs; (2) discourse relation detection and classification between them; (3) identification and classification of ADUs based on the classified discourse relations; and (4) argument completion,"A novel approach to assessing  the structural  strength and inferential weakness of arguments as merits of argument cogency. The approach relies on the discourse-based reconstruction of argumentation schemes. SOTA discourse parsers are applied to this and machine learning models to reconstruct argument graphs where the identified discourse units are represented as nodes and the classified discourse relations between them as edges. A Graph Neural Network (GNN) model is built to predict the quality (low vs high) of the reconstructed argument grphs in terms of argument acceptability, relevance,  sufficiency and overall cogency.",,semantic parsing,graph neural network (GNN),compiled,4. Neuro: Symbolic → Neuro,Dagstuhl15512 ArgQuality,Argument graph is constructed from argument text by parsing. These graphs are checked for quality using GNNs. Multiple settings are considered for evaluation purpose. 1. Only graph structure 2. Glove embedding of EDU 3. BERt embedding of EDU,F1,Upto 65,0.7222222222,Argument graph is reconstructed from argument text by parsing the text.  These argument graphs are checked for cogency using GNN.,"Argumentative Discourse Units (ADUs) – text segments corresponding to propositions that are argumentatively relevant and have their own argumentative function. An EDU can function as a claim, as an evidence or as a conclusion.
We marked this as Rules for KR even though the input to the GNN is an argument graph (semantic net). The rules are the discourse relations used for argument graph construction.",TRUE,TRUE,rules,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,They haven't tested on large dataset but for training they have considered small dataset.,,
cp,\cite{Gupta_Ghosal_Ekbal_2021},A Neuro-Symbolic Approach for Question Answering on Research Articles,"The number of research articles is increasing exponentially. It has become difficult for researchers to stay updated with the latest development in science with the deluge of papers. Hence, keeping abreast with the current literature is one of the most significant challenges to present-day researchers. However, if one can query a scientific article, they can quickly comprehend it and elicit the required information. Hence, a question-answering (QA) system on scholarly articles would be a helpful assistant for researchers to survey the literature. Recently logic-infused deep networks have been showing good  romise for solving several downstream NLP tasks. Here in this paper, we implement a neural network-based symbolic approach for QA on scholarly articles. We incorporate logical boolean functions into the deep network, significantly  mproving the model’s performance without additional parameters. Further, we reduce the dependency on domain-specific training data by using external knowledge from the ConceptNet. We perform our experiments on the benchmark  ScholarlyRead dataset and achieve significant performance improvement (∼ double) over the  baseline approach. We would make our codebase available here 1",https://aclanthology.org/2021.paclic-1.5,,,2021,Information extraction,information extraction,supervised,augmenting the network with conceptnet improves model performance,"Our main contribution in the current work is to build a robust QA system by injecting logical rules into a deep neural framework that achieves state-of-the-art performance for the benchmark ScholaryRead dataset (Saikh et al., 2020).",,"first order logic (FOL),
ontology","recurrent neural network (RNN),
BiDAF",compiled,4. Neuro: Symbolic → Neuro,ScholarlyRead,augmented BiDAF. this is a bi redirectional LSTM with extra neurons with logic functions for conceptnet relations,F1,2x of original BiDAF,0.5555555556,"introducing conceptnet and logic into the network nodes is an interesting approach. Unfortunately, the authors do not comapre their system to other QA benchmarks. The authors do not address any promises. There is a link to a github repo, but the repo is empty.","I originally labeled this L5 because the logic rules are constraints on the nodes. That's not strictly how L5 is defined by Kautz where loss function is constrained by logic. Perhaps this should be labeled L4 - either way, it's ""compiled"".
KBANN,CILP - constrain the model parameters (L4)",FALSE,TRUE,logical,explicit,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,0,,,
cp,\cite{Langton_Srihasam_2021},Applied Medical Code Mapping with Character-based Deep Learning Models and Word-based Logic,"Logical Observation Identifiers Names and Codes (LOINC) is a standard set of codes that enable clinicians to communicate about medical tests. Laboratories depend on LOINC to identify what tests a doctor orders for a patient. However, clinicians often use site specific, custom codes in their medical records systems that can include shorthand, spelling mistakes, and invented acronyms. Software solutions must map from these custom codes to the LOINC standard to support data interoperability. A key challenge is that LOINC is comprised of six elements. Mapping requires not only extracting those elements, but also combining them according to LOINC logic. We found that character-based deep learning excels at extracting LOINC elements while logic based methods are more effective for combining those elements into complete LOINC values. In this paper, we present an ensemble of machine learning and logic that is currently used in several medical facilities to map from",https://aclanthology.org/2021.naloma-1.2,,,2021,Annotation,information extraction,supervised,Combined character based GRU with logical statements,,,case based reasoning (CBR),recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,LOINC,Character based GRU is modeled to extract codes from noisy text. Followed by kind of priority based if else is applied to identify most suitable matching element and codes are generated. Based on weighted average of final code is selected.,Accuracy,Upto 90%,0.4444444444,Poor writing. Logical component is not that great. Even though authors mention that they have a neural component followed by inference engine. I belive instead of mentioning it as inference engine authors could use CBR. Inference engine is basically working of if else (I would say cbr) statements followed by weighted average.,,TRUE,FALSE,logical,explicit,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,0,,,
cp,\cite{Zhou_Richardson_2021},Temporal Reasoning on Implicit Events from Distant Supervision,"We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events{---}events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SymTime, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. SymTime outperforms strong baseline systems on TRACIE by 5{\%}, and by 11{\%} in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1{\%}-9{\%} on MATRES, an explicit event benchmark.",https://aclanthology.org/2021.naacl-main.107,10.18653/v1/2021.naacl-main.107,,2021,Causal Reasoning,reasoning,supervised,The goal of TRACIE is to test a system’s ability to compare start and end times of non-extractive implicit  event phrases instead of extractive triggers from the context.  Label for the dataset is whether implicit event occured before/after (temporal relationship) explicit event.,"Dataset TRACIE is introduced (crowd sourcing). It consists 1. Context story 2. implicit event 3. explicit event 4. comparator 5. Label, temporal relation",,temporal logic,"recurrent neural network (RNN),
transformer",compiled,4. Neuro: Symbolic → Neuro,TRACIE,They have built two models PTNTIME and SYMTIME. PTNTIME; a pre-trained sequence-to-sequence (RNN) model as our base model and additionally pre-train this model using TRACIE. Maily PTNTIME learns temporal sequence and adjusts the weights. SYMTIME is transformer based model which considers PTNTIME weights.,Accuracy,Upto 75%,0.7222222222,"A transfomer model is pretrained on the dataset to understand distance between implicit and explicit events. Semantic Role Labelling of AllenNLPs is used to identify temporal verbs. PTNTIME serves as new set of temporally-aware model weights that can be used in place of existing pre-trained models and finetuned on TRACIE (Basically cts as generating embeddings for TRACIE dataset). SYMTIME, makes end-time comparisons by symbolically combining start time distance and duration from separate predictions.",,TRUE,FALSE,rules,implicit,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,2,generalization,,
cp,\cite{Verga_Sun_Baldini_2021},Adaptable and Interpretable Neural {M}emory{O}ver Symbolic Knowledge,"Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a {``}fact memory{''}. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5{\%} of the parameters. Most interestingly, we demonstrate that the model can be modified, without \textit{any} re-training, by updating the fact memory.",https://aclanthology.org/2021.naacl-main.288,10.18653/v1/2021.naacl-main.288,,2021,Question answering,information extraction,supervised,"LM is augmented with KB facts in memory modules indoe the transformer architecture. Through a variety of data filtering techniques, the authors show that augmenting with facts improves perfomance on out-of-distribution inputs compared to LLMs which ostensibly memorize the training set and are thus not able to generalize OOD.","FILM, a neural LM with an interpretable symbolically bound fact memory.",,knowledge graph (KG),transformer,compiled,4. Neuro: Symbolic → Neuro,"WebQuestionsSP,
TriviaQA",symbolic knowledge is embedded inside a transformer,Accuracy,significantly above baselines,0.7777777778,27 point increase in accuracy in some scenarios while using only 5% of the parameters (much smaller model),,FALSE,TRUE,semantic network,implicit,FALSE,TRUE,FALSE,FALSE,FALSE,TRUE,1,,,
cp,\cite{Zhang_Wang_Yu_Wang_Wang_Jiang_Lim_2021},{NOAHQA}: Numerical Reasoning with Interpretable Graph Question Answering Dataset,"While diverse question answering (QA) datasets have been proposed and contributed significantly to the development of deep learning models for QA tasks, the existing datasets fall short in two aspects. First, we lack QA datasets covering complex questions that involve answers as well as the reasoning processes to get them. As a result, the state-of-the-art QA research on numerical reasoning still focuses on simple calculations and does not provide the mathematical expressions or evidence justifying the answers. Second, the QA community has contributed a lot of effort to improve the interpretability of QA models. However, they fail to explicitly show the reasoning process, such as the evidence order for reasoning and the interactions between different pieces of evidence. To address the above shortcoming, we introduce NOAHQA, a conversational and bilingual QA dataset with questions requiring numerical reasoning with compound mathematical expressions. With NOAHQA, we develop an interpretable reasoning graph as well as the appropriate evaluation metric to measure the answer quality. We evaluate the state-of-the-art QA models trained using existing QA datasets on NOAHQA and show that the best among them can only achieve 55.5 exact match scores, while the human performance is 89.7. We also present a new QA model for generating a reasoning graph where the reasoning graph metric still has a large gap compared with that of humans, eg, 28 scores.",https://aclanthology.org/2021.findings-emnlp.350,10.18653/v1/2021.findings-emnlp.350,,2021,Question answering,reasoning,supervised,"Intepretebility, reasoning are not provided by existing QA models for complex maths questions. Task is to generate answers for all questions in conversations. given a background passage P, a history conversation QA1:(t−1) and the next question Qt , the task is to return a textual answer At to the next question Qt and generate a reasoning graph Gˆrt.","For evaluating of reasoning graph, we introduce an automatic evaluation method named DAGsim, considering the structural and semantic similarity at the same time between the predicted and ground-truth reasoning graphs. To facilitate the research along with reasoning graph, we also contribute a new model named Reasoning Graph Network (RGNet) for generating the reasoning graph.",,graph representation,convolutional neural network (CNN),compiled,4. Neuro: Symbolic → Neuro,"NOAHQA, DropQA, Math23k","three compononets in RGNet. 1. Encoder: segments in passage followed by ground truth QA followed by actual question. 2. Reasoning graph: A graph is built by considering question as root node, and BFS is used to construct RG. BFS traverses untill it finds passages in the leaf node.  GCN is applied to update weights of edges and nodes, in the last layer edge classification is applied. 3. Prediction model. 6 types of questions are divided into 2 categories, which is predicted by prediction model for further implementation (solving the math).",DAG_sim,Upto 64%,0.6666666667,"4 levels of prediction 1. answer type prediction, final layer to predict span or PGNet (span prediction for locating text span, and PGNet for producing arithmetic expressions) 2. Span prediction 3. Sequence generation (P1,P2, ... passage generation from text) 4. Edge classifier",,TRUE,TRUE,semantic network,explicit,TRUE,FALSE,TRUE,TRUE,TRUE,FALSE,4,,,
cp,\cite{Chaudhury_Sen_Ono_2021},Neuro-Symbolic Approaches for Text-Based Policy Learning,"Text-Based Games (TBGs) have emerged as important testbeds for reinforcement learning (RL) in the natural language domain. Previous methods using LSTM-based action policies are uninterpretable and often overfit the training games showing poor performance to unseen test games. We present SymboLic Action policy for Textual Environments (SLATE), that learns interpretable action policy rules from symbolic abstractions of textual observations for improved generalization. We outline a method for end-to-end differentiable symbolic rule learning and show that such symbolic policies outperform previous state-of-the-art methods in text-based RL for the coin collector environment from 5-10x fewer training games. Additionally, our method provides human-understandable policy rules that can be readily verified for their logical consistency and can be easily debugged.",https://aclanthology.org/2021.emnlp-main.245,10.18653/v1/2021.emnlp-main.245,,2021,Text games,reasoning,reinforcement,LNN behave as a logical conjunction connective with the forward function as the weighted Łukasiewicz t-norm.,A method for interpretable policy learning in text-based games from  symbolic state representation.,,rule induction,Logical Neural Network,compiled,4. Neuro: Symbolic → Neuro,Coin collecter,Exctration of symbolic facts -> (Symbolic MLP / LNN) { LNN in combination with lifted symbolic state and unlifted rules),Average success rate,Upto 100%,0.7777777778,"A neuro-symbolic approach for action policy learning in text-based  games using a differentiable rule learner from first-order symbolic inputs. Symbolic data is represeted in terms of triples. Depending on reward for vising each room directions are given to move. Ultimate goal is to exit the board from starting room with a huge reward. Directions are given as go, take. Two learning models are built i.e. Symbolic MLP and Logical Neural network.","Lifted models define patterns from which specific (ground) models can be unfolded.
Our goal is to learn symbolic rules as logical connectives for generating action commands by gradient-based training. We present a symbolic rule learning framework using both MLP with symbolic inputs and Logical Neural Network (LNN)
Also uses ConcetNet to extract structures from the input text.",TRUE,TRUE,logical,implicit,TRUE,TRUE,TRUE,TRUE,TRUE,TRUE,5,zero shot,,
cp,\cite{Shi_Ding_Du_Liu_Qin_2021},Neural Natural Logic Inference for Interpretable Question Answering,"Many open-domain question answering problems can be cast as a textual entailment task, where a question and candidate answers are concatenated to form hypotheses. A QA system then determines if the supporting knowledge bases, regarded as potential premises, entail the hypotheses. In this paper, we investigate a neural-symbolic QA approach that integrates natural logic reasoning within deep learning architectures, towards developing effective and yet explainable question answering models. The proposed model gradually bridges a hypothesis and candidate premises following natural logic inference steps to build proof paths. Entailment scores between the acquired intermediate hypotheses and candidate premises are measured to determine if a premise entails the hypothesis. As the natural logic reasoning process forms a tree-like, hierarchical structure, we embed hypotheses and premises in a Hyperbolic space rather than Euclidean space to acquire more precise representations. Empirically, our method outperforms prior work on answering multiple-choice science questions, achieving the best results on two publicly available datasets. The natural logic inference process inherently provides evidence to help explain the prediction process.",https://aclanthology.org/2021.emnlp-main.298,10.18653/v1/2021.emnlp-main.298,,2021,Question answering,classification,supervised,combining natural logic with neural networks for interpretable question answering,"1) We introduce a novel framework NeuNLI, which combines the advantages of natural logic and deep neural networks for question answering. (2) Our proposed model provides step-by-step explanation for how the prediction was derived. (3) The proposed model achieves new state-of-the-art performance on two QA datasets.",,Natural Logic,transformer,cooperative,3. Neuro; Symbolic,"QA-S,
QA-L",input text -> extract candidate hypotheses using cos sim of Glove -> remove stopwords and other types of words with the help of POS tags -> use BERT masked words to find replacements -> NatLog to filter out erronuous mutations ->,F1,higher than baselines,0.7777777778,this work build on Manning. the whole pipeline seems pretty complicated and computaional cost is not reported.,,TRUE,FALSE,logical,explicit,FALSE,FALSE,TRUE,FALSE,FALSE,TRUE,1,,,
cp,\cite{Das_Zaheer_Thai_2021},Case-based Reasoning for Natural Language Queries over Knowledge Bases,"It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions {---} a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11{\%} on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases \textit{without} any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.",https://aclanthology.org/2021.emnlp-main.755,10.18653/v1/2021.emnlp-main.755,,2021,Question answering,information extraction,semi-supervised,using a combination of ocs similarity and nearest neighbor to find the logical forms of natural language queries. these LF are then revised based on neural KB completion techniques,"(a) We present a neural CBR approach for KBQA capable of generating complex logical forms conditioned on similar retrieved questions and their logical forms. (b) Since CBR-KBQA explicitly learns to reuse cases, we show it is able to generalize to unseen relations at test time, when relevant cases are provided. (c) We also show the efficacy of our revise step of CBR-KBQA which allows to correct generated output by aligning it to local neighborhood of the query entity. (d) Lastly, we show that CBR-KBQA significantly outperforms other competitive models on several KBQA benchmarks.",,case based reasoning (CBR),"TransE,
transformer",cooperative,3. Neuro; Symbolic,COMPLEXWEBQUESTIONS,"train a model using natural language and lofgical form pairs. where the input for inference is a natural language query and the output is an embedding of a logical form. next, find most similar embeddings of logical forms. use the LF to query a KB. Revise: vector space alignment between generated relation with actual KB relations.",F1,significantly above baselines,0.7777777778,"training data consists of pairs of natural language queries and their logical forms (eg. SPARQL query). cosine sim is used to extract similar queries using RoBerta. CBR-KBQA derives the logical form (LF) for a new query from the LFs of other retrieved queries from the case-memory. However, the derived LF might not execute because of missing edges in the KB. The revise step aligns any such missing edges (relations) with existing semantically-similar edges in the KB. 

A limitation of this approach is the need for logical forms of many queries for the case base, and a KB relevant to the domain.",,FALSE,FALSE,rules,explicit,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,2,,,
ar,\cite{Wang_Pan_2021},Variational Deep Logic Network for Joint Inference of Entities and Relations,"Abstract Currently, deep learning models have been widely adopted and achieved promising results on various application domains. Despite their intriguing performance, most deep learning models function as black boxes, lacking explicit reasoning capabilities and explanations, which are usually essential for complex problems. Take joint inference in information extraction as an example. This task requires the identification of multiple structured knowledge from texts, which is inter-correlated, including entities, events, and the relationships between them. Various deep neural networks have been proposed to jointly perform entity extraction and relation prediction, which only propagate information implicitly via representation learning. However, they fail to encode the intensive correlations between entity types and relations to enforce their coexistence. On the other hand, some approaches adopt rules to explicitly constrain certain relational facts, although the separation of rules with representation learning usually restrains the approaches with error propagation. Moreover, the predefined rules are inflexible and might result in negative effects when data is noisy. To address these limitations, we propose a variational deep logic network that incorporates both representation learning and relational reasoning via the variational EM algorithm. The model consists of a deep neural network to learn high-level features with implicit interactions via the self-attention mechanism and a relational logic network to explicitly exploit target interactions. These two components are trained interactively to bring the best of both worlds. We conduct extensive experiments ranging from fine-grained sentiment terms extraction, end-to-end relation prediction, to end-to-end event extraction to demonstrate the effectiveness of our proposed method.",https://aclanthology.org/2021.cl-4.26,10.1162/coli_a_00415,Computational Linguistics,2021,Information extraction,classification,supervised,knowledge is encoded via learned rules and/or manually designed ones. The improvement in performance is attributed to the automatically learned rules and EM algorithm,"novel marriage between deep feature learning and relational logic reasoning, named Variational Deep Logic Network (VDLN), for joint inference in the IE domain.
the logic reasoning module is flexible enough to achieve both rule learning given some simple rule templates and integration of predefined logic rules.
our proposed model is able to learn different combinations of logic atoms to form the rules and it is also flexible to incorporate predefined knowledge. Moreover, our EM training algorithm alternates between an inference step and a learning step to achieve mutual enhancement which globally enforces the learning of both modules",,first order logic (FOL),transformer,cooperative,3. Neuro; Symbolic,SemEval 2014,text input -> E-step transformer: output: sequence of probailisitic labels -> M-step Logic netwrok: output: final probabilistic evaluations,F1,slightly better than baselines,0.8888888889,"FOL via t-norms. Performance improves with larger transformer models. Without any of the other NeSy goals being met, I'm not sure the extra complexity is worth the small performance gains. Future work: ""We also plan to design more interpretable networks in terms of logic reasoning so that the learned rules can be explicitly explained.""","Aspect and Opinion Extraction,
Relation Extraction,
Event Extraction",TRUE,FALSE,logical,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,1,,,
ar,\cite{Kouris_Alexandridis_Stafylopatis_2021},Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization,"Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.",https://aclanthology.org/2021.cl-4.27,10.1162/coli_a_00417,Computational Linguistics,2021,Text summarization,generative,unsupervised,"the said framework is an appropriate sequence-to-sequence deep learning neural network, aided by a knowledge-based methodology",augmenting input data with ontological inofrmation and POS and NER for word sense diambiguation,,"knowledge graph (KG),
ontology","recurrent neural network (RNN),
transformer",sequential,1. symbolic Neuro symbolic,"Gigaword,
DUC 2004,
CNN/DailyMail","(i) the pre-processing task, (ii) the machine learning methodology, and (iii) the post-processing task. The first step achieves text generalization through the utilization of knowledge-based semantic ontologies and named entity recognition (NER) in order to extract named entities, concepts, and senses from the original document. Subsequently, the generalized text is provided to a seq2seq deep learning model of an attentive encoder-decoder architecture, which learns to predict a generalized version of the summary. Lastly, the post-processing task creates the final summary, based on heuristic algorithms and text similarity metrics that match the concepts of the generalized summary to specific ones.",ROUGE,better than baselines,0.7777777778,"a novel knowledge-based framework that enhances neural-based abstractive Text Summarization, by combining characteristics of structure and semantic-based methodologies. In particular, in the current work, the content generalization methodology has been redesigned through the inclusion of Word Sense Disambiguation for more accurate concept generalization, assuming new generalization strategies.

The symbolic part comes in the preprocessing phase. Linguistic structure information is concatenated with words from which embeddings are learned using a BOW NN.",,TRUE,TRUE,semantic network,implicit,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,0,,,
cp,\cite{Li_Srikumar_2019},Augmenting Neural Networks with First-order Logic,"Today, the dominant paradigm for training neural networks involves minimizing task loss on a large dataset. Using world knowledge to inform a model, and yet retain the ability to perform end-to-end training remains an open question. In this paper, we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign. We evaluate our modeling strategy on three tasks: machine comprehension, natural language inference, and text chunking. Our experiments show that knowledge-augmented networks can strongly improve over baselines, especially in low-data regimes.",https://aclanthology.org/P19-1028/,10.18653/v1/P19-1028,,2019,Reading comprehension,classification,supervised,netprok neurons are constrained using logical distnace functions. distance functions are implemented using t-norms,we present a novel framework for introducing declarative knowledge to neural network architectures in order to guide training and prediction. Our framework systematically compiles logical statements into computation graphs that augment a neural network without extra learnable parameters or manual redesign.,,"ontology,
first order logic (FOL)",transformer,compiled,5. Neuro_Symbolic,"SQuAD,
SNLI,
CoNLL2000","given a conditional statement and a computation graph: (1) Convert the antecedent into a conjunctive or a disjunctive normal form if necessary. (2) Convert the conjunctive/disjunctive antecedent into distance functions using t-norms (with appropriate corrections for negations). (3) Use the distance functions to construct constrained layers and/or auxiliary layers to augment the computation graph by replacing the original layer with constrained one. (4) Finally, use the augmented network for end-to-end training and inference",F1,slightly better than baselines,0.7777777778,"uses ConceptNet for external knowledge to get alignments. Conditional statements (FOL rules) are given - assuming these are hand-crafted.
The architecture gives improvements of up to 4% points (F1) over NN when training data is very small, and very slight improvement or no improvemtn (depending on task) when using 100% of the training data. 
No other NeSy goals are discussed.",,FALSE,FALSE,logical,implicit,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,1,,,
cp,\cite{Dehua_Keting_Jianrong_2021},{BDCN}: Semantic Embedding Self-explanatory Breast Diagnostic Capsules Network,Building an interpretable AI diagnosis system for breast cancer is an important embodiment ofAI assisted medicine. Traditional breast cancer diagnosis methods based on machine learning areeasy to explain but the accuracy is very low. Deep neural network greatly improves the accuracy of diagnosis but the black box model does not provide transparency and interpretation. In this work we propose a semantic embedding self-explanatory Breast Diagnostic Capsules Network(BDCN). This model is the first to combine the capsule network with semantic embedding for theAI diagnosis of breast tumors using capsules to simulate semantics. We pre-trained the extrac-tion word vector by embedding the semantic tree into the BERT and used the capsule network to improve the semantic representation of multiple heads of attention to construct the extraction feature the capsule network was extended from the computer vision classification task to the text classification task. Simultaneously both the back propagation principle and dynamic routing algorithm are used to realize the local interpretability of the diagnostic model. The experimental results show that this breast diagnosis model improves the model performance and has good interpretability which is more suitable for clinical situations.IntroductionBreast cancer is an important killer threatening women{'}s health because of rising incidence. Early detection and diagnosis are the key to reduce the mortality rate of breast cancer and improve the quality of life of patients. Mammary gland molybdenum target report contains rich semantic information whichcan directly reflect the results of breast cancer screening (CACA-CBCS 2019) and AI-assisted diagno-sis of breast cancer is an important means. Therefore various diagnostic models were born. Mengwan(2020) used support vector machine(SVM) and Naive Bayes to classify morphological features with anaccuracy of 91.11{\%}. Wei (2009) proposed a classification method of breast cancer based on SVM andthe accuracy of the classifier experiment is 79.25{\%}. These traditional AI diagnoses of breast tumors havelimited data volume and low accuracy. Deep Neural Networks (DNN) enters into the ranks of the diagno-sis of breast tumor. Wang (2019) put forward a kind of based on feature fusion with CNN deep features of breast computer-aided diagnosis methods the accuracy is 92.3{\%}. Zhao (2018) investigated capsule networks with dynamic routing for text classification which proves the feasibility of text categorization. Existing models have poor predictive effect and lack of interpretation which can not meet the clinical needs.,https://aclanthology.org/2021.ccl-1.105,,,2021,Decision support,classification,supervised,"The semantic segmentation layer of inspection report, the semantic tree knowledge embedding  extraction word vector layer (Sem-Bert), and the capsule network assisted target feature multi-head attention representation classification layer (Muti-Cap).",Semantic segmentation algorithm is used to segment breast cancer lesions. Semantic tree is integrated into Bidirectional Encoder  representation from Transformers(BERT) pre-training to obtain word vectors. A capsule network with multi-head attention mechanism was proposed to predict breast tumors.,,semantic net,transformer,compiled,4. Neuro: Symbolic → Neuro,Private,input text -> Semantic segmentation -> Semantic tree -> Multiple levels of embeddings -> Capsule network,F1,Upto 91,0.7777777778,"Clinician has to semantically segment the report by using some specific keywords. SemBert layer is responsible to generate embeddings from semantic tree representation of the semantically segmented report. Finally, layer converts the word vectors of the pre-training layer into capsules, uses the capsule network to obtain the required prediction capsules, and combines effective  information from multiple attention heads to achieve better classification","Semantic tree has obvious advantages
of context hierarchy, so the construction of semantic tree can help to solve the problem of unreasonable
word segmentation and context incoherence",TRUE,TRUE,semantic network,implicit,FALSE,FALSE,TRUE,FALSE,FALSE,FALSE,1,,,
cp,\cite{Jiang_Gurajada_Lu_2021},{LNN}-{EL}: A Neuro-Symbolic Approach to Short-text Entity Linking,"Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches. Furthermore, our framework has the benefits of extensibility and transferability. We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods. As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3{\%} increase in F1 score relative to previous SoTA. Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy.",https://aclanthology.org/2021.acl-long.64,10.18653/v1/2021.acl-long.64,,2021,Entity Linking,information extraction,supervised,combination of embeddings and rules to train LNN,"first neuro-symbolic method for entity linking (coined “LNN-EL"") that provides a principled approach to learning EL rules.",,"first order logic (FOL),
knowledge graph (KG)","Logical Neural Network,
transformer",compiled,4. Neuro: Symbolic → Neuro,"LC-QuAD 1.0
QALD-9
WebQSP
",hand craft rules and feature extraction (including cos sim) -> LLN -> candidate selection,F1,better than (BERT) baselines.,0.8888888889,Transformers are used for calculating cos sim between entities (an alternative to Spacy's word2vec). Also BERT with box embeddings which approximate the relations between entities are used. The NeSy component is the modified LNN using handcrafted rules. That is the limitation.,"the authors say the solution  has transferability, but they also say ""By leveraging rules, the learned model shows a desirable transferability property: it performs well not only on the dataset on which it was trained, but also on other datasets from the same domain without further training."" This is more like OOD, whereas transferability deals with different domains.",FALSE,FALSE,rules,explicit,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,4,,,
cp,\cite{Qin_Liang_Hong_Tang_Lin_2021},Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks,"Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers. Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: a) self-supervised number prediction task predicting both number quantity and number locations; b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; d) duality exploiting task exploiting the quasi-duality between symbolic equation generation and problem{'}s part-of-speech generation to enhance the understanding ability of a solver. Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new largescale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples. Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods.",https://aclanthology.org/2021.acl-long.456,10.18653/v1/2021.acl-long.456,,2021,Reading comprehension,reasoning,supervised,Model is optimized by four novel auxiliary objectives that enforce four levels of problem understanding and symbolic reasoning. 1. Number prediction task to predict both the number quantity and number location in he problem in a self-supervised manner. 2. Commonsense constant prediction task to predict what prior commonsense knowledge (e.g. how many legs a chicken has) is required for our solver. 3. Program consistency checker to compute the semantic loss between the predicted program and ground-truth equation to ensure reasonable equation mapping. 4. Duality exploiting task that exploits the quasi duality between symbolic grounded equation generation and the problem’s part-of-speech generation to  enhance the understanding ability of the solver.,Neural-Symbolic Solver(NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by four auxiliary tasks,,graph representation,recurrent neural network (RNN),cooperative,3. Neuro; Symbolic,"CM17K, Math23K",Problem reader represents question in the form of embedding using 2 layer bidirectional GRU. Programmer decodes ouput of problem reader as a tree. Using sympy python library math problem is solved.,Accuracy,Upto 75%,0.7777777778,"A problem reader that encodes the math word problems into vector representations, a programmer to generate the symbolic grounded programs in prefixorder, and a symbolic executor to obtain final results.  4 levels of auxillary tasks are perfomed in programmer.","marked s type 3 due to the dual learning process in the programmer module, which is akin to RL. But this paper was not clear, and it's still not clear to me whihc category if any it falls into -Kyle",TRUE,FALSE,rules,implicit,TRUE,FALSE,FALSE,FALSE,TRUE,TRUE,2,generalization,,
cp,\cite{Pinhanez_Cavalin_Alves_2021},Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems,"In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40{\%} of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge. The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the chatbots, with decreases of 0.05 or more in FAR in almost 40{\%} of the chatbots. When considering only the well-developed workspaces with a high level use of taxonomies, FAR decreased more than 0.05 in 77{\%} of them, and more than 0.1 in 39{\%} of the chatbots.",https://aclanthology.org/2021.acl-long.545,10.18653/v1/2021.acl-long.545,,2021,Reading comprehension,information extraction,supervised,The key idea behind our algorithms is to substitute the typical softmax used in the output layer of a ML text classifier with a space of embeddings of the taxonomic descriptions of the intents.,"By using neurosymbolic  algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition.",,semantic net,recurrent neural network (RNN),compiled,4. Neuro: Symbolic → Neuro,ChatWorks,Embeddings are generated on KG by considering deep walk algorithm and  Convolutional Deep Structured Semantic Model. Neural models are modeled using LSTM and Universal Sentence Enconder. Combinations embeddings and neural models are considered to identify better perfoming model.,Far Acceptance Rate (FAR ),Upto 40%,0.6666666667,Knowledge graphs are populated by considering external datasets.,,TRUE,TRUE,semantic network,implicit,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,0,,,
cp,\cite{Hu_Wei_Huai_2021},{D}ialogue{CRN}: Contextual Reasoning Networks for Emotion Recognition in Conversations,"Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. Inspired by the Cognitive Theory of Emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.",https://aclanthology.org/2021.acl-long.547,10.18653/v1/2021.acl-long.547,,2021,Emotion recognition,classification,supervised,Multiple LSTM models are modeled to extract emotion at different context.,,,semantic enrichement,recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,"SEMAINE, IEMOCAP, MELD","Model is divided into three phases. Perception phase: , Two bi-directional LSTM networks are leveraged to  capture situationlevel and speaker-level context  dependencies. Cognitive phase: Mutli turn reasoning modules to iteratively extract emotional clues (LSTM with attention). Finally emotion is classified.",Accuracy,Upto 66,0.8888888889,"Emotion is recognised by considering speaker and situation context. The model consists of three phases. First two phases tries to recognise the emotion independent of context. Based on contextual clues, in the final phase emotion is classified.",,TRUE,FALSE,frames,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,,,
cp,\cite{Yabloko_2020},{ETHAN} at {S}em{E}val-2020 Task 5: Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing,I present ETHAN: Experimental Testing of Hybrid AI Node implemented entirely on free cloud computing infrastructure. The ultimate goal of this research is to create modular reusable hybrid neuro-symbolic architecture for Artificial Intelligence. As a test case I model natural language comprehension of causal relations from open domain text corpus that combines semi-supervised language model (Huggingface Transformers) with constituency and dependency parsers (Allen Institute for Artificial Intelligence.),https://aclanthology.org/2020.semeval-1.83,10.18653/v1/2020.semeval-1.83,,2020,Causal Reasoning,classification,semi-supervised,Expansion can be achieved by layering symbolic modules with neural networks on a basis of the universal dependency scheme.,Universal Dependency (UD) parsing in the NeSy pipeline,,"dependency parsing,
constituency parsing",transformer,compiled,4. Neuro: Symbolic → Neuro,SemEval 2020,"constiuency parse -> puts corresponding textual chunks into sets of candidates for the binary classification -> Finally, at the end, ETHAN once more applies symbolic logic in order to determine which of the candidate chunks should be classified as antecedents and which as consequents. This time the hybrid AI becomes braided again since ETHAN applies neural attention-based (Dozat and Manning, 2017) dependency parser 6 to each of the candidates in order to determine the direction of causality by following the dependency tree.",F1,"0.878
2nd place on leaderboar, on par with SOTA",0.7222222222,"ETHAN relies on hand-crafted rules for detecting the counterfactual conditionals.
Note how in this case the hybrid AI becomes braided (neural precedes symbolic which in turn is succeeded by neural filtering)",,TRUE,FALSE,rules,explicit,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,0,,,
cp,\cite{Sen_Danilevsky_Li_2020},Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification,"Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.",https://aclanthology.org/2020.emnlp-main.345,10.18653/v1/2020.emnlp-main.345,,2020,Text classification,classification,supervised,RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding,explainable NN,,first order logic (FOL),recurrent neural network (RNN),compiled,4. Neuro: Symbolic → Neuro,TREC,logical expressions are learned from labeld data. the NN learns FOL using t-norms,F1,on par with baselines,0.6666666667,this paper was difficult to understand. I think the NN learns logic expressions using t-norms to simuate FOL. Are these functions the layers of the network? The authors illustrate these functoins as matix/vecot operations. The paper lacks examples.,The main point here is that the proposed architecture is interpretable while retaining performance on par with blck box models.,TRUE,FALSE,rules,explicit,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,3,,,
cp,\cite{Wu_Wang_Pan_2020},{D}eep {W}eighted {M}ax{SAT} for {A}spect-based {O}pinion {E}xtraction,"Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process. Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT). We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework. The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent. Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.",https://aclanthology.org/2020.emnlp-main.453,10.18653/v1/2020.emnlp-main.453,,2020,Opnion extraction,classification,supervised,incorporates logic rules into DNN with the ability to correct erronous predictions post hoc,"a novel attention-based weighted MaxSAT solver that can selectively rectify and update deep learning predictions according to the relevance of specific rules.
An end-to-end joint model associating DNNs, logic reasoning and structured learning",,first order logic (FOL),transformer,cooperative,3. Neuro; Symbolic,"SemEval 2014,
SemEval 2016","word embeddings + POS tags -> Transformer generates contextual embeddings -> Using manulally constructed clauses and atoms, MaXSAT generates probabilistic predictions -> CRF layer combines previous two layers and generates the final structured predictions.
The joint model can be trained in an end-to-end manner via gradient descent",F1,only slighty higher than baselines (BERT by itself),0.8333333333,"To make the logic knowledge more effective that is able to directly rectify the erroneous predictions made by deep learning models, and at the same time adapt its rules selectively according to different data instances, we propose a neuralsymbolic integration by incorporating an attentionbased weighted MaxSAT layer.
In the meantime, the partial gradient of the final loss with respect to the MaxSAT output is backpropagated to the DNN parameters, making logic rules as a form of indirect supervision to the training of the DNN. In this sense this is a coopertative architecture where each of the symbolic and sub-symbolic modules inform each other during training. ",It seems like a lot of effort just to eek out a tiny improvement in performance while not acheiving any of the stated NeSy goals.,TRUE,FALSE,logical,implicit,FALSE,FALSE,FALSE,FALSE,FALSE,TRUE,0,,,
cp,\cite{Chen_Xu_Cheng_2020},Question Directed Graph Attention Network for Numerical Reasoning over Text,"Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.",https://aclanthology.org/2020.emnlp-main.549,10.18653/v1/2020.emnlp-main.549,,2020,Reading comprehension,reasoning,supervised,"(1) We use a heterogeneous graph containing entities and different types of numbers to encode the relations among the entities and numbers, rather than the relations from numerical comparison; (2) We use the question embedding to modulate the attention over graph neighbors and update the representation to achieve reasoning",,,"dependency parsing,
graph reasoning,
graph representation","graph neural network (GNN),
transformer",sequential,1. symbolic Neuro symbolic,"DROP,
RACE","The model is composed of three main components, i.e., a representation extractor module, a reasoning module, and a prediction module.
RoBERTa (input: CLS passage SEP question CLS) -> Graph construction w/nodes: numbers and entities -> message passing graph reasoning module -> classification module (input: roberta embeddings + graph) classifying each number into one of (−1, 0, +1), which is then used as the coefficient of the number in the -> numerical expression to arrive at the final answer.",F1,better than baselines,0.7222222222,It's clear that clever combination of transformers and GNNs results in perfomance improvements. The symbolic contribution consits of a cleverly constructed graph which is then used as input to implicit reasoning. A final explict computation module is used to perform the arithmentic.,,TRUE,TRUE,semantic network,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,,,
cp,\cite{Kogkalidis_Moortgat_Moot_2020},Neural Proof Nets,"Linear logic and the linear λ-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on {\AE}Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear λ-calculus with an accuracy of as high as 70{\%}.",https://aclanthology.org/2020.conll-1.3,10.18653/v1/2020.conll-1.3,,2020,N2F,reasoning,supervised,"Two key components. The first is an encoder/decoder-based supertagger that converts raw text sentences into linear logic judgements by dynamically constructing contextual  type assignments, one primitive symbol at a time. The second is a bi-modal encoder that contextualizes the generated judgement in conjunction with the input sentence. The contextualized representations are fed into a Sinkhorn layer, tasked with finding the valid permutation that brings primitive symbol occurrences into alignment.","We have introduced neural proof  nets, a data-driven perspective on the proof nets of ILL(, and  successfully employed them on the demanding task of transcribing raw text to proofs and computational terms of the linear λ-calculus.",,Vector-symbolic architecture,transformer,compiled,4. Neuro: Symbolic → Neuro,Æthel,We first encode sentences using BERTje. We then decode into proof frame sequences using a Transformer-like decoder,Area Under the Curve (AUC),Upto 85%,0.5555555556,,,TRUE,FALSE,frames,implicit,TRUE,TRUE,TRUE,FALSE,TRUE,TRUE,4,generalization,,
cp,\cite{Lima_Espinasse_Freitas_2019},The Impact of Semantic Linguistic Features in Relation Extraction: A Logical Relational Learning Approach,"Relation Extraction (RE) consists in detecting and classifying semantic relations between entities in a sentence. The vast majority of the state-of-the-art RE systems relies on morphosyntactic features and supervised machine learning algorithms. This paper tries to answer important questions concerning both the impact of semantic based features, and the integration of external linguistic knowledge resources on RE performance. For that, a RE system based on a logical and relational learning algorithm was used and evaluated on three reference datasets from two distinct domains. The yielded results confirm that the classifiers induced using the proposed richer feature set outperformed the classifiers built with morphosyntactic features in average 4{\%} (F1-measure).",https://aclanthology.org/R19-1076,10.26615/978-954-452-056-4_076,,2019,Relation extraction,information extraction,supervised,Rich language features improve perfomance of ILP,a feature engineering step composed by a substantial body of deep linguistic knowledge in combination with an expressive inductive learning technique can generate effective RE models.,,"semantic enrichement,
inductive logic prgramming (ILP),
first order logic (FOL),
ontology",neural network (NN),sequential,1. symbolic Neuro symbolic,"reACE,
IPEA",NLP feature extraction with deep learning -> ILP,F1,better than baselines,0.7777777778,"Indeed, the performance improves as more features are used, starting with the F-measure of 77.77 and reaching 81.80. However, the authors don't provide any baselines which are not their own work. They discussed related work, but then didn't provide any results to comapre their approach to.",Logical Relational Learning,TRUE,TRUE,logical,explicit,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,2,,,
cp,\cite{Xu_Li_2019},Relation Embedding with Dihedral Group in Knowledge Graph,"Link prediction is critical for the application of incomplete knowledge graph (KG) in the downstream tasks. As a family of effective approaches for link predictions, embedding methods try to learn low-rank representations for both entities and relations such that the bilinear form defined therein is a well-behaved scoring function. Despite of their successful performances, existing bilinear forms overlook the modeling of relation compositions, resulting in lacks of interpretability for reasoning on KG. To fulfill this gap, we propose a new model called DihEdral, named after dihedral symmetry group. This new model learns knowledge graph embeddings that can capture relation compositions by nature. Furthermore, our approach models the relation embeddings parametrized by discrete values, thereby decrease the solution space drastically. Our experiments show that DihEdral is able to capture all desired properties such as (skew-) symmetry, inversion and (non-) Abelian composition, and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as ConvE.",https://aclanthology.org/P19-1026,10.18653/v1/P19-1026,,2019,KG Completion / link prediction,classification,unsupervised,Embedding generation ; AdaGrad optimazation is used to generate embeddings. Contribution of the paper is that they consider non-abelian group to generate embeddings.,"DihEdral for KG relation embedding. By leveraging the desired properties of dihedral group, relation (skew-) symmetry, inversion, and (non-)  belian compositions are all supported.",,rule based,neural network (NN),sequential,1. symbolic Neuro symbolic,FB15K,Grid search with best MRR. AdaGrad optimization with Gumbel-Softmax function.,Hits@K,Upto 64,0.7777777778,Embedding generation based on mathematical properties i.e. Abelian and non-abelian group.,,FALSE,TRUE,frames,explicit,FALSE,FALSE,TRUE,FALSE,TRUE,FALSE,2,,,
cp,\cite{AltszylerBBBV21},Zero-shot Multi-Domain Dialog State Tracking Using Prescriptive Rules,"In this work, we present a framework for incorporating declarative logical rules in state-of-the-art neural networks, enabling them to learn how to handle unseen labels without the introduction of any new training data. The rules are integrated into existing networks without modifying their architecture, through an additional term in the network’s loss function that penalizes states of the network that do not obey the designed rules. As a case study, the framework is applied to an existing neural-based Dialog State Tracker. Our experiments demonstrate that the inclusion of logical rules allows the prediction of unseen labels, without deteriorating the predictive capacity of the original system.",,,15th International Workshop on Neural-Symbolic Learning and Reasoning,2021,Dialog system,generative,supervised,"In this work we show how a set of rules can be incorporated into a Neural Network to predict a new category that did not exist in the training set. It is worth noting that this rule-based setup completely depends on the coverage of a set of rules established by the domain experts, and also, that the rules/weight/predicate and values/slot names are problem specific","a) we enhance a Neural-based Dialog State Tracker with logic rules, without degrading the  performance of the base system, b) we show that the addition of the logic rules allow the  redictions of unseen labels which can be very useful in the case of unlabeled or partially  labeled data.",,fuzzy logic,recurrent neural network (RNN),compiled,5. Neuro_Symbolic,MultiWOZ 2.0,MDNBT is implemented as multi-layer networks with Bi-LSTMs to model the user and system utterance and with RNNs with a memory cell to model the flow of the conversation.,F1,0.7% relative decrease,0.7777777778,"A rules-dependent loss term is introduced into the system’s loss  function as a way of integrating rules to the learning process and as a way of allowing the use of off-the-shelf optimizers. By including rules as part of the training process, we generate extra cost when the network  does not satisfy a certain rule.",,FALSE,TRUE,rules,implicit,FALSE,TRUE,FALSE,TRUE,FALSE,TRUE,2,,,
cp,\cite{cowen2019neural},Neural Variational Inference For Estimating Knowledge Graph Embedding Uncertainty,"Recent advances in Neural Variational Inference allowed for a renaissance in latent variable models in avariety of domains involving high-dimensional data. While traditional variational methods derive an analytical approximation for the intractable distribution  over the latent variables, here we construct an inference network conditioned on the symbolic representation of entities and relation types in the Knowledge  Graph, to provide the variational distributions. The new framework results in a highly-scalable method.  Under a Bernoulli sampling framework, we provide an alternative justification for commonly used techniques in large-scale stochastic variational inference, which drastically reduce training time at a cost of an additional approximation to the variational lower  bound. We introduce two models from this highly  scalable probabilistic framework, namely the Latent  Information and Latent Fact models, for reasoning over knowledge graph-based representations. Our  Latent  information and Latent Fact models improve upon baseline performance under certain conditions.  We use the learnt embedding variance to estimate  predictive uncertainty during link prediction, and discuss the quality of these learnt uncertainty estimates. Our source code and datasets are publicly   available online ",,,International Joint Conference on Artificial Intelligence,2019,KG Completion / link prediction,classification,unsupervised,Embedding generation.,"We introduce a framework for creating a family of highly scalable probabilistic models for knowledge graph  epresentation The framework improves model performance under certain conditions, while reducing the parameter search by one hyper-parameter, as the unit Gaussian prior is selfregularising",,Vector-symbolic architecture,neural network (NN),sequential,1. symbolic Neuro symbolic,WN18,Embedding generation by latent information model and latent fact model.,Hits@K,0.952,0.6666666667,"DistMult and ComplEx are modified to handle uncertainiity.  An inference network id built that is conditioned on the symbolic representation of entities and relation types in the Knowledge Graph, to provide the variational distributions we construct an inference network  conditioned on the symbolic representation of entities and relation types in the Knowledge  Graph, to provide the variational distributions",,TRUE,FALSE,frames,implicit,FALSE,TRUE,FALSE,FALSE,TRUE,FALSE,2,,,
cp,\cite{gu2019local},Local ABox Consistency Prediction with Transparent TBoxes Using Gated Graph Neural Networks,We address the problem of local ABox consistency prediction with transparent TBoxes and propose a new algorithm based on gated graph neural networks to solve it. Experiments on our proposed  benchmark demonstrate that our  ew model can outperform previous methods. We also share some insights on under what circumstances our method  should be favored.,,,International Joint Conference on Artificial Intelligence,2019,Entity Resolution,classification,supervised,Resolving A-Box inconsistency using GGNN,A new benchmark to cover inconsistency patterns of different difficulties.,,path finding and reasoning,graph neural network (GNN),sequential,1. symbolic Neuro symbolic,DBpedia,"GGNN is a general neural network architecture defined  ccording to a graph structure G = (V, E). Firstly, each node  V is manually annotated as a numerical vector xv with dimension D. Node annotation is task dependent. Secondly, graphs are mapped to outputs by GGNN via two steps, i.e., propagation model and output model.",F1,Upto 85%,0.6666666667,Resolved consisetncy of ABox using TBox  for DbPedia and YAGO.,,FALSE,TRUE,semantic network,implicit,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,2,,,
cp,\cite{Brasoveanu2019656},Semantic Fake News Detection: A Machine Learning Perspective,,2-s2.0-85067488804,10.1007/978-3-030-20521-8_54,"15th International Work-Conference on Artificial Neural Networks, IWANN 2019",2019,Text classification,classification,supervised,,"Semantic features (sentiment analysis, named entities and relations) will be added to a set of syntactic features (POS - part-of-speech and NPs - Noun Phrases) and to the features of the original input dataset. On the resulted augmented dataset various classifiers, such as LSTM, CNN, and Capsule Networks are applied.",,semantic enrichment,recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,Liar data set,1. Data augmentation,Accuracy,64%,0.78,"Data is preprocessed to add various semantic and syntax level details. Metadata contains : sentiment (mostly aspect-based and sentence level), named entities (extracted from NLP library) and named entity links (wrappers on top of DBPedia spotlight). Relation extraction contains :  POS tags ( extracted directly from news sentence), triples (extracted from DBPedia). Experiments were conduscted using pretrained embeddings (Glove, fastText and word2vec). Both classical and DL based models were implemented to compare impact of added relations.",,TRUE,FALSE,frames,implicit,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,0,,,no mention of reasoning. essentially this is another example of semantic enrichement
cp,\cite{Graziani2019185},Jointly Learning to Detect Emotions and Predict Facebook Reactions,,2-s2.0-85072850367,10.1007/978-3-030-30490-4_16,"28th International Conference on Artificial Neural Networks, ICANN 2019",2019,Sentiment analysis,classification,semi-supervised,The provided experimental analysis has shown that bridging these two tasks (emotion detection and reaction prediction) by means of FOL-based constraints leads to improvements in the prediction quality that clearly goes beyond more naive approaches in which artificial labels are generated in the data preprocessing stage.,a neural network-based model to jointly learn the task of emotion detection and the task of predicting Facebook reactions.,,first order logic (FOL),recurrent neural network (RNN),compiled,5. Neuro_Symbolic,"AffectiveText, ISEAR, and Fairy Tales, Facebook posts",text -> LSTM (word embeddings ) -> 2 MLPs (one for task) with logical constraints,F1,on par or better than SOTA in some intances,0.78,The model jointly predicts the reactions and the emotions of facebook posts by using FOL as constraints to the optimization problem. The FOL formulas are mapped using t-norm functions and added to the Loss function for each reaction and emotion. The system is inspired by Learning from Constraints. The procedure doesn't require an emotion lexicon which other SOTA systems employ.,,FALSE,FALSE,logical,explicit,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,0,on par with baselines,,FOL is embedded as constraints. none of the promises are relevant here.
cp,\cite{Bianchi2019161},Complementing logical reasoning with sub-symbolic commonsense,,2-s2.0-85075592929,10.1007/978-3-030-31095-0_11,"3rd International Joint Conference on Rules and Reasoning, RuleML+RR 2019",2019,KG Completion / link prediction,reasoning,supervised,"the major contribution of this work is to show that combining commonsense knowledge under the form of text-based entity embeddings with LTNs is not only simple, but it is also promising","In this paper, we have shown that the combination of sub-symoblic commonsense representations, under the form of entity embeddings generated from text, and logical reasoning in vector spaces is flexible and can be used to solve completion tasks. Since LTNs are based on Neural Networks, they reach similar results while also achieving high explainability due to the fact that they ground first-order logic. The real advantage comes from the fact that LTNs allow us to get the best of both the symbolic and connective worlds and to easily integrate additional knowledge like sub-symbolic commonsense knowledge.
",Neuro-symbolic integration is a current field of investigation in which symbolic approaches are combined with deep learning ones.,"first order logic (FOL),
knowledge graph (KG)",logic tensor network (LTN),compiled,5. Neuro_Symbolic,DBpedia,word embeddings -> logic tensor network -> reaoning,F1,on par with SOTA,1.00,"quantifiers make LTNs computationally inefficient.
LTN_EE can be used for inference after training, while DNN cannot.",https://github.com/vinid/logical commonsense.,FALSE,TRUE,semantic network,both,TRUE,TRUE,TRUE,FALSE,FALSE,TRUE,3,on par with baselines,higher than baselines,"Logic Tensor Networks (LTNs). 
The authors describe two types of reasoning: ""subsymbolic commonsense"" (strongly correlated with associative learning), and ""axiomatic"" knowledge (predicates and logic formulas) for structured inference.
""LTNs can be be used to do after-training reasoning over combinations of axioms on which it was not trained on""
""Since LTNs are based on Neural Networks, they reach similar results while also achieving high explainability due to the fact that they ground first-order logic.""

Subsymbolic commonsense (word vectors) do not in my opinion represent the kind of commonsense we refer to when we talk about people's ability to use common sense, as in not touching a hot stove, or not 'cutting a door in half to fit the table through it' (as in the example from Marcus on GPT3). Should we make a distinction between commonsense knowledge and commonsense reasoning? and how would we do that? Is it justifiable to call word vetors commonsense knowledge, or is this just linguistic gymnastics to fit an agenda?
"
cp,\cite{Lemos2020647},Neural-Symbolic Relational Reasoning on Graph Models: Effective Link Inference and Computation from Knowledge Bases,,2-s2.0-85096590382,10.1007/978-3-030-61609-0_51,"29th International Conference on Artificial Neural Networks, ICANN 2020",2020,KG Completion / link prediction,classification,supervised,Trains a model to reason directly over graphs. enabling reasoning over many paths at once.,,,semantic enrichment,graph neural network (GNN),cooperative,3. Neuro; Symbolic,"ClueWeb, Freebase",GNN to learn embeddings followed by MLP for prediction,"Mean Average Precision, MAP)",Upto 92%,0.78,"Authors have created 3 models, GNN-Relation, GNN-Mean and GNN-sum ; GNN relation considers only relations where as other 2 models considers both entity and relations to store/compute embeddings. These embeddings are fed into DL model (GNN ; LSTM cells and Relu activation function)","this seems like the appropriate Type 3 label since the entire system is backpropageted. Hense, MLP module feeds the GNN module and vice versa. Comparing to row 55 (relation extraction, the modules are not jointly learned. Hense row 55 was marked type 1)",FALSE,TRUE,frames,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,TRUE,1,better than baselines,,"""reasoning and inference on link prediction""
The authors talk about reasoning over graphs to mean link prediction."
cp,\cite{Cui2021419},Sememes-Based Framework for Knowledge Graph Embedding with Comprehensive-Information,,2-s2.0-85113718164,10.1007/978-3-030-82147-0_34,"14th International Conference on Knowledge Science, Engineering and Management, KSEM 2021",2021,KG Completion / link prediction,classification,supervised,"Rather than using TransE, TransR etc they have proposed a new embedding model using pretrained BERT.","sememes-based framework for knowledge graph to streamline the semantic space of entities. Replace entity descriptions with a finite set of semantics and encode the sememe labels of entities using a pre-trained Bert model, and finally jointly learning the symbolic triples and sememe labels",,sememe,neural network (NN),sequential,1. symbolic Neuro symbolic,"FB15K, WN18","Sememes-based framework for knowledge graph embedding architecture, where grammatical sememes and pragmatic sememes  co-represent entity’s comprehensive-information. Pretrained BERT is used to generate embeddings by considering pragmatic and grammatical sememes seperately. One-vs-rest training strategy is used to train the logistic regression model to make multi-label predictions. Multi-label classification task with 50 classes, which means that for each entity, the method should provide a set of types instead of a single type.  Semantic enrichment -> BERT -> LR ?","Mean Average Precision, MAP)",Upto 95%,0.78,Embeddings are generated by classifying text into grammatical and pragmatic sememes.  Pretrained BERT is used to classify of grammatical and pragmatic sememes. They have translated english words back to chinese to compare their models with others.,"A sememe is an indivisible semantic unit for human languages defined bylinguists. I see this as a NN approach enriched by additonal information about each node (in the form of sememes). One could argue that this additional knowledge is symbolic, and conclude that the neuro-symbolic approach outperforms SOTA NN link prediction, such asTRANS-E.",TRUE,TRUE,frames,implicit,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,0,,,link prediction. reasoning is not mentioned explicitly.
ar,\cite{Skrlj2021989},autoBOT: evolving neuro-symbolic representations for explainable low resource text classification,,JN4IZETG,10.1007/s10994-021-05968-x,Machine Learning,2021,Text classification,classification,supervised,"autoBOT is explainable, efficient, and performs on par with language models like BERT.","a novel approach for text classification with limited data and resources, but comparable in erformance to BERT. Hyperparameters are optimized using an evolutionary algorithm. Novel feature types: document keywords, relational features, and first order features (based on grounded relations from ConceptNet)",NA,"symbolic learner,
symbolic representations,
ConceptNet KG","SVM,
neuroevolution (NE)",cooperative,3. Neuro; Symbolic,"semeval2019, semeval2019,
mbti,
Fox,
BBC,
etc..","autoBOT (automatic Bags-Of-Tokens),
The proposed approach consists of an evolutionary algorithm that jointly optimizes various sparse representations of a given text (including word, subword, POS tag, keyword-based, knowledge graph-based and relational features) and two types of document embeddings (non-sparse representations). The key idea of autoBOT is that, instead of evolving at the learner level, evolution is conducted at the representation level.
",F1,up to 99% on some tasks,1.00,"autoBot is a genetic algorithm for text data that learns both the representations and the models for classification jointly. The inclusion of novel symbolic features improves performance over traditional linear models such as SVM and Logistic Regression, and performs on par with large language models such as BERT. The advantage of autoBot is its explainability (feature importances), and efficiency in a low resource setting (small data/feature sparsity, and limited compute power). What makes AutoBOT neuro-symbolic is the combination of symbolic and non-symbolic features. The study was conducted using off the shelf implementations and a very basic evolutionary algorithm, thus leaving plenty of room for improvements. Classification is performed at the document level, but it would be worth exploring this pipeline for sentence level and span level classification on for example, the semeval2020 task, which already has a leaderboard with BERT based solutions for comaprison.

L3:cooperative - unstructured input to symbolic representations (here different symbolic features, POS, relations, etc) - solved by a symbolic reasoner [the genetic algorithm which learns representations] - fed to classifer
""As the solutions encode both the weights at the feature subspace level, as well as weights of individual features, autoBOT offers two distinct views of feature importances""","Does not fall into any of Kautz's categories. However, the use of linear classifiers (ie symbolic learners) in combination with sub-symbolic representations qualifies for the neuro-symbolic moniker.",TRUE,TRUE,semantic network,implicit,FALSE,TRUE,TRUE,TRUE,TRUE,TRUE,4,on par with baselines,lower than baselines,"""autoBOT also explores novel representation types such as e.g., knowledgegraph based features, capable of exploiting the knowledge beyond the textual training data considered.""
no mention of reasoning.
""The key idea of autoBOT is that, instead of evolving at the learner level, evolution is conducted at the representation level.""
"
ar,\cite{Schon2019293},The CoRg Project: Cognitive Reasoning,,7MMJY5BM,10.1007/s13218-019-00601-5,KI - Kunstliche Intelligenz,2019,Question answering,reasoning,supervised,"Authors argue that combining logic reasoning with ML improves performance and gerates explainable results. The challenge is in finding an appropriate logics for cognitive (human-like) reasoning. The objective of CoRg is to develop a system for cognitive computing.
","So far we implemented a first prototype of the CoRg system, using KNEWS, WordNet, Hyper, ConceptNet Numberbatch and neural network techniques and evaluated it on COPA.",,logic,recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,COPA benchmark set (Choice of Plausible Alternatives),raw text -> FOL (using KNEWS) -> augment with bg knwledge bases -> theorem prover -> NN,,,0.56,"The study explores using a variety of logic systems for Cognitive Reasoning. The authors posit that combining logic programming with NN should produce not only more accurate results, but also provide expalainbility. It is a work in progress. Results are not reported. The model output is a decision between two possible answers. It is inference in the sense that the answer is inout to the model to determine the value (yes/no), even though the value s binary, this is unlike a classification problem where the label is a class. Here the labels are sentences.",,FALSE,TRUE,logical,both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,2,wip,,"""The objective of CoRg is to develop a system for cognitive computing.""
""The aim of the CoRg project (Cognitive Reasoning) is to successfully complete a reasoning task with commonsense reasoning.""
""we have to find appropriate logics for cognitive reasoning. For a successful reasoning system, nowadays it seems to be important to combine automated reasoning with machine learning technology like recurrent neural networks."" The authors don't explictly state that this is neuro-symbolic. They state that in order to emulate human reasoning, systems need to be flexible, be able to deal with contradicting evidence, evolving evidence, have access to enourmous amounts of background knowledge, and include a combination of different techniques and logics.
"
ar,\cite{Jiang2020},Medical knowledge embedding based on recursive neural network for multi-disease diagnosis,,JGU2SECC,10.1016/j.artmed.2019.101772,Artificial Intelligence in Medicine,2020,Decision support,classification,supervised,Using Huffman trees to build symbols and logic into a Recursive NN which can be optimized with GD.,"Knowledge-based neural network, in which nerve cells in a hidden layer consist of first-order logic.
For neural network parameter learning, a complete theoretical framework based on back-propagation was established to minimize the cross-entropy error.
",,"first order logic (FOL),
Huffman tree","recursive neural knowledge network (RNKN),
recursive neural network (RcNN)",compiled,4. Neuro: Symbolic → Neuro,CEMR,"text -> KG triples -> Huffman tree -> logic functions as nerve cells -> softmax,
optimized using GD/backProp","p@k,
Discounted Cummulative Gain (DCG)",[0.36 - 0.72],0.78,"The study encodes knowledge in the form of huffman trees made of triples, and logic expressions, in order to jointly learn embeddings and model weights. The first layer consists of entities, the second layer consists of relations (x-> y). Higher layers compute logic rules. The root node is the final embedding representing a document (in this case a single health record). Softmax is used to calculate class probabilities. back propagation is used for optimization. Sub-symbolic representations are learned from symbolic features and rules iteratively.",interesting approach that would be worth trying for propaganda detection,FALSE,TRUE,logical,both,TRUE,FALSE,TRUE,FALSE,FALSE,FALSE,2,better than baselines,,"Embeddings are learned by following the logic expressions encoded in the huffman trees which have been built from the data. In that sense it's fair to say that reasoning is taking place to generate the embeddings. Embeddings are then processed in the usual NN manner.

Kyle: If we could come up with a logic for language (as in news articles), then perhaps this is a promising approach to either finetune LLM, or bild a model from much smaller data.

each node of the tree () is a logic expression, thus hidden layers are interpretable.
Using Huffman trees to represent deep first-order logic knowledge - these are fed into a NN to learn vector embeddings, which in turn are used for classification.
Here knowledge is embedded in the represenations.As such it can be considered a case of semantic enrichment.
""We developed a knowledge-based neural network, in which nerve cells in a hidden layer consist of first-order logic.""
""knowledge embedding (i.e., high-dimensional and continuous vectors) is a feasible approach to complex reasoning that can not only retain the semantic information of knowledge, but also establish the quantifiable relationship among embeddings""
""the core argument against embeddings is the inability to capture more complex patterns of reasoning such as those enabled by first-order logic""



"
ar,\cite{Hussain20181662},Semi-supervised learning for big social data analysis,,GS3TRUYZ,10.1016/j.neucom.2017.10.010,Neurocomputing,2018,Sentiment analysis,classification,semi-supervised,The addtion of prior knowledge to bias the classification model improves the accuracy of sentiment predictions.,"AffectNet: combined ConceptNet and WNA - commonsense and emotional knowledge are melded together.
AffectNet2: AffectNet with reduced dimentionality.
a flexible framework for semisupervised learning comprising of two parts: clustering and regularized classification.",,"commonsense knowledge,
graph representation",SVM,compiled,5. Neuro_Symbolic,"Pang and Lee,
AffectNet benchmark",clutering -> classification regularized by output of clustering,Accuracy,88.50%,0.78,"The study embeds prior knowledge in the form of a graph representing concepts and relations from natural language. These are projected into multi-dimentional space and a clustering algorithm is employed to generate the solution space. Subsequently, a regularized classification algorithm is employed where the output of the clustering stage is used in the regularization term. I have classified this as Kautz category 5. However, it differs from the cat5 defition in that the regularization term is not a logic rule, but prior knowledge.",,TRUE,TRUE,semantic network,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,better than baselines,,"AffectiveSpace - see also Chaturvedi2019264
""The key to performing commonsense reasoning is to find a good trade-off for knowledge representation."" Here this translates to the dimanetionality of the AffectiveSpace.
Humans reason by way of analogy. ""concepts and features that are aligned and possess high dot yields are ideal contenders for analogies.""
commonsense reasoning is defeasable.
commonsense knowledge is represented in ConceptNet, a semantic network based on WordNet and Cyc which includes word phrases and relations (isA, etc..) Compare to {Bianchi2019161} where (sub-symbolic) commonsense is the raw data. 
benchmarks are from 5 and 6 years prior to the study at hand.

"
ar,\cite{Liu2021260},Heterogeneous graph reasoning for knowledge-grounded medical dialogue system,,2PNFHS7L,10.1016/j.neucom.2021.02.021,Neurocomputing,2021,Dialog system,classification,supervised,RNN (encoder) + GAT + RNN (decoder),heterogeneous graph reasoning (HGR) model to unify the dialogue context understanding and entity-correlation reasoning.,,knowledge graph (KG),recurrent neural network (RNN),compiled,4. Neuro: Symbolic → Neuro,"MDG-C, MDG-D",RNN -> GAT -> RNN,"F1, Precision/recall, Bleu score",Upto 44%,0.78,"Though it is medical application, might be useful. Heterogeneous graph reasoning model to efficiently incorporate the contextual information and domain knowledge for entity reasoning in medical dialogue.",,FALSE,TRUE,semantic network,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,,,reasoning on graphs - domain knowledge is leveraged. supervised dialog data is used for answer generation.
ar,\cite{Yao201842},Learning to activate logic rules for textual reasoning,,NB39QA35,10.1016/j.neunet.2018.06.012,Neural Networks,2018,Question answering,reasoning,reinforcement,"Image Scheme (IS) is acquired from early life experience, which is also our hypothesis in this work
our work aims at precise reasoning, good interpretability of models, and perfect performances on small dataset","We propose a novel memory-augmented neural network framework making use of logic rules inducted from existing theories of Image Schema and Cognitive Model.
We redefine textual reasoning tasks as interactive-feedback process with human working memory. Under certain assumptions, we jointly solve two main problems: variable binding and relation activating, via deep reinforcement learning.
Our experimental results show the existence of common properties among real-world relations, and the probability to partially re-construct human logic system to boost performances on language comprehension tasks.
Variable-Relation Reasoning Machine (VRRM)

",NA,"Sequencial decision making,
predicate logic,
relational logic,
rule based,
rule induction,
cognitive linguistics","Memory Network,
neural network (NN)",cooperative,3. Neuro; Symbolic,bAbI-20,"We redefine textual reasoning as a sequential decision-making problem, and solve it with reinforcement learning (RL).",Test error rate,0.70%,0.67,"This work makes the hypothesis that human reasoning can be modeled by Image Schemas (IS). Schemas are made up of logical rules on (Entity1,Relation,Entitity2) tuples, such as transitivity, or inversion. Input sentences and questions are encoded as the concatenation of individual word embeddings with zero paddings. An MLP module is used to learn a set of tuples by performing simulations and utilizing Reinforcement Learning. This work is limited by the nature of the synthetic data set. It is not shown how well it might generalize to a real world scenario. There is also a limited number of schemas, and as the authors point out, future work should include additional schemas more representative of human reasoning. (See also https://arxiv.org/pdf/1706.01427.pdf from DeepMind 2017)",,FALSE,TRUE,rules,explicit,TRUE,FALSE,TRUE,TRUE,FALSE,FALSE,3,better than baselines,lower than baselines,"authors take the explicit stance that LLMs do not reason (hence, reasoning is/should be explicit). ""There is close, intrinsic relationship between reasoning and human language.""
""our work aims at precise reasoning, good interpretability of models, and perfect performances on small dataset (1k)""
""We redefine textual reasoning as a sequential decision-making problem, and solve it with reinforcement learning (RL). ""
""we generate symbolic relational tuples to take part in external reasoning process.""
"
ar,\cite{Chaturvedi2019264},Fuzzy commonsense reasoning for multimodal sentiment analysis,,B47SSE6P,10.1016/j.patrec.2019.04.024,Pattern Recognition Letters,2019,Sentiment analysis,classification,supervised,multi-modal sentiment analysis. The computational cost is reduced by projecting the feature embeddings onto a 4 dimentional affective space. Accuracy is increased by the use of fuzzy rules.,"Convolutional Fuzzy Sentiment Classifier (CFSN). in this paper we use a low-dimensional RNN to classify concepts learned via deep learning. By projecting temporal features onto AffectiveSpace, we are able to interpret the features learned. Lastly, we take into account that most sentences have mixed emotions such as sarcasm that can only be modeled effectively using fuzzy membership functions. Hence, we predict the final accuracy of the classifier using fuzzy blending over each pair of simple emotions",,"fuzzy logic,
fuzzy neural network (FNN),
logical connectives","convolutional neural network (CNN),
recurrent neural network (RNN),
deep belief network (DBN)",nested,2. Symbolic[Neuro],"multiple, multi-modal",text -> NN -> n-grams -> low-dimenstion ->  RNN -> CDBN (convolutional deep belief network) -> Fuzzy Logic calssification,Accuracy,10-20% accuracy improvement over baselines,0.78,"Not strictly text input. Text is only part of the input, and has been converted using pretrained embeddings. sentiment analysis. includes text datasets. This pipeline includes selection of logical connectives at the input phase, and a fuzzy logic classifier at the output phase.",,FALSE,FALSE,logical,explicit,TRUE,FALSE,FALSE,FALSE,TRUE,FALSE,2,better than baselines,,"Reasoning is only mentioned in the title
AffectiveSpace - see also Hussain20181662
""AffectiveSpace [8], a vector space model of commonsense concepts such as ‘beautiful painting’ or ‘poor writing’ [9].""
here transferability pertains to different languages rather than different domains, and addresses the issue of the lack of annotations in other languages, in particular, Spanish.
The authors use fuzzy logic to classify sentences with emotions. This is the symbolic part, as it requires incorporating explicit membership functions (which are leanred). "
ar,\cite{Bounabi2021229},A new neutrosophic TF-IDF term weighting for text mining tasks: text classification use case,,ACDMDN8W,10.1108/IJWIS-11-2020-0067,International Journal of Web Information Systems,2021,Text classification,classification,supervised,Neutrosophic reasoning to analyze and generate weights for terms in natural languages.,"a new term weighting method, which uses the term frequencies as components to define the relevance and the ambiguity of term;",,"fuzzy logic,
Neutrosophy",neural network (NN),sequential,1. symbolic Neuro symbolic,"BBC News database, BBC SPORTS database, Kaagle news","SVM, feedforward network",Area Under the Curve (AUC),Upto 97%,0.78,"Instead of directly using TF-IDF, a new weighting method is proposed. This combines the advantages of the neutrosophic reasoning and ambiguity membership function to determine the weight for a given term from a document.",,FALSE,FALSE,logical,both,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,,,"reasoning is refering to the neutrosophic rules for membership. this is mathematical reasoning, not human reasoning."
ar,\cite{Honda2019152368},Question Answering Systems with Deep Learning-Based Symbolic Processing,,EGI547RA,10.1109/ACCESS.2019.2948081,IEEE Access,2019,Text classification,classification,supervised,Question answering = NMT ; Embedding for symbolic ; NMT -> Seq2Seq model and Transformer ;,Prolog-like processing system using deep learning,,symbolic processing,transformer,sequential,1. symbolic Neuro symbolic,"Kinsources, Geoquery",Transformers,"Accuracy, Rate",Upto 99%,0.56,They tried to build prolog like system (in terms of unification and inference) using deep learning and achieved the same using NMT. Embedding is generated with the help of word2vec n graycode. Unification is also done using word2vec.  Data is reconsutrcuted using augmentation (proper nouns  are replicated).,,FALSE,TRUE,logical,explicit,FALSE,TRUE,FALSE,TRUE,FALSE,FALSE,2,better than baselines,,"reasoning is logical/mathematical. the authors don't use the term reasoning, instead they call it processing."
ar,\cite{Gong202030885},Hierarchical Graph Transformer-Based Deep Learning Model for Large-Scale Multi-Label Text Classification,,3YFVRRKE,10.1109/ACCESS.2020.2972751,IEEE Access,2020,Text classification,classification,supervised,Text -> Graph -> embedding ; attention mechanism at various levels to capture semantics,"graph-based document modeling, hierarchical transformer encoder architecture for feature extraction, and weight-directed loss for label classification.",,semantic net,"convolutional neural network (CNN),
recurrent neural network (RNN)",sequential,1. symbolic Neuro symbolic,"Reuters Corpus Volume 1, RCV1-2K, AmazonCat-13K",Graph embeddings ; CNN for feature extraction ; Transformer LSTM,"NDCG@K, p@k",Upto 95%,0.83,"combination of graph-based document modeling and the hierarchical transformer. By capturing the logical structure in the text, performance is improved.",,TRUE,TRUE,semantic network,implicit,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,0,on par with baselines,,learning text representations by leveraging the linguistic structure (semantic/symbolic part). no mention of reasoning or other benefits.
ar,\cite{Es-Sabery202117943},Sentence-Level Classification Using Parallel Fuzzy Deep Learning Classifier,,BAMGPUCX,10.1109/ACCESS.2021.3053917,IEEE Access,2021,Sentiment analysis,classification,supervised,"Data cleaning and preprocessing, as well as the addtion of the fuzzy logic component (MFS) significantly improves performance on sentiment analysis tasks on Twitter datsets compared to SOTA. In addition, parallelization using hadoop map reduce significantly reduces computation time.","we develop a new hybrid fuzzy-deep learning approach, that basically integrates the CNN, FFNN deep learning networks with the MFS fuzzy logic system.",NA,"fuzzy logic,
fuzzy rules,
mamdani fuzzy system (MFS)","convolutional neural network (CNN),
neural network (NN)",sequential,1. symbolic Neuro symbolic,"sentiment140, sentiment140,
COVID-19_Sentiments","TREC,
SST-1,
MR,
SST-2","F1,
Accuracy,
Precision/Recall,
True Positive Rate (TPR), True Negative Rate (TNR) or Specificity, False Positive Rate (FPR), False Negative Rate (FNR), Error Rate (ER), Precision (PR), Classification Rate or Accuracy (AC), Kappa Statistic (KS), F1-score (FS) and Time Consumption (TC)",upwards of 99% F1 and accuracy,0.89,"A pipeline is proposed for sentence level sentiment analysis on two Twitter datasets. The pipeline comprises of data cleaning and preprocessing (this effort alone reduces the error rate by an order of magnitude), learning embeddings, CNN+FFNN for classification, and a MFS (Mandani Fuzzy System) classification step. The algorithms are implemented on hadoop map reduce. The best models achieve F1 and accuracy scores above the SOTA, as well as improve computational efficiency and cost complexity. For the MFS, the choices of linguistic terms and fuzzy rules are made manually relying on domain expertise.  It would be interesting to see how well this system performs on less emotionally charged datasets. A future neural-symbolic direction might be to learn these ""parameters"" automatically with a view to a generalizable system. 
The system doesn't fall into any of the Kautz categories, as it's not strictly speaking an integration of neuro-symnolic techniques, but rather a sequential pipeline.",,TRUE,FALSE,logical,explicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,better than baselines,lower than baselines,"reasoning takes palce at the fuzzy logic stage. 
""Fuzzy logic is proposed to handle uncertainty and vagueness data. The strength of fuzzy logic is its resemblance to human reasoning and natural language."" statement is backed up with several studies using fuzzy logic to deal with ambiguity, as well as comparing fuzzy logic to human reasoning."
cp,\cite{Amin2019133},Cases without borders: Automating knowledge acquisition approach using deep autoencoders and siamese networks in case-based reasoning,,2-s2.0-85081087069,10.1109/ICTAI.2019.00027,"31st IEEE International Conference on Tools with Artificial Intelligence, ICTAI 2019",2019,Question answering,similarity,unsupervised,"This paper presents an enhanced version of DeepKAF, where a hybrid CBR approach performs automatic feature extractions and defines similarity measures across the entire textual case base.","enhancements in the DeepKAF architecture for hybrid Textual Case-based Reasoning. Using Skip-thought + MaLSTM (Manhatan siamese network).
",,case based reasoning (CBR),recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,Private,Skipthought & MaLSTM -> similarities across all cases.,p@k,,0.89,"Improvemnet of the overall framework performance in terms of less training time, faster retrieval and more accurate results. The unsupervised approach of generating text representations beat the supervised approach. SImilarities between the learned representations and cases were more accurate. AE helped speeding up the overall model training process of and improved the overall retrieval time as well.
","I'm not sure if this can be considered Neuro-symbolic. The case base could be considered symbolic, but it's a stretch since all we are doing is comparing sub-symbolic representations.",FALSE,FALSE,rules,explicit,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,2,better than baselines,lower than baselines,"CBR - reasoning consists of comparing output of NN to cases and selecting the most similar one. To call this reasoning is a stretch, but that's the name of the technique."
cp,\cite{Sutherland2019},Leveraging Recursive Processing for Neural-Symbolic Affect-Target Associations,,2-s2.0-85073214635,10.1109/IJCNN.2019.8851875,"2019 International Joint Conference on Neural Networks, IJCNN 2019",2019,Sentiment analysis,classification,unsupervised,Using a rules to identify subtrees in a dependency parse so that each subtree can be evaluated using its own local context. Subtrees are fed into a BiLSTM for classification.,a novel approach of combining the recursive and structured sentiment parse provided by the recursive Tree-LSTM with symbolic rules to perform targeted affect labeling.,,rule based,recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,SemEval 2016,Dependency parse using Spacy -> rules based subtree extraction -> BiLSTM classification,Accuracy,67%,0.78,"As far as I can make out, the symbolic component comprises of rules to extract relevant subtrees and to identify affective targets from a dependency parse. These subtrees are used as input to a BiLSTM classification model. For each subtree (target), a sentiment label is predicted. The result is interpretable given the rules based approach to the identification of targets and local context.","The paper is written with lots of jargon and run-on sentences and as such, it is unnecesarily difficult to understand. As in many other papers, the term eraoning is bandied about in a hand-wavy fashion, even though no actual reasoning is performed by the system. In other words, the papaer is padded with keywords such as 'commonsense', 'reasonning', and 'neuro-symbolic', but these are buzzwords rather than actual techniques used.",TRUE,FALSE,rules,implicit,TRUE,FALSE,TRUE,FALSE,FALSE,TRUE,2,better than baselines,,"leverages symbolic structure but does not reason in the sense that there is no logic, just rules. The authors refer to this as 'symbolic reasoning'. 
I'm not sure why the authors feel the need to mention ""commonsense principles"".  There is a single sentence in the text which is a tautology. ""Commonsense principles dictate that the context in which an entity exists will be local to the position of the entity in a sentence.""
""Our approach uses a mixture of commonsense principles, symbolic reasoning, and recursive neural learning to construct associations between affective labels and identified targets in natural language expressions."""
cp,\cite{Ayyanar2019},Causal relation classification using convolutional neural networks and grammar tags,,2-s2.0-85083026315,10.1109/INDICON47234.2019.9028985,"16th IEEE India Council International Conference, INDICON 2019",2019,Causal Reasoning,classification,supervised,Without using wordnet and word2vec it is possible to identify cause-effect pair with the help of grammar tags.,,,constituency parsing,convolutional neural network (CNN),sequential,1. symbolic Neuro symbolic,SemEval-2010,The input is grammar tags of three sections and the parse tree distance between two nominals. Fully connected CNN produces binary value which indicated the presence of cause-effect relation in a given sentence.,F1,86%,0.67,"A novel approach using only the grammar tags has been proposed for the classification of a cause-effect pair.  CNNs are trained iwth three different sections of a sentence. These sections are generated using parse tree in which 1st and 2nd nominal words are identified (before NW, B/w NW and after NW). The output layer has one output which gives binary value representing the presence of cause-effect relation in a given sentence.",,TRUE,FALSE,rules,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,better than baselines,,"""The underlying hypothesis of the proposed approach establishes the link between language constructs and human thought constructs such as cause-effect pair."" The authors don't call it reasoning, but ""human thought constructs"" can be interpreted as human reasoning. it would be interesting to see if this approach meets the OOD , reduced data, and transferability promises. 
"
ar,\cite{Zhou20212015},Relation Extraction in Dialogues: A Deep Learning Model Based on the Generality and Specialty of Dialogue Text,,RDSQSBN7,10.1109/TASLP.2021.3082295,IEEE/ACM Transactions on Audio Speech and Language Processing,2021,Relation extraction,classification,supervised,the combination of 4 different modules improves overall performance for this task and supercedes the SOTA,a novel end-to-end model for the novel dialogue-based relation extraction task with code.,NA,"graph representation,
graph reasoning,
semantic enrichment","transformer,
graph neural network (GNN),
neural network (NN)",sequential,1. symbolic Neuro symbolic,DialogRE,the context encoder -> attention layer -> graph layer -> classifier,F1,64.89%,0.78,"Attention using BERT followed by graph ""reasoning"" with GNN. The system exploits the structure of the data to extract relations. It is not clear what is actually being learned by the graph module, but the authors do provide a code repo. The representations learned by the first 3 modules facilitate the final classification task.",this is basically semantic enrichment with BERT and GNN. Those representations are fed into a NN for classification,FALSE,TRUE,semantic network,implicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,better than baselines,,"""the graph reasoning layer can learn the representation features of different nodes and the mentions of the same entity have similar representations in the feature space."" reasoning over graphs can produce better representations in vector space."
cp,\cite{Manda2020},Automated ontology-based annotation of scientific literature using deep learning,,2-s2.0-85087103850,10.1145/3391274.3393636,"2020 International Workshop on on Semantic Big Data, SBD 2020 - In conjunction with the 2020 ACM SIGMOD/PODS Conference",2020,Annotation,classification,supervised,"Inclusion of ontology semantics via subsumption reasoning. Even with a small training dataset, the domain specific embeddings performed better than large pretrained language models",domain-specic embedding model trained on the CRAFT,,ontology,recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,CRAFT,"Three types of inputs are used - character, word, and part-of-speech (POS) embeddings. Character embeddings after a 30% dropout layer are passed through a 150 unit GRU model with a 50% recurrent dropout. The output from this GRU model is concatenated with word and part-of-speech embeddings is passed to a 200 unit bi-directional GRU unit.","F1,
Precision/recall,
Jaccard semantic similarity
",0.79 F1,0.67,They have created their own embeddings based on CRAFT (Colorado Richly Annotated Full Text). Trained GRU and LSTM with multiple embedding techniques. As per theory semantically enriched.,"I did not understand the significance. 
I think the unique approach here is to encode the ontology heirarchy. However, it did not result in better performance. They provide some future directions to improve how this heirarchy is encoded.
I'm not sure I understand how they measured the accuracy when the top two predictions were considered. Did they pick one out of these two, or did they just score a hit if any of the two were a hit. If the latter, then it is not surprising that the accuracy has increased, and therefore I do not understand why this is a ""surprising"" result.",TRUE,FALSE,semantic network,implicit,TRUE,FALSE,FALSE,TRUE,FALSE,FALSE,2,better than baselines,,"""Inclusion of ontology semantics via subsumption reasoning yielded modest performance improvement.""
""Augmenting the models with subsumption semantics from the ontology hierarchy showed modest but certain gains in both F-1 score and semantic similarity"".
subsumption reasoning follows heirarchical rules. Here reasoning is rules. this component does not significantly improve performance."
cp,\cite{Chen2021328},Web question answering with neurosymbolic program synthesis,,2-s2.0-85108908770,10.1145/3453483.3454047,"42nd ACM SIGPLAN International Conference on Programming Language Design and Implementation, PLDI 2021",2021,Question answering,classification,semi-supervised,leverages the structure of HTML and Large pretrained language models to significantly outperform all the baselines,new technique for web question answering that is based on optimal neurosymbolic program synthesis. a DSL for web information extraction that combines pre-trained NLP models with traditional language constructs for string manipulation and tree traversal.,,"domain specific language (DSL),
transductive learning,
program synthesis",neural network (NN),nested,2. Symbolic[Neuro],Private,NN -> DSL -> informatin extraction,"F1, Precision/recall",[0.7 - 0.9] - varies per task. much higher than baselines,0.89,"the sytem is made up of existing NLP models (like BERT), for tasks such as keyword extractiion and NER, and a custom DSL for traversing the HTML structure of web pages. The DSL is selected via transductive learning on unlabeled web pages.",,FALSE,FALSE,rules,explicit,FALSE,FALSE,FALSE,TRUE,FALSE,TRUE,1,better than baselines,,The authors don't explictly talk about reasoning. their system can be said to be doing reasoning by following rules using a DSL. 
cp,\cite{Fazlic20191025},A novel NLP-fuzzy system prototype for information extraction from medical guidelines,,2-s2.0-85070277710,10.23919/MIPRO.2019.8756929,"42nd International Convention on Information and Communication Technology, Electronics and Microelectronics, MIPRO 2019",2019,Information extraction,classification,supervised,"Our NLP-FUZZY system prototype performs a semantic extraction of a medical guideline using Bi-directional Long short-term memory (bi-LSTM) in the first step. In the second step, using the extracted semantic classes, the NLP-FUZZY system creates fuzzy rules and a database that recognizes a new case while predicting a priority of a recommendation.",Our NLP-FUZZY system prototype allows the classification of recommendations by semantic meaning and prediction of the recommendation grade.,,"fuzzy logic,
fuzzy rules",recurrent neural network (RNN),sequential,1. symbolic Neuro symbolic,"Medical Guideline Central, word2vec -> lstm (5 class prediction for each word) -> 125 successfully obtained fuzzy rules in the fuzzy rules database in MATLAB -> defuzzification
",LSTM -> Fuzzy logic,F1,12 points above SOTA LSTM only system.,0.67,"First a NN predicts 1 of 5 classes for each word, then a fuzzy rule set is generated broadly based on word frequencies as far as I can understand. The defuzzification process results in a score for each recommendation.",,FALSE,FALSE,logical,explicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,better than baselines,,authors refer to reasoning in the explicit sense of following rules. No additioinal discussion of reasoning is present.
cp,\cite{Mao2019},"The neuro-symbolic concept learner: Interpreting scenes, words, and sentences from natural supervision",,2-s2.0-85083954234,,"7th International Conference on Learning Representations, ICLR 2019",2019,Question answering,"classification,
reasoning",reinforcement,joint learning of vision and natural language (semantic query representations) with zero annotations.,"neuro-symbolic concept learner (NS-CL), which jointly learns visual perception, words, and semantic language parsing from images and question-answer pairs",,domain specific language (DSL),recurrent neural network (RNN),cooperative,3. Neuro; Symbolic,CLEVR,Object detection to latent representations -> semantic parsing of question using DSL (GRU) -> Quasi symbolic program executor,Accuracy,almost perfect,0.89,"This work acheives SOTA results without explicit annotations or class labels. It also does so even on a small subset of the data (10%), where other approaches degrade in accuracy by 30% or more. The limitation is the hand crafted DSL and no improvements in accuracy when the data is non-trivial, such as images from real life (as opposed to the synthetic world of CLEVR which contains only basic 3d shapes and colors).",,TRUE,FALSE,rules,explicit,TRUE,TRUE,TRUE,TRUE,FALSE,TRUE,4,better than baselines,lower than baselines,"The authors acknowledge impllict and explicit reasoning, but do not take a stance as to which is the more legitimate.

""We propose to use neural symbolic reasoning as a bridge to jointly learn visual concepts, words, and semantic parsing of sentences.""
""we propose an alternative representation learning approach through joint reasoning with language.""
""Our program executor works in a symbolic and deterministic manner. This feature ensures a transparent execution trace of the program.""
""The explicit program semantics enjoys compositionality, interpretability, and generalizability.""
""Using only 10% of the training images, our model is able to achieve a comparable results with the baselines trained on the full dataset.""
""Our model outperforms all baselines on data efficiency. This comes from the full disentanglement of visual concept learning and symbolic reasoning""
Future work:
Second, our model assumes a domain-specific language for describing formal semantics. The integration of formal semantics into the processing of complex natural language would be meaningful future work (Artzi & Zettlemoyer, 2013; Oh et al., 2017). We hope our paper could motivate future research in visual concept learning, language learning, and compositionality.
"
cp,\cite{DSouza201990},Team SVMrank: Leveraging feature-rich support vector machines for ranking explanations to elementary science questions,,2-s2.0-85085038180,,"13th Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs 2019, in conjunction with the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",2019,Question answering,classification,supervised,Ranking of explantions for answers ; Dataset used is highschool/primary science questions. Model is built using SVM which is again refined by rule based technique.,,,rule based,SVM,sequential,1. symbolic Neuro symbolic,MIER-19,"SVM -> Rules ; Model is trained by considering (Q, A) as features and E as label. To rank the answers, rule based technique is used.","Mean Average Precision, MAP)",39%,0.67,"The MIER-19 task focused on computing a ranked list of explanation sentences for a question and its correct answer (QA) from an unordered collection of explanation sentences. Ranking is computed using features such as lexical, relations (from OpenIE) and semantics (ConceptNet). Later pairwise relations are learned and model is built as binary classification model.",Part of Multi-Hop  Inference for Explanation Regeneration (MIER) 2019 challenge.,FALSE,FALSE,rules,explicit,TRUE,FALSE,FALSE,FALSE,FALSE,FALSE,1,worse than baselines,,"athors choose to report performance in terms of the weakest baseline. In fact, their solution is the worst of all entries. Rules are the reasoning component, and the knowledgebase of explanations is refered to as commonsense knowledge."
cp,\cite{Huang20191344},Attentive tensor product learning,,2-s2.0-85080593464,,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Annual Conference on Innovative Applications of Artificial Intelligence, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",2019,Image captioning,generative,supervised,"ATPL exploits Tensor Product Representations (TPR), a structured neural-symbolic model developed in cognitive science, to integrate deep learning with explicit language structures and rules.",a novel attention-based neural network architecture for learning the unbinding vectors ut to serve the core at ATPL,,constituency parsing,"recursive neural network (RcNN),
Tensor Product Representation (TPR)",compiled,5. Neuro_Symbolic,"COCO, Penn TreeBank",TPR combined with LSTM with attention mechanism,"BLEU, METEOR, CIDEr",improves on LSTM,0.78,"The purpose of ATPR is to replace the LSTM core layers. Future work will incorporate this into better performing systems. Our findings in this paper show great promise of TPRs. The model has a novel architecture based on a rationale derived from the use of Tensor Product Representations for encoding and processing symbolic structure through neural network computation.

",,TRUE,FALSE,rules,implicit,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,0,better than baselines,,"Authors do not explicitly mention reasoning.
TPR with attention replaces LSTM.
""The novelty of our approach lies in its ability to extract the grammatical structure of a sentence"". Perhaps it can be used in downstream tasks that can leverage this structure."
cp,\cite{Tato2019623},Hybrid deep neural networks to predict socio-moral reasoning skills,,2-s2.0-85081092649,,"12th International Conference on Educational Data Mining, EDM 2019",2019,Text classification,classification,supervised,combine a priori knowledge with deep learning architecture using the attentional mechanism,incorporating expert knowledge via an attention mechanism into a DNN,,semantic enrichment,"recurrent neural network (RNN),
convolutional neural network (CNN),
attention network (Attn)",sequential,1. symbolic Neuro symbolic,SoMoral,text -> LSTM (with attention which holds the expert knowledge) -> prediction,F1,0.73,0.56,"The authors combine expert knowledge - a vector of a priori weights or each class. This vector is used to weight the importance of the DNN hidden layers. The main improvement over SOTA is due to the fact that there are separate models learned for each class in addition to the expert knowledge, while the attention mechanism provides only a minimal improvement. Also, this study is self referencial. The results are only compared with the authors' own prior research, and are not put into a braoder context of research.",,FALSE,FALSE,frames,implicit,FALSE,FALSE,FALSE,FALSE,FALSE,FALSE,0,,,"even though it has reasoning in the title, this is a classification model where expert knowledge is represented in the attention mechanism.
reasoning is in reference to predicting humans ability to reason not the models ability to reason."
cp,\cite{Huo2019159},Graph enhanced cross-domain text-to-SQL generation,,2-s2.0-85085049747,,"13th Workshop on Graph-Based Methods for Natural Language Processing, TextGraphs 2019, in conjunction with the 2019 Conference on Empirical Methods in Natural Language Processing and 9th International Joint Conference on Natural Language Processing, EMNLP-IJCNLP 2019",2019,N2F,generative,unsupervised,translation of text to sql queries. A database graph is constructed and a graph neural network is used to encode the columns.,"a novel column embedding technique that additionally includes a graph of the tables, connected through shared column names.",,"symbolic node embedding,
graph representation,
semantic parsing",graph neural network (GNN),compiled,4. Neuro: Symbolic → Neuro,Spider,"create graph from column names, table names, and column types from multiple databases -> GNN learn node representations -> feed into SyntaxSQLNet",Accuracy,57%,0.78,"Our model is based on the framework of SyntaxSQLNet (Yu et al., 2018b) but extends it with our approach for generating column embeddings. Our idea is to use a graph to connect the column names across all tables and databases and compute representations of them by using a graph neural network. In this way, the information of different domains is passed to each other, so that one can learn a better column embedding that generalizes across domains.  (I am calssifying this as unsupervised because the part of the system which this study describes is unsupervised. However, the system as a whole is supervised).
The results show that the graph representations outperform vanilla SyntaxSQLNet, but do not perform as well as a data-augmentation technique. However, the data augmentation technique relies on expensive hand-annotated data, whereas the graph technique does not have this overhead.
","I am not sure if this is neuro-symbolic. The symbolic part is the graph. Embeddings are learnded which capture the relationships between nodes in a sub-symbolic representation. However, there is no reasoning component.",TRUE,TRUE,rules,implicit,FALSE,TRUE,FALSE,TRUE,TRUE,FALSE,3,better than baselines,,"using GNN is better than baselines, but not better than traditional data augmentation"
cp,\cite{Chen20201544},Mapping natural-language problems to formal-language solutions using structured neural representations,,2-s2.0-85105153943,,"37th International Conference on Machine Learning, ICML 2020",2020,N2F,generative,supervised,"Encoder binding, decoder unbinding. State-of-the-art performance on two recently developed N2F tasks shows that the TPN2F model has significant structure learning ability on tasks requiring symbolic reasoning through program synthesis.","a new scheme for neural-symbolic relational representations and a new architecture, TP-N2F, for formal-language generation from natural-language descriptions.",,binding problem,"Tensor Product Representation (TPR),
recurrent neural network (RNN)",compiled,5. Neuro_Symbolic,MathQA,natural language to TPR -> LSTM encoder ( binding) -> LSTM decoder (unbinding) -> formal language,Accuracy,71.89%,0.78,"Natural- to formal-language generation. Using tensor product representations in an emcoder-decoder architecture.
",,TRUE,FALSE,frames,implicit,TRUE,FALSE,TRUE,FALSE,TRUE,FALSE,3,better than baselines,,"TPR
""State-of-the-art performance on two recently developed N2F tasks shows that the TPN2F model has significant structure learning ability on tasks requiring symbolic reasoning through program synthesis.""
""Next, we will combine largescale deep learning models such as BERT with TP-N2F to take advantage of structure learning for other generation tasks.""
The mapping step between the encoder and decoder is refered to as a ""reasoning module"", but no justification for this nomenclature is given. This is simply a MLP which transforms the encoded text input into a TPR. I cannot see the ""reasoning"" part here.
""When people perform explicit reasoning, they can typically describe the way to the conclusion step by step via relational descriptions.""

"
cp,\cite{Demeter20207634},Just Add Functions: A Neural-Symbolic Language Model,,2-s2.0-85106687657,,"34th AAAI Conference on Artificial Intelligence, AAAI 2020",2020,Language modeling,generative,supervised,"adding explicit logic functions to a language model improves performance especially on small datasets. It does this by enhancing the inductive bias of NNLM. In other words, it makes the assumption that ""you are the company you keep"" (refered to as the distributional hypothesis of language) even stronger.","NSLM - neuro symbolic language model. A method for embedding domain specific logic and/or knowledge into a language model, thus enhancing the models ability to predict words in said domain. The method can be extended to any domain as long as the architect can find appropriate logical expressions for her domain. A Neural-Symbolic Language Model (NSLM) is a hierarchical NNLM that incorporates simple functions to enhance inductive bias.",,"rule based,
semantic enrichment",recurrent neural network (RNN),compiled,4. Neuro: Symbolic → Neuro,Wikitext,"1. Identify a class C, which has an inductive bias that can be encoded as a simple mathematical or logical expression. 
2. generate vocabulary for class C 
3. Add/delete candidate metric functions fc(w(c) t , w(c) t−1) and PDFs to be considered. 
4. Train a hierarchical NNLM on words and class labels and select a {metric function,PDF} pair.
",perplexity,outperforms NHLM but not LLM.,0.67,"We seek to establish a general method to tune language models for domain-specific tasks by enhancing the inductive bias with symbolic expressions that encode domain knowledge (Grounding with External Knowledge). 
A NNLM is finetuned with a ""micro-model"" which is a function (learned by grid search) over a class distribution obtained from an external knowledge base.
It's a language model, so it can be used for sentence completion, measuring similarity, and other downstream tasks. ",,FALSE,FALSE,rules,explicit,TRUE,TRUE,FALSE,FALSE,FALSE,TRUE,2,better than baselines,,"NNLM (neural network language model) cannot learn a function such as f(x)=x+1 even when the corpus is very large. By contrast augmenting an NNLM (neural network language model) with a simple micro-models allows the model to generalize well on this simple function.
The downside is that the system requires domain expert knowledge to come up with metric functions and to identify classes of interest. So this is a domain specific solution rather than a general improvement over LLMs. What would be interesting to see is how this technique comapares to other domain specific finetuning techniques. Still, theoretically, the promise of OOD has been met which is likely to not be the case with simple finetuing of domain specific data.
""Our work is related to three active subareas within language modeling research: numeracy, predicting geographic locations and integrating symbolic reasoning with NNLMs."""