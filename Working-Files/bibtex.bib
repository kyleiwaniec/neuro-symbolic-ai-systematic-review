@inproceedings{branco-etal-2021-shortcutted,
 abstract = {Commonsense is a quintessential human capacity that has been a core challenge to Artificial Intelligence since its inception. Impressive results in Natural Language Processing tasks, including in commonsense reasoning, have consistently been achieved with Transformer neural language models, even matching or surpassing human performance in some benchmarks. Recently, some of these advances have been called into question: so called data artifacts in the training data have been made evident as spurious correlations and shallow shortcuts that in some cases are leveraging these outstanding results. In this paper we seek to further pursue this analysis into the realm of commonsense related language processing tasks. We undertake a study on different prominent benchmarks that involve commonsense reasoning, along a number of key stress experiments, thus seeking to gain insight on whether the models are learning transferable generalizations intrinsic to the problem at stake or just taking advantage of incidental shortcuts in the data items. The results obtained indicate that most datasets experimented with are problematic, with models resorting to non-robust features and appearing not to be learning and generalizing towards the overall tasks intended to be conveyed or exemplified by the datasets.},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Branco, Ruben  and
Branco, Ant{\'o}nio  and
Ant{\'o}nio Rodrigues, Jo{\~a}o  and
Silva, Jo{\~a}o Ricardo},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.113},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {1504--1521},
 publisher = {Association for Computational Linguistics},
 title = {Shortcutted Commonsense: Data Spuriousness in Deep Learning of Commonsense Reasoning},
 url = {https://aclanthology.org/2021.emnlp-main.113},
 volume = {nan},
 year = {2021}
}

@inproceedings{byszuk-etal-2020-detecting,
 abstract = {Fictional prose can be broadly divided into narrative and discursive forms with direct speech being central to any discourse representation (alongside indirect reported speech and free indirect discourse). This distinction is crucial in digital literary studies and enables interesting forms of narratological or stylistic analysis. The difficulty of automatically detecting direct speech, however, is currently under-estimated. Rule-based systems that work reasonably well for modern languages struggle with (the lack of) typographical conventions in 19th-century literature. While machine learning approaches to sequence modeling can be applied to solve the task, they typically face a severed skewness in the availability of training material, especially for lesser resourced languages. In this paper, we report the result of a multilingual approach to direct speech detection in a diverse corpus of 19th-century fiction in 9 European languages. The proposed method finetunes a transformer architecture with multilingual sentence embedder on a minimal amount of annotated training in each language, and improves performance across languages with ambiguous direct speech marking, in comparison to a carefully constructed regular expression baseline.},
 address = {Marseille, France},
 author = {Byszuk, Joanna  and
Wo{\'z}niak, Micha{\l}  and
Kestemont, Mike  and
Le{\'s}niak, Albert  and
{\L}ukasik, Wojciech  and
{\v{S}}e{\c{l}}a, Artjoms  and
Eder, Maciej},
 booktitle = {Proceedings of LT4HALA 2020 - 1st Workshop on Language Technologies for Historical and Ancient Languages},
 doi = {nan},
 editor = {nan},
 isbn = {979-10-95546-53-5},
 journal = {nan},
 language = {English},
 month = {May},
 note = {nan},
 number = {nan},
 pages = {100--104},
 publisher = {European Language Resources Association (ELRA)},
 title = {Detecting Direct Speech in Multilingual Collection of 19th-century Novels},
 url = {https://aclanthology.org/2020.lt4hala-1.15},
 volume = {nan},
 year = {2020}
}

@inproceedings{chang-etal-2020-benchmark,
 abstract = {Much of biomedical and healthcare data is encoded in discrete, symbolic form such as text and medical codes. There is a wealth of expert-curated biomedical domain knowledge stored in knowledge bases and ontologies, but the lack of reliable methods for learning knowledge representation has limited their usefulness in machine learning applications. While text-based representation learning has significantly improved in recent years through advances in natural language processing, attempts to learn biomedical concept embeddings so far have been lacking. A recent family of models called knowledge graph embeddings have shown promising results on general domain knowledge graphs, and we explore their capabilities in the biomedical domain. We train several state-of-the-art knowledge graph embedding models on the SNOMED-CT knowledge graph, provide a benchmark with comparison to existing methods and in-depth discussion on best practices, and make a case for the importance of leveraging the multi-relational nature of knowledge graphs for learning biomedical knowledge representation. The embeddings, code, and materials will be made available to the community.},
 address = {Online},
 author = {Chang, David  and
Bala{\v{z}}evi{\'c}, Ivana  and
Allen, Carl  and
Chawla, Daniel  and
Brandt, Cynthia  and
Taylor, Andrew},
 booktitle = {Proceedings of the 19th SIGBioMed Workshop on Biomedical Language Processing},
 doi = {10.18653/v1/2020.bionlp-1.18},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {July},
 note = {nan},
 number = {nan},
 pages = {167--176},
 publisher = {Association for Computational Linguistics},
 title = {Benchmark and Best Practices for Biomedical Knowledge Graph Embeddings},
 url = {https://aclanthology.org/2020.bionlp-1.18},
 volume = {nan},
 year = {2020}
}

@inproceedings{chaudhury-etal-2021-neuro,
 abstract = {Text-Based Games (TBGs) have emerged as important testbeds for reinforcement learning (RL) in the natural language domain. Previous methods using LSTM-based action policies are uninterpretable and often overfit the training games showing poor performance to unseen test games. We present SymboLic Action policy for Textual Environments (SLATE), that learns interpretable action policy rules from symbolic abstractions of textual observations for improved generalization. We outline a method for end-to-end differentiable symbolic rule learning and show that such symbolic policies outperform previous state-of-the-art methods in text-based RL for the coin collector environment from 5-10x fewer training games. Additionally, our method provides human-understandable policy rules that can be readily verified for their logical consistency and can be easily debugged.},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Chaudhury, Subhajit  and
Sen, Prithviraj  and
Ono, Masaki  and
Kimura, Daiki  and
Tatsubori, Michiaki  and
Munawar, Asim},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.245},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {3073--3078},
 publisher = {Association for Computational Linguistics},
 title = {Neuro-Symbolic Approaches for Text-Based Policy Learning},
 url = {https://aclanthology.org/2021.emnlp-main.245},
 volume = {nan},
 year = {2021}
}

@inproceedings{chen-etal-2020-question,
 abstract = {Numerical reasoning over texts, such as addition, subtraction, sorting and counting, is a challenging machine reading comprehension task, since it requires both natural language understanding and arithmetic computation. To address this challenge, we propose a heterogeneous graph representation for the context of the passage and question needed for such reasoning, and design a question directed graph attention network to drive multi-step numerical reasoning over this context graph. Our model, which combines deep learning and graph reasoning, achieves remarkable results in benchmark datasets such as DROP.},
 address = {Online},
 author = {Chen, Kunlong  and
Xu, Weidi  and
Cheng, Xingyi  and
Xiaochuan, Zou  and
Zhang, Yuyu  and
Song, Le  and
Wang, Taifeng  and
Qi, Yuan  and
Chu, Wei},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.549},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {6759--6768},
 publisher = {Association for Computational Linguistics},
 title = {Question Directed Graph Attention Network for Numerical Reasoning over Text},
 url = {https://aclanthology.org/2020.emnlp-main.549},
 volume = {nan},
 year = {2020}
}

@inproceedings{chen-etal-2021-de,
 abstract = {Logical table-to-text generation aims to automatically generate fluent and logically faithful text from tables. The task remains challenging where deep learning models often generated linguistically fluent but logically inconsistent text. The underlying reason may be that deep learning models often capture surface-level spurious correlations rather than the causal relationships between the table $\boldsymbol{x}$ and the sentence $\boldsymbol{y}$. Specifically, in the training stage, a model can get a low empirical loss without understanding $\boldsymbol{x}$ and use spurious statistical cues instead. In this paper, we propose a de-confounded variational encoder-decoder (DCVED) based on causal intervention, learning the objective $p(\boldsymbol{y}|\textrm{do}(\boldsymbol{x}))$. Firstly, we propose to use variational inference to estimate the confounders in the latent space and cooperate with the causal intervention based on Pearl{'}s do-calculus to alleviate the spurious correlations. Secondly, to make the latent confounder meaningful, we propose a back-prediction process to predict the not-used entities but linguistically similar to the exactly selected ones. Finally, since our variational model can generate multiple candidates, we train a table-text selector to find out the best candidate sentence for the given table. An extensive set of experiments show that our model outperforms the baselines and achieves new state-of-the-art performance on two logical table-to-text datasets in terms of logical fidelity.},
 address = {Online},
 author = {Chen, Wenqing  and
Tian, Jidong  and
Li, Yitian  and
He, Hao  and
Jin, Yaohui},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.430},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {5532--5542},
 publisher = {Association for Computational Linguistics},
 title = {De-Confounded Variational Encoder-Decoder for Logical Table-to-Text Generation},
 url = {https://aclanthology.org/2021.acl-long.430},
 volume = {nan},
 year = {2021}
}

@inproceedings{chen-etal-2021-neurallog,
 abstract = {Deep learning (DL) based language models achieve high performance on various benchmarks for Natural Language Inference (NLI). And at this time, symbolic approaches to NLI are receiving less attention. Both approaches (symbolic and DL) have their advantages and weaknesses. However, currently, no method combines them in a system to solve the task of NLI. To merge symbolic and deep learning methods, we propose an inference framework called NeuralLog, which utilizes both a monotonicity-based logical inference engine and a neural network language model for phrase alignment. Our framework models the NLI task as a classic search problem and uses the beam search algorithm to search for optimal inference paths. Experiments show that our joint logic and neural inference system improves accuracy on the NLI task and can achieve state-of-art accuracy on the SICK and MED datasets.},
 address = {Online},
 author = {Chen, Zeming  and
Gao, Qiyue  and
Moss, Lawrence S.},
 booktitle = {Proceedings of *SEM 2021: The Tenth Joint Conference on Lexical and Computational Semantics},
 doi = {10.18653/v1/2021.starsem-1.7},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {78--88},
 publisher = {Association for Computational Linguistics},
 title = {{N}eural{L}og: Natural Language Inference with Joint Neural and Logical Reasoning},
 url = {https://aclanthology.org/2021.starsem-1.7},
 volume = {nan},
 year = {2021}
}

@inproceedings{chrupala-alishahi-2019-correlating,
 abstract = {Analysis methods which enable us to better understand the representations and functioning of neural models of language are increasingly needed as deep learning becomes the dominant approach in NLP. Here we present two methods based on Representational Similarity Analysis (RSA) and Tree Kernels (TK) which allow us to directly quantify how strongly the information encoded in neural activation patterns corresponds to information represented by symbolic structures such as syntax trees. We first validate our methods on the case of a simple synthetic language for arithmetic expressions with clearly defined syntax and semantics, and show that they exhibit the expected pattern of results. We then our methods to correlate neural representations of English sentences with their constituency parse trees.},
 address = {Florence, Italy},
 author = {Chrupa{\l}a, Grzegorz  and
Alishahi, Afra},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1283},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {July},
 note = {nan},
 number = {nan},
 pages = {2952--2962},
 publisher = {Association for Computational Linguistics},
 title = {Correlating Neural and Symbolic Representations of Language},
 url = {https://aclanthology.org/P19-1283},
 volume = {nan},
 year = {2019}
}

@inproceedings{das-etal-2021-case,
 abstract = {It is often challenging to solve a complex problem from scratch, but much easier if we can access other similar problems with their solutions {---} a paradigm known as case-based reasoning (CBR). We propose a neuro-symbolic CBR approach (CBR-KBQA) for question answering over large knowledge bases. CBR-KBQA consists of a nonparametric memory that stores cases (question and logical forms) and a parametric model that can generate a logical form for a new question by retrieving cases that are relevant to it. On several KBQA datasets that contain complex questions, CBR-KBQA achieves competitive performance. For example, on the CWQ dataset, CBR-KBQA outperforms the current state of the art by 11{\%} on accuracy. Furthermore, we show that CBR-KBQA is capable of using new cases \textit{without} any further training: by incorporating a few human-labeled examples in the case memory, CBR-KBQA is able to successfully generate logical forms containing unseen KB entities as well as relations.},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Das, Rajarshi  and
Zaheer, Manzil  and
Thai, Dung  and
Godbole, Ameya  and
Perez, Ethan  and
Lee, Jay Yoon  and
Tan, Lizhen  and
Polymenakos, Lazaros  and
McCallum, Andrew},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.755},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {9594--9611},
 publisher = {Association for Computational Linguistics},
 title = {Case-based Reasoning for Natural Language Queries over Knowledge Bases},
 url = {https://aclanthology.org/2021.emnlp-main.755},
 volume = {nan},
 year = {2021}
}

@inproceedings{davis-etal-2021-computational,
 abstract = {This paper presents a finite-state computational model of the verbal morphology of Michif. Michif, the official language of the M{\'e}tis peoples, is a uniquely mixed language with Algonquian and French origins. It is spoken across the M{\'e}tis homelands in what is now called Canada and the United States, but it is highly endangered with less than 100 speakers. The verbal morphology is remarkably complex, as the already polysynthetic Algonquian patterns are combined with French elements and unique morpho-phonological interactions.The model presented in this paper, LI VERB KAA-OOSHITAHK DI MICHIF handles this complexity by using a series of composed finite-state transducers to model the concatenative morphology and phonological rule alternations that are unique to Michif. Such a rule-based approach is necessary as there is insufficient language data for an approach that uses machine learning. A language model such as LI VERB KAA-OOSHITAHK DI MICHIF furthers the goals of Indigenous computational linguistics in Canada while also supporting the creation of tools for documentation, education, and revitalization that are desired by the M{\'e}tis community.},
 address = {Online},
 author = {Davis, Fineen  and
Santos, Eddie Antonio  and
Souter, Heather},
 booktitle = {Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume},
 doi = {10.18653/v1/2021.eacl-main.226},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {April},
 note = {nan},
 number = {nan},
 pages = {2631--2636},
 publisher = {Association for Computational Linguistics},
 title = {On the Computational Modelling of {M}ichif Verbal Morphology},
 url = {https://aclanthology.org/2021.eacl-main.226},
 volume = {nan},
 year = {2021}
}

@inproceedings{dehua-etal-2021-bdcn,
 abstract = {Building an interpretable AI diagnosis system for breast cancer is an important embodiment ofAI assisted medicine. Traditional breast cancer diagnosis methods based on machine learning areeasy to explain but the accuracy is very low. Deep neural network greatly improves the accuracy of diagnosis but the black box model does not provide transparency and interpretation. In this work we propose a semantic embedding self-explanatory Breast Diagnostic Capsules Network(BDCN). This model is the first to combine the capsule network with semantic embedding for theAI diagnosis of breast tumors using capsules to simulate semantics. We pre-trained the extrac-tion word vector by embedding the semantic tree into the BERT and used the capsule network to improve the semantic representation of multiple heads of attention to construct the extraction feature the capsule network was extended from the computer vision classification task to the text classification task. Simultaneously both the back propagation principle and dynamic routing algorithm are used to realize the local interpretability of the diagnostic model. The experimental results show that this breast diagnosis model improves the model performance and has good interpretability which is more suitable for clinical situations.IntroductionBreast cancer is an important killer threatening women{'}s health because of rising incidence. Early detection and diagnosis are the key to reduce the mortality rate of breast cancer and improve the quality of life of patients. Mammary gland molybdenum target report contains rich semantic information whichcan directly reflect the results of breast cancer screening (CACA-CBCS 2019) and AI-assisted diagno-sis of breast cancer is an important means. Therefore various diagnostic models were born. Mengwan(2020) used support vector machine(SVM) and Naive Bayes to classify morphological features with anaccuracy of 91.11{\%}. Wei (2009) proposed a classification method of breast cancer based on SVM andthe accuracy of the classifier experiment is 79.25{\%}. These traditional AI diagnoses of breast tumors havelimited data volume and low accuracy. Deep Neural Networks (DNN) enters into the ranks of the diagno-sis of breast tumor. Wang (2019) put forward a kind of based on feature fusion with CNN deep features of breast computer-aided diagnosis methods the accuracy is 92.3{\%}. Zhao (2018) investigated capsule networks with dynamic routing for text classification which proves the feasibility of text categorization. Existing models have poor predictive effect and lack of interpretation which can not meet the clinical needs.},
 address = {Huhhot, China},
 author = {Dehua, Chen  and
Keting, Zhong  and
Jianrong, He},
 booktitle = {Proceedings of the 20th Chinese National Conference on Computational Linguistics},
 doi = {nan},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {English},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {1178--1189},
 publisher = {Chinese Information Processing Society of China},
 title = {{BDCN}: Semantic Embedding Self-explanatory Breast Diagnostic Capsules Network},
 url = {https://aclanthology.org/2021.ccl-1.105},
 volume = {nan},
 year = {2021}
}

@inproceedings{ding-etal-2020-hit,
 abstract = {We describe our system for Task 5 of SemEval 2020: Modelling Causal Reasoning in Language: Detecting Counterfactuals. Despite deep learning has achieved significant success in many fields, it still hardly drives today{'}s AI to strong AI, as it lacks of causation, which is a fundamental concept in human thinking and reasoning. In this task, we dedicate to detecting causation, especially counterfactuals from texts. We explore multiple pre-trained models to learn basic features and then fine-tune models with counterfactual data and pseudo-labeling data. Our team HIT-SCIR wins the first place (1st) in Sub-task 1 {---} Detecting Counterfactual Statements and is ranked 4th in Sub-task 2 {---} Detecting Antecedent and Consequence. In this paper we provide a detailed description of the approach, as well as the results obtained in this task.},
 address = {Barcelona (online)},
 author = {Ding, Xiao  and
Hao, Dingkui  and
Zhang, Yuewei  and
Liao, Kuo  and
Li, Zhongyang  and
Qin, Bing  and
Liu, Ting},
 booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation},
 doi = {10.18653/v1/2020.semeval-1.43},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {December},
 note = {nan},
 number = {nan},
 pages = {354--360},
 publisher = {International Committee for Computational Linguistics},
 title = {{HIT}-{SCIR} at {S}em{E}val-2020 Task 5: Training Pre-trained Language Model with Pseudo-labeling Data for Counterfactuals Detection},
 url = {https://aclanthology.org/2020.semeval-1.43},
 volume = {nan},
 year = {2020}
}

@inproceedings{dsouza-etal-2019-team,
 abstract = {The TextGraphs 2019 Shared Task on Multi-Hop Inference for Explanation Regeneration (MIER-19) tackles explanation generation for answers to elementary science questions. It builds on the AI2 Reasoning Challenge 2018 (ARC-18) which was organized as an advanced question answering task on a dataset of elementary science questions. The ARC-18 questions were shown to be hard to answer with systems focusing on surface-level cues alone, instead requiring far more powerful knowledge and reasoning. To address MIER-19, we adopt a hybrid pipelined architecture comprising a featurerich learning-to-rank (LTR) machine learning model, followed by a rule-based system for reranking the LTR model predictions. Our system was ranked fourth in the official evaluation, scoring close to the second and third ranked teams, achieving 39.4{\%} MAP.},
 address = {Hong Kong},
 author = {D{'}Souza, Jennifer  and
Mulang{'}, Isaiah Onando  and
Auer, S{\"o}ren},
 booktitle = {Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)},
 doi = {10.18653/v1/D19-5312},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {90--100},
 publisher = {Association for Computational Linguistics},
 title = {Team {SVM}rank: Leveraging Feature-rich Support Vector Machines for Ranking Explanations to Elementary Science Questions},
 url = {https://aclanthology.org/D19-5312},
 volume = {nan},
 year = {2019}
}

@inproceedings{guo-choi-2021-enhancing,
 abstract = {We present a novel deep learning-based framework to generate embedding representations of fine-grained emotions that can be used to computationally describe psychological models of emotions. Our framework integrates a contextualized embedding encoder with a multi-head probing model that enables to interpret dynamically learned representations optimized for an emotion classification task. Our model is evaluated on the Empathetic Dialogue dataset and shows the state-of-the-art result for classifying 32 emotions. Our layer analysis can derive an emotion graph to depict hierarchical relations among the emotions. Our emotion representations can be used to generate an emotion wheel directly comparable to the one from Plutchik{'}s model, and also augment the values of missing emotions in the PAD emotional state model.},
 address = {Online},
 author = {Guo, Yuting  and
Choi, Jinho D.},
 booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
 doi = {10.18653/v1/2021.cmcl-1.18},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {June},
 note = {nan},
 number = {nan},
 pages = {141--148},
 publisher = {Association for Computational Linguistics},
 title = {Enhancing Cognitive Models of Emotions with Representation Learning},
 url = {https://aclanthology.org/2021.cmcl-1.18},
 volume = {nan},
 year = {2021}
}

@inproceedings{gupta-etal-2021-neuro,
 abstract = {nan},
 address = {Shanghai, China},
 author = {Gupta, Komal  and
Ghosal, Tirthankar  and
Ekbal, Asif},
 booktitle = {Proceedings of the 35th Pacific Asia Conference on Language, Information and Computation},
 doi = {nan},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {11},
 note = {nan},
 number = {nan},
 pages = {40--49},
 publisher = {Association for Computational Lingustics},
 title = {A Neuro-Symbolic Approach for Question Answering on Research Articles},
 url = {https://aclanthology.org/2021.paclic-1.5},
 volume = {nan},
 year = {2021}
}

@inproceedings{gupta-sharma-2021-nlpiitr,
 abstract = {This paper describes and examines different systems to address Task 6 of SemEval-2021: Detection of Persuasion Techniques In Texts And Images, Subtask 1. The task aims to build a model for identifying rhetorical and psycho- logical techniques (such as causal oversimplification, name-calling, smear) in the textual content of a meme which is often used in a disinformation campaign to influence the users. The paper provides an extensive comparison among various machine learning systems as a solution to the task. We elaborate on the pre-processing of the text data in favor of the task and present ways to overcome the class imbalance. The results show that fine-tuning a RoBERTa model gave the best results with an F1-Micro score of 0.51 on the development set.},
 address = {Online},
 author = {Gupta, Vansh  and
Sharma, Raksha},
 booktitle = {Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)},
 doi = {10.18653/v1/2021.semeval-1.147},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {1061--1067},
 publisher = {Association for Computational Linguistics},
 title = {{NLPIITR} at {S}em{E}val-2021 Task 6: {R}o{BERT}a Model with Data Augmentation for Persuasion Techniques Detection},
 url = {https://aclanthology.org/2021.semeval-1.147},
 volume = {nan},
 year = {2021}
}

@inproceedings{han-etal-2020-explaining,
 abstract = {Modern deep learning models for NLP are notoriously opaque. This has motivated the development of methods for interpreting such models, e.g., via gradient-based saliency maps or the visualization of attention weights. Such approaches aim to provide explanations for a particular model prediction by highlighting important words in the corresponding input text. While this might be useful for tasks where decisions are explicitly influenced by individual tokens in the input, we suspect that such highlighting is not suitable for tasks where model decisions should be driven by more complex reasoning. In this work, we investigate the use of influence functions for NLP, providing an alternative approach to interpreting neural text classifiers. Influence functions explain the decisions of a model by identifying influential training examples. Despite the promise of this approach, influence functions have not yet been extensively evaluated in the context of NLP, a gap addressed by this work. We conduct a comparison between influence functions and common word-saliency methods on representative tasks. As suspected, we find that influence functions are particularly useful for natural language inference, a task in which {`}saliency maps{'} may not have clear interpretation. Furthermore, we develop a new quantitative measure based on influence functions that can reveal artifacts in training data.},
 address = {Online},
 author = {Han, Xiaochuang  and
Wallace, Byron C.  and
Tsvetkov, Yulia},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/2020.acl-main.492},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {July},
 note = {nan},
 number = {nan},
 pages = {5553--5563},
 publisher = {Association for Computational Linguistics},
 title = {Explaining Black Box Predictions and Unveiling Data Artifacts through Influence Functions},
 url = {https://aclanthology.org/2020.acl-main.492},
 volume = {nan},
 year = {2020}
}

@inproceedings{haruta-etal-2020-logical,
 abstract = {Comparative constructions pose a challenge in Natural Language Inference (NLI), which is the task of determining whether a text entails a hypothesis. Comparatives are structurally complex in that they interact with other linguistic phenomena such as quantifiers, numerals, and lexical antonyms. In formal semantics, there is a rich body of work on comparatives and gradable expressions using the notion of degree. However, a logical inference system for comparatives has not been sufficiently developed for use in the NLI task. In this paper, we present a compositional semantics that maps various comparative constructions in English to semantic representations via Combinatory Categorial Grammar (CCG) parsers and combine it with an inference system based on automated theorem proving. We evaluate our system on three NLI datasets that contain complex logical inferences with comparatives, generalized quantifiers, and numerals. We show that the system outperforms previous logic-based systems as well as recent deep learning-based models.},
 address = {Online},
 author = {Haruta, Izumi  and
Mineshima, Koji  and
Bekki, Daisuke},
 booktitle = {Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics: Student Research Workshop},
 doi = {10.18653/v1/2020.acl-srw.35},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {July},
 note = {nan},
 number = {nan},
 pages = {263--270},
 publisher = {Association for Computational Linguistics},
 title = {Logical Inferences with Comparatives and Generalized Quantifiers},
 url = {https://aclanthology.org/2020.acl-srw.35},
 volume = {nan},
 year = {2020}
}

@inproceedings{hu-etal-2021-dialoguecrn,
 abstract = {Emotion Recognition in Conversations (ERC) has gained increasing attention for developing empathetic machines. Recently, many approaches have been devoted to perceiving conversational context by deep learning models. However, these approaches are insufficient in understanding the context due to lacking the ability to extract and integrate emotional clues. In this work, we propose novel Contextual Reasoning Networks (DialogueCRN) to fully understand the conversational context from a cognitive perspective. Inspired by the Cognitive Theory of Emotion, we design multi-turn reasoning modules to extract and integrate emotional clues. The reasoning module iteratively performs an intuitive retrieving process and a conscious reasoning process, which imitates human unique cognitive thinking. Extensive experiments on three public benchmark datasets demonstrate the effectiveness and superiority of the proposed model.},
 address = {Online},
 author = {Hu, Dou  and
Wei, Lingwei  and
Huai, Xiaoyong},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.547},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {7042--7052},
 publisher = {Association for Computational Linguistics},
 title = {{D}ialogue{CRN}: Contextual Reasoning Networks for Emotion Recognition in Conversations},
 url = {https://aclanthology.org/2021.acl-long.547},
 volume = {nan},
 year = {2021}
}

@inproceedings{huo-etal-2019-graph,
 abstract = {Semantic parsing is a fundamental problem in natural language understanding, as it involves the mapping of natural language to structured forms such as executable queries or logic-like knowledge representations. Existing deep learning approaches for semantic parsing have shown promise on a variety of benchmark data sets, particularly on text-to-SQL parsing. However, most text-to-SQL parsers do not generalize to unseen data sets in different domains. In this paper, we propose a new cross-domain learning scheme to perform text-to-SQL translation and demonstrate its use on Spider, a large-scale cross-domain text-to-SQL data set. We improve upon a state-of-the-art Spider model, SyntaxSQLNet, by constructing a graph of column names for all databases and using graph neural networks to compute their embeddings. The resulting embeddings offer better cross-domain representations and SQL queries, as evidenced by substantial improvement on the Spider data set compared to SyntaxSQLNet.},
 address = {Hong Kong},
 author = {Huo, Siyu  and
Ma, Tengfei  and
Chen, Jie  and
Chang, Maria  and
Wu, Lingfei  and
Witbrock, Michael},
 booktitle = {Proceedings of the Thirteenth Workshop on Graph-Based Methods for Natural Language Processing (TextGraphs-13)},
 doi = {10.18653/v1/D19-5319},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {159--163},
 publisher = {Association for Computational Linguistics},
 title = {Graph Enhanced Cross-Domain Text-to-{SQL} Generation},
 url = {https://aclanthology.org/D19-5319},
 volume = {nan},
 year = {2019}
}

@inproceedings{imanigooghari-etal-2021-parcoure,
 abstract = {With more than 7000 languages worldwide, multilingual natural language processing (NLP) is essential both from an academic and commercial perspective. Researching typological properties of languages is fundamental for progress in multilingual NLP. Examples include assessing language similarity for effective transfer learning, injecting inductive biases into machine learning models or creating resources such as dictionaries and inflection tables. We provide ParCourE, an online tool that allows to browse a word-aligned parallel corpus, covering 1334 languages. We give evidence that this is useful for typological research. ParCourE can be set up for any parallel corpus and can thus be used for typological research on other corpora as well as for exploring their quality and properties.},
 address = {Online},
 author = {ImaniGooghari, Ayyoob  and
Jalili Sabet, Masoud  and
Dufter, Philipp  and
Cysou, Michael  and
Sch{\"u}tze, Hinrich},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations},
 doi = {10.18653/v1/2021.acl-demo.8},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {63--72},
 publisher = {Association for Computational Linguistics},
 title = {{P}ar{C}our{E}: A Parallel Corpus Explorer for a Massively Multilingual Corpus},
 url = {https://aclanthology.org/2021.acl-demo.8},
 volume = {nan},
 year = {2021}
}

@inproceedings{jahan-finlayson-2019-character,
 abstract = {Characters are a key element of narrative and so character identification plays an important role in automatic narrative understanding. Unfortunately, most prior work that incorporates character identification is not built upon a clear, theoretically grounded concept of character. They either take character identification for granted (e.g., using simple heuristics on referring expressions), or rely on simplified definitions that do not capture important distinctions between characters and other referents in the story. Prior approaches have also been rather complicated, relying, for example, on predefined case bases or ontologies. In this paper we propose a narratologically grounded definition of character for discussion at the workshop, and also demonstrate a preliminary yet straightforward supervised machine learning model with a small set of features that performs well on two corpora. The most important of the two corpora is a set of 46 Russian folktales, on which the model achieves an F1 of 0.81. Error analysis suggests that features relevant to the plot will be necessary for further improvements in performance.},
 address = {Minneapolis, Minnesota},
 author = {Jahan, Labiba  and
Finlayson, Mark},
 booktitle = {Proceedings of the First Workshop on Narrative Understanding},
 doi = {10.18653/v1/W19-2402},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {June},
 note = {nan},
 number = {nan},
 pages = {12--18},
 publisher = {Association for Computational Linguistics},
 title = {Character Identification Refined: A Proposal},
 url = {https://aclanthology.org/W19-2402},
 volume = {nan},
 year = {2019}
}

@inproceedings{janicki-2019-finite,
 abstract = {The research on machine learning of morphology often involves formulating morphological descriptions directly on surface forms of words. As the established two-level morphology paradigm requires the knowledge of the underlying structure, it is not widely used in such settings. In this paper, we propose a formalism describing structural relationships between words based on theories of morphology that reject the notions of internal word structure and morpheme. The formalism covers a wide variety of morphological phenomena (including non-concatenative ones like stem vowel alternation) without the need of workarounds and extensions. Furthermore, we show that morphological rules formulated in such way can be easily translated to FSTs, which enables us to derive performant approaches to morphological analysis, generation and automatic rule discovery.},
 address = {Dresden, Germany},
 author = {Janicki, Maciej},
 booktitle = {Proceedings of the 14th International Conference on Finite-State Methods and Natural Language Processing},
 doi = {10.18653/v1/W19-3107},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {September},
 note = {nan},
 number = {nan},
 pages = {37--45},
 publisher = {Association for Computational Linguistics},
 title = {Finite State Transducer Calculus for Whole Word Morphology},
 url = {https://aclanthology.org/W19-3107},
 volume = {nan},
 year = {2019}
}

@inproceedings{jiang-etal-2021-lnn,
 abstract = {Entity linking (EL) is the task of disambiguating mentions appearing in text by linking them to entities in a knowledge graph, a crucial task for text understanding, question answering or conversational systems. In the special case of short-text EL, which poses additional challenges due to limited context, prior approaches have reached good performance by employing heuristics-based methods or purely neural approaches. Here, we take a different, neuro-symbolic approach that combines the advantages of using interpretable rules based on first-order logic with the performance of neural learning. Even though constrained to use rules, we show that we reach competitive or better performance with SoTA black-box neural approaches. Furthermore, our framework has the benefits of extensibility and transferability. We show that we can easily blend existing rule templates given by a human expert, with multiple types of features (priors, BERT encodings, box embeddings, etc), and even with scores resulting from previous EL methods, thus improving on such methods. As an example of improvement, on the LC-QuAD-1.0 dataset, we show more than 3{\%} increase in F1 score relative to previous SoTA. Finally, we show that the inductive bias offered by using logic results in a set of learned rules that transfers from one dataset to another, sometimes without finetuning, while still having high accuracy.},
 address = {Online},
 author = {Jiang, Hang  and
Gurajada, Sairam  and
Lu, Qiuhao  and
Neelam, Sumit  and
Popa, Lucian  and
Sen, Prithviraj  and
Li, Yunyao  and
Gray, Alexander},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.64},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {775--787},
 publisher = {Association for Computational Linguistics},
 title = {{LNN}-{EL}: A Neuro-Symbolic Approach to Short-text Entity Linking},
 url = {https://aclanthology.org/2021.acl-long.64},
 volume = {nan},
 year = {2021}
}

@inproceedings{kalouli-etal-2020-hy,
 abstract = {Despite the advances in Natural Language Inference through the training of massive deep models, recent work has revealed the generalization difficulties of such models, which fail to perform on adversarial datasets with challenging linguistic phenomena. Such phenomena, however, can be handled well by symbolic systems. Thus, we propose Hy-NLI, a hybrid system that learns to identify an NLI pair as linguistically challenging or not. Based on that, it uses its symbolic or deep learning component, respectively, to make the final inference decision. We show how linguistically less complex cases are best solved by robust state-of-the-art models, like BERT and XLNet, while hard linguistic phenomena are best handled by our implemented symbolic engine. Our thorough evaluation shows that our hybrid system achieves state-of-the-art performance across mainstream and adversarial datasets and opens the way for further research into the hybrid direction.},
 address = {Barcelona, Spain (Online)},
 author = {Kalouli, Aikaterini-Lida  and
Crouch, Richard  and
de Paiva, Valeria},
 booktitle = {Proceedings of the 28th International Conference on Computational Linguistics},
 doi = {10.18653/v1/2020.coling-main.459},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {December},
 note = {nan},
 number = {nan},
 pages = {5235--5249},
 publisher = {International Committee on Computational Linguistics},
 title = {Hy-{NLI}: a Hybrid system for Natural Language Inference},
 url = {https://aclanthology.org/2020.coling-main.459},
 volume = {nan},
 year = {2020}
}

@inproceedings{kimura-etal-2021-loa,
 abstract = {We present Logical Optimal Actions (LOA), an action decision architecture of reinforcement learning applications with a neuro-symbolic framework which is a combination of neural network and symbolic knowledge acquisition approach for natural language interaction games. The demonstration for LOA experiments consists of a web-based interactive platform for text-based games and visualization for acquired knowledge for improving interpretability for trained rules. This demonstration also provides a comparison module with other neuro-symbolic approaches as well as non-symbolic state-of-the-art agent models on the same text-based games. Our LOA also provides open-sourced implementation in Python for the reinforcement learning environment to facilitate an experiment for studying neuro-symbolic agents. Demo site: https://ibm.biz/acl21-loa, Code: https://github.com/ibm/loa},
 address = {Online},
 author = {Kimura, Daiki  and
Chaudhury, Subhajit  and
Ono, Masaki  and
Tatsubori, Michiaki  and
Agravante, Don Joven  and
Munawar, Asim  and
Wachi, Akifumi  and
Kohita, Ryosuke  and
Gray, Alexander},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations},
 doi = {10.18653/v1/2021.acl-demo.27},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {227--231},
 publisher = {Association for Computational Linguistics},
 title = {{LOA}: Logical Optimal Actions for Text-based Interaction Games},
 url = {https://aclanthology.org/2021.acl-demo.27},
 volume = {nan},
 year = {2021}
}

@inproceedings{kimura-etal-2021-neuro,
 abstract = {Deep reinforcement learning (RL) methods often require many trials before convergence, and no direct interpretability of trained policies is provided. In order to achieve fast convergence and interpretability for the policy in RL, we propose a novel RL method for text-based games with a recent neuro-symbolic framework called Logical Neural Network, which can learn symbolic and interpretable rules in their differentiable network. The method is first to extract first-order logical facts from text observation and external word meaning network (ConceptNet), then train a policy in the network with directly interpretable logical operators. Our experimental results show RL training with the proposed method converges significantly faster than other state-of-the-art neuro-symbolic methods in a TextWorld benchmark.},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Kimura, Daiki  and
Ono, Masaki  and
Chaudhury, Subhajit  and
Kohita, Ryosuke  and
Wachi, Akifumi  and
Agravante, Don Joven  and
Tatsubori, Michiaki  and
Munawar, Asim  and
Gray, Alexander},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.283},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {3505--3511},
 publisher = {Association for Computational Linguistics},
 title = {Neuro-Symbolic Reinforcement Learning with First-Order Logic},
 url = {https://aclanthology.org/2021.emnlp-main.283},
 volume = {nan},
 year = {2021}
}

@inproceedings{kogkalidis-etal-2020-neural,
 abstract = {Linear logic and the linear λ-calculus have a long standing tradition in the study of natural language form and meaning. Among the proof calculi of linear logic, proof nets are of particular interest, offering an attractive geometric representation of derivations that is unburdened by the bureaucratic complications of conventional prooftheoretic formats. Building on recent advances in set-theoretic learning, we propose a neural variant of proof nets based on Sinkhorn networks, which allows us to translate parsing as the problem of extracting syntactic primitives and permuting them into alignment. Our methodology induces a batch-efficient, end-to-end differentiable architecture that actualizes a formally grounded yet highly efficient neuro-symbolic parser. We test our approach on {\AE}Thel, a dataset of type-logical derivations for written Dutch, where it manages to correctly transcribe raw text sentences into proofs and terms of the linear λ-calculus with an accuracy of as high as 70{\%}.},
 address = {Online},
 author = {Kogkalidis, Konstantinos  and
Moortgat, Michael  and
Moot, Richard},
 booktitle = {Proceedings of the 24th Conference on Computational Natural Language Learning},
 doi = {10.18653/v1/2020.conll-1.3},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {26--40},
 publisher = {Association for Computational Linguistics},
 title = {Neural Proof Nets},
 url = {https://aclanthology.org/2020.conll-1.3},
 volume = {nan},
 year = {2020}
}

@article{kouris-etal-2021-abstractive,
 abstract = {Abstract Nowadays, most research conducted in the field of abstractive text summarization focuses on neural-based models alone, without considering their combination with knowledge-based approaches that could further enhance their efficiency. In this direction, this work presents a novel framework that combines sequence-to-sequence neural-based text summarization along with structure and semantic-based methodologies. The proposed framework is capable of dealing with the problem of out-of-vocabulary or rare words, improving the performance of the deep learning models. The overall methodology is based on a well-defined theoretical model of knowledge-based content generalization and deep learning predictions for generating abstractive summaries. The framework is composed of three key elements: (i) a pre-processing task, (ii) a machine learning methodology, and (iii) a post-processing task. The pre-processing task is a knowledge-based approach, based on ontological knowledge resources, word sense disambiguation, and named entity recognition, along with content generalization, that transforms ordinary text into a generalized form. A deep learning model of attentive encoder-decoder architecture, which is expanded to enable a coping and coverage mechanism, as well as reinforcement learning and transformer-based architectures, is trained on a generalized version of text-summary pairs, learning to predict summaries in a generalized form. The post-processing task utilizes knowledge resources, word embeddings, word sense disambiguation, and heuristic algorithms based on text similarity methods in order to transform the generalized version of a predicted summary to a final, human-readable form. An extensive experimental procedure on three popular data sets evaluates key aspects of the proposed framework, while the obtained results exhibit promising performance, validating the robustness of the proposed approach.},
 address = {Cambridge, MA},
 author = {Kouris, Panagiotis  and
Alexandridis, Georgios  and
Stafylopatis, Andreas},
 booktitle = {nan},
 doi = {10.1162/coli_a_00417},
 editor = {nan},
 isbn = {nan},
 journal = {Computational Linguistics},
 language = {nan},
 month = {December},
 note = {nan},
 number = {4.0},
 pages = {813--859},
 publisher = {MIT Press},
 title = {Abstractive Text Summarization: Enhancing Sequence-to-Sequence Models Using Word Sense Disambiguation and Semantic Content Generalization},
 url = {https://aclanthology.org/2021.cl-4.27},
 volume = {47.0},
 year = {2021}
}

@inproceedings{langton-srihasam-2021-applied,
 abstract = {Logical Observation Identifiers Names and Codes (LOINC) is a standard set of codes that enable clinicians to communicate about medical tests. Laboratories depend on LOINC to identify what tests a doctor orders for a patient. However, clinicians often use site specific, custom codes in their medical records systems that can include shorthand, spelling mistakes, and invented acronyms. Software solutions must map from these custom codes to the LOINC standard to support data interoperability. A key challenge is that LOINC is comprised of six elements. Mapping requires not only extracting those elements, but also combining them according to LOINC logic. We found that character-based deep learning excels at extracting LOINC elements while logic based methods are more effective for combining those elements into complete LOINC values. In this paper, we present an ensemble of machine learning and logic that is currently used in several medical facilities to map from},
 address = {Groningen, the Netherlands (online)},
 author = {Langton, John  and
Srihasam, Krishna},
 booktitle = {Proceedings of the 1st and 2nd Workshops on Natural Logic Meets Machine Learning (NALOMA)},
 doi = {nan},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {June},
 note = {nan},
 number = {nan},
 pages = {7--11},
 publisher = {Association for Computational Linguistics},
 title = {Applied Medical Code Mapping with Character-based Deep Learning Models and Word-based Logic},
 url = {https://aclanthology.org/2021.naloma-1.2},
 volume = {nan},
 year = {2021}
}

@inproceedings{lima-etal-2019-impact,
 abstract = {Relation Extraction (RE) consists in detecting and classifying semantic relations between entities in a sentence. The vast majority of the state-of-the-art RE systems relies on morphosyntactic features and supervised machine learning algorithms. This paper tries to answer important questions concerning both the impact of semantic based features, and the integration of external linguistic knowledge resources on RE performance. For that, a RE system based on a logical and relational learning algorithm was used and evaluated on three reference datasets from two distinct domains. The yielded results confirm that the classifiers induced using the proposed richer feature set outperformed the classifiers built with morphosyntactic features in average 4{\%} (F1-measure).},
 address = {Varna, Bulgaria},
 author = {Lima, Rinaldo  and
Espinasse, Bernard  and
Freitas, Frederico},
 booktitle = {Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019)},
 doi = {10.26615/978-954-452-056-4_076},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {September},
 note = {nan},
 number = {nan},
 pages = {648--654},
 publisher = {INCOMA Ltd.},
 title = {The Impact of Semantic Linguistic Features in Relation Extraction: A Logical Relational Learning Approach},
 url = {https://aclanthology.org/R19-1076},
 volume = {nan},
 year = {2019}
}

@inproceedings{liu-etal-2019-aifu,
 abstract = {AiFu has won the first place in the SemEval-2019 Task 10 - {''}Math Question Answering{''}competition. This paper is to describe how it works technically and to report and analyze some essential experimental results},
 address = {Minneapolis, Minnesota, USA},
 author = {Liu, Yifan  and
Ding, Keyu  and
Zhou, Yi},
 booktitle = {Proceedings of the 13th International Workshop on Semantic Evaluation},
 doi = {10.18653/v1/S19-2154},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {June},
 note = {nan},
 number = {nan},
 pages = {900--906},
 publisher = {Association for Computational Linguistics},
 title = {{A}i{F}u at {S}em{E}val-2019 Task 10: A Symbolic and Sub-symbolic Integrated System for {SAT} Math Question Answering},
 url = {https://aclanthology.org/S19-2154},
 volume = {nan},
 year = {2019}
}

@inproceedings{ma-etal-2019-towards,
 abstract = {Non-extractive commonsense QA remains a challenging AI task, as it requires systems to reason about, synthesize, and gather disparate pieces of information, in order to generate responses to queries. Recent approaches on such tasks show increased performance, only when models are either pre-trained with additional information or when domain-specific heuristics are used, without any special consideration regarding the knowledge resource type. In this paper, we perform a survey of recent commonsense QA methods and we provide a systematic analysis of popular knowledge resources and knowledge-integration methods, across benchmarks from multiple commonsense datasets. Our results and analysis show that attention-based injection seems to be a preferable choice for knowledge integration and that the degree of domain overlap, between knowledge bases and datasets, plays a crucial role in determining model success.},
 address = {Hong Kong, China},
 author = {Ma, Kaixin  and
Francis, Jonathan  and
Lu, Quanyang  and
Nyberg, Eric  and
Oltramari, Alessandro},
 booktitle = {Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing},
 doi = {10.18653/v1/D19-6003},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {22--32},
 publisher = {Association for Computational Linguistics},
 title = {Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering},
 url = {https://aclanthology.org/D19-6003},
 volume = {nan},
 year = {2019}
}

@inproceedings{manchanda-grunin-2020-domain,
 abstract = {Neural Machine Translation (NMT) is a deep learning based approach that has achieved outstanding results lately in the translation community. The performance of NMT systems, however, is dependent on the availability of large amounts of in-domain parallel corpora. The business enterprises in domains such as legal and healthcare require specialized vocabulary but translation systems trained for a general purpose do not cater to these needs. The data in these domains is either hard to acquire or is very small in comparison to public data sets. This is a detailed report of using an open-source library to implement a machine translation system and successfully customizing it for the needs of a particular client in the healthcare domain. This report details the chronological development of every component of this system, namely, extraction of data from in-domain healthcare documents, a pre-processing pipeline for the data, data alignment and augmentation, training and a fully automated and robust deployment pipeline. This work proposes an efficient way for the continuous deployment of newly trained deep learning models. The deployed translation models are optimized for both inference time and cost.},
 address = {Lisboa, Portugal},
 author = {Manchanda, Sahil  and
Grunin, Galina},
 booktitle = {Proceedings of the 22nd Annual Conference of the European Association for Machine Translation},
 doi = {nan},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {255--261},
 publisher = {European Association for Machine Translation},
 title = {Domain Informed Neural Machine Translation: Developing Translation Services for Healthcare Enterprise},
 url = {https://aclanthology.org/2020.eamt-1.27},
 volume = {nan},
 year = {2020}
}

@inproceedings{moghimifar-etal-2021-neural,
 abstract = {Commonsense reasoning aims to incorporate sets of commonsense facts, retrieved from Commonsense Knowledge Graphs (CKG), to draw conclusion about ordinary situations. The dynamic nature of commonsense knowledge postulates models capable of performing multi-hop reasoning over new situations. This feature also results in having large-scale sparse Knowledge Graphs, where such reasoning process is needed to predict relations between new events. However, existing approaches in this area are limited by considering CKGs as a limited set of facts, thus rendering them unfit for reasoning over new unseen situations and events. In this paper, we present a neural-symbolic reasoner, which is capable of reasoning over large-scale dynamic CKGs. The logic rules for reasoning over CKGs are learned during training by our model. In addition to providing interpretable explanation, the learned logic rules help to generalise prediction to newly introduced events. Experimental results on the task of link prediction on CKGs prove the effectiveness of our model by outperforming the state-of-the-art models.},
 address = {Online},
 author = {Moghimifar, Farhad  and
Qu, Lizhen  and
Zhuo, Terry Yue  and
Haffari, Gholamreza  and
Baktashmotlagh, Mahsa},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)},
 doi = {10.18653/v1/2021.acl-short.100},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {797--802},
 publisher = {Association for Computational Linguistics},
 title = {Neural-Symbolic Commonsense Reasoner with Relation Predictors},
 url = {https://aclanthology.org/2021.acl-short.100},
 volume = {nan},
 year = {2021}
}

@article{murawaki-2019-bayesian,
 abstract = {We borrow the concept of representation learning from deep learning research, and we argue that the quest for Greenbergian implicational universals can be reformulated as the learning of good latent representations of languages, or sequences of surface typological features. By projecting languages into latent representations and performing inference in the latent space, we can handle complex dependencies among features in an implicit manner. The most challenging problem in turning the idea into a concrete computational model is the alarmingly large number of missing values in existing typological databases. To address this problem, we keep the number of model parameters relatively small to avoid overfitting, adopt the Bayesian learning framework for its robustness, and exploit phylogenetically and/or spatially related languages as additional clues. Experiments show that the proposed model recovers missing values more accurately than others and that some latent variables exhibit phylogenetic and spatial signals comparable to those of surface features.},
 address = {Cambridge, MA},
 author = {Murawaki, Yugo},
 booktitle = {nan},
 doi = {10.1162/coli_a_00346},
 editor = {nan},
 isbn = {nan},
 journal = {Computational Linguistics},
 language = {nan},
 month = {June},
 note = {nan},
 number = {2.0},
 pages = {199--228},
 publisher = {MIT Press},
 title = {{B}ayesian Learning of Latent Representations of Language Structures},
 url = {https://aclanthology.org/J19-2001},
 volume = {45.0},
 year = {2019}
}

@inproceedings{nicolai-yarowsky-2019-learning,
 abstract = {A large percentage of computational tools are concentrated in a very small subset of the planet{'}s languages. Compounding the issue, many languages lack the high-quality linguistic annotation necessary for the construction of such tools with current machine learning methods. In this paper, we address both issues simultaneously: leveraging the high accuracy of English taggers and parsers, we project morphological information onto translations of the Bible in 26 varied test languages. Using an iterative discovery, constraint, and training process, we build inflectional lexica in the target languages. Through a combination of iteration, ensembling, and reranking, we see double-digit relative error reductions in lemmatization and morphological analysis over a strong initial system.},
 address = {Florence, Italy},
 author = {Nicolai, Garrett  and
Yarowsky, David},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1172},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {July},
 note = {nan},
 number = {nan},
 pages = {1765--1774},
 publisher = {Association for Computational Linguistics},
 title = {Learning Morphosyntactic Analyzers from the {B}ible via Iterative Annotation Projection across 26 Languages},
 url = {https://aclanthology.org/P19-1172},
 volume = {nan},
 year = {2019}
}

@article{pacheco-goldwasser-2021-modeling,
 abstract = {Building models for realistic natural language tasks requires dealing with long texts and accounting for complicated structural dependencies. Neural-symbolic representations have emerged as a way to combine the reasoning capabilities of symbolic methods, with the expressiveness of neural networks. However, most of the existing frameworks for combining neural and symbolic representations have been designed for classic relational learning tasks that work over a universe of symbolic entities and relations. In this paper, we present DRaiL, an open-source declarative framework for specifying deep relational models, designed to support a variety of NLP scenarios. Our framework supports easy integration with expressive language encoders, and provides an interface to study the interactions between representation, inference and learning.},
 address = {Cambridge, MA},
 author = {Pacheco, Maria Leonor  and
Goldwasser, Dan},
 booktitle = {nan},
 doi = {10.1162/tacl_a_00357},
 editor = {nan},
 isbn = {nan},
 journal = {Transactions of the Association for Computational Linguistics},
 language = {nan},
 month = {nan},
 note = {nan},
 number = {nan},
 pages = {100--119},
 publisher = {MIT Press},
 title = {Modeling Content and Context with Deep Relational Learning},
 url = {https://aclanthology.org/2021.tacl-1.7},
 volume = {9.0},
 year = {2021}
}

@inproceedings{pinhanez-etal-2021-using,
 abstract = {In this paper we explore the improvement of intent recognition in conversational systems by the use of meta-knowledge embedded in intent identifiers. Developers often include such knowledge, structure as taxonomies, in the documentation of chatbots. By using neuro-symbolic algorithms to incorporate those taxonomies into embeddings of the output space, we were able to improve accuracy in intent recognition. In datasets with intents and example utterances from 200 professional chatbots, we saw decreases in the equal error rate (EER) in more than 40{\%} of the chatbots in comparison to the baseline of the same algorithm without the meta-knowledge. The meta-knowledge proved also to be effective in detecting out-of-scope utterances, improving the false acceptance rate (FAR) in two thirds of the chatbots, with decreases of 0.05 or more in FAR in almost 40{\%} of the chatbots. When considering only the well-developed workspaces with a high level use of taxonomies, FAR decreased more than 0.05 in 77{\%} of them, and more than 0.1 in 39{\%} of the chatbots.},
 address = {Online},
 author = {Pinhanez, Claudio  and
Cavalin, Paulo  and
Alves Ribeiro, Victor Henrique  and
Appel, Ana  and
Candello, Heloisa  and
Nogima, Julio  and
Pichiliani, Mauro  and
Guerra, Melina  and
de Bayser, Maira  and
Malfatti, Gabriel  and
Ferreira, Henrique},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.545},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {7014--7027},
 publisher = {Association for Computational Linguistics},
 title = {Using Meta-Knowledge Mined from Identifiers to Improve Intent Recognition in Conversational Systems},
 url = {https://aclanthology.org/2021.acl-long.545},
 volume = {nan},
 year = {2021}
}

@inproceedings{qin-etal-2021-neural,
 abstract = {Previous math word problem solvers following the encoder-decoder paradigm fail to explicitly incorporate essential math symbolic constraints, leading to unexplainable and unreasonable predictions. Herein, we propose Neural-Symbolic Solver (NS-Solver) to explicitly and seamlessly incorporate different levels of symbolic constraints by auxiliary tasks. Our NS-Solver consists of a problem reader to encode problems, a programmer to generate symbolic equations, and a symbolic executor to obtain answers. Along with target expression supervision, our solver is also optimized via 4 new auxiliary objectives to enforce different symbolic reasoning: a) self-supervised number prediction task predicting both number quantity and number locations; b) commonsense constant prediction task predicting what prior knowledge (e.g. how many legs a chicken has) is required; c) program consistency checker computing the semantic loss between predicted equation and target equation to ensure reasonable equation mapping; d) duality exploiting task exploiting the quasi-duality between symbolic equation generation and problem{'}s part-of-speech generation to enhance the understanding ability of a solver. Besides, to provide a more realistic and challenging benchmark for developing a universal and scalable solver, we also construct a new largescale MWP benchmark CM17K consisting of 4 kinds of MWPs (arithmetic, one-unknown linear, one-unknown non-linear, equation set) with more than 17K samples. Extensive experiments on Math23K and our CM17k demonstrate the superiority of our NS-Solver compared to state-of-the-art methods.},
 address = {Online},
 author = {Qin, Jinghui  and
Liang, Xiaodan  and
Hong, Yining  and
Tang, Jianheng  and
Lin, Liang},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.456},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {5870--5881},
 publisher = {Association for Computational Linguistics},
 title = {Neural-Symbolic Solver for Math Word Problems with Auxiliary Tasks},
 url = {https://aclanthology.org/2021.acl-long.456},
 volume = {nan},
 year = {2021}
}

@inproceedings{rajaby-faghihi-etal-2021-domiknows,
 abstract = {We demonstrate a library for the integration of domain knowledge in deep learning architectures. Using this library, the structure of the data is expressed symbolically via graph declarations and the logical constraints over outputs or latent variables can be seamlessly added to the deep models. The domain knowledge can be defined explicitly, which improves the explainability of the models in addition to their performance and generalizability in the low-data regime. Several approaches for such integration of symbolic and sub-symbolic models have been introduced; however, there is no library to facilitate the programming for such integration in a generic way while various underlying algorithms can be used. Our library aims to simplify programming for such integration in both training and inference phases while separating the knowledge representation from learning algorithms. We showcase various NLP benchmark tasks and beyond. The framework is publicly available at Github(https://github.com/HLR/DomiKnowS).},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Rajaby Faghihi, Hossein  and
Guo, Quan  and
Uszok, Andrzej  and
Nafar, Aliakbar  and
Kordjamshidi, Parisa},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations},
 doi = {10.18653/v1/2021.emnlp-demo.27},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {231--241},
 publisher = {Association for Computational Linguistics},
 title = {{D}omi{K}now{S}: A Library for Integration of Symbolic Domain Knowledge in Deep Learning},
 url = {https://aclanthology.org/2021.emnlp-demo.27},
 volume = {nan},
 year = {2021}
}

@inproceedings{rivera-martinez-2019-deep,
 abstract = {In this work, we introduce a Deep Learning architecture for pharmaceutical and chemical Named Entity Recognition in Spanish clinical cases texts. We propose a hybrid model approach based on two Bidirectional Long Short-Term Memory (Bi-LSTM) network and Conditional Random Field (CRF) network using character, word, concept and sense embeddings to deal with the extraction of semantic, syntactic and morphological features. The approach was evaluated on the PharmaCoNER Corpus obtaining an F-measure of 85.24{\%} for subtask 1 and 49.36{\%} for subtask2. These results prove that deep learning methods with specific domain embedding representations can outperform the state-of-the-art approaches.},
 address = {Hong Kong, China},
 author = {Rivera, Renzo  and
Mart{\'\i}nez, Paloma},
 booktitle = {Proceedings of The 5th Workshop on BioNLP Open Shared Tasks},
 doi = {10.18653/v1/D19-5707},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {38--46},
 publisher = {Association for Computational Linguistics},
 title = {Deep neural model with enhanced embeddings for pharmaceutical and chemical entities recognition in {S}panish clinical text},
 url = {https://aclanthology.org/D19-5707},
 volume = {nan},
 year = {2019}
}

@inproceedings{ruder-etal-2021-xtreme,
 abstract = {Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Ruder, Sebastian  and
Constant, Noah  and
Botha, Jan  and
Siddhant, Aditya  and
Firat, Orhan  and
Fu, Jinlan  and
Liu, Pengfei  and
Hu, Junjie  and
Garrette, Dan  and
Neubig, Graham  and
Johnson, Melvin},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.802},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {10215--10245},
 publisher = {Association for Computational Linguistics},
 title = {{XTREME}-{R}: Towards More Challenging and Nuanced Multilingual Evaluation},
 url = {https://aclanthology.org/2021.emnlp-main.802},
 volume = {nan},
 year = {2021}
}

@article{saha-etal-2019-complex,
 abstract = {Recent years have seen increasingly complex question-answering on knowledge bases (KBQA) involving logical, quantitative, and comparative reasoning over KB subgraphs. Neural Program Induction (NPI) is a pragmatic approach toward modularizing the reasoning process by translating a complex natural language query into a multi-step executable program. While NPI has been commonly trained with the {`}{`}gold{'}{'} program or its sketch, for realistic KBQA applications such gold programs are expensive to obtain. There, practically only natural language queries and the corresponding answers can be provided for training. The resulting combinatorial explosion in program space, along with extremely sparse rewards, makes NPI for KBQA ambitious and challenging. We present Complex Imperative Program Induction from Terminal Rewards (CIPITR), an advanced neural programmer that mitigates reward sparsity with auxiliary rewards, and restricts the program space to semantically correct programs using high-level constraints, KB schema, and inferred answer type. CIPITR solves complex KBQA considerably more accurately than key-value memory networks and neural symbolic machines (NSM). For moderately complex queries requiring 2- to 5-step programs, CIPITR scores at least 3{\mbox{$\times$}} higher F1 than the competing systems. On one of the hardest class of programs (comparative reasoning) with 5{--}10 steps, CIPITR outperforms NSM by a factor of 89 and memory networks by 9 times.},
 address = {Cambridge, MA},
 author = {Saha, Amrita  and
Ansari, Ghulam Ahmed  and
Laddha, Abhishek  and
Sankaranarayanan, Karthik  and
Chakrabarti, Soumen},
 booktitle = {nan},
 doi = {10.1162/tacl_a_00262},
 editor = {nan},
 isbn = {nan},
 journal = {Transactions of the Association for Computational Linguistics},
 language = {nan},
 month = {nan},
 note = {nan},
 number = {nan},
 pages = {185--200},
 publisher = {MIT Press},
 title = {Complex Program Induction for Querying Knowledge Bases in the Absence of Gold Programs},
 url = {https://aclanthology.org/Q19-1012},
 volume = {7.0},
 year = {2019}
}

@inproceedings{salam-etal-2021-generating,
 abstract = {HESIP is a hybrid explanation system for image predictions that combines sub-symbolic and symbolic machine learning techniques to explain the predictions of image classification tasks. The sub-symbolic component makes a prediction for an image and the symbolic component learns probabilistic symbolic rules in order to explain that prediction. In HESIP, the explanations are generated in controlled natural language from the learned probabilistic rules using a bi-directional logic grammar. In this paper, we present an explanation modification method where a human-in-the-loop can modify an incorrect explanation generated by the HESIP system and afterwards, the modified explanation is used by HESIP to learn a better explanation.},
 address = {Online},
 author = {Salam, Abdus  and
Schwitter, Rolf  and
Orgun, Mehmet},
 booktitle = {Proceedings of the The 19th Annual Workshop of the Australasian Language Technology Association},
 doi = {nan},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {December},
 note = {nan},
 number = {nan},
 pages = {149--157},
 publisher = {Australasian Language Technology Association},
 title = {Generating and Modifying Natural Language Explanations},
 url = {https://aclanthology.org/2021.alta-1.15},
 volume = {nan},
 year = {2021}
}

@inproceedings{salam-etal-2021-human,
 abstract = {nan},
 address = {Amsterdam, Netherlands},
 author = {Salam, Abdus  and
Schwitter, Rolf  and
Orgun, Mehmet},
 booktitle = {Proceedings of the Seventh International Workshop on Controlled Natural Language (CNL 2020/21)},
 doi = {nan},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {September},
 note = {nan},
 number = {nan},
 pages = {nan},
 publisher = {Special Interest Group on Controlled Natural Language},
 title = {Human-understandable and Machine-processable Explanations for Sub-symbolic Predictions},
 url = {https://aclanthology.org/2021.cnl-1.12},
 volume = {nan},
 year = {2021}
}

@inproceedings{saveleva-etal-2021-graph,
 abstract = {The paper presents a novel discourse-based approach to argument quality assessment defined as a graph classification task, where the depth of reasoning (argumentation) is evident from the number and type of detected discourse units and relations between them. We successfully applied state-of-the-art discourse parsers and machine learning models to reconstruct argument graphs with the identified and classified discourse units as nodes and relations between them as edges. Then Graph Neural Networks were trained to predict the argument quality assessing its acceptability, relevance, sufficiency and overall cogency. The obtained accuracy ranges from 74.5{\%} to 85.0{\%} and indicates that discourse-based argument structures reflect qualitative properties of natural language arguments. The results open many interesting prospects for future research in the field of argumentation mining.},
 address = {Held Online},
 author = {Saveleva, Ekaterina  and
Petukhova, Volha  and
Mosbach, Marius  and
Klakow, Dietrich},
 booktitle = {Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)},
 doi = {nan},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {September},
 note = {nan},
 number = {nan},
 pages = {1268--1280},
 publisher = {INCOMA Ltd.},
 title = {Graph-based Argument Quality Assessment},
 url = {https://aclanthology.org/2021.ranlp-1.143},
 volume = {nan},
 year = {2021}
}

@inproceedings{saxena-etal-2021-leveraging,
 abstract = {Domain-specific conceptual bases use key concepts to capture domain scope and relevant information. Conceptual bases serve as a foundation for various downstream tasks, including ontology construction, information mapping, and analysis. However, building conceptual bases necessitates domain awareness and takes time. Wikipedia navigational templates offer multiple articles on the same/similar domain. It is possible to use the templates to recognize fundamental concepts that shape the domain. Earlier work in this domain used Wikipedia{'}s structured and unstructured data to construct open-domain ontologies, domain terminologies, and knowledge bases. We present a novel method for leveraging navigational templates to create domain-specific fuzzy conceptual bases in this work. Our system generates knowledge graphs from the articles mentioned in the template, which we then process using Wikidata and machine learning algorithms. We filter important concepts using fuzzy logic on network metrics to create a crude conceptual base. Finally, the expert helps by refining the conceptual base. We demonstrate our system using an example of RNA virus antiviral drugs.},
 address = {Online},
 author = {Saxena, Krati  and
Singh, Tushita  and
Patil, Ashwini  and
Sunkle, Sagar  and
Kulkarni, Vinay},
 booktitle = {Proceedings of the Second Workshop on Data Science with Human in the Loop: Language Advances},
 doi = {10.18653/v1/2021.dash-1.1},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {June},
 note = {nan},
 number = {nan},
 pages = {1--7},
 publisher = {Association for Computational Linguistics},
 title = {Leveraging {W}ikipedia Navigational Templates for Curating Domain-Specific Fuzzy Conceptual Bases},
 url = {https://aclanthology.org/2021.dash-1.1},
 volume = {nan},
 year = {2021}
}

@inproceedings{sen-etal-2019-heidl,
 abstract = {While the role of humans is increasingly recognized in machine learning community, representation of and interaction with models in current human-in-the-loop machine learning (HITL-ML) approaches are too low-level and far-removed from human{'}s conceptual models. We demonstrate HEIDL, a prototype HITL-ML system that exposes the machine-learned model through high-level, explainable linguistic expressions formed of predicates representing semantic structure of text. In HEIDL, human{'}s role is elevated from simply evaluating model predictions to interpreting and even updating the model logic directly by enabling interaction with rule predicates themselves. Raising the currency of interaction to such semantic levels calls for new interaction paradigms between humans and machines that result in improved productivity for text analytics model development process. Moreover, by involving humans in the process, the human-machine co-created models generalize better to unseen data as domain experts are able to instill their expertise by extrapolating from what has been learned by automated algorithms from few labelled data.},
 address = {Florence, Italy},
 author = {Sen, Prithviraj  and
Li, Yunyao  and
Kandogan, Eser  and
Yang, Yiwei  and
Lasecki, Walter},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations},
 doi = {10.18653/v1/P19-3023},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {July},
 note = {nan},
 number = {nan},
 pages = {135--140},
 publisher = {Association for Computational Linguistics},
 title = {{HEIDL}: Learning Linguistic Expressions with Deep Learning and Human-in-the-Loop},
 url = {https://aclanthology.org/P19-3023},
 volume = {nan},
 year = {2019}
}

@inproceedings{sen-etal-2020-learning,
 abstract = {Interpretability of predictive models is becoming increasingly important with growing adoption in the real-world. We present RuleNN, a neural network architecture for learning transparent models for sentence classification. The models are in the form of rules expressed in first-order logic, a dialect with well-defined, human-understandable semantics. More precisely, RuleNN learns linguistic expressions (LE) built on top of predicates extracted using shallow natural language understanding. Our experimental results show that RuleNN outperforms statistical relational learning and other neuro-symbolic methods, and performs comparably with black-box recurrent neural networks. Our user studies confirm that the learned LEs are explainable and capture domain semantics. Moreover, allowing domain experts to modify LEs and instill more domain knowledge leads to human-machine co-creation of models with better performance.},
 address = {Online},
 author = {Sen, Prithviraj  and
Danilevsky, Marina  and
Li, Yunyao  and
Brahma, Siddhartha  and
Boehm, Matthias  and
Chiticariu, Laura  and
Krishnamurthy, Rajasekar},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.345},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {4211--4221},
 publisher = {Association for Computational Linguistics},
 title = {Learning Explainable Linguistic Expressions with Neural Inductive Logic Programming for Sentence Classification},
 url = {https://aclanthology.org/2020.emnlp-main.345},
 volume = {nan},
 year = {2020}
}

@inproceedings{shi-etal-2021-neural,
 abstract = {Many open-domain question answering problems can be cast as a textual entailment task, where a question and candidate answers are concatenated to form hypotheses. A QA system then determines if the supporting knowledge bases, regarded as potential premises, entail the hypotheses. In this paper, we investigate a neural-symbolic QA approach that integrates natural logic reasoning within deep learning architectures, towards developing effective and yet explainable question answering models. The proposed model gradually bridges a hypothesis and candidate premises following natural logic inference steps to build proof paths. Entailment scores between the acquired intermediate hypotheses and candidate premises are measured to determine if a premise entails the hypothesis. As the natural logic reasoning process forms a tree-like, hierarchical structure, we embed hypotheses and premises in a Hyperbolic space rather than Euclidean space to acquire more precise representations. Empirically, our method outperforms prior work on answering multiple-choice science questions, achieving the best results on two publicly available datasets. The natural logic inference process inherently provides evidence to help explain the prediction process.},
 address = {Online and Punta Cana, Dominican Republic},
 author = {Shi, Jihao  and
Ding, Xiao  and
Du, Li  and
Liu, Ting  and
Qin, Bing},
 booktitle = {Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing},
 doi = {10.18653/v1/2021.emnlp-main.298},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {3673--3684},
 publisher = {Association for Computational Linguistics},
 title = {Neural Natural Logic Inference for Interpretable Question Answering},
 url = {https://aclanthology.org/2021.emnlp-main.298},
 volume = {nan},
 year = {2021}
}

@article{sun-etal-2019-dream,
 abstract = {We present DREAM, the first dialogue-based multiple-choice reading comprehension data set. Collected from English as a Foreign Language examinations designed by human experts to evaluate the comprehension level of Chinese learners of English, our data set contains 10,197 multiple-choice questions for 6,444 dialogues. In contrast to existing reading comprehension data sets, DREAM is the first to focus on in-depth multi-turn multi-party dialogue understanding. DREAM is likely to present significant challenges for existing reading comprehension systems: 84{\%} of answers are non-extractive, 85{\%} of questions require reasoning beyond a single sentence, and 34{\%} of questions also involve commonsense knowledge. We apply several popular neural reading comprehension models that primarily exploit surface information within the text and find them to, at best, just barely outperform a rule-based approach. We next investigate the effects of incorporating dialogue structure and different kinds of general world knowledge into both rule-based and (neural and non-neural) machine learning-based reading comprehension models. Experimental results on the DREAM data set show the effectiveness of dialogue structure and general world knowledge. DREAM is available at https://dataset.org/dream/.},
 address = {Cambridge, MA},
 author = {Sun, Kai  and
Yu, Dian  and
Chen, Jianshu  and
Yu, Dong  and
Choi, Yejin  and
Cardie, Claire},
 booktitle = {nan},
 doi = {10.1162/tacl_a_00264},
 editor = {nan},
 isbn = {nan},
 journal = {Transactions of the Association for Computational Linguistics},
 language = {nan},
 month = {nan},
 note = {nan},
 number = {nan},
 pages = {217--231},
 publisher = {MIT Press},
 title = {{DREAM}: A Challenge Data Set and Models for Dialogue-Based Reading Comprehension},
 url = {https://aclanthology.org/Q19-1014},
 volume = {7.0},
 year = {2019}
}

@inproceedings{tsou-etal-2019-towards,
 abstract = {The emergence of China as a global economic power in the 21st Century has brought about surging needs for cross-lingual and cross-cultural mediation, typically performed by translators. Advances in Artificial Intelligence and Language Engineering have been bolstered by Machine learning and suitable Big Data cultivation. They have helped to meet some of the translator{'}s needs, though the technical specialists have not kept pace with the practical and expanding requirements in language mediation. One major technical and linguistic hurdle involves words outside the vocabulary of the translator or the lexical database he/she consults, especially Multi-Word Expressions (Compound Words) in technical subjects. A further problem is in the multiplicity of renditions of a term in the target language. This paper discusses a proactive approach following the successful extraction and application of sizable bilingual Multi-Word Expressions (Compound Words) for language mediation in technical subjects, which do not fall within the expertise of typical translators, who have inadequate appreciation of the range of new technical tools available to help him/her. Our approach draws on the personal reflections of translators and teachers of translation and is based on the prior R{\&}D efforts relating to 300,000 comparable Chinese-English patents. The subsequent protocol we have developed aims to be proactive in meeting four identified practical challenges in technical translation (e.g. patents). It has broader economic implication in the Age of Big Data (Tsou et al, 2015) and Trade War, as the workload, if not, the challenges, increasingly cannot be met by currently available front-line translators. We shall demonstrate how new tools can be harnessed to spearhead the application of language technology not only in language mediation but also in the {``}teaching{''} and {``}learning{''} of translation. It shows how a better appreciation of their needs may enhance the contributions of the technical specialists, and thus enhance the resultant synergetic benefits.},
 address = {Varna, Bulgaria},
 author = {Tsou, Benjamin K.  and
Chow, Kapo  and
Nie, Junru  and
Yuan, Yuan},
 booktitle = {Proceedings of the Human-Informed Translation and Interpreting Technology Workshop (HiT-IT 2019)},
 doi = {10.26615/issn.2683-0078.2019_014},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {September},
 note = {nan},
 number = {nan},
 pages = {116--121},
 publisher = {Incoma Ltd., Shoumen, Bulgaria},
 title = {Towards a Proactive {MWE} Terminological Platform for Cross-Lingual Mediation in the Age of Big Data},
 url = {https://aclanthology.org/W19-8714},
 volume = {nan},
 year = {2019}
}

@inproceedings{verga-etal-2021-adaptable,
 abstract = {Past research has demonstrated that large neural language models (LMs) encode surprising amounts of factual information: however, augmenting or modifying this information requires modifying a corpus and retraining, which is computationally expensive. To address this problem, we develop a neural LM that includes an interpretable neuro-symbolic KB in the form of a {``}fact memory{''}. Each element of the fact memory is formed from a triple of vectors, where each vector corresponds to a KB entity or relation. Our LM improves performance on knowledge-intensive question-answering tasks, sometimes dramatically, including a 27 point increase in one setting of WebQuestionsSP over a state-of-the-art open-book model, despite using 5{\%} of the parameters. Most interestingly, we demonstrate that the model can be modified, without \textit{any} re-training, by updating the fact memory.},
 address = {Online},
 author = {Verga, Pat  and
Sun, Haitian  and
Baldini Soares, Livio  and
Cohen, William},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.288},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {June},
 note = {nan},
 number = {nan},
 pages = {3678--3691},
 publisher = {Association for Computational Linguistics},
 title = {Adaptable and Interpretable Neural {M}emory{O}ver Symbolic Knowledge},
 url = {https://aclanthology.org/2021.naacl-main.288},
 volume = {nan},
 year = {2021}
}

@inproceedings{virk-etal-2021-deep,
 abstract = {Linguistic typology is an area of linguistics concerned with analysis of and comparison between natural languages of the world based on their certain linguistic features. For that purpose, historically, the area has relied on manual extraction of linguistic feature values from textural descriptions of languages. This makes it a laborious and time expensive task and is also bound by human brain capacity. In this study, we present a deep learning system for the task of automatic extraction of linguistic features from textual descriptions of natural languages. First, textual descriptions are manually annotated with special structures called semantic frames. Those annotations are learned by a recurrent neural network, which is then used to annotate un-annotated text. Finally, the annotations are converted to linguistic feature values using a separate rule based module. Word embeddings, learned from general purpose text, are used as a major source of knowledge by the recurrent neural network. We compare the proposed deep learning system to a previously reported machine learning based system for the same task, and the deep learning system wins in terms of F1 scores with a fair margin. Such a system is expected to be a useful contribution for the automatic curation of typological databases, which otherwise are manually developed.},
 address = {Held Online},
 author = {Virk, Shafqat Mumtaz  and
Foster, Daniel  and
Sheikh Muhammad, Azam  and
Saleem, Raheela},
 booktitle = {Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2021)},
 doi = {nan},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {September},
 note = {nan},
 number = {nan},
 pages = {1480--1489},
 publisher = {INCOMA Ltd.},
 title = {A Deep Learning System for Automatic Extraction of Typological Linguistic Information from Descriptive Grammars},
 url = {https://aclanthology.org/2021.ranlp-1.166},
 volume = {nan},
 year = {2021}
}

@inproceedings{wang-etal-2021-counterfactual-adversarial,
 abstract = {Deep learning models exhibit a preference for statistical fitting over logical reasoning. Spurious correlations might be memorized when there exists statistical bias in training data, which severely limits the model performance especially in small data scenarios. In this work, we introduce Counterfactual Adversarial Training framework (CAT) to tackle the problem from a causality perspective. Particularly, for a specific sample, CAT first generates a counterfactual representation through latent space interpolation in an adversarial manner, and then performs Counterfactual Risk Minimization (CRM) on each original-counterfactual pair to adjust sample-wise loss weight dynamically, which encourages the model to explore the true causal effect. Extensive experiments demonstrate that CAT achieves substantial performance improvement over SOTA across different downstream tasks, including sentence classification, natural language inference and question answering.},
 address = {Punta Cana, Dominican Republic},
 author = {Wang, Wei  and
Wang, Boxin  and
Shi, Ning  and
Li, Jinfeng  and
Zhu, Bingyu  and
Liu, Xiangyu  and
Zhang, Rong},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 doi = {10.18653/v1/2021.findings-emnlp.413},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {4809--4820},
 publisher = {Association for Computational Linguistics},
 title = {Counterfactual Adversarial Learning with Representation Interpolation},
 url = {https://aclanthology.org/2021.findings-emnlp.413},
 volume = {nan},
 year = {2021}
}

@article{wang-pan-2021-variational,
 abstract = {Abstract Currently, deep learning models have been widely adopted and achieved promising results on various application domains. Despite their intriguing performance, most deep learning models function as black boxes, lacking explicit reasoning capabilities and explanations, which are usually essential for complex problems. Take joint inference in information extraction as an example. This task requires the identification of multiple structured knowledge from texts, which is inter-correlated, including entities, events, and the relationships between them. Various deep neural networks have been proposed to jointly perform entity extraction and relation prediction, which only propagate information implicitly via representation learning. However, they fail to encode the intensive correlations between entity types and relations to enforce their coexistence. On the other hand, some approaches adopt rules to explicitly constrain certain relational facts, although the separation of rules with representation learning usually restrains the approaches with error propagation. Moreover, the predefined rules are inflexible and might result in negative effects when data is noisy. To address these limitations, we propose a variational deep logic network that incorporates both representation learning and relational reasoning via the variational EM algorithm. The model consists of a deep neural network to learn high-level features with implicit interactions via the self-attention mechanism and a relational logic network to explicitly exploit target interactions. These two components are trained interactively to bring the best of both worlds. We conduct extensive experiments ranging from fine-grained sentiment terms extraction, end-to-end relation prediction, to end-to-end event extraction to demonstrate the effectiveness of our proposed method.},
 address = {Cambridge, MA},
 author = {Wang, Wenya  and
Pan, Sinno Jialin},
 booktitle = {nan},
 doi = {10.1162/coli_a_00415},
 editor = {nan},
 isbn = {nan},
 journal = {Computational Linguistics},
 language = {nan},
 month = {December},
 note = {nan},
 number = {4.0},
 pages = {775--812},
 publisher = {MIT Press},
 title = {Variational Deep Logic Network for Joint Inference of Entities and Relations},
 url = {https://aclanthology.org/2021.cl-4.26},
 volume = {47.0},
 year = {2021}
}

@inproceedings{wu-etal-2020-deep,
 abstract = {Though deep learning has achieved significant success in various NLP tasks, most deep learning models lack the capability of encoding explicit domain knowledge to model complex causal relationships among different types of variables. On the other hand, logic rules offer a compact expression to represent the causal relationships to guide the training process. Logic programs can be cast as a satisfiability problem which aims to find truth assignments to logic variables by maximizing the number of satisfiable clauses (MaxSAT). We adopt the MaxSAT semantics to model logic inference process and smoothly incorporate a weighted version of MaxSAT that connects deep neural networks and a graphical model in a joint framework. The joint model feeds deep learning outputs to a weighted MaxSAT layer to rectify the erroneous predictions and can be trained via end-to-end gradient descent. Our proposed model associates the benefits of high-level feature learning, knowledge reasoning, and structured learning with observable performance gain for the task of aspect-based opinion extraction.},
 address = {Online},
 author = {Wu, Meixi  and
Wang, Wenya  and
Pan, Sinno Jialin},
 booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
 doi = {10.18653/v1/2020.emnlp-main.453},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {5618--5628},
 publisher = {Association for Computational Linguistics},
 title = {{D}eep {W}eighted {M}ax{SAT} for {A}spect-based {O}pinion {E}xtraction},
 url = {https://aclanthology.org/2020.emnlp-main.453},
 volume = {nan},
 year = {2020}
}

@inproceedings{xu-li-2019-relation,
 abstract = {Link prediction is critical for the application of incomplete knowledge graph (KG) in the downstream tasks. As a family of effective approaches for link predictions, embedding methods try to learn low-rank representations for both entities and relations such that the bilinear form defined therein is a well-behaved scoring function. Despite of their successful performances, existing bilinear forms overlook the modeling of relation compositions, resulting in lacks of interpretability for reasoning on KG. To fulfill this gap, we propose a new model called DihEdral, named after dihedral symmetry group. This new model learns knowledge graph embeddings that can capture relation compositions by nature. Furthermore, our approach models the relation embeddings parametrized by discrete values, thereby decrease the solution space drastically. Our experiments show that DihEdral is able to capture all desired properties such as (skew-) symmetry, inversion and (non-) Abelian composition, and outperforms existing bilinear form based approach and is comparable to or better than deep learning models such as ConvE.},
 address = {Florence, Italy},
 author = {Xu, Canran  and
Li, Ruijiang},
 booktitle = {Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics},
 doi = {10.18653/v1/P19-1026},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {July},
 note = {nan},
 number = {nan},
 pages = {263--272},
 publisher = {Association for Computational Linguistics},
 title = {Relation Embedding with Dihedral Group in Knowledge Graph},
 url = {https://aclanthology.org/P19-1026},
 volume = {nan},
 year = {2019}
}

@inproceedings{yabloko-2020-ethan,
 abstract = {I present ETHAN: Experimental Testing of Hybrid AI Node implemented entirely on free cloud computing infrastructure. The ultimate goal of this research is to create modular reusable hybrid neuro-symbolic architecture for Artificial Intelligence. As a test case I model natural language comprehension of causal relations from open domain text corpus that combines semi-supervised language model (Huggingface Transformers) with constituency and dependency parsers (Allen Institute for Artificial Intelligence.)},
 address = {Barcelona (online)},
 author = {Yabloko, Len},
 booktitle = {Proceedings of the Fourteenth Workshop on Semantic Evaluation},
 doi = {10.18653/v1/2020.semeval-1.83},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {December},
 note = {nan},
 number = {nan},
 pages = {645--652},
 publisher = {International Committee for Computational Linguistics},
 title = {{ETHAN} at {S}em{E}val-2020 Task 5: Modelling Causal Reasoning in Language Using Neuro-symbolic Cloud Computing},
 url = {https://aclanthology.org/2020.semeval-1.83},
 volume = {nan},
 year = {2020}
}

@inproceedings{zellers-etal-2021-piglet,
 abstract = {We propose PIGLeT: a model that learns physical commonsense knowledge through interaction, and then uses this knowledge to ground language. We factorize PIGLeT into a physical dynamics model, and a separate language model. Our dynamics model learns not just what objects are but also what they do: glass cups break when thrown, plastic ones don{'}t. We then use it as the interface to our language model, giving us a unified model of linguistic form and grounded meaning. PIGLeT can read a sentence, simulate neurally what might happen next, and then communicate that result through a literal symbolic representation, or natural language. Experimental results show that our model effectively learns world dynamics, along with how to communicate them. It is able to correctly forecast what happens next given an English sentence over 80{\%} of the time, outperforming a 100x larger, text-to-text approach by over 10{\%}. Likewise, its natural language summaries of physical interactions are also judged by humans as more accurate than LM alternatives. We present comprehensive analysis showing room for future work.},
 address = {Online},
 author = {Zellers, Rowan  and
Holtzman, Ari  and
Peters, Matthew  and
Mottaghi, Roozbeh  and
Kembhavi, Aniruddha  and
Farhadi, Ali  and
Choi, Yejin},
 booktitle = {Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)},
 doi = {10.18653/v1/2021.acl-long.159},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {August},
 note = {nan},
 number = {nan},
 pages = {2040--2050},
 publisher = {Association for Computational Linguistics},
 title = {{PIGL}e{T}: Language Grounding Through Neuro-Symbolic Interaction in a 3{D} World},
 url = {https://aclanthology.org/2021.acl-long.159},
 volume = {nan},
 year = {2021}
}

@inproceedings{zeng-etal-2020-fancy,
 abstract = {Automatic or semi-automatic conversion of protocols specifying steps in performing a lab procedure into machine-readable format benefits biological research a lot. These noisy, dense, and domain-specific lab protocols processing draws more and more interests with the development of deep learning. This paper presents our teamwork on WNUT 2020 shared task-1: wet lab entity extract, that we conducted studies in several models, including a BiLSTM CRF model and a Bert case model which can be used to complete wet lab entity extraction. And we mainly discussed the performance differences of Bert case under different situations such as transformers versions, case sensitivity that may don{'}t get enough attention before.},
 address = {Online},
 author = {Zeng, Qingcheng  and
Fang, Xiaoyang  and
Liang, Zhexin  and
Meng, Haoding},
 booktitle = {Proceedings of the Sixth Workshop on Noisy User-generated Text (W-NUT 2020)},
 doi = {10.18653/v1/2020.wnut-1.39},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {299--304},
 publisher = {Association for Computational Linguistics},
 title = {Fancy Man Launches Zippo at {WNUT} 2020 Shared Task-1: A Bert Case Model for Wet Lab Entity Extraction},
 url = {https://aclanthology.org/2020.wnut-1.39},
 volume = {nan},
 year = {2020}
}

@inproceedings{zhang-etal-2021-noahqa-numerical,
 abstract = {While diverse question answering (QA) datasets have been proposed and contributed significantly to the development of deep learning models for QA tasks, the existing datasets fall short in two aspects. First, we lack QA datasets covering complex questions that involve answers as well as the reasoning processes to get them. As a result, the state-of-the-art QA research on numerical reasoning still focuses on simple calculations and does not provide the mathematical expressions or evidence justifying the answers. Second, the QA community has contributed a lot of effort to improve the interpretability of QA models. However, they fail to explicitly show the reasoning process, such as the evidence order for reasoning and the interactions between different pieces of evidence. To address the above shortcoming, we introduce NOAHQA, a conversational and bilingual QA dataset with questions requiring numerical reasoning with compound mathematical expressions. With NOAHQA, we develop an interpretable reasoning graph as well as the appropriate evaluation metric to measure the answer quality. We evaluate the state-of-the-art QA models trained using existing QA datasets on NOAHQA and show that the best among them can only achieve 55.5 exact match scores, while the human performance is 89.7. We also present a new QA model for generating a reasoning graph where the reasoning graph metric still has a large gap compared with that of humans, eg, 28 scores.},
 address = {Punta Cana, Dominican Republic},
 author = {Zhang, Qiyuan  and
Wang, Lei  and
Yu, Sicheng  and
Wang, Shuohang  and
Wang, Yang  and
Jiang, Jing  and
Lim, Ee-Peng},
 booktitle = {Findings of the Association for Computational Linguistics: EMNLP 2021},
 doi = {10.18653/v1/2021.findings-emnlp.350},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {November},
 note = {nan},
 number = {nan},
 pages = {4147--4161},
 publisher = {Association for Computational Linguistics},
 title = {{NOAHQA}: Numerical Reasoning with Interpretable Graph Question Answering Dataset},
 url = {https://aclanthology.org/2021.findings-emnlp.350},
 volume = {nan},
 year = {2021}
}

@inproceedings{zhou-etal-2021-temporal,
 abstract = {We propose TRACIE, a novel temporal reasoning dataset that evaluates the degree to which systems understand implicit events{---}events that are not mentioned explicitly in natural language text but can be inferred from it. This introduces a new challenge in temporal reasoning research, where prior work has focused on explicitly mentioned events. Human readers can infer implicit events via commonsense reasoning, resulting in a more comprehensive understanding of the situation and, consequently, better reasoning about time. We find, however, that state-of-the-art models struggle when predicting temporal relationships between implicit and explicit events. To address this, we propose a neuro-symbolic temporal reasoning model, SymTime, which exploits distant supervision signals from large-scale text and uses temporal rules to combine start times and durations to infer end times. SymTime outperforms strong baseline systems on TRACIE by 5{\%}, and by 11{\%} in a zero prior knowledge training setting. Our approach also generalizes to other temporal reasoning tasks, as evidenced by a gain of 1{\%}-9{\%} on MATRES, an explicit event benchmark.},
 address = {Online},
 author = {Zhou, Ben  and
Richardson, Kyle  and
Ning, Qiang  and
Khot, Tushar  and
Sabharwal, Ashish  and
Roth, Dan},
 booktitle = {Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies},
 doi = {10.18653/v1/2021.naacl-main.107},
 editor = {nan},
 isbn = {nan},
 journal = {nan},
 language = {nan},
 month = {June},
 note = {nan},
 number = {nan},
 pages = {1361--1371},
 publisher = {Association for Computational Linguistics},
 title = {Temporal Reasoning on Implicit Events from Distant Supervision},
 url = {https://aclanthology.org/2021.naacl-main.107},
 volume = {nan},
 year = {2021}
}

